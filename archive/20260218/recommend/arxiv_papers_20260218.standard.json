{
  "mode": "standard",
  "generated_at": "2026-02-18T06:18:19.464221+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 0,
    "deep_cap": 8,
    "deep_selected": 0,
    "quick_candidates": 5,
    "quick_skim_target": 13,
    "quick_selected": 5
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.15128v1",
      "title": "PolyNODE: Variable-dimension Neural ODEs on M-polyfolds",
      "abstract": "Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .",
      "authors": [
        "Per Åhag",
        "Alexander Friedrich",
        "Fredrik Ohlsson",
        "Viktor Vigren Näslund"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 19:11:06+00:00",
      "link": "https://arxiv.org/pdf/2602.15128v1",
      "tags": [
        "keyword:FM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Variable-dimensional flow-based model within the flow matching paradigm",
      "llm_evidence_cn": "流匹配范式下的变维流模型",
      "llm_evidence": "流匹配范式下的变维流模型",
      "llm_tldr_en": "Introduces PolyNODEs, the first variable-dimensional flow-based model for geometric deep learning.",
      "llm_tldr_cn": "引入了PolyNODEs，这是几何深度学习中第一个变维流模型。",
      "llm_tldr": "引入了PolyNODEs，这是几何深度学习中第一个变维流模型。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.15396v1",
      "title": "Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching",
      "abstract": "Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.",
      "authors": [
        "Jeongwoo Shin",
        "Jinhwan Sul",
        "Joonseok Lee",
        "Jaewong Choi",
        "Jaemoo Choi"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-17 07:06:20+00:00",
      "link": "https://arxiv.org/pdf/2602.15396v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Methodological bridge for generative modeling and bridge matching related to flow matching and diffusion.",
      "llm_evidence_cn": "生成模型和桥接匹配的方法论桥梁，与流匹配和扩散模型相关。",
      "llm_evidence": "生成模型和桥接匹配的方法论桥梁，与流匹配和扩散模型相关。",
      "llm_tldr_en": "Proposes Adjoint Schrodinger Bridge Matching to improve generative trajectories beyond standard diffusion models.",
      "llm_tldr_cn": "提出伴随薛定谔桥匹配框架，通过非记忆性过程优化生成轨迹并提升采样效率。",
      "llm_tldr": "提出伴随薛定谔桥匹配框架，通过非记忆性过程优化生成轨迹并提升采样效率。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.15567v1",
      "title": "Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions",
      "abstract": "Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.",
      "authors": [
        "Jieting Long",
        "Dechuan Liu",
        "Weidong Cai",
        "Ian Manchester",
        "Weiming Zhi"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-17 13:27:05+00:00",
      "link": "https://arxiv.org/pdf/2602.15567v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "streaming flow models for robot trajectory generation",
      "llm_evidence_cn": "用于机器人轨迹生成的流模型",
      "llm_evidence": "用于机器人轨迹生成的流模型",
      "llm_tldr_en": "Introduces Constraint-Aware Streaming Flow for generating and adapting robot trajectories using velocity fields.",
      "llm_tldr_cn": "引入约束感知流框架，利用速度场生成并调整机器人运动轨迹。",
      "llm_tldr": "引入约束感知流框架，利用速度场生成并调整机器人运动轨迹。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.15733v1",
      "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction",
      "abstract": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.",
      "authors": [
        "Qiang Zhang",
        "Jiahao Ma",
        "Peiran Liu",
        "Shuai Shi",
        "Zeran Su",
        "Zifan Wang",
        "Jingkai Sun",
        "Wei Cui",
        "Jialin Yu",
        "Gang Han",
        "Wen Zhao",
        "Pihai Sun",
        "Kangning Yin",
        "Jiaxu Wang",
        "Jiahang Cao",
        "Lingfeng Zhang",
        "Hao Cheng",
        "Xiaoshuai Hao",
        "Yiding Ji",
        "Junwei Liang",
        "Jian Tang",
        "Renjing Xu",
        "Yijie Guo"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-02-17 17:09:45+00:00",
      "link": "https://arxiv.org/pdf/2602.15733v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "humanoid motion learning and synthesis",
      "llm_evidence_cn": "类人运动学习与合成",
      "llm_evidence": "类人运动学习与合成",
      "llm_tldr_en": "Learns humanoid motion through 3D scene reconstruction to ensure physical consistency in synthesis.",
      "llm_tldr_cn": "通过3D场景重建学习类人运动，以确保运动合成中的物理一致性。",
      "llm_tldr": "通过3D场景重建学习类人运动，以确保运动合成中的物理一致性。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.15827v1",
      "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching",
      "abstract": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.",
      "authors": [
        "Zhen Wu",
        "Xiaoyu Huang",
        "Lujie Yang",
        "Yuanhang Zhang",
        "Koushil Sreenath",
        "Xi Chen",
        "Pieter Abbeel",
        "Rocky Duan",
        "Angjoo Kanazawa",
        "Carmelo Sferrazza",
        "Guanya Shi",
        "C. Karen Liu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "published": "2026-02-17 18:59:11+00:00",
      "link": "https://arxiv.org/pdf/2602.15827v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Long-horizon skill composition and motion matching for humanoid robots",
      "llm_evidence_cn": "人形机器人的长时程技能组合与运动匹配",
      "llm_evidence": "人形机器人的长时程技能组合与运动匹配",
      "llm_tldr_en": "A modular framework for humanoid parkour using motion matching to compose long-horizon skills.",
      "llm_tldr_cn": "一种利用运动匹配组合长时程技能的人形机器人跑酷模块化框架。",
      "llm_tldr": "一种利用运动匹配组合长时程技能的人形机器人跑酷模块化框架。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    }
  ]
}
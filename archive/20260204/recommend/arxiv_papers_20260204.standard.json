{
  "mode": "standard",
  "generated_at": "2026-02-04T05:43:32.033077+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 0,
    "deep_cap": 8,
    "deep_selected": 0,
    "quick_candidates": 11,
    "quick_skim_target": 13,
    "quick_selected": 11
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.03126v1",
      "title": "Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models",
      "abstract": "3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at https://github.com/fsnelgar/diffusion_pose .",
      "authors": [
        "Francis Snelgar",
        "Ming Xu",
        "Stephen Gould",
        "Liang Zheng",
        "Akshay Asthana"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-03 05:35:37+00:00",
      "link": "https://arxiv.org/pdf/2602.03126v1",
      "tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "diffusion models for probabilistic human pose estimation",
      "llm_evidence_cn": "用于概率人体姿态估计的扩散模型",
      "llm_evidence": "用于概率人体姿态估计的扩散模型",
      "llm_tldr_en": "A diffusion-based framework for sampling plausible 3D human poses consistent with 2D image constraints.",
      "llm_tldr_cn": "一种基于扩散模型的框架，用于采样与2D图像约束一致的合理3D人体姿态。",
      "llm_tldr": "一种基于扩散模型的框架，用于采样与2D图像约束一致的合理3D人体姿态。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.02831v1",
      "title": "Adaptive Linear Path Model-Based Diffusion",
      "abstract": "The interest in combining model-based control approaches with diffusion models has been growing. Although we have seen many impressive robotic control results in difficult tasks, the performance of diffusion models is highly sensitive to the choice of scheduling parameters, making parameter tuning one of the most critical challenges. We introduce Linear Path Model-Based Diffusion (LP-MBD), which replaces the variance-preserving schedule with a flow-matching-inspired linear probability path. This yields a geometrically interpretable and decoupled parameterization that reduces tuning complexity and provides a stable foundation for adaptation. Building on this, we propose Adaptive LP-MBD (ALP-MBD), which leverages reinforcement learning to adjust diffusion steps and noise levels according to task complexity and environmental conditions. Across numerical studies, Brax benchmarks, and mobile-robot trajectory tracking, LP-MBD simplifies scheduling while maintaining strong performance, and ALP-MBD further improves robustness, adaptability, and real-time efficiency.",
      "authors": [
        "Yutaka Shimizu",
        "Masayoshi Tomizuka"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-02 21:33:03+00:00",
      "link": "https://arxiv.org/pdf/2602.02831v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Combines flow-matching with diffusion for robotic control",
      "llm_evidence_cn": "结合流匹配与扩散模型用于机器人控制",
      "llm_evidence": "结合流匹配与扩散模型用于机器人控制",
      "llm_tldr_en": "Introduces a flow-matching inspired linear path for diffusion models to improve stability in robotic control.",
      "llm_tldr_cn": "引入受流匹配启发的线性路径扩散模型，提高了机器人控制任务中的调参稳定性和性能。",
      "llm_tldr": "引入受流匹配启发的线性路径扩散模型，提高了机器人控制任务中的调参稳定性和性能。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03564v1",
      "title": "CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting",
      "abstract": "Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at https://github.com/liuyaguo/_CoGenCast.",
      "authors": [
        "Yaguo Liu",
        "Mingyue Cheng",
        "Daoyu Wang",
        "Xiaoyu Tao",
        "Qi Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03 14:08:10+00:00",
      "link": "https://arxiv.org/pdf/2602.03564v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Flow-matching mechanism for continuous temporal dynamics forecasting",
      "llm_evidence_cn": "用于连续时间动态预测的流匹配机制",
      "llm_evidence": "用于连续时间动态预测的流匹配机制",
      "llm_tldr_en": "Combines LLMs with flow-matching for effective time series forecasting and continuous dynamics.",
      "llm_tldr_cn": "结合大语言模型与流匹配机制，实现有效的时间序列预测和连续动态建模。",
      "llm_tldr": "结合大语言模型与流匹配机制，实现有效的时间序列预测和连续动态建模。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.02895v1",
      "title": "Moving On, Even When You're Broken: Fail-Active Trajectory Generation via Diffusion Policies Conditioned on Embodiment and Task",
      "abstract": "Robot failure is detrimental and disruptive, often requiring human intervention to recover. Maintaining safe operation under impairment to achieve task completion, i.e. fail-active operation, is our target. Focusing on actuation failures, we introduce DEFT, a diffusion-based trajectory generator conditioned on the robot's current embodiment and task constraints. DEFT generalizes across failure types, supports constrained and unconstrained motions, and enables task completion under arbitrary failure. We evaluated DEFT in both simulation and real-world scenarios using a 7-DoF robotic arm. In simulation over thousands of joint-failure cases across multiple tasks, DEFT outperformed the baseline by up to 2 times. On failures unseen during training, it continued to outperform the baseline, indicating robust generalization in simulation. Further, we performed real-world evaluations on two multi-step tasks, drawer manipulation and whiteboard erasing. These experiments demonstrated DEFT succeeding on tasks where classical methods failed. Our results show that DEFT achieves fail-active manipulation across arbitrary failure configurations and real-world deployments.",
      "authors": [
        "Gilberto G. Briscoe-Martinez",
        "Yaashia Gautam",
        "Rahul Shetty",
        "Anuj Pasricha",
        "Marco M. Nicotra",
        "Alessandro Roncone"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-02-02 23:02:48+00:00",
      "link": "https://arxiv.org/pdf/2602.02895v1",
      "tags": [
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Diffusion-based trajectory generation for robot motion",
      "llm_evidence_cn": "基于扩散模型的机器人运动轨迹生成",
      "llm_evidence": "基于扩散模型的机器人运动轨迹生成",
      "llm_tldr_en": "Introduces DEFT, a diffusion-based trajectory generator for robot motion under failure conditions.",
      "llm_tldr_cn": "引入DEFT，一种用于故障条件下机器人运动生成的扩散轨迹生成器。",
      "llm_tldr": "引入DEFT，一种用于故障条件下机器人运动生成的扩散轨迹生成器。",
      "llm_tags": [
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03796v1",
      "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation",
      "abstract": "Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.",
      "authors": [
        "Zhixue Fang",
        "Xu He",
        "Songlin Tang",
        "Haoxian Zhang",
        "Qingfeng Li",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Kun Gai"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-03 17:59:09+00:00",
      "link": "https://arxiv.org/pdf/2602.03796v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Discusses human motion control and video generation using implicit 3D-aware representations.",
      "llm_evidence_cn": "讨论了使用隐式3D感知表示的人体运动控制和视频生成。",
      "llm_evidence": "讨论了使用隐式3D感知表示的人体运动控制和视频生成。",
      "llm_tldr_en": "Proposes implicit motion control for view-adaptive human video generation to improve 3D awareness.",
      "llm_tldr_cn": "提出一种用于视角自适应人体视频生成的隐式运动控制方法，增强3D感知力。",
      "llm_tldr": "提出一种用于视角自适应人体视频生成的隐式运动控制方法，增强3D感知力。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.02928v1",
      "title": "Distance Marching for Generative Modeling",
      "abstract": "Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.",
      "authors": [
        "Zimo Wang",
        "Ishit Mehta",
        "Haolin Lu",
        "Chung-En Sun",
        "Ge Yan",
        "Tsui-Wei Weng",
        "Tzu-Mao Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03 00:00:37+00:00",
      "link": "https://arxiv.org/pdf/2602.02928v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Discusses flow matching and generative modeling improvements",
      "llm_evidence_cn": "讨论流匹配和生成模型的改进",
      "llm_evidence": "讨论流匹配和生成模型的改进",
      "llm_tldr_en": "Proposes Distance Marching, a time-unconditional generative approach that outperforms flow matching on benchmarks.",
      "llm_tldr_cn": "提出一种无需时间条件的生成建模方法 Distance Marching，在多个基准上优于流匹配。",
      "llm_tldr": "提出一种无需时间条件的生成建模方法 Distance Marching，在多个基准上优于流匹配。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02958v1",
      "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
      "abstract": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
      "authors": [
        "Haocheng Xi",
        "Shuo Yang",
        "Yilong Zhao",
        "Muyang Li",
        "Han Cai",
        "Xingyang Li",
        "Yujun Lin",
        "Zhuoyang Zhang",
        "Jintao Zhang",
        "Xiuyu Li",
        "Zhiying Xu",
        "Jun Wu",
        "Chenfeng Xu",
        "Ion Stoica",
        "Song Han",
        "Kurt Keutzer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03 00:54:32+00:00",
      "link": "https://arxiv.org/pdf/2602.02958v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Addresses long-horizon consistency and spatiotemporal redundancy in autoregressive video diffusion.",
      "llm_evidence_cn": "解决了自回归视频扩散中的长时一致性和时空冗余问题。",
      "llm_evidence": "解决了自回归视频扩散中的长时一致性和时空冗余问题。",
      "llm_tldr_en": "Presents Quant VideoGen for efficient long video generation using KV-cache quantization and spatiotemporal awareness.",
      "llm_tldr_cn": "提出Quant VideoGen，利用KV缓存量化和时空感知实现高效的长视频生成。",
      "llm_tldr": "提出Quant VideoGen，利用KV缓存量化和时空感知实现高效的长视频生成。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03188v1",
      "title": "Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives",
      "abstract": "Imitation learning (IL) enables robots to acquire human-like motion skills from demonstrations, but it still requires extensive high-quality data and retraining to handle complex or long-horizon tasks. To improve data efficiency and adaptability, this study proposes a hierarchical IL framework that integrates motion primitives with proportion-based motion synthesis. The proposed method employs a two-layer architecture, where the upper layer performs long-term planning, while a set of lower-layer models learn individual motion primitives, which are combined according to specific proportions. Three model variants are introduced to explore different trade-offs between learning flexibility, computational cost, and adaptability: a learning-based proportion model, a sampling-based proportion model, and a playback-based proportion model, which differ in how the proportions are determined and whether the upper layer is trainable. Through real-robot pick-and-place experiments, the proposed models successfully generated complex motions not included in the primitive set. The sampling-based and playback-based proportion models achieved more stable and adaptable motion generation than the standard hierarchical model, demonstrating the effectiveness of proportion-based motion integration for practical robot learning.",
      "authors": [
        "Yu-Han Shu",
        "Toshiaki Tsuji",
        "Sho Sakaino"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-03 06:57:06+00:00",
      "link": "https://arxiv.org/pdf/2602.03188v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "long-term motion planning and hierarchical framework",
      "llm_evidence_cn": "长期运动规划与分层框架",
      "llm_evidence": "长期运动规划与分层框架",
      "llm_tldr_en": "A hierarchical framework for long-term motion generation by integrating motion primitives.",
      "llm_tldr_cn": "一种通过整合运动基元进行长期运动生成的层次化框架。",
      "llm_tldr": "一种通过整合运动基元进行长期运动生成的层次化框架。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03376v1",
      "title": "PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer",
      "abstract": "Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer",
      "authors": [
        "Constantin Selzer",
        "Fabina B. Flohr"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2026-02-03 10:55:05+00:00",
      "link": "https://arxiv.org/pdf/2602.03376v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "trajectory prediction and motion forecasting",
      "llm_evidence_cn": "轨迹预测与运动预测",
      "llm_evidence": "轨迹预测与运动预测",
      "llm_tldr_en": "PlanTRansformer integrates goal-conditioned motion prediction and planning using a Transformer framework.",
      "llm_tldr_cn": "PlanTRansformer 使用 Transformer 框架集成了目标条件下的运动预测与规划。",
      "llm_tldr": "PlanTRansformer 使用 Transformer 框架集成了目标条件下的运动预测与规划。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03762v1",
      "title": "Conditional Flow Matching for Visually-Guided Acoustic Highlighting",
      "abstract": "Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling.",
      "authors": [
        "Hugo Malard",
        "Gael Le Lan",
        "Daniel Wong",
        "David Lou Alon",
        "Yi-Chiao Wu",
        "Sanjeel Parekh"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "published": "2026-02-03 17:24:47+00:00",
      "link": "https://arxiv.org/pdf/2602.03762v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "conditional flow matching framework",
      "llm_evidence_cn": "条件流匹配框架",
      "llm_evidence": "条件流匹配框架",
      "llm_tldr_en": "Introduces a Conditional Flow Matching framework for visually-guided acoustic highlighting.",
      "llm_tldr_cn": "引入了一种用于视觉引导音频增强的条件流匹配框架。",
      "llm_tldr": "引入了一种用于视觉引导音频增强的条件流匹配框架。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03789v1",
      "title": "Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants",
      "abstract": "Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the assumption of Gaussian data, we identify lazy schedule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statistically optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the usefulness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating images in fewer steps without retraining the model.",
      "authors": [
        "Gabriel Damsholt",
        "Jes Frellsen",
        "Susanne Ditlevsen"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-03 17:48:34+00:00",
      "link": "https://arxiv.org/pdf/2602.03789v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "fast sampling for flows and diffusions",
      "llm_evidence_cn": "流和扩散模型的快速采样",
      "llm_evidence": "流和扩散模型的快速采样",
      "llm_tldr_en": "This paper provides a theoretical framework for converting sample paths between different flow and diffusion schedules.",
      "llm_tldr_cn": "该论文提供了一个在不同流和扩散调度之间转换采样路径的理论框架。",
      "llm_tldr": "该论文提供了一个在不同流和扩散调度之间转换采样路径的理论框架。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "quick_tier": "6"
    }
  ]
}
{
  "mode": "standard",
  "generated_at": "2026-02-05T05:49:47.908707+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 2,
    "deep_cap": 8,
    "deep_selected": 2,
    "quick_candidates": 7,
    "quick_skim_target": 13,
    "quick_selected": 7
  },
  "deep_dive": [
    {
      "id": "2602.04188v1",
      "title": "DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding",
      "abstract": "Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.",
      "authors": [
        "Ning Zhang",
        "Zhengyu Li",
        "Kwong Weng Loh",
        "Mingxi Xu",
        "Qi Wang",
        "Zhengyu Wen",
        "Xiaoyu He",
        "Wei Zhao",
        "Kehong Gong",
        "Mingyuan Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-04 04:01:02+00:00",
      "link": "https://arxiv.org/pdf/2602.04188v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "discrete diffusion modeling for motion generation",
      "llm_evidence_cn": "用于动作生成的离散扩散模型",
      "llm_evidence": "用于动作生成的离散扩散模型",
      "llm_tldr_en": "A discrete diffusion framework for bidirectional text-motion understanding and generation with quality-latency trade-offs.",
      "llm_tldr_cn": "一种用于双向文本-动作理解与生成的离散扩散框架，支持质量与延迟的权衡。",
      "llm_tldr": "一种用于双向文本-动作理解与生成的离散扩散框架，支持质量与延迟的权衡。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ]
    },
    {
      "id": "2602.04292v1",
      "title": "Event-T2M: Event-level Conditioning for Complex Text-to-Motion Synthesis",
      "abstract": "Text-to-motion generation has advanced with diffusion models, yet existing systems often collapse complex multi-action prompts into a single embedding, leading to omissions, reordering, or unnatural transitions. In this work, we shift perspective by introducing a principled definition of an event as the smallest semantically self-contained action or state change in a text prompt that can be temporally aligned with a motion segment. Building on this definition, we propose Event-T2M, a diffusion-based framework that decomposes prompts into events, encodes each with a motion-aware retrieval model, and integrates them through event-based cross-attention in Conformer blocks. Existing benchmarks mix simple and multi-event prompts, making it unclear whether models that succeed on single actions generalize to multi-action cases. To address this, we construct HumanML3D-E, the first benchmark stratified by event count. Experiments on HumanML3D, KIT-ML, and HumanML3D-E show that Event-T2M matches state-of-the-art baselines on standard tests while outperforming them as event complexity increases. Human studies validate the plausibility of our event definition, the reliability of HumanML3D-E, and the superiority of Event-T2M in generating multi-event motions that preserve order and naturalness close to ground-truth. These results establish event-level conditioning as a generalizable principle for advancing text-to-motion generation beyond single-action prompts.",
      "authors": [
        "Seong-Eun Hong",
        "JaeYoung Seon",
        "JuYeong Hwang",
        "JongHwan Shin",
        "HyeongYeop Kang"
      ],
      "primary_category": "cs.GR",
      "categories": [
        "cs.GR"
      ],
      "published": "2026-02-04 07:45:21+00:00",
      "link": "https://arxiv.org/pdf/2602.04292v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Diffusion-based human motion synthesis with temporal alignment",
      "llm_evidence_cn": "基于扩散模型的人体运动合成与时序对齐",
      "llm_evidence": "基于扩散模型的人体运动合成与时序对齐",
      "llm_tldr_en": "Introduces Event-T2M, a diffusion-based framework for complex text-to-motion synthesis using event-level conditioning.",
      "llm_tldr_cn": "引入 Event-T2M，一种基于扩散模型的框架，利用事件级条件进行复杂的文本到运动合成。",
      "llm_tldr": "引入 Event-T2M，一种基于扩散模型的框架，利用事件级条件进行复杂的文本到运动合成。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.03973v1",
      "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
      "abstract": "Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/",
      "authors": [
        "Shuo Liu",
        "Ishneet Sukhvinder Singh",
        "Yiqing Xu",
        "Jiafei Duan",
        "Ranjay Krishna"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2026-02-03 19:50:16+00:00",
      "link": "https://arxiv.org/pdf/2602.03973v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "steering pretrained diffusion or flow-matching policies",
      "llm_evidence_cn": "引导预训练的扩散或流匹配策略",
      "llm_evidence": "引导预训练的扩散或流匹配策略",
      "llm_tldr_en": "A training-free framework for adapting frozen generative robot policies like flow-matching at inference time.",
      "llm_tldr_cn": "一种无需训练的框架，用于在推理时适配冻结的生成式机器人策略（如流匹配）。",
      "llm_tldr": "一种无需训练的框架，用于在推理时适配冻结的生成式机器人策略（如流匹配）。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.04271v1",
      "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
      "abstract": "4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/",
      "authors": [
        "Lifan Wu",
        "Ruijie Zhu",
        "Yubo Ai",
        "Tianzhu Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "published": "2026-02-04 07:00:44+00:00",
      "link": "https://arxiv.org/pdf/2602.04271v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Skeleton-driven 4D motion generation",
      "llm_evidence_cn": "骨架驱动的4D运动生成",
      "llm_evidence": "骨架驱动的4D运动生成",
      "llm_tldr_en": "Decomposes motion into rigid skeleton-driven and non-rigid components for editable 4D generation.",
      "llm_tldr_cn": "将运动分解为骨架驱动的刚性运动和非刚性组件，用于可编辑的4D生成。",
      "llm_tldr": "将运动分解为骨架驱动的刚性运动和非刚性组件，用于可编辑的4D生成。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04204v1",
      "title": "AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting",
      "abstract": "Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.",
      "authors": [
        "Chao Li",
        "Rui Zhang",
        "Siyuan Huang",
        "Xian Zhong",
        "Hongbo Jiang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-04 04:42:57+00:00",
      "link": "https://arxiv.org/pdf/2602.04204v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Multimodal human trajectory forecasting",
      "llm_evidence_cn": "多模态人类轨迹预测",
      "llm_evidence": "多模态人类轨迹预测",
      "llm_tldr_en": "Proposes AGMA to improve human trajectory forecasting through scene-adaptive Gaussian mixture priors.",
      "llm_tldr_cn": "提出AGMA，通过场景自适应高斯混合先验改进人类轨迹预测。",
      "llm_tldr": "提出AGMA，通过场景自适应高斯混合先验改进人类轨迹预测。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.04329v1",
      "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
      "abstract": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment.",
      "authors": [
        "Shuo Pei",
        "Yong Wang",
        "Yuanchen Zhu",
        "Chen Sun",
        "Qin Li",
        "Yanan Zhao",
        "Huachun Tan"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-04 08:46:05+00:00",
      "link": "https://arxiv.org/pdf/2602.04329v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Diffusion-based trajectory planning for dynamic agents",
      "llm_evidence_cn": "基于扩散模型的动态智能体轨迹规划",
      "llm_evidence": "基于扩散模型的动态智能体轨迹规划",
      "llm_tldr_en": "Develops a diffusion-based planner for safe and stylized trajectory generation in autonomous driving.",
      "llm_tldr_cn": "开发了一种基于扩散模型的规划器，用于自动驾驶中安全且具风格的轨迹生成。",
      "llm_tldr": "开发了一种基于扩散模型的规划器，用于自动驾驶中安全且具风格的轨迹生成。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04780v1",
      "title": "Dynamical Regimes of Multimodal Diffusion Models",
      "abstract": "Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.",
      "authors": [
        "Emil Albrychiewicz",
        "Andrés Franco Valiente",
        "Li-Ching Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04 17:16:12+00:00",
      "link": "https://arxiv.org/pdf/2602.04780v1",
      "tags": [
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Theoretical framework for multimodal diffusion models and temporal resolution",
      "llm_evidence_cn": "多模态扩散模型与时间分辨率的理论框架",
      "llm_evidence": "多模态扩散模型与时间分辨率的理论框架",
      "llm_tldr_en": "Explores the dynamical regimes and synchronization gaps in multimodal diffusion generative processes.",
      "llm_tldr_cn": "探讨了多模态扩散生成过程中的动力学机制和同步间隙。",
      "llm_tldr": "探讨了多模态扩散生成过程中的动力学机制和同步间隙。",
      "llm_tags": [
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04851v1",
      "title": "PDF-HR: Pose Distance Fields for Humanoid Robots",
      "abstract": "Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.",
      "authors": [
        "Yi Gu",
        "Yukang Gao",
        "Yangchen Zhou",
        "Xingyu Chen",
        "Yixiao Feng",
        "Mingle Zhao",
        "Yunyang Mo",
        "Zhaorui Wang",
        "Lixin Xu",
        "Renjing Xu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2026-02-04 18:38:51+00:00",
      "link": "https://arxiv.org/pdf/2602.04851v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "humanoid motion priors and retargeted poses",
      "llm_evidence_cn": "人形机器人运动先验与重定向姿态",
      "llm_evidence": "人形机器人运动先验与重定向姿态",
      "llm_tldr_en": "A lightweight prior representing robot pose distributions as a continuous manifold for optimization and control.",
      "llm_tldr_cn": "一种轻量级先验，将机器人姿态分布表示为连续流形，用于优化与控制。",
      "llm_tldr": "一种轻量级先验，将机器人姿态分布表示为连续流形，用于优化与控制。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04876v1",
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.",
      "authors": [
        "Jiahao Zhan",
        "Zizhang Li",
        "Hong-Xing Yu",
        "Jiajun Wu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-04 18:58:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04876v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "long-horizon action-conditioned 4D scene generation",
      "llm_evidence_cn": "长时程动作条件4D场景生成",
      "llm_evidence": "长时程动作条件4D场景生成",
      "llm_tldr_en": "A hybrid generative simulator for long-horizon 4D scene generation with closed-loop physical and visual updates.",
      "llm_tldr_cn": "一种用于长时程4D场景生成的混合生成模拟器，具有闭环物理和视觉更新功能。",
      "llm_tldr": "一种用于长时程4D场景生成的混合生成模拟器，具有闭环物理和视觉更新功能。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    }
  ]
}
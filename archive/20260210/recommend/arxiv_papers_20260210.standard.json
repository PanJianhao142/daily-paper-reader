{
  "mode": "standard",
  "generated_at": "2026-02-10T06:44:27.414795+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 2,
    "deep_cap": 8,
    "deep_selected": 2,
    "quick_candidates": 13,
    "quick_skim_target": 13,
    "quick_selected": 13
  },
  "deep_dive": [
    {
      "id": "2602.08462v1",
      "title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation",
      "abstract": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.",
      "authors": [
        "Yiyang Cao",
        "Yunze Deng",
        "Ziyu Lin",
        "Bin Feng",
        "Xinggang Wang",
        "Wenyu Liu",
        "Dandan Zheng",
        "Jingdong Chen"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-09 10:12:13+00:00",
      "link": "https://arxiv.org/pdf/2602.08462v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "text-to-motion generation and spatial-temporal modeling",
      "llm_evidence_cn": "文本到运动生成与时空建模",
      "llm_evidence": "文本到运动生成与时空建模",
      "llm_tldr_en": "Proposes a tri-domain causal framework for joint spatial, temporal, and frequency optimization in motion generation.",
      "llm_tldr_cn": "提出了一种三域因果框架，用于运动生成中的空间、时间和频域联合优化。",
      "llm_tldr": "提出了一种三域因果框架，用于运动生成中的空间、时间和频域联合优化。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ]
    },
    {
      "id": "2602.07967v1",
      "title": "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation",
      "abstract": "In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.",
      "authors": [
        "Xiaofeng Tan",
        "Wanjiang Weng",
        "Haodong Lei",
        "Hongsong Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-08 13:29:46+00:00",
      "link": "https://arxiv.org/pdf/2602.07967v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "diffusion-based motion generation and step-aware fine-tuning",
      "llm_evidence_cn": "基于扩散的运动生成与步进感知微调",
      "llm_evidence": "基于扩散的运动生成与步进感知微调",
      "llm_tldr_en": "Introduces EasyTune for efficient step-aware fine-tuning of diffusion-based motion generation models.",
      "llm_tldr_cn": "引入EasyTune，用于扩散运动生成模型的高效步进感知微调。",
      "llm_tldr": "引入EasyTune，用于扩散运动生成模型的高效步进感知微调。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.07498v1",
      "title": "IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation",
      "abstract": "Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.",
      "authors": [
        "Zhufeng Xu",
        "Xuan Gao",
        "Feng-Lin Liu",
        "Haoxian Zhang",
        "Zhixue Fang",
        "Yu-Kun Lai",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Lin Gao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-07 11:17:20+00:00",
      "link": "https://arxiv.org/pdf/2602.07498v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Implicit motion representation in video diffusion models",
      "llm_evidence_cn": "视频扩散模型中的隐式运动表示",
      "llm_evidence": "视频扩散模型中的隐式运动表示",
      "llm_tldr_en": "Introduces IM-Animation, using 1D motion tokens in diffusion models to handle varying body scales in animation.",
      "llm_tldr_cn": "引入IM-Animation，在扩散模型中使用1D运动令牌处理动画中不同的身体比例。",
      "llm_tldr": "引入IM-Animation，在扩散模型中使用1D运动令牌处理动画中不同的身体比例。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.07413v1",
      "title": "Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity",
      "abstract": "There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.",
      "authors": [
        "Yunhai Han",
        "Linhao Bai",
        "Ziyu Xiao",
        "Zhaodong Yang",
        "Yogita Choudhary",
        "Krishna Jha",
        "Chuizheng Kong",
        "Shreyas Kousik",
        "Harish Ravichandar"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-07 07:18:00+00:00",
      "link": "https://arxiv.org/pdf/2602.07413v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Diffusion-based behavioral models for temporal coherence in motion",
      "llm_evidence_cn": "基于扩散的行为模型，用于运动的时间连贯性",
      "llm_evidence": "基于扩散的行为模型，用于运动的时间连贯性",
      "llm_tldr_en": "Learns dexterous skills as coupled dynamical systems using diffusion backbones for temporal coherence.",
      "llm_tldr_cn": "使用扩散骨干网将灵巧技能学习为耦合动力系统，以实现时间连贯性。",
      "llm_tldr": "使用扩散骨干网将灵巧技能学习为耦合动力系统，以实现时间连贯性。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.07928v1",
      "title": "A Kinetic-Energy Perspective of Flow Matching",
      "abstract": "Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.",
      "authors": [
        "Ziyun Li",
        "Huancheng Hu",
        "Soon Hoe Lim",
        "Xuyu Li",
        "Fei Gao",
        "Enmao Diao",
        "Zezhen Ding",
        "Michalis Vazirgiannis",
        "Henrik Bostrom"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-08 11:51:50+00:00",
      "link": "https://arxiv.org/pdf/2602.07928v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Theoretical analysis of Flow Matching and trajectory energy",
      "llm_evidence_cn": "流匹配和轨迹能量的理论分析",
      "llm_evidence": "流匹配和轨迹能量的理论分析",
      "llm_tldr_en": "Introduces Kinetic Path Energy to analyze Flow Matching models through a physics-inspired lens.",
      "llm_tldr_cn": "引入动能路径能量，通过物理视角分析流匹配模型。",
      "llm_tldr": "引入动能路径能量，通过物理视角分析流匹配模型。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.08012v1",
      "title": "A Unified Density Operator View of Flow Control and Merging",
      "abstract": "Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.",
      "authors": [
        "Riccardo De Santi",
        "Malte Franke",
        "Ya-Ping Hsieh",
        "Andreas Krause"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-08 15:27:28+00:00",
      "link": "https://arxiv.org/pdf/2602.08012v1",
      "tags": [
        "keyword:FM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Discusses flow matching and reward-guided flow merging for generative models",
      "llm_evidence_cn": "讨论了生成模型的流匹配和奖励引导的流合并",
      "llm_evidence": "讨论了生成模型的流匹配和奖励引导的流合并",
      "llm_tldr_en": "Introduces a framework for merging pre-trained flow models and reward-guided adaptation in probability space.",
      "llm_tldr_cn": "引入了一个在概率空间中合并预训练流模型和奖励引导自适应的框架。",
      "llm_tldr": "引入了一个在概率空间中合并预训练流模型和奖励引导自适应的框架。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08318v1",
      "title": "Is Flow Matching Just Trajectory Replay for Sequential Data?",
      "abstract": "Flow matching (FM) is increasingly used for time-series generation, but it is not well understood whether it learns a general dynamical structure or simply performs an effective \"trajectory replay\". We study this question by deriving the velocity field targeted by the empirical FM objective on sequential data, in the limit of perfect function approximation. For the Gaussian conditional paths commonly used in practice, we show that the implied sampler is an ODE whose dynamics constitutes a nonparametric, memory-augmented continuous-time dynamical system. The optimal field admits a closed-form expression as a similarity-weighted mixture of instantaneous velocities induced by past transitions, making the dataset dependence explicit and interpretable. This perspective positions neural FM models trained by stochastic optimization as parametric surrogates of an ideal nonparametric solution. Using the structure of the optimal field, we study sampling and approximation schemes that improve the efficiency and numerical robustness of ODE-based generation. On nonlinear dynamical system benchmarks, the resulting closed-form sampler yields strong probabilistic forecasts directly from historical transitions, without training.",
      "authors": [
        "Soon Hoe Lim",
        "Shizheng Lin",
        "Michael W. Mahoney",
        "N. Benjamin Erichson"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "nlin.CD"
      ],
      "published": "2026-02-09 06:48:45+00:00",
      "link": "https://arxiv.org/pdf/2602.08318v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "flow matching for sequential data",
      "llm_evidence_cn": "用于序列数据的流匹配",
      "llm_evidence": "用于序列数据的流匹配",
      "llm_tldr_en": "Analyzes whether flow matching learns general dynamical structures or just replays trajectories in time-series.",
      "llm_tldr_cn": "分析流匹配在时间序列中是学习了通用动力学结构还是仅仅重放轨迹。",
      "llm_tldr": "分析流匹配在时间序列中是学习了通用动力学结构还是仅仅重放轨迹。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.08245v1",
      "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction",
      "abstract": "Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.",
      "authors": [
        "Jinhao Li",
        "Yuxuan Cong",
        "Yingqiao Wang",
        "Hao Xia",
        "Shan Huang",
        "Yijia Zhang",
        "Ningyi Xu",
        "Guohao Dai"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-02-09 03:50:40+00:00",
      "link": "https://arxiv.org/pdf/2602.08245v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "spatiotemporal consistency in diffusion policies",
      "llm_evidence_cn": "扩散策略中的时空一致性",
      "llm_evidence": "扩散策略中的时空一致性",
      "llm_tldr_en": "STEP uses spatiotemporal consistency to warm-start diffusion policies for faster robotic control.",
      "llm_tldr_cn": "STEP 利用时空一致性预热扩散策略，以实现更快的机器人控制。",
      "llm_tldr": "STEP 利用时空一致性预热扩散策略，以实现更快的机器人控制。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08337v1",
      "title": "Language-Guided Transformer Tokenizer for Human Motion Generation",
      "abstract": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.",
      "authors": [
        "Sheng Yan",
        "Yong Wang",
        "Xin Du",
        "Junsong Yuan",
        "Mengyuan Liu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-09 07:22:14+00:00",
      "link": "https://arxiv.org/pdf/2602.08337v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "motion discrete tokenization for efficient motion generation",
      "llm_evidence_cn": "用于高效运动生成的运动离散标记化",
      "llm_evidence": "用于高效运动生成的运动离散标记化",
      "llm_tldr_en": "Proposes Language-Guided Tokenization to align natural language with motion for high-quality generation.",
      "llm_tldr_cn": "提出语言引导的标记化方法，将自然语言与运动对齐以实现高质量的运动生成。",
      "llm_tldr": "提出语言引导的标记化方法，将自然语言与运动对齐以实现高质量的运动生成。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.08538v1",
      "title": "Trajectory Stitching for Solving Inverse Problems with Flow-Based Models",
      "abstract": "Flow-based generative models have emerged as powerful priors for solving inverse problems. One option is to directly optimize the initial latent code (noise), such that the flow output solves the inverse problem. However, this requires backpropagating through the entire generative trajectory, incurring high memory costs and numerical instability. We propose MS-Flow, which represents the trajectory as a sequence of intermediate latent states rather than a single initial code. By enforcing the flow dynamics locally and coupling segments through trajectory-matching penalties, MS-Flow alternates between updating intermediate latent states and enforcing consistency with observed data. This reduces memory consumption while improving reconstruction quality. We demonstrate the effectiveness of MS-Flow over existing methods on image recovery and inverse problems, including inpainting, super-resolution, and computed tomography.",
      "authors": [
        "Alexander Denker",
        "Moshe Eliasof",
        "Zeljko Kereta",
        "Carola-Bibiane Schönlieb"
      ],
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.LG"
      ],
      "published": "2026-02-09 11:36:41+00:00",
      "link": "https://arxiv.org/pdf/2602.08538v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Flow-based generative models and trajectory matching",
      "llm_evidence_cn": "基于流的生成模型和轨迹匹配",
      "llm_evidence": "基于流的生成模型和轨迹匹配",
      "llm_tldr_en": "Proposes MS-Flow to solve inverse problems by representing trajectories as intermediate states with matching penalties.",
      "llm_tldr_cn": "提出MS-Flow，通过将轨迹表示为中间状态并使用匹配惩罚来解决逆问题。",
      "llm_tldr": "提出MS-Flow，通过将轨迹表示为中间状态并使用匹配惩罚来解决逆问题。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08558v1",
      "title": "FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction",
      "abstract": "We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.",
      "authors": [
        "Guan Yuan Tan",
        "Ngoc Tuan Vu",
        "Arghya Pal",
        "Sailaja Rajanala",
        "Raphael Phan C. -W.",
        "Mettu Srinivas",
        "Chee-Ming Ting"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GT"
      ],
      "published": "2026-02-09 11:55:15+00:00",
      "link": "https://arxiv.org/pdf/2602.08558v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "flow-guided global motion and temporal deformation",
      "llm_evidence_cn": "流引导的全局运动与时间变形",
      "llm_evidence": "流引导的全局运动与时间变形",
      "llm_tldr_en": "FLAG-4D uses a dual-deformation network to capture complex point motions and dynamic details in 4D reconstruction.",
      "llm_tldr_cn": "FLAG-4D利用双变形网络捕捉4D重建中复杂的点运动和细粒度动态细节。",
      "llm_tldr": "FLAG-4D利用双变形网络捕捉4D重建中复杂的点运动和细粒度动态细节。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08753v1",
      "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization",
      "abstract": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.",
      "authors": [
        "Tianyu Sun",
        "Zhoujie Fu",
        "Bang Zhang",
        "Guosheng Lin"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-09 14:55:21+00:00",
      "link": "https://arxiv.org/pdf/2602.08753v1",
      "tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Character animation framework with temporal consistency and 3D structures",
      "llm_evidence_cn": "具有时间一致性和3D结构的字符动画框架",
      "llm_evidence": "具有时间一致性和3D结构的字符动画框架",
      "llm_tldr_en": "MVAnimate enhances character animation by optimizing multi-view 2D and 3D information for temporal coherence.",
      "llm_tldr_cn": "MVAnimate通过优化多视图2D和3D信息来增强角色动画的时间连贯性。",
      "llm_tldr": "MVAnimate通过优化多视图2D和3D信息来增强角色动画的时间连贯性。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08905v1",
      "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models",
      "abstract": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.",
      "authors": [
        "Jiawei Liu",
        "Xiting Wang",
        "Yuanyuan Zhong",
        "Defu Lian",
        "Yu Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 17:04:23+00:00",
      "link": "https://arxiv.org/pdf/2602.08905v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Spatio-temporal pruning in diffusion models",
      "llm_evidence_cn": "扩散模型中的时空剪枝",
      "llm_evidence": "扩散模型中的时空剪枝",
      "llm_tldr_en": "Proposes spatio-temporal pruning to improve efficiency and stability in diffusion language models.",
      "llm_tldr_cn": "提出时空剪枝框架，通过压缩生成过程中的冗余来提高扩散语言模型的效率。",
      "llm_tldr": "提出时空剪枝框架，通过压缩生成过程中的冗余来提高扩散语言模型的效率。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08962v1",
      "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting",
      "abstract": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D",
      "authors": [
        "Guangxun Zhu",
        "Xuan Liu",
        "Nicolas Pugeault",
        "Chongfeng Wei",
        "Edmond S. L. Ho"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2026-02-09 17:58:53+00:00",
      "link": "https://arxiv.org/pdf/2602.08962v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "3D pedestrian pose forecasting and motion prediction",
      "llm_evidence_cn": "3D 行人姿态预测与运动预测",
      "llm_evidence": "3D 行人姿态预测与运动预测",
      "llm_tldr_en": "Proposes a vehicle-conditioned pedestrian pose forecasting framework using TBIFormer and cross-attention modules.",
      "llm_tldr_cn": "提出一种利用 TBIFormer 和交叉注意力机制的车辆条件行人姿态预测框架。",
      "llm_tldr": "提出一种利用 TBIFormer 和交叉注意力机制的车辆条件行人姿态预测框架。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.09013v1",
      "title": "Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction",
      "abstract": "Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.",
      "authors": [
        "Hongyi Chen",
        "Tony Dong",
        "Tiancheng Wu",
        "Liquan Wang",
        "Yash Jangir",
        "Yaru Niu",
        "Yufei Ye",
        "Homanga Bharadhwaj",
        "Zackory Erickson",
        "Jeffrey Ichnowski"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2026-02-09 18:56:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09013v1",
      "tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Reconstructs 4D human motions for robot manipulation learning",
      "llm_evidence_cn": "重建4D人体运动用于机器人操作学习",
      "llm_evidence": "重建4D人体运动用于机器人操作学习",
      "llm_tldr_en": "Learns dexterous manipulation by reconstructing 4D hand-object trajectories and human motions from RGB videos.",
      "llm_tldr_cn": "通过从RGB视频中重建4D手物轨迹和人体运动来学习灵巧操作。",
      "llm_tldr": "通过从RGB视频中重建4D手物轨迹和人体运动来学习灵巧操作。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    }
  ]
}
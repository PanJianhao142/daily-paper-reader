{
  "mode": "standard",
  "generated_at": "2026-02-12T06:21:16.117902+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 1,
    "deep_cap": 8,
    "deep_selected": 1,
    "quick_candidates": 13,
    "quick_skim_target": 13,
    "quick_selected": 13
  },
  "deep_dive": [
    {
      "id": "2602.10285v1",
      "title": "Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning",
      "abstract": "Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.",
      "authors": [
        "Ananya Trivedi",
        "Anjian Li",
        "Mohamed Elnoor",
        "Yusuf Umut Ciftci",
        "Avinash Singh",
        "Jovin D'sa",
        "Sangjae Bae",
        "David Isele",
        "Taskin Padir",
        "Faizan M. Tariq"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-10 20:57:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10285v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "flow matching for motion prediction",
      "llm_evidence_cn": "用于运动预测的流匹配",
      "llm_evidence": "用于运动预测的流匹配",
      "llm_tldr_en": "Proposes a conditional flow matching framework for real-time future motion prediction in autonomous driving.",
      "llm_tldr_cn": "提出了一种基于条件流匹配的框架，用于自动驾驶中的实时未来运动预测。",
      "llm_tldr": "提出了一种基于条件流匹配的框架，用于自动驾驶中的实时未来运动预测。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.09449v1",
      "title": "Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing",
      "abstract": "Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \\emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \\emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.",
      "authors": [
        "Yan Luo",
        "Henry Huang",
        "Todd Y. Zhou",
        "Mengyu Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 06:34:47+00:00",
      "link": "https://arxiv.org/pdf/2602.09449v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "flow matching framework for trajectory adjustment",
      "llm_evidence_cn": "用于轨迹调整的流匹配框架",
      "llm_evidence": "用于轨迹调整的流匹配框架",
      "llm_tldr_en": "Proposes training-free latent-trajectory adjustment for flow matching to improve image generation.",
      "llm_tldr_cn": "提出了一种无需训练的流匹配潜轨迹调整方法，以优化生成质量。",
      "llm_tldr": "提出了一种无需训练的流匹配潜轨迹调整方法，以优化生成质量。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.09713v1",
      "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models",
      "abstract": "Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.",
      "authors": [
        "Ruisi Zhao",
        "Haoren Zheng",
        "Zongxin Yang",
        "Hehe Fan",
        "Yi Yang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 12:17:00+00:00",
      "link": "https://arxiv.org/pdf/2602.09713v1",
      "tags": [
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Latent diffusion for skeletal graph generation and 3D animation",
      "llm_evidence_cn": "用于骨骼图生成和3D动画的潜扩散模型",
      "llm_evidence": "用于骨骼图生成和3D动画的潜扩散模型",
      "llm_tldr_en": "Generates rigged 3D meshes from 2D strokes using a skeletal graph VAE and latent diffusion models.",
      "llm_tldr_cn": "利用骨骼图VAE和潜扩散模型，根据2D笔触生成带蒙皮的3D网格。",
      "llm_tldr": "利用骨骼图VAE和潜扩散模型，根据2D笔触生成带蒙皮的3D网格。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.09722v1",
      "title": "Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization",
      "abstract": "While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard \"scale data\" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla",
      "authors": [
        "Ye Wang",
        "Sipeng Zheng",
        "Hao Luo",
        "Wanpeng Zhang",
        "Haoqi Yuan",
        "Chaoyi Xu",
        "Haiweng Xu",
        "Yicheng Feng",
        "Mingyang Yu",
        "Zhiyu Kang",
        "Zongqing Lu",
        "Qin Jin"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-10 12:25:43+00:00",
      "link": "https://arxiv.org/pdf/2602.09722v1",
      "tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Combines vision-language backbone with flow-matching for robot control",
      "llm_evidence_cn": "结合视觉语言骨干与流匹配进行机器人控制",
      "llm_evidence": "结合视觉语言骨干与流匹配进行机器人控制",
      "llm_tldr_en": "Studies VLA model scaling using flow-matching for generalist robot control across diverse embodiments.",
      "llm_tldr_cn": "研究使用流匹配的VLA模型缩放，用于跨不同实体的通用机器人控制。",
      "llm_tldr": "研究使用流匹配的VLA模型缩放，用于跨不同实体的通用机器人控制。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10094v1",
      "title": "4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere",
      "abstract": "We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.",
      "authors": [
        "Yihang Luo",
        "Shangchen Zhou",
        "Yushi Lan",
        "Xingang Pan",
        "Chen Change Loy"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 18:57:04+00:00",
      "link": "https://arxiv.org/pdf/2602.10094v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Utilizes a spatio-temporal latent space for querying motion dynamics from video.",
      "llm_evidence_cn": "利用时空潜空间从视频中查询运动动力学。",
      "llm_evidence": "利用时空潜空间从视频中查询运动动力学。",
      "llm_tldr_en": "A 4D reconstruction framework using a transformer to encode spatio-temporal motion and geometry from monocular video.",
      "llm_tldr_cn": "一种利用 Transformer 从单目视频中编码时空运动和几何的 4D 重建框架。",
      "llm_tldr": "一种利用 Transformer 从单目视频中编码时空运动和几何的 4D 重建框架。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10099v1",
      "title": "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders",
      "abstract": "Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF",
      "authors": [
        "Amandeep Kumar",
        "Vishal M. Patel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-02-10 18:58:04+00:00",
      "link": "https://arxiv.org/pdf/2602.10099v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "proposes Riemannian Flow Matching to improve diffusion transformer convergence",
      "llm_evidence_cn": "提出黎曼流匹配以改进扩散Transformer的收敛性",
      "llm_evidence": "提出黎曼流匹配以改进扩散Transformer的收敛性",
      "llm_tldr_en": "Introduces Riemannian Flow Matching to resolve geometric interference in diffusion-based generative modeling.",
      "llm_tldr_cn": "引入黎曼流匹配技术，解决扩散生成模型中的几何干涉问题，提升模型性能。",
      "llm_tldr": "引入黎曼流匹配技术，解决扩散生成模型中的几何干涉问题，提升模型性能。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10102v1",
      "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos",
      "abstract": "Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.",
      "authors": [
        "Zhongwei Ren",
        "Yunchao Wei",
        "Xiao Yu",
        "Guixun Luo",
        "Yao Zhao",
        "Bingyi Kang",
        "Jiashi Feng",
        "Xiaojie Jin"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 18:58:19+00:00",
      "link": "https://arxiv.org/pdf/2602.10102v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "latent dynamics and long-horizon reasoning",
      "llm_evidence_cn": "潜空间动力学与长程推理",
      "llm_evidence": "潜空间动力学与长程推理",
      "llm_tldr_en": "A video diffusion model that decouples action dynamics for long-horizon task reasoning and policy learning.",
      "llm_tldr_cn": "一种解耦动作动力学的视频扩散模型，支持长程任务推理和策略学习。",
      "llm_tldr": "一种解耦动作动力学的视频扩散模型，支持长程任务推理和策略学习。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10221v1",
      "title": "DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions",
      "abstract": "In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\\bf 1)} geometric key feature extraction and {\\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.",
      "authors": [
        "El Hadji S. Diop",
        "Thierno Fall",
        "Mohamed Daoudi"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 19:13:47+00:00",
      "link": "https://arxiv.org/pdf/2602.10221v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Denoising diffusion models for geometric feature extraction",
      "llm_evidence_cn": "用于几何特征提取的去噪扩散模型",
      "llm_evidence": "用于几何特征提取的去噪扩散模型",
      "llm_tldr_en": "Enhances DDPMs with Riemannian equivariant group morphological convolutions for better geometric feature extraction.",
      "llm_tldr_cn": "通过黎曼等变群形态卷积增强 DDPM，以实现更好的几何特征提取。",
      "llm_tldr": "通过黎曼等变群形态卷积增强 DDPM，以实现更好的几何特征提取。",
      "llm_tags": [
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10326v1",
      "title": "Flow Matching with Uncertainty Quantification and Guidance",
      "abstract": "Despite the remarkable success of sampling-based generative models such as flow matching, they can still produce samples of inconsistent or degraded quality. To assess sample reliability and generate higher-quality outputs, we propose uncertainty-aware flow matching (UA-Flow), a lightweight extension of flow matching that predicts the velocity field together with heteroscedastic uncertainty. UA-Flow estimates per-sample uncertainty by propagating velocity uncertainty through the flow dynamics. These uncertainty estimates act as a reliability signal for individual samples, and we further use them to steer generation via uncertainty-aware classifier guidance and classifier-free guidance. Experiments on image generation show that UA-Flow produces uncertainty signals more highly correlated with sample fidelity than baseline methods, and that uncertainty-guided sampling further improves generation quality.",
      "authors": [
        "Juyeop Han",
        "Lukas Lao Beyer",
        "Sertac Karaman"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-10 22:03:13+00:00",
      "link": "https://arxiv.org/pdf/2602.10326v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Flow matching with uncertainty for generation",
      "llm_evidence_cn": "带不确定性的流匹配生成",
      "llm_evidence": "带不确定性的流匹配生成",
      "llm_tldr_en": "Introduces UA-Flow, an extension of flow matching that predicts velocity fields with uncertainty quantification.",
      "llm_tldr_cn": "引入UA-Flow，这是一种预测带不确定性量化速度场的流匹配扩展方法。",
      "llm_tldr": "引入UA-Flow，这是一种预测带不确定性量化速度场的流匹配扩展方法。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10420v1",
      "title": "Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning",
      "abstract": "Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.",
      "authors": [
        "Jiadong Hong",
        "Lei Liu",
        "Xinyu Bian",
        "Wenjie Wang",
        "Zhaoyang Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT",
        "eess.IV",
        "eess.SP"
      ],
      "published": "2026-02-11 02:02:30+00:00",
      "link": "https://arxiv.org/pdf/2602.10420v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "investigates flow matching prediction-loss alignment",
      "llm_evidence_cn": "研究流匹配预测与损失的对齐",
      "llm_evidence": "研究流匹配预测与损失的对齐",
      "llm_tldr_en": "Formalizes prediction-loss alignment in flow matching to improve robust learning on binary manifolds.",
      "llm_tldr_cn": "形式化了流匹配中的预测-损失对齐，以增强离散数据生成的鲁棒性。",
      "llm_tldr": "形式化了流匹配中的预测-损失对齐，以增强离散数据生成的鲁棒性。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.10659v1",
      "title": "Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation",
      "abstract": "We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.",
      "authors": [
        "Yin Wang",
        "Ziyao Zhang",
        "Zhiying Leng",
        "Haitian Liu",
        "Frederick W. B. Li",
        "Mu Li",
        "Xiaohui Liang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 09:04:28+00:00",
      "link": "https://arxiv.org/pdf/2602.10659v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "3D human-object interaction motion generation",
      "llm_evidence_cn": "3D人体-物体交互运动生成",
      "llm_evidence": "3D人体-物体交互运动生成",
      "llm_tldr_en": "A framework for text-driven 3D human-object interaction motion generation using multimodal priors.",
      "llm_tldr_cn": "一种利用多模态先验进行文本驱动的3D人体-物体交互运动生成的框架。",
      "llm_tldr": "一种利用多模态先验进行文本驱动的3D人体-物体交互运动生成的框架。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.11105v1",
      "title": "FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference",
      "abstract": "Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.",
      "authors": [
        "Divya Jyoti Bajpai",
        "Dhruv Bhardwaj",
        "Soumya Roy",
        "Tejas Duseja",
        "Harsh Agarwal",
        "Aashay Sandansing",
        "Manjesh Kumar Hanawal"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 18:21:11+00:00",
      "link": "https://arxiv.org/pdf/2602.11105v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "accelerating flow matching models for video generation",
      "llm_evidence_cn": "加速用于视频生成的流匹配模型",
      "llm_evidence": "加速用于视频生成的流匹配模型",
      "llm_tldr_en": "FastFlow accelerates flow matching models using bandit inference for faster image and video generation.",
      "llm_tldr_cn": "FastFlow通过老虎机推理加速流匹配模型，实现更快的图像和视频生成。",
      "llm_tldr": "FastFlow通过老虎机推理加速流匹配模型，实现更快的图像和视频生成。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.11117v1",
      "title": "HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion",
      "abstract": "We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.",
      "authors": [
        "Di Chang",
        "Ji Hou",
        "Aljaz Bozic",
        "Assaf Neuberger",
        "Felix Juefei-Xu",
        "Olivier Maury",
        "Gene Wei-Chin Lin",
        "Tuur Stuyck",
        "Doug Roble",
        "Mohammad Soleymani",
        "Stephane Grabli"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 18:31:47+00:00",
      "link": "https://arxiv.org/pdf/2602.11117v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Diffusion-based human hair motion synthesis",
      "llm_evidence_cn": "基于扩散的人体头发运动合成",
      "llm_evidence": "基于扩散的人体头发运动合成",
      "llm_tldr_en": "Presents HairWeaver, a video diffusion pipeline for realistic hair motion synthesis from a single image.",
      "llm_tldr_cn": "提出 HairWeaver，一种用于从单张图像合成逼真头发运动的视频扩散流水线。",
      "llm_tldr": "提出 HairWeaver，一种用于从单张图像合成逼真头发运动的视频扩散流水线。",
      "llm_tags": [
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.11146v1",
      "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "abstract": "Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.",
      "authors": [
        "Gongye Liu",
        "Bo Yang",
        "Yida Zhi",
        "Zhizhou Zhong",
        "Lei Ke",
        "Didan Deng",
        "Han Gao",
        "Yongxiang Huang",
        "Kaihao Zhang",
        "Hongbo Fu",
        "Wenhan Luo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-11 18:57:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11146v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "preference optimization for flow-matching and diffusion models",
      "llm_evidence_cn": "流匹配和扩散模型的偏好优化",
      "llm_evidence": "流匹配和扩散模型的偏好优化",
      "llm_tldr_en": "A latent reward model for preference learning in diffusion and flow-matching generators.",
      "llm_tldr_cn": "一种用于扩散和流匹配生成器偏好学习的潜空间奖励模型。",
      "llm_tldr": "一种用于扩散和流匹配生成器偏好学习的潜空间奖励模型。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "quick_tier": "6"
    }
  ]
}
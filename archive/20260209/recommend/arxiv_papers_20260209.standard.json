{
  "mode": "standard",
  "generated_at": "2026-02-09T06:20:03.435839+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 0,
    "deep_cap": 8,
    "deep_selected": 0,
    "quick_candidates": 5,
    "quick_skim_target": 13,
    "quick_selected": 5
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.06548v1",
      "title": "NECromancer: Breathing Life into Skeletons via BVH Animation",
      "abstract": "Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/",
      "authors": [
        "Mingxi Xu",
        "Qi Wang",
        "Zhengyu Wen",
        "Phong Dao Thien",
        "Zhengyu Li",
        "Ning Zhang",
        "Xiaoyu He",
        "Wei Zhao",
        "Kehong Gong",
        "Mingyuan Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-06 09:52:29+00:00",
      "link": "https://arxiv.org/pdf/2602.06548v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Universal motion tokenizer for arbitrary skeletons and motion sequences",
      "llm_evidence_cn": "适用于任意骨架和运动序列的通用运动分词器",
      "llm_evidence": "适用于任意骨架和运动序列的通用运动分词器",
      "llm_tldr_en": "Proposes NECromancer, a universal motion tokenizer for diverse skeletal morphologies and BVH animations.",
      "llm_tldr_cn": "提出NECromancer，一种适用于多种骨架形态和BVH动画的通用运动分词器。",
      "llm_tldr": "提出NECromancer，一种适用于多种骨架形态和BVH动画的通用运动分词器。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.06698v1",
      "title": "Crowd-FM: Learned Optimal Selection of Conditional Flow Matching-generated Trajectories for Crowd Navigation",
      "abstract": "Safe and computationally efficient local planning for mobile robots in dense, unstructured human crowds remains a fundamental challenge. Moreover, ensuring that robot trajectories are similar to how a human moves will increase the acceptance of the robot in human environments. In this paper, we present Crowd-FM, a learning-based approach to address both safety and human-likeness challenges. Our approach has two novel components. First, we train a Conditional Flow-Matching (CFM) policy over a dataset of optimally controlled trajectories to learn a set of collision-free primitives that a robot can choose at any given scenario. The chosen optimal control solver can generate multi-modal collision-free trajectories, allowing the CFM policy to learn a diverse set of maneuvers. Secondly, we learn a score function over a dataset of human demonstration trajectories that provides a human-likeness score for the flow primitives. At inference time, computing the optimal trajectory requires selecting the one with the highest score. Our approach improves the state-of-the-art by showing that our CFM policy alone can produce collision-free navigation with a higher success rate than existing learning-based baselines. Furthermore, when augmented with inference-time refinement, our approach can outperform even expensive optimisation-based planning approaches. Finally, we validate that our scoring network can select trajectories closer to the expert data than a manually designed cost function.",
      "authors": [
        "Antareep Singha",
        "Laksh Nanwani",
        "Mathai Mathew P.",
        "Samkit Jain",
        "Phani Teja Singamaneni",
        "Arun Kumar Singh",
        "K. Madhava Krishna"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-06 13:36:46+00:00",
      "link": "https://arxiv.org/pdf/2602.06698v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Uses Conditional Flow Matching for human-like trajectory generation",
      "llm_evidence_cn": "使用条件流匹配生成类人运动轨迹",
      "llm_evidence": "使用条件流匹配生成类人运动轨迹",
      "llm_tldr_en": "Crowd-FM uses Conditional Flow Matching to generate safe, human-like robot trajectories in dense crowds.",
      "llm_tldr_cn": "Crowd-FM利用条件流匹配生成安全且具有类人特征的机器人避障轨迹。",
      "llm_tldr": "Crowd-FM利用条件流匹配生成安全且具有类人特征的机器人避障轨迹。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.06825v1",
      "title": "AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models",
      "abstract": "Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.   To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.   Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.   By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.",
      "authors": [
        "Yuming Li",
        "Qingyu Li",
        "Chengyu Bai",
        "Xiangyang Luo",
        "Zeyue Xue",
        "Wenyu Qin",
        "Meng Wang",
        "Yikai Wang",
        "Shanghang Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-06 16:09:50+00:00",
      "link": "https://arxiv.org/pdf/2602.06825v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Policy optimization for diffusion and flow models",
      "llm_evidence_cn": "扩散和流模型的策略优化",
      "llm_evidence": "扩散和流模型的策略优化",
      "llm_tldr_en": "Proposes an adaptive entropy-guided policy optimization method to improve sampling efficiency in diffusion and flow models.",
      "llm_tldr_cn": "提出了一种自适应熵引导策略优化方法，以提高扩散和流模型的采样效率。",
      "llm_tldr": "提出了一种自适应熵引导策略优化方法，以提高扩散和流模型的采样效率。",
      "llm_tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.06827v1",
      "title": "DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization",
      "abstract": "In this paper, we introduce DynaRetarget, a complete pipeline for retargeting human motions to humanoid control policies. The core component of DynaRetarget is a novel Sampling-Based Trajectory Optimization (SBTO) framework that refines imperfect kinematic trajectories into dynamically feasible motions. SBTO incrementally advances the optimization horizon, enabling optimization over the entire trajectory for long-horizon tasks. We validate DynaRetarget by successfully retargeting hundreds of humanoid-object demonstrations and achieving higher success rates than the state of the art. The framework also generalizes across varying object properties, such as mass, size, and geometry, using the same tracking objective. This ability to robustly retarget diverse demonstrations opens the door to generating large-scale synthetic datasets of humanoid loco-manipulation trajectories, addressing a major bottleneck in real-world data collection.",
      "authors": [
        "Victor Dhedin",
        "Ilyass Taouil",
        "Shafeef Omar",
        "Dian Yu",
        "Kun Tao",
        "Angela Dai",
        "Majid Khadiv"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-06 16:14:27+00:00",
      "link": "https://arxiv.org/pdf/2602.06827v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Long-horizon human motion trajectory optimization",
      "llm_evidence_cn": "长周期人体运动轨迹优化",
      "llm_evidence": "长周期人体运动轨迹优化",
      "llm_tldr_en": "Presents a trajectory optimization framework for retargeting human motions to humanoid robots.",
      "llm_tldr_cn": "提出一种轨迹优化框架，用于将人体运动重定向到类人机器人控制策略。",
      "llm_tldr": "提出一种轨迹优化框架，用于将人体运动重定向到类人机器人控制策略。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.06949v1",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "abstract": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
      "authors": [
        "Shenyuan Gao",
        "William Liang",
        "Kaiyuan Zheng",
        "Ayaan Malik",
        "Seonghyeon Ye",
        "Sihyun Yu",
        "Wei-Cheng Tseng",
        "Yuzhu Dong",
        "Kaichun Mo",
        "Chen-Hsuan Lin",
        "Qianli Ma",
        "Seungjun Nah",
        "Loic Magne",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Ruijie Zheng",
        "Dantong Niu",
        "You Liang Tan",
        "K. R. Zentner",
        "George Kurian",
        "Suneel Indupuru",
        "Pooya Jannaty",
        "Jinwei Gu",
        "Jun Zhang",
        "Jitendra Malik",
        "Pieter Abbeel",
        "Ming-Yu Liu",
        "Yuke Zhu",
        "Joel Jang",
        "Linxi \"Jim\" Fan"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-06 18:49:43+00:00",
      "link": "https://arxiv.org/pdf/2602.06949v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "World model for human video and dexterous control",
      "llm_evidence_cn": "人类视频和灵巧控制的世界模型",
      "llm_evidence": "人类视频和灵巧控制的世界模型",
      "llm_tldr_en": "A foundation world model trained on 44k hours of human videos to simulate interactions and robot controls.",
      "llm_tldr_cn": "一个在4.4万小时人类视频上训练的基础世界模型，用于模拟交互和机器人控制。",
      "llm_tldr": "一个在4.4万小时人类视频上训练的基础世界模型，用于模拟交互和机器人控制。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    }
  ]
}
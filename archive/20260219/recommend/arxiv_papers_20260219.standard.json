{
  "mode": "standard",
  "generated_at": "2026-02-19T06:18:54.287492+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 0,
    "deep_cap": 8,
    "deep_selected": 0,
    "quick_candidates": 4,
    "quick_skim_target": 13,
    "quick_selected": 4
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.16020v1",
      "title": "MolCrystalFlow: Molecular Crystal Structure Prediction via Flow Matching",
      "abstract": "Molecular crystal structure prediction represents a grand challenge in computational chemistry due to large sizes of constituent molecules and complex intra- and intermolecular interactions. While generative modeling has revolutionized structure discovery for molecules, inorganic solids, and metal-organic frameworks, extending such approaches to fully periodic molecular crystals is still elusive. Here, we present MolCrystalFlow, a flow-based generative model for molecular crystal structure prediction. The framework disentangles intramolecular complexity from intermolecular packing by embedding molecules as rigid bodies and jointly learning the lattice matrix, molecular orientations, and centroid positions. Centroids and orientations are represented on their native Riemannian manifolds, allowing geodesic flow construction and graph neural network operations that respects geometric symmetries. We benchmark our model against state-of-the-art generative models for large-size periodic crystals and rule-based structure generation methods on two open-source molecular crystal datasets. We demonstrate an integration of MolCrystalFlow model with universal machine learning potential to accelerate molecular crystal structure prediction, paving the way for data-driven generative discovery of molecular crystals.",
      "authors": [
        "Cheng Zeng",
        "Harry W. Sullivan",
        "Thomas Egg",
        "Maya M. Martirossyan",
        "Philipp Höllmer",
        "Jirui Jin",
        "Richard G. Hennig",
        "Adrian Roitberg",
        "Stefano Martiniani",
        "Ellad B. Tadmor",
        "Mingjie Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-17 21:22:08+00:00",
      "link": "https://arxiv.org/pdf/2602.16020v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "flow matching for structure prediction",
      "llm_evidence_cn": "用于结构预测的流匹配",
      "llm_evidence": "用于结构预测的流匹配",
      "llm_tldr_en": "Presents MolCrystalFlow, a flow-matching generative model for molecular crystal structure prediction.",
      "llm_tldr_cn": "提出 MolCrystalFlow，一种用于分子晶体结构预测的流匹配生成模型。",
      "llm_tldr": "提出 MolCrystalFlow，一种用于分子晶体结构预测的流匹配生成模型。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.16169v1",
      "title": "Discrete Stochastic Localization for Non-autoregressive Generation",
      "abstract": "Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \\emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \\textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \\textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \\(\\sim\\)4$\\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.",
      "authors": [
        "Yunshu Wu",
        "Jiayi Cheng",
        "Partha Thakuria",
        "Rob Brekelmans",
        "Evangelos E. Papalexakis",
        "Greg Ver Steeg"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-18 04:05:40+00:00",
      "link": "https://arxiv.org/pdf/2602.16169v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "masked diffusion language models and iterative refinement",
      "llm_evidence_cn": "掩码扩散语言模型与迭代优化",
      "llm_evidence": "掩码扩散语言模型与迭代优化",
      "llm_tldr_en": "Proposes Discrete Stochastic Localization to improve the efficiency of masked diffusion model sampling.",
      "llm_tldr_cn": "提出离散随机定位方法，旨在提高掩码扩散模型采样的步骤效率。",
      "llm_tldr": "提出离散随机定位方法，旨在提高掩码扩散模型采样的步骤效率。",
      "llm_tags": [
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.16198v1",
      "title": "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform",
      "abstract": "Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.",
      "authors": [
        "Qijie Zhu",
        "Zeqi Ye",
        "Han Liu",
        "Zhaoran Wang",
        "Minshuo Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 05:44:19+00:00",
      "link": "https://arxiv.org/pdf/2602.16198v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "adaptation of diffusion models",
      "llm_evidence_cn": "扩散模型的自适应",
      "llm_evidence": "扩散模型的自适应",
      "llm_tldr_en": "Proposes DOIT, a training-free adaptation method for diffusion models using Doob's h-transform.",
      "llm_tldr_cn": "提出 DOIT，一种利用 Doob h-变换的扩散模型免训练自适应方法。",
      "llm_tldr": "提出 DOIT，一种利用 Doob h-变换的扩散模型免训练自适应方法。",
      "llm_tags": [
        "keyword:MDM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.16412v1",
      "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
      "abstract": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.",
      "authors": [
        "Daichi Yashima",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Komei Sugiura"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18 12:37:35+00:00",
      "link": "https://arxiv.org/pdf/2602.16412v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "refined motion representation for long-video understanding",
      "llm_evidence_cn": "用于长视频理解的精细运动表示",
      "llm_evidence": "用于长视频理解的精细运动表示",
      "llm_tldr_en": "Proposes ReMoRa, a video MLLM using refined motion representations to handle long-form video dynamics.",
      "llm_tldr_cn": "提出ReMoRa，一种利用精细运动表示处理长视频动态的视频大模型。",
      "llm_tldr": "提出ReMoRa，一种利用精细运动表示处理长视频动态的视频大模型。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "6"
    }
  ]
}
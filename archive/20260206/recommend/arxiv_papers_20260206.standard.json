{
  "mode": "standard",
  "generated_at": "2026-02-06T05:49:52.028057+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 1,
    "deep_cap": 8,
    "deep_selected": 1,
    "quick_candidates": 6,
    "quick_skim_target": 13,
    "quick_selected": 6
  },
  "deep_dive": [
    {
      "id": "2602.05755v1",
      "title": "FMPose3D: monocular 3D pose estimation via flow matching",
      "abstract": "Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.",
      "authors": [
        "Ti Wang",
        "Xiaohang Yu",
        "Mackenzie Weygandt Mathis"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05 15:25:35+00:00",
      "link": "https://arxiv.org/pdf/2602.05755v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Flow Matching for 3D pose estimation and probabilistic generation",
      "llm_evidence_cn": "用于3D姿态估计和概率生成的流匹配技术",
      "llm_evidence": "用于3D姿态估计和概率生成的流匹配技术",
      "llm_tldr_en": "FMPose3D uses Flow Matching for efficient 3D pose estimation via conditional distribution transport.",
      "llm_tldr_cn": "FMPose3D利用流匹配技术通过条件分布传输实现高效的3D姿态估计。",
      "llm_tldr": "FMPose3D利用流匹配技术通过条件分布传输实现高效的3D姿态估计。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.05174v1",
      "title": "Total Variation Rates for Riemannian Flow Matching",
      "abstract": "Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\\mathrm{TV}\\le C_{\\mathrm{Lip}}\\,h + C_{\\varepsilon}\\,\\varepsilon$ (with an additional higher-order $\\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\\varepsilon$ is the target accuracy. Instantiations yield \\emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.",
      "authors": [
        "Yunrui Guan",
        "Krishnakumar Balasubramanian",
        "Shiqian Ma"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-05 01:06:53+00:00",
      "link": "https://arxiv.org/pdf/2602.05174v1",
      "tags": [
        "keyword:FM"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Riemannian flow matching theory",
      "llm_evidence_cn": "黎曼流匹配理论",
      "llm_evidence": "黎曼流匹配理论",
      "llm_tldr_en": "Provides a convergence analysis for Riemannian flow matching samplers on manifolds.",
      "llm_tldr_cn": "为流形上的黎曼流匹配采样器提供了收敛性分析。",
      "llm_tldr": "为流形上的黎曼流匹配采样器提供了收敛性分析。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.05319v1",
      "title": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
      "abstract": "Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.",
      "authors": [
        "Yinan Huang",
        "Hans Hao-Hsun Hsu",
        "Junran Wang",
        "Bo Dai",
        "Pan Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-05 05:37:14+00:00",
      "link": "https://arxiv.org/pdf/2602.05319v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Sequential flow matching for trajectory prediction",
      "llm_evidence_cn": "用于轨迹预测的序列流匹配",
      "llm_evidence": "用于轨迹预测的序列流匹配",
      "llm_tldr_en": "Introduces a Bayesian filtering framework for sequential flow matching to model complex, multi-modal trajectories.",
      "llm_tldr_cn": "引入了一种基于贝叶斯滤波的序列流匹配框架，用于建模复杂的、多模态的轨迹。",
      "llm_tldr": "引入了一种基于贝叶斯滤波的序列流匹配框架，用于建模复杂的、多模态的轨迹。",
      "llm_tags": [
        "keyword:FM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.05435v1",
      "title": "Stable Velocity: A Variance Perspective on Flow Matching",
      "abstract": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
      "authors": [
        "Donglin Yang",
        "Yongxing Zhang",
        "Xin Yu",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Xiaojuan Qi",
        "Renjie Liao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05 08:25:05+00:00",
      "link": "https://arxiv.org/pdf/2602.05435v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "variance reduction in flow matching for stable optimization",
      "llm_evidence_cn": "流匹配中的方差减少以实现稳定优化",
      "llm_evidence": "流匹配中的方差减少以实现稳定优化",
      "llm_tldr_en": "Introduces Stable Velocity to reduce variance in flow matching training and improve sampling stability.",
      "llm_tldr_cn": "引入Stable Velocity以减少流匹配训练中的方差并提高采样稳定性。",
      "llm_tldr": "引入Stable Velocity以减少流匹配训练中的方差并提高采样稳定性。",
      "llm_tags": [
        "keyword:FM"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.05551v1",
      "title": "FastVMT: Eliminating Redundancy in Video Motion Transfer",
      "abstract": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.",
      "authors": [
        "Yue Ma",
        "Zhikai Wang",
        "Tianhao Ren",
        "Mingzhe Zheng",
        "Hongyu Liu",
        "Jiayi Guo",
        "Mark Fong",
        "Yuxuan Xue",
        "Zixiang Zhao",
        "Konrad Schindler",
        "Qifeng Chen",
        "Linfeng Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05 11:15:59+00:00",
      "link": "https://arxiv.org/pdf/2602.05551v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Addresses motion transfer using Diffusion Transformers and temporal redundancy",
      "llm_evidence_cn": "使用扩散Transformer处理视频动作迁移并解决时间冗余问题",
      "llm_evidence": "使用扩散Transformer处理视频动作迁移并解决时间冗余问题",
      "llm_tldr_en": "FastVMT accelerates video motion transfer by eliminating redundancy in Diffusion Transformer architectures.",
      "llm_tldr_cn": "FastVMT通过消除扩散Transformer架构中的冗余来加速视频动作迁移。",
      "llm_tldr": "FastVMT通过消除扩散Transformer架构中的冗余来加速视频动作迁移。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.05590v1",
      "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
      "abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.",
      "authors": [
        "Haojie Cheng",
        "Shaun Jing Heng Ong",
        "Shaoyu Cai",
        "Aiden Tat Yang Koh",
        "Fuxi Ouyang",
        "Eng Tat Khoo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.GR"
      ],
      "published": "2026-02-05 12:17:35+00:00",
      "link": "https://arxiv.org/pdf/2602.05590v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "spatiotemporal reasoning for human pose",
      "llm_evidence_cn": "人体姿态的时空推理",
      "llm_evidence": "人体姿态的时空推理",
      "llm_tldr_en": "Presents EgoPoseVR for accurate egocentric full-body pose estimation using spatiotemporal multi-modal reasoning.",
      "llm_tldr_cn": "提出 EgoPoseVR 框架，利用时空多模态推理实现精确的全身姿态估计。",
      "llm_tldr": "提出 EgoPoseVR 框架，利用时空多模态推理实现精确的全身姿态估计。",
      "llm_tags": [
        "query:课题"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.05871v1",
      "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
      "abstract": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.",
      "authors": [
        "Xunzhi Xiang",
        "Zixuan Duan",
        "Guiyu Zhang",
        "Haiyu Zhang",
        "Zhe Gao",
        "Junta Wu",
        "Shaofeng Zhang",
        "Tengfei Wang",
        "Qi Fan",
        "Chunchao Guo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-05 16:50:39+00:00",
      "link": "https://arxiv.org/pdf/2602.05871v1",
      "tags": [
        "keyword:FM",
        "keyword:MDM",
        "query:课题"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "long video generation with diffusion",
      "llm_evidence_cn": "扩散模型长视频生成",
      "llm_evidence": "扩散模型长视频生成",
      "llm_tldr_en": "Addresses error accumulation in long-sequence generation for distilled diffusion models.",
      "llm_tldr_cn": "解决了蒸馏扩散模型在长序列生成中的误差累积问题。",
      "llm_tldr": "解决了蒸馏扩散模型在长序列生成中的误差累积问题。",
      "llm_tags": [
        "keyword:MDM",
        "query:课题"
      ],
      "quick_tier": "6"
    }
  ]
}
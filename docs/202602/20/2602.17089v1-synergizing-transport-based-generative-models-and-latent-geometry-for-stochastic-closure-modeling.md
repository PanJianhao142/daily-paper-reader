---
title: Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling
title_zh: 协同基于输运的生成模型与潜几何进行随机闭合建模
authors: "Xinghao Dong, Huchen Yang, Jin-long Wu"
date: 2026-02-19
pdf: "https://arxiv.org/pdf/2602.17089v1"
tags: ["keyword:FM"]
score: 6.0
evidence: 用于随机建模的流匹配
tldr: 本研究针对扩散模型在随机闭合建模中采样速度慢的问题，提出在低维潜空间中结合流匹配（Flow Matching）技术。通过对比隐式正则化与显式几何约束（如保度量和几何感知），研究发现该方法能实现单步采样，速度比传统扩散模型快两个数量级。此外，该方法能有效捕捉原动力系统的拓扑特征，在减少训练数据需求的同时保证了物理保真度。
motivation: 扩散模型虽能生成高质量随机闭合样本但采样速度慢，限制了其在复杂动力系统实时建模中的应用。
method: 提出在低维潜空间中使用流匹配技术，并引入保度量（MP）和几何感知（GA）等显式正则化手段来控制潜空间畸变。
result: 在2D Kolmogorov流实验中，该方法实现了比迭代扩散模型快两个数量级的单步采样，并能有效继承原系统的流形拓扑信息。
conclusion: 结合流匹配与潜空间几何约束的生成模型，为高效、低数据依赖的随机闭合建模提供了一种兼顾速度与物理保真度的有效方案。
---

## 摘要
最近为生成式人工智能任务开发的扩散模型能够产生高质量样本，同时保持样本多样性以促进模式覆盖，为学习随机闭合模型提供了一条极具前景的途径。与 GAN 和 VAE 等其他类型的生成式人工智能模型相比，采样速度是扩散模型的一个主要劣势。通过在二维 Kolmogorov 流的数值示例上系统地比较基于输运的生成模型，我们表明在低维潜空间中进行流匹配（flow matching）适用于随机闭合模型的快速采样，其实现的单步采样速度比基于迭代扩散的方法快两个数量级。为了控制潜空间失真并确保采样闭合项的物理保真度，我们将联合训练方案提供的隐式正则化与两种显式正则化器（保度量 (MP) 和几何感知 (GA) 约束）进行了对比。除了提供更快的采样速度外，显式和隐式正则化的潜空间都继承了原始复杂动力系统低维流形的关键拓扑信息，从而能够在不需要海量训练数据的情况下学习随机闭合模型。

## Abstract
Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.
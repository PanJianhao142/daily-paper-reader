---
title: Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching
title_zh: 利用条件流匹配实现从刚性到软体机器人夹爪的抓取合成匹配
authors: "Tanisha Parulekar, Ge Shi, Josh Pinskier, David Howard, Jen Jen Chung"
date: 2026-02-19
pdf: "https://arxiv.org/pdf/2602.17110v1"
tags: ["keyword:FM"]
score: 6.0
evidence: 利用条件流匹配进行姿态映射
tldr: 针对刚性与软体抓取器间的表示差异，本文提出一种基于条件流匹配（CFM）的框架，将 Anygrasp 生成的刚性抓取位姿映射到软体 Fin-ray 抓取器。通过 U-Net 提取物体几何特征并结合 CFM 学习复杂的位姿转换，有效解决了软体抓取器合规性建模难的问题。实验证明该方法在已知和未知物体上的抓取成功率显著优于直接使用刚性位姿，为软体机器人抓取策略迁移提供了高效方案。
motivation: 现有的抓取合成方法多针对刚性抓取器，难以直接应用于具有独特顺应行为且建模复杂的软体抓取器。
method: 利用条件流匹配（CFM）结合 U-Net 提取的深度图像几何特征，学习从刚性抓取位姿到稳定软体抓取位姿的连续映射。
result: "在 7 自由度机器人实验中，CFM 生成的位姿在未知物体上的抓取成功率从 25% 提升至 46%，尤其在圆柱和球形物体上表现优异。"
conclusion: 本研究证明了 CFM 是实现刚性到软体抓取策略迁移的一种数据高效且可扩展的有效方法。
---

## 摘要
刚性夹爪与软体夹爪的抓取合成之间存在表示鸿沟。Anygrasp [1] 及许多其他抓取合成方法是为刚性平行夹爪设计的，将它们适配到软体夹爪时，往往无法捕捉其独特的顺应性行为，导致模型数据密集且不准确。为了弥补这一差距，本文提出了一种新颖的框架，用于将抓取位姿从刚性夹爪模型映射到软体 Fin-ray 夹爪。我们利用生成模型——条件流匹配（Conditional Flow Matching, CFM）来学习这种复杂的变换。我们的方法包括一个数据采集流水线，用于生成成对的刚性-软体抓取位姿。U-Net 自编码器根据深度图像中的物体几何形状对 CFM 模型进行条件约束，使其能够学习从初始 Anygrasp 位姿到稳定 Fin-ray 夹爪位姿的连续映射。我们在 7 自由度（7-DOF）机器人上验证了该方法，结果表明，当由软体夹爪执行时，CFM 生成的位姿在已知和未知物体上的总体成功率（分别为 34% 和 46%）均高于基准刚性位姿（分别为 6% 和 25%）。该模型表现出显著改进，特别是对于圆柱体（已知和未知物体的成功率分别为 50% 和 100%）和球形物体（已知和未知物体的成功率分别为 25% 和 31%），并成功泛化到未知物体。本研究表明 CFM 是一种数据高效且有效的抓取策略迁移方法，为其他软体机器人系统提供了一种可扩展的方法论。

## Abstract
A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.
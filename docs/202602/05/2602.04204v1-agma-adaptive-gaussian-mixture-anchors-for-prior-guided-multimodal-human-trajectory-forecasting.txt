Title: AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting

URL Source: https://arxiv.org/pdf/2602.04204v1

Published Time: Thu, 05 Feb 2026 01:32:44 GMT

Number of Pages: 14

Markdown Content:
# AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting 

Chao Li 1 Rui Zhang 1 Siyuan Huang 2 Xian Zhong 1 Hongbo Jiang 3

# Abstract 

Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors of-ten fail to capture the full distribution of plausi-ble futures, limiting both prediction accuracy and diversity. We theoretically establish that predic-tion error is lower-bounded by prior quality, mak-ing prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adap-tive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demon-strate that AGMA achieves state-of-the-art perfor-mance, confirming the critical role of high-quality priors in trajectory forecasting. 

# 1. Introduction 

Trajectory forecasting is a cornerstone capability for au-tonomous systems, enabling safe navigation in human-populated environments (Ettinger et al., 2021; Sadat et al., 2020; Floreano & Wood, 2015; Rudenko et al., 2020). The task demands predicting not only accurate future paths but also the multimodality of human behaviors (Rudenko et al., 2020). To address this inherent multimodality, recent methods in-troduce auxiliary mechanisms such as destination (Man-galam et al., 2020b), patterns (Shi et al., 2023), and Gaus-sian distribution (Maeda & Ukita, 2023). From a Bayesian perspective (Kingma & Welling, 2013; Sohn et al., 2015), these mechanisms can be interpreted as priors that guide 

> 1

School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, China 2Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA 

> 3

College of Computer Science and Electronic Engineering, Hu-nan University, Changsha, China. Correspondence to: Chao Li 

<302476@whut.edu.cn >.

Preprint. February 5, 2026. 

Figure 1. Qualitative comparison of priors at a three-way inter-section with two agents: (a) Implicit Gaussian priors: Simple Gaussian priors collapse to a single dominant mode during train-ing, yielding limited diversity. (b) Discrete anchor priors: Fixed discrete priors produce repetitive predictions that fail to adapt to scene-specific contexts. (c) AGMA (Ours): Adaptive Gaussian Mixture Anchors generate diverse, semantically aligned predic-tions through explicit, scene-aware prior construction. 

the model in exploring the space of possible future trajecto-ries (Lee et al., 2017; Gupta et al., 2018) and fundamentally drive the generation of diverse predictions. For example, at an intersection, such priors may encode typical behavioral patterns such as turning left, going straight, or turning right. Despite the established role of priors in multimodal forecast-ing, recent research has focused primarily on application-driven improvements, such as scene understanding and agent interaction modeling (Shi et al., 2024; Chen et al., 2025; Bae et al., 2022). While these advances have im-proved overall performance, the prior mechanisms involved are often treated as fixed auxiliary components.This over-sight has left their formulation and impact largely underex-plored. We identify two prevalent paradigms for handling priors in trajectory forecasting. Implicit Gaussian priors (Fang et al., 2025; Fu et al., 2025; Bhattacharyya et al., 2019; Mao et al., 2023a) leverage continuous latent variables sampled from certain distributions (e.g., standard Gaussians), aiming to in-duce diversity through stochastic sampling. These methods typically employ variational inference frameworks in which the prior is learned implicitly as a byproduct of optimiz-ing reconstruction objectives. Discrete anchor priors (Fang et al., 2025; Sun et al., 2021; Chen et al., 2024; Chai et al., 2020; Phan-Minh et al., 2020), in contrast, explicitly de-fine a finite set of behavioral modes, typically derived from dataset-level clustering (e.g., k-means on future trajectories). 1

> arXiv:2602.04204v1 [cs.CV] 4 Feb 2026 Submission and Formatting Instructions for ICML 2026

Each anchor represents a prototypical trajectory pattern, and predictions are generated by selecting and refining these discrete templates. However, Implicit Gaussian priors are prone to mode col-lapse during training (Yan et al., 2020; Nicoli et al., 2023; Dai et al., 2020)(Fig. 1a). Discrete anchor priors, while avoiding collapse through explicit mode enumeration, offer a finite set of discrete patterns, resulting in predictions con-fined to these limited anchors (Madjid et al., 2025).(Fig. 1b). For example, at a three-way intersection, fixed anchors may generate left-turn predictions even when the road geometry prohibits such maneuvers. These phenomena suggest that prior knowledge plays a more central role in trajectory forecasting than previously assumed. We further examine this point through theoreti-cal analysis (detailed in Sect. 3). We show that improving prior quality is essential to achieve accurate and diverse predictions. Despite employing different mechanisms, both existing paradigms suffer from prior misalignment: the im-plicit Gaussian prior collapses to average behaviors, while discrete candidate point–based priors approximate continu-ous behaviors in a coarse and often suboptimal manner. Therefore, we aim to improve prior quality by address-ing misalignment and validating our theoretical insights through empirical comparisons with state-of-the-art meth-ods. To this end, we propose Adaptive Gaussian Mixture Anchors (AGMA), a prior modeling framework based on two key strategies: (1) A batch-wise clustering autoen-coder , which clusters trajectory embeddings within each batch and decodes cluster assignments back to trajectory space, producing batch-specific priors; and (2) An optimal transport-based distillation , which aggregates batch priors into a global GMM using cross attention to form global priors. Extensive experiments on three benchmarks (ETH-UCY (Lerner et al., 2007; Pellegrini et al., 2009), Stanford Drone (Robicquet et al., 2016), ego-centric JRDB (Mart ´ın-Mart ´ın et al., 2023)) confirm our thesis: AGMA achieves competitive performance, outperforming methods with com-plex decoders but misalignment priors. In summary, our main contributions are as follows. • We identify prior quality as the critical factor for multi-modal trajectory forecasting through theoretical anal-ysis. Specifically, we prove that high-quality priors are necessary for achieving accurate predictions and faithful distribution matching. • We propose AGMA, a framework for explicitly opti-mizing prior quality via batch-wise clustering autoen-coding and optimal transfer-based distillation, effec-tively mitigating prior misalignment. • Experiments on ETH/UCY and JRDB show that AGMA achieves state-of-the-art performance, improv-ing mADE 20 by 5.26% and mFDE 20 by 9.38% on ETH/UCY. 

# 2. Related work 

2.1. Gaussian-based Priors 

A major line of multimodal forecasting approaches intro-duces a latent variable z, typically drawn from a Gaussian distribution, to implicitly encode the underlying behavioral patterns. Various strategies have been explored under this paradigm. Apratim et al. (Bhattacharyya et al., 2019) pro-posed CF-VAE using conditional normalizing flows, while Mao et al. (Mao et al., 2023a) and Fu et al. (Fu et al., 2025) improved the inference efficiency for diffusion and flow-matching models, respectively. Parallel efforts enhance interaction and context modeling to improve prediction ro-bustness. Despite this progress, these methods remain fundamentally constrained by the unimodal nature of their Gaussian pri-ors. The intrinsic lack of multimodality forces models to compensate for limited diversity via increasingly complex architectures. For example, Neuralized Markov Random Field (NMRF) (Fang et al., 2025) and Trajectory-Scene-Cell (TSC) (HU & Cham, 2025) incorporate specialized decoders or interaction modules to capture richer dynamics, effectively counterbalancing the simplicity of their latent priors. 

2.2. Mode-based Priors 

In contrast, another family of approaches introduces a dis-crete latent variable z to explicitly represent distinct behav-ioral patterns, thereby directly addressing multimodality. These works typically define patterns as representative tra-jectory prototypes. Sun et al. (Sun et al., 2021) proposed PCCSNet, which clusters historical paths into modality sets, classifies observations into these patterns, and synthesizes the corresponding futures. Shi et al. (Shi et al., 2023) intro-duced TUTR, which relies on a set of pre-computed motion patterns to generate probabilistic future trajectories without post-processing. A related branch constructs structured, data-driven latent spaces for mode representation. Bae et al. (Bae et al., 2023; 2024) applied SVD to form compact latent spaces for effi-cient trajectory encoding. Chen et al. (Chen et al., 2024) further advanced this direction with Mixed Gaussian Flow (MGF), which clusters training data into multimodal Gaus-sian priors serving as expressive base distributions for flow-based modeling. However, whether patterns are defined as prototypes, latent 2Submission and Formatting Instructions for ICML 2026 

components, or clustered priors, their quantity and struc-ture are typically fixed before training, which limits their adaptability compared to true priors. This static ”one-size-fits-all” design restricts the model’s ability to effectively handle diversity human motion scenes. 

# 3. The Necessity of Priors 

3.1. Problem Formulation 

Consider a scene containing M pedestrians. We denote the collection of trajectory pairs in the scene as: 

S = {(xj , y j )}Mj=1 , xj ∈ RTobs ×d, y j ∈ RTpred ×d (1) where M is the number of pedestrians, Tobs and Tpred denote the observation and prediction time steps respectively, and 

d is the spatial dimension (typically d = 2 for 2D coor-dinates). The trajectory forecasting task is to predict the future trajectories {yj }Mj=1 given the observed trajectories 

{xj }Mj=1 .However, in real-world scenarios with complex interactions and diverse human intentions, predicting a single determinis-tic future trajectory from observations is unrealistic. Instead, the task becomes predicting a distribution over plausible fu-ture trajectories (Liang et al., 2020; Salzmann et al., 2020). To formalize this, we treat the future trajectory Yj as a ran-dom variable taking values in the possible future trajectory space, and denote the true conditional distribution as: 

p(Yj | X) = p(Yj | { xk}Mk=1 ), (2) which assigns probability densities to all possible future paths of pedestrian j, conditioned on the observed trajecto-ries X = {xk}Mk=1 .For the discrete anchors and implicit Gaussian priors, we unify their perspectives through variable z:

p(Yj | X) = 

> C

X

> c=1

p(Yj | X, z c)

| {z }

> sampler

· p(zc | X)

| {z }

> prior

, (discrete z)or 

p(Yj | X) = 

Z

p(Yj | X, z )

| {z }

> sampler

· p(z | X)

| {z }

> prior

dz, (continuous z)(3) While Eq. (3) offers a principled view of multimodal predic-tion, the prior term p(z|X) remains is what we concerned in this work. 

3.2. How Prior Matters 

Trajectory prediction pursues two fundamental objectives: 

Prediction Accuracy : Minimizing the expected distance between predicted and ground-truth trajectories: 

Lacc = EX,Y j



min  

> k

∥Yj − ˆY (k) 

> j

∥2



, (4) where ˆY (1)  

> j

, . . . , ˆY (K) 

> j

are K sampled predictions. 

Distribution Matching : Aligning the predicted distribution with the true conditional distribution: 

Ldist = EX

KL  p(Yj | X) ∥ q(Yj | X) . (5) To expose this limitation, we decompose Ldist into prior and sampler components. Let q(Yj | X) denote the predictive distribution produced by model q, which factorizes into a learned prior q(z | X) and a conditional sampler q(Yj |

X, z ).

Theorem 3.1. For a given scene with observed trajectories 

X and target agent j, define the prior error and sampler error as: 

ϵprior (X) = 

Z

(p(z|X) − q(z|X)) q(Yj |X, z ) dz 

> 1

,

(6) 

ϵsample (X) = 

Z

p(z|X) ( p(Yj |X, z ) − q(Yj |X, z )) dz 

> 1

,

(7) 

where ϵprior (X) measures the distributional mismatch be-tween the true prior p(z|X) and the learned prior q(z|X),and ϵsample (X) quantifies the sampler’s reconstruction ac-curacy. Then the prediction loss satisfies: 

Ldist (X) ≥ 12 (ϵprior (X) − ϵsample (X)) 2 . (8) 

Proof. See Supplementary Materials .3.2.1. STAGE 1: P RIOR LIMITS PREDICTION ACCURACY 

The sampler’s ability to reconstruct Yj from latent code z is fundamentally constrained by the information content of z.By the data processing inequality: 

I(Yj ; ˆYj |X) ≤ I(Yj ; z|X), (9) with equality achieved only when z is a sufficient statistic for Yj given X.

Proposition 3.2. For a fixed prior q(z|X), the sampler error is lower-bounded by: 

ϵsample (X) ≥ C (H(Yj |X) − I(Yj ; z|X)) , (10) 

where C(h) = √2σ2h is a monotonically increasing func-tion, H(Yj |X) is the conditional entropy of trajectories, and 

σ2 characterizes the noise level in trajectory distributions. 

3Submission and Formatting Instructions for ICML 2026 

Proof. Consider the optimal sampler q∗(Yj |X, z ) =

E[Yj |X, z ] that minimizes the expected reconstruction error. The residual uncertainty satisfies: 

H(Yj |X, z ) = H(Yj |X) − I(Yj ; z|X) ≥ 0. (11) By Fano’s inequality for continuous variables, the mean squared reconstruction error is bounded by: 

Ep(Yj ,z |X)

∥Yj − E[Yj |X, z ]∥22

 ≥ σ2H(Yj |X, z ), (12) where σ2 depends on the effective dimensionality and vari-ance of Yj . Converting to L1 distance via Cauchy-Schwarz and applying Pinsker’s inequality: 

ϵsample (X) = Ep(z|X) [∥p(Yj |X, z ) − q∗(Yj |X, z )∥1]

(13) 

≥

q

2 · KL (p(Yj |X, z )∥q∗(Yj |X, z )) (14) 

≥

q

2σ2H(Yj |X, z ) (15) 

= C (H(Yj |X) − I(Yj ; z|X)) , (16) where the second inequality follows from the relationship between KL divergence and entropy for Gaussian-like dis-tributions. Proposition 3.2 reveals that when the prior q(z|X) fails to capture sufficient information about Yj (i.e., I(Yj ; z|X) ≪

H(Yj |X)), the sampler faces an irreducible error floor de-termined purely by the information gap. When the learned prior q(z|X) is informationally deficient, we have: 

I(Yj ; z|X) < H (Yj |X) − δinfo , (17) for some information gap δinfo > 0. In this regime, even with an optimal sampler q∗(Yj |X, z ) = E[Yj |X, z ], the sampler error remains bounded away from zero: 

ϵmin sample (X) ≥ C (δinfo ) > 0. (18) 

Remark 3.3 . Gradient-based optimization of Lacc with re-spect to sampler parameters θsample satisfies: 

lim  

> t→∞

ϵ(t)sample (X) = ϵmin sample (X) = H(Yj |X) − I(Yj ; z|X),

(19) where t indexes training iterations. This plateau is reached when the sampler has extracted all available information from z, leaving only the irreducible uncertainty H(Yj |X, z ).3.2.2. S TAGE 2: P RIOR LIMITS DISTRIBUTION 

MATCHING 

Combining Theorem 3.1 with the sampler performance ceil-ing from Proposition 3.2, we establish the necessity of high-quality priors: 

Corollary 3.4. For any target distribution matching error 

δ > 0, achieving Ldist (X) < δ requires the prior error to satisfy when ϵsample < ϵ prior :

ϵprior (X) < √2δ + ϵmin sample (X), (20) 

where ϵmin sample (X) = C(H(Yj |X) − I(Yj ; z|X)) is the information-theoretic lower bound from Eq. (10) .Proof. See Supplementary Materials .

# 4. Methodology 

4.1. Overview 

According to Theorem 3.1 and Corollary 3.4, minimizing prediction error fundamentally requires minimizing the mis-match between learned prior q(z|X) and true prior p(z|X).To validate this insight, we propose AGMA, a two-stage framework that explicitly model priors: 

Stage 1: Batch Prior Extraction (§4.2). We encode com-plete trajectories and apply graph-based clustering to dis-cover future trajectory patterns. Clustering is optimized by decoding cluster assignments back to trajectories, maxi-mizing I(Y ; z|X) through reconstruction, generating batch-level priors q(z|X(b), Y (b)) that preserve long-tail patterns. 

Stage 2: Global Prior Distillation (§4.3). Batch-discovered patterns are distilled into a global GMM through optimal transport, with cross-attention selecting relevant components and forming p(z|X) in global. To isolate the effect of prior quality, we employ a simple MLP decoder, demonstrating that strong performance stems from expressive priors rather than complex samplers. 

4.2. Batch Prior Extraction 

The Proposition 3.2 reveals that the performance of the sam-pler is fundamentally limited by I(Yj ; z|X). To construct informative priors, we discover behavioral patterns through graph clustering, then optimize the clustering to maximize mutual information by decoding cluster assignments back to trajectories. For each training batch B = {S(b)}Bbatch  

> b=1

, where S(b) de-notes a scene containing M (b) agents, we collect all agent trajectories across scenes: 

Bagents =

> Bbatch

[

> b=1

{(xb,j , y b,j )}M (b) 

> j=1

, (21) 4Submission and Formatting Instructions for ICML 2026 

Figure 2. AGMA architecture. Left: Graph-based clustering discovers behavioral patterns within each batch, forming batch-level GMM priors. Right: Optimal transport distills batch priors into a global GMM, refined via trajectory prediction with a shared decoder. 

where (xb,j , y b,j ) denotes the trajectory pair of agent j in scene b. The total number of agents in the batch is Nbatch =PBbatch  

> b=1

M (b).We extract trajectory representations using two encoders 

Φpast and Φfull that share the same architecture: temporal convolutional layers and GRU units first encode the input into time-dependent features, which are then processed by a self-attention layer to capture socially-aware representations. For each agent (b, j ) in the batch: 

f past  

> b,j

= Φ past (xb,j ), f full  

> b,j

= Φ full (xb,j , y b,j ) ∈ Rd. (22) Recall that f past  

> b,j

encodes the observed context while f full 

> b,j

encodes the complete trajectory. We employ graph-based clustering that explicitly models be-havioral similarity in trajectory space across all agents in the batch. For each agent (b, j ), we compute dual projections of its full trajectory embedding via MLPs: 

sb,j = Φ sim (f full  

> b,j

), rb,j = Φ rep (f full  

> b,j

), (23) where I[·] denotes the indicator function. Two agents are connected if they exhibit high similarity ( ˜S > θ sim ) and low repulsion ( ˜R < θ rep ), indicating membership in the same behavioral mode. We enable end-to-end training of the thresholds via the Straight-Through Estimator (Bae et al., 2022) (details in Appendix B). The graph adjacency matrix is constructed using learnable thresholds θsim and θrep . For agents (b, j ) and (b′, j ′):

A(b,j ),(b′,j ′) = I[ ˜S(b,j ),(b′,j ′) > θ sim ]∧I[ ˜R(b,j ),(b′,j ′) < θ rep ],

(24) where I[·] denotes the indicator function, and differentiation through discrete thresholding is enabled through the Gumbel softmax (Jang et al., 2017) (details in Appendix C). Two agents are connected if they exhibit high similarity and low repulsion, indicating membership in the same behavioral mode. The Connected-component analysis on graph A partitions all agents in the batch into mode clusters {I k}Npatterns  

> k=1

. For each cluster k with cardinality nk = |I k|, we estimate a Gaussian component as follows: 

πB,k = nk

Nbatch 

, μB,k = 1

nk

X

> (b,j )∈I k

f full  

> b,j

, (25) 

σ2 

> B,k

= 1max( nk − 1, 1) 

X

> (b,j )∈I k

(f full  

> b,j

− μB,k )⊙2. (26) Aggregating all clusters yields the batch prior in GMM form: 

q(z|X(B), Y (B)) = 

> Npatterns

X

> k=1

πB,k N (z; μB,k , diag( σ2

> B,k

)) ,

(27) which serves as an approximation of the true prior p(z|X)

restricted to the current batch. 

4.3. Global Prior Distillation 

We parameterize a global prior as a GMM with Kg compo-nents in the latent space z:GMM global = (πg , μg , Σg ) Kg 

> g=1

, (28) 5Submission and Formatting Instructions for ICML 2026 

where π = ( π1, . . . , π Kg ) are the base mixture weights and 

Σg = diag( σ2 

> g

) are diagonal covariances. To construct the context-conditioned prior qθ (z|x), we em-ploy a cross-attention mechanism that selects and reweights components from the GMM global based on the observed trajectories. Specifically, given the past trajectory feature 

f past  

> b,j

= Φ past (xb,j ) as the query and the global component centers {μg }Kg 

> g=1

as keys, we compute attention scores: 

ab,j,g = CrossAttention  f past  

> b,j

, {μg }Kg

> g=1

 

> g

, g = 1 , . . . , K g ,

(29) where ab,j,g ∈ [0 , 1] and PKg 

> g=1

ab,j,g = 1 .The context-dependent prior for agent (b, j ) is then formed as a reweighted sub-GMM: 

qθ (z|xb,j ) = 

> Kg

X

> g=1

ab,j,g N (z | μg , Σg ), (30) where the attention weights ab,j,g dynamically emphasize patterns relevant to the observed trajectory xb,j .

4.4. Sampler for Prior 

Given a past trajectory feature f past  

> b,j

for agent j in scene b

and a latent mode code z(n) 

> b,j

, we decode future trajectories through a shared MLP decoder Dϕ:

ˆY (n) 

> b,j

= Dϕ

 f past  

> b,j

, z(n)

> b,j

, n = 1 , . . . , N, (31) producing N candidate future trajectories per agent. The decoder Dϕ is shared across both the batch prior extraction (§4.2) and the global prior inference stages. 

4.5. Training and Inference 

We jointly optimize three components: (1) batch-level clus-tering quality, (2) global prior distillation, and (3) trajectory reconstruction accuracy. For each agent (b, j ) assigned to cluster cb,j , we sample latent codes from its cluster’s Gaussian: 

z(n) 

> b,j,B

∼ N (μB,c b,j , ΣB,c b,j ), ˆY (n) 

> b,j,B

= Dϕ(f past  

> b,j

, z(n) 

> b,j,B

),

(32) and minimize best-of-N ADE (Fang et al., 2025): 

LB = 1

Nbatch 

X

> b,j

min  

> n

ADE( ˆY (n) 

> b,j,B

, Y b,j ). (33) We align the cross-attention-weighted global GMM with batch priors via entropic optimal transport: 

Ldistill = min  

> P

⟨P, C⟩+εH (P), s.t. P1 = ¯ a, P⊤1 = βB ,

(34) where ¯a are batch-averaged attention weights, 

βB are batch prior weights, and Cgk =

W 22 (N (μg , Σg ), N (μB,k , ΣB,k )) measures Wasser-stein distance (Panaretos & Zemel, 2019) between global and batch components. We sample from the attention-weighted global prior and decode: 

g ∼ Cat (ab,j ), z(n) 

> b,j,G

∼ N (μg , Σg ),

LG = 1

Nbatch 

X

> b,j

min  

> n

ADE( ˆY (n)

> b,j,G

, Y b,j ). (35) The total objective is: 

Ltotal = LB + LG + λLdistill . (36) At test time, we use only the global prior. For each agent, cross-attention selects relevant GMM components based on 

f past , we sample z(n) ∼ qθ (z|x), and decode via Dϕ to produce N diverse predictions. 

# 5. Experimental Results 

We conduct extensive experiments to validate the effective-ness of the proposed AGMA framework. We first introduce the benchmark datasets and evaluation metrics (see §5.1), followed by theory-guided ablation studies and quantitative comparisons with state-of-the-art methods (see §5.3). 

5.1. Datasets and Metrics Datasets. We evaluate AGMA on three pedestrian trajec-tory benchmarks covering diverse interaction scenarios: 

ETH-UCY (Pellegrini et al., 2009; Lerner et al., 2007) con-tains five subsets (ETH, HOTEL, UNIV, ZARA1, ZARA2). Following (Gupta et al., 2018), we use leave-one-out cross-validation, predicting 12 future frames (4.8s) from 8 ob-served frames (3.2s). 

Stanford Drone (SDD) (Robicquet et al., 2016) provides bird’s-eye view trajectories on a university campus. We predict 12 frames (4.8s) from 8 frames (3.2s) and report metrics in both pixel and meter units. 

JRDB (Mart ´ın-Mart ´ın et al., 2023) is a large-scale ego-centric dataset recorded by a social robot. We follow the official train-validation-test split, predicting 12 frames from 9 frames at 2.5 FPS. Trajectories are transformed to global coordinates using provided odometry. To address the validation-test leakage issue identified in (Fang et al., 2025), we strictly separate validation (for hyperparameter tuning) and test sets (for final evaluation only). 

Evaluation Metrics. We report Average Displacement Error (ADE) and Final Displacement Error (FDE). For 6Submission and Formatting Instructions for ICML 2026 

Table 1. Quantitative results on the ETH-UCY dataset. We report mADE 20 /mFDE 20 (m). Best bold , second-best underlined. Method Venue ETH HOTEL UNIV ZARA1 ZARA2 AVG mADE 20 mFDE 20 mADE 20 mFDE 20 mADE 20 mFDE 20 mADE 20 mFDE 20 mADE 20 mFDE 20 mADE 20 mFDE 20 

Trajectron++ ECCV’20 0.61 1.03 0.32 0.55 0.37 0.70 0.29 0.53 0.25 0.45 0.37 0.65 PECNET ECCV’20 0.64 1.13 0.22 0.38 0.35 0.57 0.25 0.45 0.18 0.31 0.33 0.57 GP-Graph ECCV’22 0.43 0.63 0.18 0.30 0.25 0.48 0.23 0.45 0.18 0.35 0.25 0.44 VIKT TITS’23 0.30 0.51 0.13 0.25 0.23 0.51 0.21 0.44 0.14 0.30 0.20 0.40 TUTR ICCV’23 0.45 0.67 0.14 0.20 0.24 0.44 0.19 0.36 0.15 0.28 0.23 0.39 EigenTraj ICCV’23 0.36 0.53 0.12 0.19 0.24 0.34 0.19 0.33 0.14 0.24 0.21 0.33 SocialCircle CVPR’24 0.27 0.42 0.13 0.16 0.29 0.51 0.19 0.33 0.14 0.25 0.20 0.33 SingularTraj CVPR’24 0.35 0.42 0.13 0.19 0.25 0.44 0.19 0.32 0.15 0.25 0.21 0.32 MGF NIPS’24 0.39 0.59 0.13 0.20 0.21 0.39 0.17 0.29 0.14 0.24 0.21 0.34 MoFlow CVPR’25 0.40 0.57 0.12 0.18 0.23 0.40 0.17 0.30 0.13 0.23 0.21 0.34 NMRF ICLR’25 0.26 0.37 0.11 0.17 0.28 0.49 0.17 0.30 0.14 0.25 0.19 0.32 AGMA (Ours) 0.24 0.35 0.10 0.15 0.25 0.44 0.17 0.29 0.14 0.23 0.18 0.29 

Table 2. Comparison on SDD. We report mADE 20 /mFDE 20 (m). Best bold , second-best underlined. Method Venue mADE 20 mFDE 20 

TUTR ICCV’23 7.76 12.69 EigenTraj ICCV’23 8.05 13.25 MGF NeurIPS’24 7.74 12.07 MoFlow CVPR’25 7.66 12.39 NMRF ICLR’25 7.20 11.29 AGMA (Ours) 7.23 10.92 

Table 3. Comparison on JRDB. We report mADE 20 /mFDE 20 (m). Best bold , second-best underlined. Method Venue mADE 20 mFDE 20 

LED CVPR’23 0.18 0.28 NMRF ICLR’25 0.17 0.27 AGMA (Ours) 0.15 0.23 

multimodal prediction, we use minimum-of-20 metrics (mADE 20 /mFDE 20 ), which measure the best prediction among 20 samples (Mangalam et al., 2020a; Mao et al., 2023b). 

5.2. Implementation Details 

Due to space constraints, we report only key hyperparam-eters mentioned in § 4. Complete Implementation De-tails are provided in the Supplementary Materials D. The learnable thresholds are initialized as θsim = 0.7 and 

θrep = 0 .3. For differentiable thresholding, we use the Gumbel-Softmax temperature τ = 0 .1. The global GMM contains Kg = 1000 components. For entropic optimal transport, the Sinkhorn temperature is set to ε = 0 .1. The distillation weight in the total objective is λ = 0 .1.

5.3. Main Results 

We conduct comprehensive studies to answer two core re-search questions that validate our theoretical framework and method implementation. 

RQ1: Can Prior Modeling Improve Performance? Re-call from Section 3.2 that our theoretical analysis decom-poses the prediction error into two sources: prior mismatch and sampler error. Theorem 1 predicts that the prior term plays a significant role when the generator is reasonably ex-pressive, suggesting that improving p(z|X) can be effective alongside refining p(Y |z, X ). We validate this prediction through comprehensive experiments across three datasets, as shown in Table 1, Table 2 and Table 3 These results conclusively validate our theoretical frame-work: by explicitly minimizing the prior mismatch through batch clustering and global distillation with a relatively poor sampler p(Y |z, X ), AGMA gains the consistent improve-ments across diverse datasets demonstrate that: (1) prior quality p(z|X) is a dominant factor in prediction accuracy, validating our theoretical analysis in Section 3.2, and (2) AGMA’s adaptive prior construction effectively minimizes the prior misalignment, thereby reducing the L identified in Theorem 1. 

RQ2: How AGMA work? We evaluate three variants by removing each loss term. Removing LB (w/o LB ) con-structs the batch prior qB purely from clustering without prediction-driven refinement. Removing LG (w/o LG) al-lows the global prior q to learn only via distillation from batch priors, with no direct supervision. Removing Ldistill (w/o Distill) trains batch and global priors independently without distillation. Our ablation study reveals the importance of each loss com-ponent. LG proves most critical: removing it causes the largest performance degradation, with ADE increasing by 18% and FDE by 22% on average, and even more dramat-ically on UNIV (+42% ADE, +46% FDE). Without direct supervision, the global prior degenerates into a smoothed replica of imperfect batch priors and loses its self-correction 7Submission and Formatting Instructions for ICML 2026                                                                                 

> Table 4. Ablation study on the ETH-UCY dataset. We report mADE 20 /mFDE 20 (m). Best results are highlighted in bold .Method ETH HOTEL UNIV ZARA1 ZARA2 AVG ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE
> w/o LB.0.27 0.40 0.11 0.15 0.30 0.51 0.18 0.30 0.15 0.25 0.20 0.32
> w/o LG.0.26 0.36 0.11 0.15 0.43 0.82 0.18 0.30 0.14 0.23 0.22 0.37
> w/o Distill 0.27 0.38 0.11 0.15 0.27 0.45 0.19 0.33 0.14 0.23 0.20 0.31 AGMA (Ours) 0.25 0.36 0.10 0.15 0.25 0.44 0.17 0.29 0.14 0.23 0.18 0.29
> Figure 3. Sensitivity to batch size and Top-K. (a) Performance degrades with larger batches, demonstrating that finer batch subdivision better captures localized priors and prevents over-smoothing of the global distribution. (b) Minimal sensitivity to Kvalidates that our model avoids overfitting p(Y|z, X )to compensate for collapsed p(z|X)—the learned prior remains informative across sampling budgets, confirming effective prior learning without mode collapse. Results on ETH dataset.

capability, confirming our theoretical analysis that once 

q̸ = p, the generator cannot compensate (Section 3.2). LB

plays a crucial refinement role: its removal increases ADE by 10% and FDE by 9%, demonstrating that while clustering alone captures trajectory diversity, it lacks accuracy-driven feedback and produces noisy mode estimates. Finally, dis-tillation loss prevents mode collapse: without Ldistill , the global prior fails to effectively aggregate batch-level knowl-edge, resulting in redundant or collapsed patterns (ADE +10%, FDE +6.5%). We further investigate the impact of batch size and top-K sampling on AGMA’s performance (Figure 3(a)). As batch size increases from 1 to 32, both ADE and FDE increase con-sistently. This validates our prior batch subdivision enables more accurate prior estimation qB , preventing the global prior q from over-averaging diverse trajectory patterns into a single smoothed distribution. Larger batches, while com-putationally efficient, fail to capture more complete prior. The top-K ablation reveals an expected trade-off inherent to our design philosophy. As shown in Figure 3(b), prediction error decreases consistently as K increases from 1 to 20. This behavior directly stems from our modeling priorities: AGMA is designed to investigate the relative importance of learning a high-quality prior p(z|X) versus learning a high-capacity sampler p(Y |z, X ).Importantly, however, the steady improvement with K also validates that AGMA avoids a critical failure mode: mode collapse where the decoder p(Y |z, X ) bypasses the prior and directly maps context X to outputs, as discussed in Sec-tion 3.2. If such collapse occurred, the prior would become uninformative and performance would plateau regardless of K. The observed consistent gains demonstrate that the learned prior p(z|X) genuinely encodes diverse, meaningful patterns, confirming the effectiveness of our prior learning framework. 

# 6. Conclusion 

In this work, we present a study on the role of priors in multi-modal human trajectory forecasting. We identify prior qual-ity as the critical factor for multimodal trajectory forecast-ing, proving that both prediction accuracy and distribution fidelity are lower-bounded by prior mismatch ϵprior (X). To validate this finding, we propose AGMA, which explicitly optimizes priors through batch prior extraction and global prior distillation. Experiments on ETH-UCY, SDD, and JRDB demonstrate that AGMA achieves state-of-the-art per-formance, with 5.26% improvement in mADE 20 and 9.38% in mFDE 20 on ETH-UCY, confirming that prior optimiza-tion is an effective way for advancing multimodal trajectory prediction. 8Submission and Formatting Instructions for ICML 2026 

# Impact Statement 

This work advances trajectory prediction for autonomous systems through principled prior modeling. We discuss key societal considerations below. 

Positive Impact. Our theoretical framework demonstrates that prior quality fundamentally limits prediction perfor-mance, identifying prior optimization as the critical factor for improving multimodal trajectory prediction. 

Key Limitations and Risks. AGMA represents an initial attempt to explicitly optimize prior quality in trajectory prediction. While our method achieves strong performance on standard benchmarks, it has inherent limitations that practitioners must consider. AGMA’s design focuses on optimizing prior quality from observed trajectories. However, it has limited capacity for modeling complex agent-agent interactions, particularly in scenarios where observed trajectories alone provide insuffi-cient information to disambiguate future interaction details. When the observation history X does not contain discrim-inative cues for highly divergent interaction patterns (e.g., yielding vs. crossing in ambiguous right-of-way situations), AGMA’s context-conditioned prior q(z|X) may fail to cap-ture the true multimodal distribution. In such interaction-heavy scenarios performance may degrade significantly. 

# References 

Bae, I., Park, J.-H., and Jeon, H.-G. Learning pedestrian group representations for multi-modal trajectory predic-tion. In Avidan, S., Brostow, G., Ciss ´e, M., Farinella, G. M., and Hassner, T. (eds.), ECCV , pp. 270–289, Cham, 2022. Springer Nature Switzerland. Bae, I., Oh, J., and Jeon, H.-G. Eigentrajectory: Low-rank descriptors for multi-modal trajectory forecasting. In 

Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 10017–10029, October 2023. Bae, I., Park, Y.-J., and Jeon, H.-G. Singulartrajectory: Uni-versal trajectory predictor using diffusion model. In Pro-ceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR) , pp. 17890–17901, June 2024. Bhattacharyya, A., Hanselmann, M., Fritz, M., Schiele, B., and Straehle, C.-N. Conditional flow variational autoen-coders for structured sequence prediction. In NeurIPS ,2019. Chai, Y., Sapp, B., Bansal, M., and Anguelov, D. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), Proceedings of the Conference on Robot Learning , volume 100 of Proceedings of Machine Learning Research , pp. 86–99, 30 Oct–01 Nov 2020. Chen, J., Cao, J., Lin, D., Kitani, K., and Pang, J. Mgf: Mixed gaussian flow for diverse trajectory prediction. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C. (eds.), NeurIPS ,volume 37, pp. 57539–57563. Curran Associates, Inc., 2024. doi: 10.52202/079017-1834. Chen, K., Zhao, X., Huang, Y., Fang, G., Song, X., Wang, R., and Wang, Z. Socialmoif: Multi-order intention fusion for pedestrian trajectory prediction. In CVPR , pp. 22465– 22475, June 2025. Dai, B. et al. Diagnosing and preventing posterior collapse of vaes. In ICLR , 2020. Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Prad-han, S., Chai, Y., Sapp, B., Qi, C. R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan, V., Mc-Cauley, A., Shlens, J., and Anguelov, D. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In ICCV , pp. 9710– 9719, October 2021. Fang, Z., Hsu, D., and Lee, G. H. Neuralized markov ran-dom field for interaction-aware stochastic human trajec-tory prediction. In The Thirteenth International Confer-ence on Learning Representations , 2025. URL https: //openreview.net/forum?id=r3cEOVj7Ze .Floreano, D. and Wood, R. J. Science, technology and the future of small autonomous drones. nature , 521(7553): 460–466, 2015. Fu, Y., Yan, Q., Wang, L., Li, K., and Liao, R. Moflow: One-step flow matching for human trajectory forecast-ing via implicit maximum likelihood estimation based distillation. In CVPR , pp. 17282–17293, June 2025. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., and Alahi, A. Social gan: Socially acceptable trajectories with gen-erative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition ,pp. 2255–2264, 2018. HU, B. and Cham, T.-J. TSC-net: Prediction of pedes-trian trajectories by trajectory-scene-cell classification. In ICLR , 2025. URL https://openreview.net/ forum?id=Xmh5gdMfRJ .Jang, E., Gu, S., and Poole, B. Categorical reparameteriza-tion with gumbel-softmax. In International Conference on Learning Representations , 2017. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013. 9Submission and Formatting Instructions for ICML 2026 

Lee, N., Choi, W., Vernaza, P., Choy, C. B., Torr, P. H., and Chandraker, M. Desire: Distant future prediction in dynamic scenes with interacting agents. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 336–345, 2017. Lerner, A., Chrysanthou, Y., and Lischinski, D. Crowds by example. In Computer graphics forum , volume 26, pp. 655–664. Wiley Online Library, 2007. Liang, J., Jiang, L., Murphy, K., Yu, T., and Hauptmann, A. The garden of forking paths: Towards multi-future trajectory prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,pp. 10508–10518, 2020. Madjid, N. A., Ahmad, A., Mebrahtu, M., Babaa, Y., Nasser, A., Malik, S., Hassan, B., Werghi, N., Dias, J., and Khonji, M. Trajectory prediction for autonomous driv-ing: Progress, limitations, and future directions. arXiv preprint arXiv:2503.03262 , 2025. Maeda, T. and Ukita, N. Fast inference and update of prob-abilistic density estimation on trajectory prediction. In 

Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 9795–9805, October 2023. Mangalam, K., Girase, H., Agarwal, S., Lee, K.-H., Adeli, E., Malik, J., and Gaidon, A. It is not the journey but the destination: Endpoint conditioned trajectory prediction. In European conference on computer vision , pp. 759–776. Springer, 2020a. Mangalam, K., Girase, H., Agarwal, S., Lee, K.-H., Adeli, E., Malik, J., and Gaidon, A. It is not the journey but the destination: Endpoint conditioned trajectory prediction. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J.-M. (eds.), ECCV , pp. 759–776, Cham, 2020b. Springer International Publishing. Mao, W., Xu, C., Zhu, Q., Chen, S., and Wang, Y. Leapfrog diffusion model for stochastic trajectory prediction. In 

CVPR , pp. 5517–5526, June 2023a. Mao, W., Xu, C., Zhu, Q., Chen, S., and Wang, Y. Leapfrog diffusion model for stochastic trajectory prediction. In 

Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 5517–5526, 2023b. Mart ´ın-Mart ´ın, R., Patel, M., Rezatofighi, H., Shenoi, A., Gwak, J., Frankel, E., Sadeghian, A., and Savarese, S. Jrdb: A dataset and benchmark of egocentric robot vi-sual perception of humans in built environments. IEEE Transactions on Pattern Analysis and Machine Intelli-gence , 45(6):6748–6765, 2023. doi: 10.1109/TPAMI. 2021.3070543. Nicoli, K. A., Anders, C. J., Hartung, T., Jansen, K., Kessel, P., and Nakajima, S. Detecting and mitigating mode-collapse for flow-based sampling of lattice field theories. 

Physical Review D , 108(11):114501, 2023. Panaretos, V. M. and Zemel, Y. Statistical aspects of wasser-stein distances. Annual review of statistics and its appli-cation , 6(1):405–431, 2019. Pellegrini, S., Ess, A., Schindler, K., and van Gool, L. You’ll never walk alone: Modeling social behavior for multi-target tracking. In 2009 IEEE 12th International Con-ference on Computer Vision , pp. 261–268, 2009. doi: 10.1109/ICCV.2009.5459260. Phan-Minh, T., Grigore, E. C., Boulton, F. A., Beijbom, O., and Wolff, E. M. Covernet: Multimodal behavior prediction using trajectory sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020. Robicquet, A., Sadeghian, A., Alahi, A., and Savarese, S. Learning social etiquette: Human trajectory understand-ing in crowded scenes. In European conference on com-puter vision , pp. 549–565. Springer, 2016. Rudenko, A., Palmieri, L., Herman, M., Kitani, K. M., Gavrila, D. M., and Arras, K. O. Human motion tra-jectory prediction: a survey. The International Jour-nal of Robotics Research , 39(8):895–935, 2020. doi: 10.1177/0278364920917446. Sadat, A., Casas, S., Ren, M., Wu, X., Dhawan, P., and Urtasun, R. Perceive, predict, and plan: Safe motion planning through interpretable semantic representations. In ECCV , pp. 414–430, 2020. Salzmann, T., Ivanovic, B., Chakravarty, P., and Pavone, M. Trajectron++: Dynamically-feasible trajectory forecast-ing with heterogeneous data. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J.-M. (eds.), ECCV , pp. 683–700, Cham, 2020. Springer International Publishing. Shi, L., Wang, L., Zhou, S., and Hua, G. Trajectory unified transformer for pedestrian trajectory prediction. In Pro-ceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 9675–9684, October 2023. Shi, S., Jiang, L., Dai, D., and Schiele, B. Mtr++: Multi-agent motion prediction with symmetric scene modeling and guided intention querying. IEEE Transactions on Pattern Analysis and Machine Intelligence , 46(5):3955– 3971, 2024. Sohn, K., Lee, H., and Yan, X. Learning structured output representation using deep conditional generative models. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information 

10 Submission and Formatting Instructions for ICML 2026 

Processing Systems , volume 28. Curran Associates, Inc., 2015. Sun, J., Li, Y., Fang, H.-S., and Lu, C. Three steps to multimodal trajectory prediction: Modality clustering, classification and synthesis. In ICCV , pp. 13250–13259, October 2021. Yan, C., Wang, S., Yang, J., Xu, T., and Huang, J. Re-balancing variational autoencoder loss for molecule se-quence generation. In Proceedings of the 11th ACM in-ternational conference on bioinformatics, computational biology and health informatics , pp. 1–7, 2020. 11 Submission and Formatting Instructions for ICML 2026 

# A. Theoretical Proofs 

In this section, we provide rigorous proofs for the theoretical framework presented in Section 3. We treat future trajectories 

Yj as continuous random variables in RTpred ×d.

A.1. Proof of Theorem 3.1 Theorem 3.1. Define the distribution matching loss as the KL divergence Ldist (X) = KL (p(Yj |X)∥q(Yj |X)) . The loss is lower-bounded by the gap between prior mismatch ϵprior and sampler error ϵsample :

Ldist (X) ≥ 12 (ϵprior (X) − ϵsample (X)) 2 . (37) 

Proof. By Pinsker’s Inequality , the KL divergence between two distributions p and q is bounded by their Total Variation (TV) distance, or equivalently, their L1 norm: 

Ldist (X) ≥ 2∥p(Yj |X) − q(Yj |X)∥2 

> TV

= 12 ∥p(Yj |X) − q(Yj |X)∥21. (38) To decompose the error, we introduce an auxiliary distribution ˜q(Yj |X) = R p(z|X)q(Yj |X, z ) dz , which represents the output of the current sampler under a perfect prior. The L1 term can be expanded as: 

∆ = ∥p(Yj |X) − q(Yj |X)∥1

= ∥(p(Yj |X) − ˜q(Yj |X)) + (˜ q(Yj |X) − q(Yj |X)) ∥1. (39) Applying the Reverse Triangle Inequality ( ∥A + B∥1 ≥ |∥ A∥1 − ∥ B∥1|), we have: 

∆ ≥ |∥ ˜q(Yj |X) − q(Yj |X)∥1 − ∥ p(Yj |X) − ˜q(Yj |X)∥1| . (40) By definition from the main text: 

ϵprior (X) = ∥˜q − q∥1 =

Z

(p(z|X) − q(z|X)) q(Yj |X, z ) dz 

> 1

, (41) 

ϵsample (X) = ∥p − ˜q∥1 =

Z

p(z|X)( p(Yj |X, z ) − q(Yj |X, z )) dz 

> 1

. (42) Substituting these into Eq. (38) yields: 

Ldist (X) ≥ 12 (|ϵprior (X) − ϵsample (X)|)2 = 12 (ϵprior (X) − ϵsample (X)) 2 . (43) 

A.2. Proof of Proposition 3.2 

Proof. For trajectory Yj of total dimension D = Tpred × d, the reconstruction error of any sampler is bounded by the prior. Specifically, for any latent variable z, the Mean Squared Error (MSE) satisfies: MSE (Yj , ˆYj ) ≥ D

2πe exp 

 2

D H(Yj |X, z )



, (44) where H(·) denotes the differential entropy. Using the identity H(Yj |X, z ) = H(Yj |X) − I(Yj ; z|X), we obtain: MSE (Yj , ˆYj ) ≥ D

2πe exp 

 2

D [H(Yj |X) − I(Yj ; z|X)] 



. (45) 12 Submission and Formatting Instructions for ICML 2026 

Since the L1 sampler error ϵsample is lower-bounded by the square root of the MSE (up to a scaling factor κ depending on the distribution shape), we define the bound function C(·) as: 

C(I) = κ ·

r D

2πe exp 

 1

D [H(Yj |X) − I]



. (46) As I(Yj ; z|X) decreases (i.e., the prior captures less information), ϵsample is forced to increase exponentially, establishing the sampler’s performance ceiling. 

A.3. Proof of Corollary 3.3 

Proof. We consider the case where ϵprior > ϵ sample , a standard condition in multimodal forecasting where mode loss dominates fitting noise. In this regime, the term inside the square of Theorem 3.1 is positive. To achieve a target matching accuracy Ldist (X) < δ , we must satisfy: 

12 (ϵprior (X) − ϵsample (X)) 2 < δ ϵprior (X) − ϵsample (X) < √2δ. (47) Rearranging for ϵprior and substituting the absolute lower bound for the sampler error ϵmin sample from Proposition 3.2: 

ϵprior (X) < √2δ + ϵmin sample (X). (48) This indicates that even with an optimal sampler ( ϵsample → ϵmin sample ), the distribution error δ cannot be reduced unless the prior mismatch ϵprior is strictly controlled, proving that high-quality priors are a necessary condition for accurate multimodal forecasting. 

# B. Application in Batch Clustering 

When building the binary adjacency matrix Aij in Section 4.2, we rely on hard thresholding conditions: 

Aij = I ˜Sij > θ sim 

 ∧ I ˜Rij < θ rep 



The indicator function I[·] is non-differentiable. To enable end-to-end training, we approximate these hard choices using sigmoid functions with temperature parameters: 

ˆAij = sigmoid ( ˜Sij − θsim )

τsim 

!

· sigmoid (θrep − ˜Rij )

τrep 

!

where τsim and τrep are temperature parameters. During training, we use τsim = τrep = 0 .1 to approximate hard thresholding while maintaining differentiability. This allows backpropagation through the graph construction, enabling joint learning of feature projections and thresholds. 

# C. Application in Global Prior Sampling 

In Section 4.3, agent (b, j ) obtains attention weights ab,j = {αb,j,g }Kg 

> g=1

over global GMM components. To sample from this discrete distribution while maintaining differentiability, we apply Gumbel-Softmax: 

˜z(n) 

> b,j,g

= exp((log αb,j,g + G(n) 

> g

)/τ )

PKg 

> j′=1

exp((log αb,j,j ′ + G(n) 

> j′

)/τ )

where G(n) 

> g

∼ Gumbel (0 , 1) and τ is the temperature parameter. We set τ = 1 .0 to maintain multimodality during sampling. This higher temperature prevents mode collapse by allowing the sampled vector ˜z(n) 

> b,j

to be ”soft” (non-one-hot), reflecting the underlying distribution ab,j and encouraging exploration of multiple plausible modes simultaneously. 13 Submission and Formatting Instructions for ICML 2026 

# D. Implementation Details 

We implement AGMA using PyTorch. In the following, we detail the network architecture, hyperparameters, and specific algorithmic settings derived from our official implementation. 

D.1. Network Architecture 

The model processes the observed trajectories of length Tobs = 8 and predicts future trajectories of length Tpred = 12 , with a hidden core dimension d = 32 . Both history and future encoders utilize a spatio-temporal encoder with self-attention, where spatial coordinates are first embedded via MLPs before being processed by the encoder. Similarity and repulsion heads ( Φsim , Φrep ) are implemented as 2-layer MLPs with sigmoid activation to output probabilities in [0 , 1] . To query the global prior, we employ an attention mechanism where the query is the past feature and the key is a concatenation of global centroids, covariances, and weights. We utilize Entmax1.5 ( ?)—a sparse alternative to softmax—to compute attention scores, encouraging the model to attend to a sparse set of relevant global modes. The sampled latent codes are refined using multi-head attention with 4 heads and an embedding dimension of 20 × d, allowing interaction modeling between the 

N = 20 samples. 

D.2. Hyperparameters and Settings Global and Batch Priors. We set the number of global Gaussian components to Kg = 100 , with centroids and covariances initialized from a normal distribution. The similarity threshold θsim and the repulsion threshold θrep are initialized as learnable parameters at 0.7 and 0.3, respectively. The temperature parameter τ used in the Gumbel-Softmax and Straight-Through Estimator is set to 0.1, which encourages sharper decision boundaries for clustering and mode selection. 

Optimal Transport (Sinkhorn). For the global-to-batch distillation stage, we solve the optimal transport problem using the Sinkhorn-Knopp algorithm with 20 iterations to approximate the optimal assignment matrix P. The cost matrix between the global component g and the batch component b is defined as the 2-Wasserstein distance approximation: 

Cgb = ∥μg − μb∥2 + ∥σg − σb∥2 

> F

, where the second term represents the trace of squared differences of standard deviations. 

Training Configuration. We sample N = 20 trajectories for both training and evaluation. The model is trained using the AdamW optimizer, with the total loss comprising student predictions (from the global prior), teacher predictions (from the batch prior), and the distillation loss. 14
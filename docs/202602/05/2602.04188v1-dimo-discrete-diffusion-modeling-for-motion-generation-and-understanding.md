---
title: "DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding"
title_zh: DiMo：用于动作生成与理解的离散扩散建模
authors: "Ning Zhang, Zhengyu Li, Kwong Weng Loh, Mingxi Xu, Qi Wang, Zhengyu Wen, Xiaoyu He, Wei Zhao, Kehong Gong, Mingyuan Zhang"
date: 2026-02-04
pdf: "https://arxiv.org/pdf/2602.04188v1"
tags: ["keyword:MDM", "query:课题"]
score: 9.0
evidence: 用于动作生成的离散扩散模型
tldr: DiMo是一个基于离散扩散风格的统一框架，旨在解决人体动作生成与理解的双向任务。不同于传统的自回归模型，它通过迭代掩码标记细化实现了文本到动作、动作到文本及动作到动作的统一建模。通过引入残差向量量化（RVQ）提高动作保真度，并利用群体相对策略优化（GRPO）增强对齐性，DiMo在保持高质量生成的同时，展现了出色的双向理解能力和推理灵活性。
motivation: 现有的掩码建模方法多局限于文本到动作生成，缺乏一个能同时处理双向理解与生成的统一高效框架。
method: 提出一种离散扩散框架，利用迭代掩码标记细化技术，并结合RVQ和GRPO算法来提升动作保真度与文本对齐性。
result: 在HumanML3D和KIT-ML数据集上，DiMo在动作质量和双向理解任务上均表现出极具竞争力的性能，并支持多种下游任务。
conclusion: DiMo成功统一了动作生成与理解任务，通过灵活的推理机制实现了生成质量与延迟的平衡，为人体动作建模提供了新思路。
---

## 摘要
先前的掩码建模动作生成方法主要研究文本到动作（text-to-motion）的生成。我们提出了 DiMo，这是一个离散扩散风格的框架，它将掩码建模扩展到了双向文本-动作理解与生成。与将动作标记化并按顺序解码的 GPT 式自回归方法不同，DiMo 执行迭代掩码标记细化，在单一模型中统一了文本到动作（T2M）、动作到文本（M2T）以及无文本动作到动作（M2M）任务。这种解码范式通过细化步骤的数量，在推理时自然地实现了质量与延迟之间的权衡。我们通过残差向量量化（RVQ）进一步提高了动作标记的保真度，并利用组相对策略优化（GRPO）增强了对齐性和可控性。在 HumanML3D 和 KIT-ML 上的实验表明，在统一框架下，该方法具有出色的动作质量和极具竞争力的双向理解能力。此外，我们还展示了该模型在无需更改架构的情况下，在无文本动作补全、文本引导动作预测和动作描述修正方面的能力。更多定性结果可在我们的项目页面查看：https://animotionlab.github.io/DiMo/。

## Abstract
Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.

---

## 论文详细总结（自动生成）

### DiMo: 用于动作生成与理解的离散扩散建模论文总结

#### 1. 核心问题与整体含义（研究动机和背景）
人体动作建模领域长期存在“生成”与“理解”任务割裂的问题。传统的自回归（AR）模型（如 GPT 风格）虽然在统一接口上有所进展，但面临三大挑战：
*   **推理延迟：** 逐标记（Token-by-token）的解码方式导致延迟随序列长度线性增长。
*   **缺乏全局修正：** 单向解码限制了模型对早期错误的修正能力，不利于动作编辑。
*   **任务统一困难：** 整合文本引导生成（T2M）、动作描述（M2T）及无文本动作补全（M2M）通常需要复杂的架构设计。
**DiMo** 提出了一种基于**离散扩散（Discrete Diffusion）**的统一框架，通过迭代掩码细化（Iterative Masked Refinement）实现双向建模，旨在平衡生成质量、推理效率与多任务通用性。

#### 2. 方法论：核心思想与关键技术
DiMo 的核心是将动作和文本均视为离散标记序列，并采用类似扩散模型的“加噪-去噪”逻辑进行处理：
*   **统一建模范式：** 采用 BERT 风格的双向掩码语言模型作为骨干。在训练时，随机掩码动作或文本标记，模型学习根据上下文恢复被掩码的部分。
    *   **T2M：** 给定文本，恢复掩码的动作标记。
    *   **M2T：** 给定动作，恢复掩码的文本标记。
    *   **M2M：** 仅针对动作标记进行自监督恢复，增强动作预测和补全能力。
*   **残差向量量化（RVQ）：** 使用多层 RVQ 将连续动作离散化。相比单层 VQ，RVQ 能以更低的比特率捕捉高频动作细节，显著降低量化误差。
*   **置信度引导的渐进式推理：** 推理时从全掩码序列开始，每一步预测所有位置的概率，仅保留置信度最高的标记，并在后续步骤中不断细化。这允许通过调整迭代步数 $K$ 来权衡质量与速度。
*   **GRPO 强化学习微调：** 引入群体相对策略优化（Group Relative Policy Optimization），利用预训练的 M2T 分支作为奖励函数来优化 T2M 的语义对齐，无需额外的判别器网络。

#### 3. 实验设计
*   **数据集：** 主要在 **HumanML3D** 和 **KIT-ML** 两个基准数据集上进行评估，并在 **Motion-X** 上进行了泛化性验证。
*   **Benchmark 与对比方法：**
    *   **T2M 任务：** 对比了 MDM, MotionDiffuse, MLD, MoMask, T2M-GPT, ReMoDiffuse 等。
    *   **统一模型对比：** 对比了 MotionGPT (v1, v2, v3), MoTe, MG-MotionLLM 等。
*   **评估指标：** 动作质量（FID）、语义对齐（R-Precision, MM Dist）、多样性（Div）以及文本生成质量（BLEU, ROUGE, CIDEr, BERTScore）。

#### 4. 资源与算力
*   **硬件：** 使用了 **16 台 32GB 显存的 Ascend 910 NPU**（华为昇腾处理器）。
*   **训练配置：** 训练 50 个 epoch，mini-batch size 为 32。
*   **优化器：** AdamW，学习率 $5 \times 10^{-5}$，包含 5k 步的线性预热。

#### 5. 实验数量与充分性
论文进行了极其详尽的实验验证：
*   **主实验：** 在两个主流数据集上验证了 T2M 和 M2T 的双向性能。
*   **消融实验：** 涵盖了骨干网络规模（ALBERT 到 BERT-Large）、RVQ 层数（1-8层）、掩码调度策略（线性 vs 预弦）、任务比例分配、CFG 尺度、以及 [PAD] 标记的惩罚因子。
*   **人类偏好研究：** 针对 R-Precision 指标可能存在的偏差，进行了 1000+ 组的人类主观评价对比。
*   **应用展示：** 验证了动作插值、动作预测、动作描述修正等扩展任务。
**评价：** 实验设计非常客观且充分，特别是对 R-Precision 指标局限性的讨论，体现了严谨的学术态度。

#### 6. 主要结论与发现
*   **质量-延迟权衡：** DiMo 仅需 5 步迭代即可在 FID 指标上超越许多 AR 模型，而增加步数（如 20-30 步）可进一步提升细节。
*   **动作保真度：** 在 HumanML3D 上，DiMo 取得了极低的 FID（0.047），证明其生成的动作在分布上最接近真实数据。
*   **强化学习有效性：** GRPO 微调显著提升了文本与动作的语义一致性。
*   **统一性优势：** 单一模型在不改变架构的情况下，能自然胜任动作补全和描述修正等多种下游任务。

#### 7. 优点（亮点）
*   **高效并行：** 摆脱了自回归的串行限制，支持并行解码，推理效率高。
*   **自修正能力：** 迭代细化机制允许模型在后续步骤中修正早期的生成错误。
*   **高保真标记化：** RVQ 的引入解决了离散动作建模中细节丢失的痛点。
*   **灵活的推理：** 用户可以根据应用场景自由选择“快而精简”或“慢而精致”的生成模式。

#### 8. 不足与局限
*   **评估指标偏差：** 论文指出 R-Precision 极度依赖于预训练评估器的偏好，导致该指标与人类主观感受有时不一致（例如真实动作的 R-Precision 反而低于某些模型）。
*   **骨干网络限制：** 目前主要基于 BERT 家族，虽然适合双向建模，但在处理超大规模语料和复杂逻辑推理方面可能弱于超大规模的自回归 LLM。
*   **应用范围：** 目前主要集中在人体骨架动作，尚未扩展到更复杂的网格（Mesh）变形或多智能体交互场景。

（完）

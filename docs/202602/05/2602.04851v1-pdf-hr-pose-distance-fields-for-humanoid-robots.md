---
title: "PDF-HR: Pose Distance Fields for Humanoid Robots"
title_zh: PDF-HR：类人机器人的姿态距离场
authors: "Yi Gu, Yukang Gao, Yangchen Zhou, Xingyu Chen, Yixiao Feng, Mingle Zhao, Yunyang Mo, Zhaorui Wang, Lixin Xu, Renjing Xu"
date: 2026-02-04
pdf: "https://arxiv.org/pdf/2602.04851v1"
tags: ["query:课题"]
score: 6.0
evidence: 人形机器人运动先验与重定向姿态
tldr: 针对人形机器人高质量运动数据稀缺导致姿态先验研究受限的问题，本文提出了PDF-HR。这是一种轻量级的姿态距离场先验，将机器人姿态分布表示为连续且可微的流形。通过预测任意姿态到大规模重定向姿态库的距离，PDF-HR提供了一种平滑的姿态合理性度量，可作为奖励项、正则化项或评分器集成到优化和控制流程中。实验证明，该方法在运动跟踪、模仿和重定向等任务中显著提升了基准性能。
motivation: 人形机器人领域由于高质量运动数据稀缺，限制了类似于人体运动恢复领域中姿态先验的应用。
method: 提出PDF-HR，通过连续可微的距离场表示机器人姿态分布，量化任意姿态与真实运动数据流形的接近程度。
result: 在运动跟踪、风格化模仿和通用运动重定向等多种任务中，PDF-HR作为即插即用组件显著增强了现有强基准模型的表现。
conclusion: PDF-HR为人形机器人提供了一个高效、通用的姿态合理性评分器，有效提升了机器人运动的自然度和控制精度。
---

## 摘要
姿态和运动先验在类人机器人领域起着至关重要的作用。尽管此类先验已在人体运动恢复（HMR）领域通过一系列模型得到了广泛研究，但由于高质量类人机器人运动数据的稀缺，它们在类人机器人中的应用仍然有限。在这项工作中，我们引入了类人机器人姿态距离场（PDF-HR），这是一种轻量级先验，将机器人姿态分布表示为一个连续且可微的流形。给定任意姿态，PDF-HR 预测其到大规模重定向机器人姿态语料库的距离，从而产生一种平滑的姿态合理性度量，非常适合优化和控制。PDF-HR 可以作为奖励塑造项、正则化项或独立的合理性评分器集成到各种流程中。我们在各种类人机器人任务上评估了 PDF-HR，包括单轨迹运动跟踪、通用运动跟踪、基于风格的运动模仿和通用运动重定向。实验表明，这种即插即用的先验能够持续且显著地增强强基准模型。代码和模型将会开源。

## Abstract
Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.
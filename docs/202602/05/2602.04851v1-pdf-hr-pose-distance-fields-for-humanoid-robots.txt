Title: PDF-HR: Pose Distance Fields for Humanoid Robots

URL Source: https://arxiv.org/pdf/2602.04851v1

Published Time: Thu, 05 Feb 2026 02:21:10 GMT

Number of Pages: 16

Markdown Content:
# PDF-HR: Pose Distance Fields for Humanoid Robots 

Yi Gu â€ 1 Yukang Gao â€ 1 Yangchen Zhou â€ 1 Xingyu Chen 1 Yixiao Feng 1 Mingle Zhao 2

Yunyang Mo 1 Zhaorui Wang 1 Lixin Xu 1 Renjing Xu 1

> 1

The Hong Kong University of Science and Technology (Guangzhou) 2University of Macau â€ Equal Contributions PDF-HR project page 

Abstract â€”Pose and motion priors play a crucial role in hu-manoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of mod-els, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released. 

I. I NTRODUCTION 

Humanoid robots are appealing because they share human-like kinematics and can operate in the same spaces and with the same tools designed for humans [9, 16, 22, 44]. However, generating reliable whole-body motion remains challenging. Feasible behavior must simultaneously respect joint limits, self-collision constraints, contact feasibility, and balance [32, 51], while staying smooth and consistent with the task over time. In practice, even small errors in perception, modeling, or retargeting can push a solution toward physically awkward or unstable configurations [45, 3]. A natural way to improve robustness is to incorporate pose or motion priors that encode which configurations are plausible for a given robot, and to use these priors to regularize estimation and control. In the human motion recovery (HMR) literature, pose and motion priors [6, 19, 33, 26, 17] have been studied extensively, supported by abundant, high-quality datasets [28] and powerful generative models. These priors help resolve ambiguity from partial observations and improve robustness to noise and occlusion. In contrast, comparatively fewer works explore similarly general and reusable priors for humanoid robots. This gap is not merely historical. Collecting large-scale, high-quality humanoid robot motion data is expensive, and directly transferring priors from human bodies to robot morphologies is nontrivial due to differences in joint limits, kinematic structure, actuation capabilities, and contact dynamics. As a result, many robotics pipelines still rely on task-specific constraints, hand-tuned regularizers, or priors that are tightly coupled to a particular controller or dataset, limiting generalization across tasks [35, 36, 27]. This paper focuses on pose priors and proposes a simple but effective alternative. Instead of modeling the full gener-ative distribution of humanoid motion, we learn a continu-ous distance-to-data function in pose space. Concretely, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior represented by an MLP that maps an arbitrary robot pose to a scalar distance to the nearest pose in a large corpus of retargeted robot poses. To obtain a well-behaved distance field, we carefully design the training distribution to cover the pose space while emphasizing high-quality near-manifold samples, and we use cross-validation to select reliable positive samples. The resulting function is continuous and provides meaningful gradients both near and far from the data manifold. Intuitively, PDF-HR acts like a â€œpose plausibility scorerâ€: poses near the dataset manifold receive low distance, while out-of-distribution or physically awkward configurations re-ceive higher distance. This distance-field formulation offers several practical advantages. First, it supports arbitrary query poses and provides a smooth signal even when the pose is far from the data manifold, which is valuable in early-stage optimization and when tracking difficult motions. Second, it is modular and reusable: PDF-HR can serve as an independent pose-quality objective without requiring additional data collec-tion, and it can be applied across tasks and policies without retraining [29]. Third, the model is compact and efficient, making it easy to integrate into existing systems. We demonstrate the breadth of this approach across rep-resentative humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and motion retargeting. For tracking, we incorporate PDF-HR as a general-purpose reward shaping term that encourages exploration and optimization within the near manifold region. Empirically, the pose reward accelerates convergence for RL tracking baselines and improves robustness on challenging motions. For retargeting, PDF-HR acts as a principled reg-ularizer that improves motion quality. Across settings, PDF-HR consistently strengthens strong baselines while remaining lightweight and easy to deploy. 

> arXiv:2602.04851v1 [cs.RO] 4 Feb 2026

Our main contributions can be summarized as follows:  

> â€¢

PDF-HR: a continuous and differentiable pose distance field for humanoid robots learned from a large corpus of retargeted robot poses.  

> â€¢

Plug-and-play integration: simple mechanisms to incorpo-rate PDF-HR as a prior across diverse humanoid applica-tions, including motion tracking and motion retargeting.  

> â€¢

Empirical validation: experiments on multiple humanoid tasks demonstrating consistent improvements in perfor-mance and robustness over strong baselines. II. R ELATED WORK 

Human Motion Priors. Data-driven human pose and motion priors are fundamental for human motion recovery [6, 25, 33, 39]. Early works mainly focused on learning explicit constraints, such as joint limits, to avoid implausible poses [1]. Subsequent approaches leveraged generative models, including Gaussian Mixture Models (GMMs) [6], Variational Autoen-coders (VAEs) [11, 33], Generative Adversarial Networks (GANs) [12, 19] and diffusion models [25, 26, 50, 40] to cap-ture the statistical distribution of natural human articulation. More recently, implicit representations like Pose-NDF [41] has been proposed to model the manifold of valid poses as the zero-level set of a learned neural distance field. However, Pose-NDF relies on Euclidean gradient descent followed by re-projection onto SO(3) during optimization, which can limit convergence speed. In contrast, NRDF [17] enforces strict geometric consistency by leveraging Riemannian gradient de-scent, ensuring that optimization iterates always remain on the manifold. Building upon this geometric metric, NRMF [47] extends the representation to the temporal domain, modeling distance fields over pose, velocity, and acceleration to ensure higher-order motion continuity and dynamic feasibility. In this work, we extend this paradigm of implicit manifold representation from humans to humanoids. Specifically, we learn PDF-HR to capture the valid configuration space of the robot. We demonstrate that this learned prior can significantly enhance performance in various downstream tasks. 

Physics-based Motion Imitation. Data-driven imitation learn-ing has emerged as a cornerstone for acquiring humanoid skills [10, 46]. DeepMimic [35] pioneered this direction by tracking reference motions to produce physically feasible behaviors. However, such tracking-based methods typically require the controller to rigidly mimic targets frame-by-frame, limiting the flexibility to adapt to new tasks [24]. Furthermore, they often necessitate labor-intensive reward tuning, a chal-lenge partially addressed by introducing Adversarial Differen-tial Discriminator (ADD) [52]. To overcome the constraints of rigid tracking and paired data alignment, distribution-matching approaches such as Generative Adversarial Imitation Learning (GAIL) [18] and Adversarial Motion Priors (AMP) [36] have been widely adopted. By learning flexible motion priors from datasets, these methods provide a task-agnostic measure of naturalness, allowing policies to produce life-like behaviors across diverse tasks. This paradigm was further expanded by ASE [37] to amplify motion diversity and repurpose skills hierarchically. The closest method to ours is SMP [29], which leverages pre-trained motion diffusion models with score distillation sampling (SDS) [38] to extract reusable, task-agnostic motion priors. Our work follows a similar modular 

and reusable philosophy, but tackles the problem from a different perspective: pose rather than motion. We also show that our prior improves style-conditioned tracking, making it theoretically complementary to SMP. 

Motion Retargeting. Retargeting motions from human demonstrations to humanoid robots introduces unique chal-lenges, particularly the strict requirements for physical plau-sibility and consistent interactions with the environment. Ge-ometric approaches [14, 15, 27, 3] typically minimize key-point errors via trajectory optimization or inverse kinematics. However, when physical constraints are weak or absent, they can violate kinematic and contact limits, leading to artifacts such as foot skating and interpenetration. To improve re-targeting fidelity, subsequent works [2, 21] introduced soft interaction penalties, and OmniRetarget [45] further unifies these objectives by minimizing the interaction mesh [30] Laplacian deformation energy under explicit hard constraints, yielding more robust physical consistency. Complementary to these optimization-based formulations, SPIDER [31] lever-ages large-scale physics-based sampling in a simulator with curriculum-style virtual contact guidance to resolve contact ambiguity and recover feasible interactions. To the best of our knowledge, PDF-HR is the first learned prior that can be introduced as a regularizer into the optimization loop of existing pipelines, encouraging solutions to remain within the valid configuration space of the humanoid and thereby improving both motion naturalness and physical feasibility. III. B ACKGROUND : R IEMANNIAN GEOMETRY 

We first establish the preliminaries needed to formalize our pose distance fields. Following recent works [4, 5, 8, 17, 47], we consider a smooth m-dimensional Riemannian manifold 

M, smoothly embedded in a higher-dimensional ambient space X (typically Rn) via an embedding Î¹ : M ,â†’ X .The manifold M is endowed with a Riemannian metric 

G â‰œ (Gx)xâˆˆM , where each Gx defines an inner product on the tangent space TxM that varies smoothly with x,forming a smooth Riemannian manifold (M, G). In composite systems, especially for articulations, we exploit the structure of product manifolds [49, 42]. Given K component manifolds 

M1, . . . , MK , their Cartesian product M1: K = M1 Ã— Â· Â· Â· Ã— MK is a smooth manifold of dimension PKi=1 dim( Mi).Moreover, equipping it with the canonical product metric yields a Riemannian manifold. When all K components are identical (i.e., Mi â‰¡ M j ), we denote the configuration space as the power manifold MK , which has dimension Km and in-herits a natural product Riemannian structure [49, 42, 17, 47]. In the context of HMR and humanoid robotics, the kine-matic state of each joint is commonly formalized as a 3D rotation in the Special Orthogonal group SO(3) . This Lie group is defined as: 

SO(3) = R âˆˆ R3Ã—3 RâŠ¤R = I, det( R) = 1 . (1) Viewing SO(3) as a submanifold embedded in R9 (identified with R3Ã—3), the full configuration space of a skeleton with K

joints can be constructed as the power manifold of Lie groups: 

MH â‰œ

> K

Y

> k=1

SO(3) k = SO(3) K . (2) Since SO(3) is a Lie group, it admits well-defined tangent spaces and the exponential/logarithmic maps, which allow us to perform local (linear) computations in the tangent space while respecting the underlying manifold geometry. 

Definition 1 (Tangent Space) . To characterize motion on the curved manifold M embedded in X , we introduce the tangent space TxM. Formally, a vector v âˆˆ X is tangent to M at x

if it is the velocity of a smooth curve passing through x. Let 

Î³ : [0 , 1] â†’ M be a smooth curve such that Î³(0) = x. The tangent space is defined as the collection of all such velocities: 

TxM â‰œ { Ë™Î³(0) | Î³ is smooth , Î³ (0) = x} âŠ‚ X . (3) where Ë™Î³(0) is understood as an element of TÎ¹(x)X via the differential of the embedding Î¹ : M ,â†’ X . While the ambient space X supports global linear operations, rigorous kinematic updates must respect the manifold structure defined on the tangent bundle, the disjoint union of tangent spaces: T M â‰œS

> xâˆˆM

{(x, v) | v âˆˆ TxM} .

Definition 2 (Exponential Map on SO(3) ). The matrix expo-nential exp : so (3) â†’ SO(3) maps a Lie algebra element to a rotation. Given a unit axis Ë†Ï‰ âˆˆ R3 and angle Î¸ âˆˆ R,Rodriguesâ€™ formula yields 

exp([ Ë† Ï‰]Î¸) = I + sin Î¸ [ Ë† Ï‰] + (1 âˆ’ cos Î¸) [ Ë† Ï‰]2, (4) where [Â·] denotes the skew-symmetric matrix operator. 

Definition 3 (Local Logarithmic Map on SO(3) ). The matrix logarithm log : SO(3) â†’ so (3) is the local inverse of exp .For two rotations R1, R2 âˆˆ SO(3) , define R â‰œ RâŠ¤ 

> 1

R2. Let 

Î¸ âˆˆ [0 , Ï€ ] satisfy cos Î¸ = tr( R)âˆ’12 . Then (for Î¸ / âˆˆ { 0, Ï€ }), the principal logarithm is given by 

[Ï‰] = log( R) = Î¸

2 sin Î¸

 R âˆ’ RâŠ¤ . (5) The geodesic distance induced by the canonical bi-invariant Riemannian metric on SO(3) is given by 

dgeo (R1, R2) =   log( RâŠ¤ 

> 1

R2)âˆ¨

> 2

, (6) where (Â·)âˆ¨ : so (3) â†’ R3 is the inverse of the skew operator, and âˆ¥ Â· âˆ¥ 2 is the Euclidean norm. IV. M ETHODS 

In contemporary humanoids, the articulated body is com-monly modeled as a kinematic tree of 1-DoF revolute joints, which enables direct actuation, high structural stiffness, and well-conditioned dynamics. This design choice facilitates ef-ficient whole-body control, optimization, and learning. In this work, we adopt the Unitree G1 humanoid as our primary plat-form. For a 1-DoF joint, the geodesic metric simplifies to the Euclidean distance. Let qi, q â€² 

> i

âˆˆ R denote two configurations of the same joint. The corresponding rotations can be expressed as: 

Ri = exp([ k]qi), âˆ¥kâˆ¥ = 1 , (7) where k âˆˆ R3 is the fixed rotation axis and [k] âˆˆ so (3) 

denotes its associated skew-symmetric matrix. The relative rotation Rrel = RâŠ¤ 

> i

Râ€² 

> i

can be calculated by: 

Rrel = exp( âˆ’[k]qi) exp([ k]qâ€²

> i

) = exp([ k]( qâ€² 

> i

âˆ’ qi)) . (8) Substituting this result into Eq. (6), the geodesic metric strictly reduces to the L1 distance: 

d(Ri, Râ€²

> i

) = |qi âˆ’ qâ€²

> i

|, (9) assuming qi is represented within its joint limits (i.e., no angle wrap-around). 

Pose Manifold and Distance Field Learning. We hypothesize that physically plausible robot poses lie on a low-dimensional manifold MHR embedded in the high-dimensional config-uration space Q âŠ† RNJ . Q is a proper subset of RNJ

due to per-joint bound constraints specified by the URDF. The instantaneous configuration of the Unitree G1 robot is represented as a vector q describing the articulation of its 

NJ = 29 revolute joints: 

q = [ q1, q 2, . . . , q NJ ]âŠ¤ âˆˆ Q , (10) where qi is a scalar joint angle. Endowed with the L1 product metric, the distance between two configurations is: 

dMHR (q, qâ€²) = 

> NJ

X

> i=1

d(Ri, Râ€²

> i

). (11) Instead of explicitly parameterizing this manifold, we model 

MHR implicitly as the zero-level set of a continuous unsigned distance function fÏ• : Q â†’ R+:

MHR = { q âˆˆ Q | fÏ•(q) = 0 }, (12) where fÏ•(q) represents the unsigned geodesic distance to the closest plausible pose on the manifold. Training an accurate distance field fÏ• requires a large num-ber of samples, including both positive (on-manifold, i.e., zero-distance) and negative (off-manifold, i.e., non-zero-distance) poses. Leveraging recent advances in retargeting, we construct a large-scale positive set Dp using the PHUMA [21], LaFAN1, and AMASS [28] retargeted datasets. We further perform cross-validation to filter unreliable positives. Despite having abundant on-manifold samples, the performance of fÏ• depends critically on the statistical coverage of the training distribution and, in particular, on incorporating informative off-manifold negatives Dn. Motivated by NRDF [17], we therefore adopt a hybrid sampling strategy to construct Dn. Details of the hybrid sampling procedure and the cross-validation protocol are provided in the Appendix A. With the resulting dataset D = Dp âˆª D n containing N

samples, we learn a continuous unsigned distance field via supervised regression. Specifically, fÏ• is trained to approxi-mate the minimum geodesic distance from a query pose q to Retargeted -G1 Dataset 

## â„’ = ð‘“ ðœ™ ð’’ âˆ’ ð‘‘  ð‘“ ðœ™ ð’’ 

ð’’ = q1, q2, â€¦ , q29 

> Pose Distance Field

ð‘“ ðœ™ ð’’ = 0  

> 0.649 0.520
> 0.170
> 0.007
> 0.001
> 0.377

ð‘‘ = min Fig. 1: We present PDF-HR , which learns the manifold of plausible G1 poses as a zero-level set. Left : The fÏ• is trained to approximate the unsigned pose distance field. Given a query pose q, we compute its distance d to the nearest dataset sample and optimize the network to regress this value. Right : The learned prior provides quantitative scores for arbitrary poses, where a larger predicted value fÏ•(q) indicates a significant deviation from the manifold, corresponding to an unnatural pose. This learned prior effectively benefits downstream tasks such as motion tracking and motion retargeting. 

the valid pose manifold MHR , which is approximated by the distance to the nearest sample in the dense dataset Dp. The optimal parameters are obtained by minimizing the objective function: 

Ï•â‹† = arg min 

> Ï•N

X

> i=1

fÏ•(qi) âˆ’ min 

> qâ€²âˆˆD p

dMHR (qi, qâ€²)

> 1

. (13) Once trained, fÏ• serves as a continuous and differentiable pose prior that explicitly captures the geometry of the valid pose manifold. Figure 1 illustrates the full training pipeline. For 1-DoF joints, the Riemannian gradient coincides with the Euclidean gradient (proven in Appendix C). Leveraging this differentiability, the projection of an arbitrary query pose 

q onto the manifold M can be formulated as an iterative gradient-based update: 

qk+1 = P



qk âˆ’ Î± Â· fÏ•(qk) Â· âˆ‡qfÏ•(qk)

âˆ¥âˆ‡ qfÏ•(qk)âˆ¥



(14) where Î± is the step size, âˆ’âˆ‡ qfÏ•(qk)/âˆ¥âˆ‡ qfÏ•(qk)âˆ¥ denotes the gradient direction towards the manifold. The operator 

P enforces physical joint limits, ensuring the updated pose remains within the valid configuration space. V. A PPLICATIONS 

A. Reinforcement Learning based Motion Tracking 

Reinforcement learning (RL) is a standard approach for motion tracking. Modern RL-based tracking methods generally fall into two categories: strict tracking [35, 52] and stylized tracking [36, 29]. In both cases, the reward can be simplified as: 

rt = wGrGt + wT rTt . (15) Here, rGt is the task reward, which defines high-level goals for the character to satisfy (e.g., walking in a target direction or punching a target). rTt denotes the tracking reward. For strict tracking, it encourages the character to imitate the reference motion as closely as possible, typically by minimizing the pose discrepancy between the simulated character and the target poses from the reference trajectory. In contrast, for stylized tracking, rTt encourages similarity in motion style and is not directly tied to pose error. wT and wG are the corresponding reward weights. Although RL-based tracking methods can successfully fol-low most motions, they typically require long training times to explore a sufficiently diverse set of states. To improve training efficiency, we introduce a pose-prior reward that encourages the policy to explore near the learned pose manifold. Specifi-cally, we first define a pose score s as 

s(qt) = clip 

 fÏ•(qt) âˆ’ dgood 

dbad âˆ’ dgood 

, 0, 1



(16) where dbad is a scalar hyperparameter. In all experiments, we set dbad = 0 .4 (i.e., if the predicted pose distance fÏ•(qt)

exceeds 0.4, we treat the pose as abnormal). dgood is computed from the reference motion as the maximum fÏ•(qt) observed along the trajectory. We clip s(qt) to [0 , 1] for training stability. We then define the pose-prior reward rPt as: 

rPt = eâˆ’Î±P s(qt). (17) Here Î±P controls the scale of the pose-prior term. Thus, our full reward can be formulated as: 

rt = wGrGt + wT rTt + wP rPt , (18) where wP is the weight of the pose reward rPt .

B. Motion Retargeting 

Physical humanoids only approximate human morphology, with significant differences in shape, proportion, and degrees of freedom; this is also called the embodiment gap. A common way to bridge this gap is to retarget human motion data to a Ours ADD Reference Fig. 2: Visualization of joint orientation distributions of Sideflip at early training stage. The visualization maps the directional vectors of the robotâ€™s links onto unit spheres centered at their respective joints. The color gradient corresponds to the probability density of the visited states. 

humanoid embodiment and then train RL policies to imitate the resulting reference trajectories. Existing works [45] have shown that high-quality retargeting algorithms can substan-tially alleviate extensive reward engineering. For clarity, we adopt GMR [3] as a representative retargeting method and illustrate how to incorporate our pose prior. GMR [3] performs retargeting by solving an inverse kine-matics (IK) problem at each frame. At each timestep t, GMR finds the robot configuration qt that matches the source key-point positions and orientations via the following optimization program: 

qâ‹†t = arg min 

> qt

X

> i

f pi (qt) âˆ’ psource   

> t,i 2

+

f Î¸i (qt) âˆ’ Î¸source  

> t,i 2

s.t. qmin â‰¤ qt â‰¤ qmax ,

(19) where f pi and f Î¸i are the robot forward kinematics for the 

i-th keypointâ€™s position and orientation, respectively. Lever-aging the Mink [48] library, GMR solves this program in a Sequential Quadratic Programming fashion. A key limitation of GMR is the need for tedious parameter tuning, especially careful per-joint weighting, to avoid opti-mization artifacts. Since our pose distance field is continuous and differentiable, we can simply add an extra prior term to Eq. (19). The resulting objective becomes: 

qâ‹†t = arg min 

> qt

X

> i

f pi (qt) âˆ’ psource   

> t,i 2

+

f Î¸i (qt) âˆ’ Î¸source   

> t,i 2

+ âˆ¥fÏ•(qt)âˆ¥2

s.t. qmin â‰¤ qt â‰¤ qmax ,

(20) VI. E XPERIMENTS 

To validate the effectiveness of our method, we integrate it into four humanoid tasks: single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and gen-eral motion retargeting. Comprehensive details regarding the implementation and experimental configurations are provided in the Appendix D. 

Metrics. We evaluate our method against baselines in terms of sample efficiency and motion tracking error. All policies are trained with 4096 parallel environments and evaluated on 4096 test episodes every 100 training iterations. We define the motion tracking success rate as the ratio of continuous tracking duration to the total episode length. The sample efficiency is measured by the number of training samples required to first reach 80% success rate. The position error Epos and rotation error Erot are defined as: 

Epos = 1

N + 1 

ï£«ï£­ X

> jâˆˆjoints

epos  

> j

+ epos root 

ï£¶ï£¸ , (21) 

Erot = 1

N + 1 

ï£«ï£­ X

> jâˆˆjoints

erot  

> j

+ erot root 

ï£¶ï£¸ , (22) where epos  

> j

, epos root , erot  

> j

, and erot root represent the time-averaged 

j-th joint position error, root position error, j-th joint rotation error, and root rotation error, respectively. Joint errors are computed in the robotâ€™s local coordinate frame, while root errors are computed in the global coordinate frame. For general motion tracking, we additionally report the detailed errors (i.e., joint position error Â¯epos  

> joint

=

P 

> jâˆˆjoints

epos 

> j



/N ,root position error epos root , joint rotation error Â¯erot  

> joint

=P 

> jâˆˆjoints

erot 

> j



/N and root rotation error erot root ) for a more fine-grained comparison. 

A. Single-trajectory Motion Tracking 

Experiment settings. We compare our method with ADD [52] in the single-trajectory tracking task using nine motion clips: three easy motions (Walk, Run, Jump), four hard motions (Spinkick, Cartwheel, Backflip, Sideflip), and two parkour motions (Double Kong, Speed Vault). The parkour skills are particularly challenging, as they require high tracking accuracy to reproduce intricate contacts with the environment. For a fair comparison, all policies are trained with 800 million samples with the same hyperparameters. 

Results. Table I presents the quantitative comparisons. Our method achieves substantially higher sample efficiency across all motions and achieves lower tracking error on most hard and parkour motions. Notably, this advantage in sample efficiency becomes more pronounced as the motion difficulty increases. To better understand the exploration behavior, we logged the actions during the early stage of training (around 125 million samples) on Sideflip. Figure 2 visualizes the resulting action distributions as heatmaps. Compared to ADD, our method produces an action distribution that aligns much more closely with the reference motion. This matters because a humanoidâ€™s action space is continuous and high-dimensional, while the set of actions that stay on a plausible motion man-ifold occupies only a small fraction of the total volume. For highly dynamic motions, the local neighborhood is dominated by invalid poses, causing ADD to waste substantial samples exploring unproductive regions. Our pose-prior guidance alle-viates this by steering exploration toward a valid, well-shaped subspace. Meanwhile, since the strict tracking objective is maintained, the policy explores primarily within this reduced space, enabling faster convergence to the optimal solution. We further provide qualitative visual comparisons in Figure 3 to corroborate this observation. ADD -1.2G 

Backflip 

Speed Vault 

Sideflip 

Double Kong 

> Ours -200M Ours -130M ADD -430M
> ADD -350M  Ours -150M Ours -300M ADD -1.3G

Fig. 3: Visual comparisons of motion tracking performance on dynamic skills between our method and ADD [52]. The number of training samples is annotated on the left of each strip. Our method successfully masters these complex skills with remarkably fewer samples, whereas the baseline frequently suffers from falls or collisions even after extensive training. 

TABLE I: Comparison of sample efficiency and motion-tracking performance between ADD [52] and our method on the single-trajectory motion-tracking task. Metrics are reported as mean Â± 1 std over three independently trained models with different random initializations. A motion is marked as Failed if all three trials fail to converge within 800 M training samples. 

Skill Length Samples ( SR â‰¥ 80% ) [M] Position Error [m] Rotation Error [rad] ADD Ours ADD Ours ADD Ours 

Walk 1.03 70 .036 Â±6.179 56 .929 Â±6.179 0.008 Â±0.000 0.009 Â±0.000 0.038 Â±0.001 0.042 Â±0.002 

Run 0.80 113 .727 Â±6.179 70 .036 Â±6.179 0.015 Â±0.000 0.017 Â±0.000 0.063 Â±0.001 0.070 Â±0.001 

Jump 1.75 222 .953 Â±10 .702 188 .001 Â±16 .348 0.022 Â±0.001 0.024 Â±0.001 0.108 Â±0.004 0.103 Â±0.003 

Spinkick 1.28 135 .572 Â±6.179 91 .881 Â±0.000 0.031 Â±0.000 0.032 Â±0.001 0.150 Â±0.002 0.161 Â±0.004 

Cartwheel 2.72 183 .632 Â±0.000 161 .787 Â±12 .358 0.031 Â±0.000 0.028 Â±0.003 0.129 Â±0.007 0.126 Â±0.010 

Backflip 1.69 Failed 183 .632 Â±18 .536 Failed 0.048 Â±0.000 Failed 0.205 Â±0.004 

Sideflip 2.38 336 .549 Â±62 .708 166 .156 Â±6.179 0.051 Â±0.002 0.050 Â±0.001 0.241 Â±0.003 0.242 Â±0.005 

Speed Vault 1.92 388 .978 Â±49 .430 122 .465 Â±6.179 0.024 Â±0.001 0.023 Â±0.000 0.118 Â±0.003 0.116 Â±0.001 

Double Kong 5.17 Failed 288 .489 Â±10 .702 Failed 0.031 Â±0.001 Failed 0.157 Â±0.003 

B. Style-based Motion Mimicry 

Experiment settings. We compare our method with AMP [36] on three easy motions (Walk, Run, Jump) and three hard motions (Spinkick, Cartwheel, Sideflip), as summarized in Table II. All policies are trained with 800 million samples for a fair comparison. Although style-based motion mimicry does not aim for precise motion tracking, we still report tracking error to facilitate comparison. 

Results. Our method consistently outperforms AMP in terms of sample efficiency while maintaining a comparable level of tracking error. The qualitative results at an intermediate train-ing stage (350 million samples for all policies) are presented in Appendix E. Even with this limited training budget, our method generally mimics all reference motions, highlighting the stability and efficiency brought by our pose-prior reward. We further evaluate our method on the Target Location 

and Heading tasks and obtain consistent results. Due to space limitations, we report the task-oriented results in Appendix F TABLE II: Quantitative results in mean Â±1 std across 3 random seeds of AMP [36] and our method in style-based motion mimicry task.                                                                                                                                       

> Skill Length Samples ( SR â‰¥80% ) [M] Position Error [m] Rotation Error [rad] AMP Ours AMP Ours AMP Ours
> Walk 1.03 91 .881 Â±0.000 65 .667 Â±0.000 0.228 Â±0.002 0.228 Â±0.002 0.301 Â±0.002 0.304 Â±0.003
> Run 0.80 104 .989 Â±0.000 87 .512 Â±6.179 0.601 Â±0.004 0.593 Â±0.002 0.708 Â±0.000 0.718 Â±0.004
> Jump 1.75 323 .442 Â±26 .933 196 .739 Â±21 .404 0.189 Â±0.010 0.163 Â±0.003 0.490 Â±0.011 0.480 Â±0.003
> Spinkick 1.28 183 .632 Â±0.000 161 .787 Â±6.179 0.319 Â±0.002 0.316 Â±0.008 1.634 Â±0.003 1.637 Â±0.003
> Cartwheel 2.72 240 .430 Â±6.179 209 .846 Â±74 .914 0.455 Â±0.004 0.399 Â±0.013 1.604 Â±0.008 1.607 Â±0.039
> Sideflip 2.38 393 .347 Â±28 .315 345 .287 Â±40 .517 0.327 Â±0.010 0.336 Â±0.005 1.254 Â±0.028 1.282 Â±0.022
> TABLE III: General motion tracking performance comparison across different episode lengths. Ours is trained with ADD [52] augmented by the pose-prior reward. Ours* is trained in two stages: 4 billion samples guided by PDF-HR, then 8 billion samples of resumed training with the original ADD objective. All policies are trained with 12 billion total samples for a fair comparison.
> Episode Length [s] Joint Position Error [m] Root Position Error [m] Joint Rotation Error [rad] Root Rotation Error [rad] ADD Ours Ours* ADD Ours Ours* ADD Ours Ours* ADD Ours Ours*
> 10 0.022 0.023 0.019 0.095 0.101 0.107 0.121 0.129 0.108 0.075 0.080 0.066 20 0.022 0.023 0.019 0.098 0.099 0.111 0.123 0.127 0.107 0.075 0.079 0.066 30 0.022 0.022 0.018 0.103 0.104 0.112 0.122 0.125 0.102 0.077 0.077 0.062
> TABLE IV: Quantitative comparisons of retargeting quality. Tracking metrics produced by an ADD policy trained on each retargeted dataset. MTL means motion tracking length in frames. BPE and RPE are short for body and root position errors. BRE and RRE are body and root rotation errors. R.M. is short for retargeting methods.
> R.M. MTL (frames) BPE [m] RPE [m] BRE [rad] RRE [rad]
> GMR 95.708 0.029 0.113 0.197 0.087 Ours 101.613 0.029 0.124 0.162 0.087

C. General Motion Tracking 

Experiment settings. For this task, both our method and the baseline ADD [52] are trained with a single general policy to track a large-scale motion dataset, utilizing a subset of LaFAN1 [13]. The subset contains 34 sequences (excluding six FallAndGetUp sequences), totaling 7887.6 seconds of motion. We adopt a two-stage training strategy (dubbed as Ours* ) to improve the tracking accuracy. For this strategy, we first train the policy with the pose-prior reward for 4 billion samples, then disable the prior and resume the training with the original ADD objective. All policies are trained with 12 billion samples in total for a fair comparison. To assess convergence under different horizon lengths, we additionally vary the episode duration and evaluate tracking horizons of 10, 20, and 30 seconds for both our method and ADD. 

Results. Quantitative results are reported in Table III, and learning curves are provided in Appendix E. Benefiting from the guidance of the PDF-HR pose-prior reward, our method exhibits significantly faster convergence compared to the baseline. This advantage becomes more pronounced as the episode length increases, demonstrating improved robustness for long-horizon tracking. We also observe that the one-stage training yields slightly higher tracking error than ADD, whereas the two-stage protocol effectively mitigates this issue and consistently achieves the lowest joint position error, joint rotation error, and root rotation error. Intuitively, the first stage uses the prior reward to quickly acquire basic locomotion and coordination skills, as reflected by the rapid rise in success rate; the second stage then focuses purely on tracking to refine accuracy. Overall, the two-stage method strikes a strong balance between fast convergence and precise motion tracking. 

D. Motion Retargeting 

Experiment settings. For the motion retargeting task, we compare our method with GMR [3] on a subset of the AMASS [28] DanceDB dataset. 30 sequences are sampled from DanceDB, totaling 7,770 frames (254.7 seconds). We retarget the same SMPL-X [33] motions to the Unitree G1 robot using both GMR and our method, producing two sets of retargeted reference motions. To evaluate how well these motions can be tracked in physics, we train an ADD-based general tracking policy on each reference set and compare the resulting tracking errors. 

Results. Figure 4 presents a visual comparison between the retargeted motions and the original SMPL-X motions. As highlighted by the red indicators, GMR often produces kine-matically implausible artifacts, including severe joint distor-tions (e.g., at the wrists, torso, and legs) and noticeable self-penetration. These artifacts not only deviate from the original human motion but are also physically infeasible, which makes them difficult to track in physics-based simulators. The quan-titative results in Table IV corroborate this observation: such inconsistencies in GMR lead to shorter continuous tracking durations and larger body rotation errors. In contrast, our PDF-HR regularizer steers the optimizer away from poor local minima without requiring tedious parameter tuning, yielding retargeted motions that are both visually faithful to the refer-ence and physically valid for tracking. Fig. 4: Visual comparison of retargeted motions produced by GMR [3] (red) and our method (green). GMR artifacts are highlighted with red markers. (a)     

> (c)
> (b)
> Fig. 5: Real-world deployment. (a) Deploying GMR-retargeted Seq. 4. Artifacts introduced by GMR lead to poor alignment with the reference motion and frequent self-collisions. (b) Deploying our retargeted Seq. 4. Our method produces smooth, physically plausible motions that closely match the reference sequence. (c) Deploying our retargeted Seq. 2 for highly dynamic skills. Our method robustly executes challenging motions, including dance and spinning jumps.

Deployment validation. We further evaluate deployability using BeyondMimic [23]. We select 10 motion sequences and compare policies trained on GMR-retargeted data versus our retargeted data. Our method successfully deploys on 9/10 sequences, whereas GMR succeeds on only 6/10. Detailed learning curves are provided in Appendix E. Figure 5 vi-sualizes real-world deployment results, where our method produces behaviors that are noticeably more natural and closer to the target motions than those from GMR. VII. C ONCLUSION 

In this work, we introduce Pose Distance Fields for Hu-manoid Robots (PDF-HR), a lightweight and differentiable prior that models the plausibility of humanoid robot poses as a continuous distance field learned from a large corpus of retargeted motions. PDF-HR is simple to integrate into existing pipelines and broadly applicable to optimization and learn-ing. Across multiple humanoid benchmarks, including single-trajectory tracking, general motion tracking, style-conditioned mimicry, and motion retargeting, PDF-HR consistently im-proves strong baselines, demonstrating the value of pose-level priors for robust humanoid behavior. Our results highlight the importance of pose priors for humanoid control and open opportunities for more data-driven approaches in humanoid motion generation and understanding. VIII. L IMITATIONS 

While PDF-HR demonstrates promising results across a range of humanoid tasks, it has several limitations:  

> â€¢

Accuracy in some tracking settings. Although PDF-HR can substantially accelerate convergence, its final tracking accuracy is occasionally lower than that of the baseline. We acknowledge this as a limitation.  

> â€¢

Mode collapse in style-conditioned tracking. In style tracking, our method can suffer from mode collapse, sim-ilar to AMP [36]. A potential remedy is to enrich the prior with additional kinematic information, e.g., velocity- and acceleration-based fields [47], to better constrain long-horizon behavior.  

> â€¢

Runtime and smoothness in retargeting. Our current implementation is slower than GMR [3], and it can sometimes exhibit more severe jitter. GPU-parallel batch processing and incorporating explicit smoothness costs could mitigate these issues and better reveal the practical advantages of our approach [20].  

> â€¢

Training data quality. Our pose field is trained on retargeted poses, which may not always be physically feasible. An important future direction is to curate or col-lect large-scale pose corpora that better satisfy physical constraints, thereby improving the fidelity and robustness of the learned field. REFERENCES 

[1] Ijaz Akhter and Michael J Black. Pose-conditioned joint angle limits for 3d human pose reconstruction. In 

Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1446â€“1455, 2015. [2] Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, and Angjoo Kanazawa. Visual imitation enables contextual humanoid control. In Proceedings of the Conference on Robot Learning (CoRL) , 2025. [3] Joao Pedro Araujo, Yanjie Ze, Pei Xu, Jiajun Wu, and C Karen Liu. Retargeting matters: General motion retargeting for humanoid motion tracking. arXiv preprint arXiv:2510.02252 , 2025. [4] Tolga Birdal and Umut Simsekli. Probabilistic permu-tation synchronization using the riemannian structure of the birkhoff polytope. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,pages 11105â€“11116, 2019. [5] Tolga Birdal, Umut Simsekli, Mustafa Onur Eken, and Slobodan Ilic. Bayesian pose graph optimization via bingham distributions and tempered geodesic mcmc. 

Advances in neural information processing systems , 31, 2018. [6] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In European conference on computer vision , pages 561â€“578. Springer, 2016. [7] Bingjie Chen, Zihan Wang, Zhe Han, Guoping Pan, Yi Cheng, and Houde Liu. Hl-ik: A lightweight imple-mentation of human-like inverse kinematics in humanoid arms. arXiv preprint arXiv:2509.20263 , 2025. [8] Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas J Guibas, and He Wang. Projective manifold gradient layer for deep rotation regression. In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6646â€“6655, 2022. [9] Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, and Xiaolong Wang. Gmt: General motion tracking for humanoid whole-body control. arXiv preprint arXiv:2506.14770 , 2025. [10] Stelian Coros, Philippe Beaudoin, and Michiel Van de Panne. Generalized biped walking control. ACM Trans-actions On Graphics (TOG) , 29(4):1â€“9, 2010. [11] Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, and Michael J Black. Tokenhmr: Advancing human mesh recovery with a tokenized pose representation. In 

Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 1323â€“1333, 2024. [12] Georgios Georgakis, Ren Li, Srikrishna Karanam, Ter-rence Chen, Jana KoË‡ seckÂ´ a, and Ziyan Wu. Hierarchical kinematic human mesh recovery. In European Con-ference on Computer Vision , pages 768â€“784. Springer, 2020. [13] FÂ´ elix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. ACM Transactions on Graphics (TOG) , 39(4):60â€“1, 2020. [14] Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning. 

arXiv preprint arXiv:2406.08858 , 2024. [15] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, et al. Asap: Aligning simula-tion and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143 ,2025. [16] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbabu, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Ki-tani, Jessica K. Hodgins, Linxi Fan, Yuke Zhu, Changliu Liu, and Guanya Shi. ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills. In Proceedings of Robotics: Science and Systems , LosAngeles, CA, USA, June 2025. doi: 10.15607/RSS.2025.XXI.066. [17] Yannan He, Garvita Tiwari, Tolga Birdal, Jan Eric Lenssen, and Gerard Pons-Moll. Nrdf: Neural riemannian distance fields for learning articulated pose priors. In 

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1661â€“1671, 2024. [18] Jonathan Ho and Stefano Ermon. Generative adversar-ial imitation learning. Advances in neural information processing systems , 29, 2016. [19] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 7122â€“ 7131, 2018. [20] Chung Min Kim*, Brent Yi*, Hongsuk Choi, Yi Ma, Ken Goldberg, and Angjoo Kanazawa. Pyroki: A mod-ular toolkit for robot kinematic optimization. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2025. URL https://arxiv.org/abs/ 2505.03728. [21] Kyungmin Lee, Sibeen Kim, Minho Park, Hyunse-ung Kim, Dongyoon Hwang, Hojoon Lee, and Jaegul Choo. Phuma: Physically-grounded humanoid locomo-tion dataset. arXiv preprint arXiv:2510.26236 , 2025. [22] Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, and Xiaolong Wang. AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control. In Proceedings of Robotics: Sci-ence and Systems , LosAngeles, CA, USA, June 2025. doi: 10.15607/RSS.2025.XXI.061. [23] Qiayuan Liao, Takara E Truong, Xiaoyu Huang, Yu-man Gao, Guy Tevet, Koushil Sreenath, and C Karen Liu. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241 , 2025. [24] Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based characters using deep q-learning. ACM Transactions on Graphics (TOG) ,36(3):1â€“14, 2017. [25] Junzhe Lu, Jing Lin, Hongkun Dou, Ailing Zeng, Yue Deng, Yulun Zhang, and Haoqian Wang. Dposer: Diffu-sion model as robust 3d human pose prior. arXiv preprint arXiv:2312.05541 , 2023. [26] Junzhe Lu, Jing Lin, Hongkun Dou, Ailing Zeng, Yue Deng, Xian Liu, Zhongang Cai, Lei Yang, Yulun Zhang, Haoqian Wang, et al. Dposer-x: Diffusion model as robust 3d whole-body human pose prior. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 9988â€“9997, 2025. [27] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 10895â€“10904, 2023. [28] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In Inter-national Conference on Computer Vision , pages 5442â€“ 5451, October 2019. [29] Yuxuan Mu, Ziyu Zhang, Yi Shi, Minami Matsumoto, Kotaro Imamura, Guy Tevet, Chuan Guo, Michael Tay-lor, Chang Shu, Pengcheng Xi, et al. Smp: Reusable score-matching motion priors for physics-based character control. arXiv preprint arXiv:2512.03028 , 2025. [30] Shinâ€™ichiro Nakaoka and Taku Komura. Interaction mesh based motion adaptation for biped humanoid robots. In 2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012) , pages 625â€“631. IEEE, 2012. [31] Chaoyi Pan, Changhao Wang, Haozhi Qi, Zixi Liu, Homanga Bharadhwaj, Akash Sharma, Tingfan Wu, Guanya Shi, Jitendra Malik, and Francois Hogan. Spider: Scalable physics-informed dexterous retargeting. arXiv preprint arXiv:2511.09484 , 2025. [32] Yixuan Pan, Ruoyi Qiao, Li Chen, Kashyap Chitta, Liang Pan, Haoguang Mai, Qingwen Bu, Hao Zhao, Cunyuan Zheng, Ping Luo, et al. Agility meets stability: Versa-tile humanoid control with heterogeneous data. arXiv preprint arXiv:2511.17373 , 2025. [33] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In Pro-ceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 10975â€“10985, 2019. [34] Xue Bin Peng. Mimickit: A reinforcement learning framework for motion imitation and control. arXiv preprint arXiv:2510.13794 , 2025. [35] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided deep re-inforcement learning of physics-based character skills. 

ACM Transactions On Graphics (TOG) , 37(4):1â€“14, 2018. [36] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transac-tions on Graphics (ToG) , 40(4):1â€“20, 2021. [37] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Transactions On Graphics (TOG) , 41 (4):1â€“17, 2022. [38] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learn-ing Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 , 2023. URL https://openreview.net/pdf?id= FjNys5c7VyY. [39] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J Guibas. Humor: 3d human motion model for robust pose estimation. In 

Proceedings of the IEEE/CVF international conference on computer vision , pages 11488â€“11499, 2021. [40] Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao. Diffusion-based 3d human pose estimation with multi-hypothesis aggregation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14761â€“14771, 2023. [41] Garvita Tiwari, Dimitrije AntiÂ´ c, Jan Eric Lenssen, Niko-laos Sarafianos, Tony Tung, and Gerard Pons-Moll. Pose-ndf: Modeling human pose manifolds with neural dis-tance fields. In European Conference on Computer Vision , pages 572â€“589. Springer, 2022. [42] Nathaniel S Woodward, Sang Eon Park, Gaia Grosso, Jeffrey Krupa, and Philip Harris. Product mani-fold machine learning for physics. arXiv preprint arXiv:2412.07033 , 2024. [43] Weiji Xie, Jiakun Zheng, and Chenjia Bai. TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control. 

https://github.com/TeleHuman/TextOp , November 2025. [44] Yufei Xue, Wentao Dong, Minghuan Liu, Weinan Zhang, and Jiangmiao Pang. A Unified and General Humanoid Whole-Body Controller for Fine-Grained Locomotion. In 

Proceedings of Robotics: Science and Systems , LosAn-geles, CA, USA, June 2025. doi: 10.15607/RSS.2025. XXI.067. [45] Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C Karen Liu, Rocky Duan, and Guanya Shi. Omniretarget: Interaction-preserving data generation for humanoid whole-body loco-manipulation and scene interaction. arXiv preprint arXiv:2509.26633 , 2025. [46] KangKang Yin, Kevin Loken, and Michiel Van de Panne. Simbicon: Simple biped locomotion control. ACM Trans-actions on Graphics (TOG) , 26(3):105â€“es, 2007. [47] Zhengdi Yu, Simone Foti, Linguang Zhang, Amy Zhao, Cem Keskin, Stefanos Zafeiriou, and Tolga Birdal. Geo-metric neural distance fields for learning human motion priors. arXiv preprint arXiv:2509.09667 , 2025. [48] Kevin Zakka. Mink: Python inverse kinematics based on MuJoCo. https://github.com/kevinzakka/mink , December 2025. [49] Sharon Zhang, Amit Moscovich, and Amit Singer. Prod-uct manifold learning. In International Conference on Artificial Intelligence and Statistics , pages 3241â€“3249. PMLR, 2021. [50] Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexan-der Winkler, Petr Kadlecek, Siyu Tang, and Federica Bogo. Rohm: Robust human motion reconstruction via diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14606â€“14617, 2024. [51] Tong Zhang, Boyuan Zheng, Ruiqian Nai, Yingdong Hu, Yen-Jen Wang, Geng Chen, Fanqi Lin, Jiongye Li, Chuye Hong, Koushil Sreenath, and Yang Gao. Hub: Learning extreme humanoid balance. In 9th Annual Conference on Robot Learning , 2025. URL https://openreview.net/ forum?id=FCpYuGtN4j. [52] Ziyu Zhang, Sergey Bashkirov, Dun Yang, Yi Shi, Michael Taylor, and Xue Bin Peng. Physics-based motion imitation with adversarial differential discriminators. In 

Proceedings of the SIGGRAPH Asia 2025 Conference Papers , pages 1â€“12, 2025. APPENDIX 

A. PDF-HR Training Details 

To learn a continuous and differentiable representation of the pose distance field, dense coverage of the state space is essen-tial. Pose-NDF [41] typically generates off-manifold samples by perturbing clean poses with Gaussian noise. Formally, given a clean pose q on the manifold, a corrupted sample is generated as Ë†q = q + Ïµ, where Ïµ âˆ¼ N (0 , Ïƒ 2I). However, this method introduces a sampling bias. In an N -dimensional space, the normalized squared Euclidean norm of the noise vector follows a Chi-squared distribution: 

|Ïµ|2

Ïƒ2 âˆ¼ Ï‡2 

> N

. (23) Consequently, the perturbation magnitudes concentrate within a narrow spherical shell around ÏƒâˆšN , preventing the network from effectively learning the distance fieldâ€™s transition across different scales. This concentration compromises the continuity of the represented manifold. To address this lim-itation, we construct a composite training dataset D derived from a high-fidelity reference dataset, denoted as Draw . The final training set D = Don âˆª D near âˆª D interp aggregates samples from three distinct generation strategies. 

On-Manifold Sampling. We directly sample raw pose states 

q from the reference dataset Draw . These samples represent the ground truth geometry of the valid pose distance field. Consequently, for every sample q âˆˆ D raw , we assign a zero-distance label: 

Don = {(q, 0) | q âˆˆ D raw }. (24) 

Near-Manifold Sampling. To characterize the distance field gradients in the immediate vicinity of the manifold, we employ a decoupled perturbation strategy. Unlike standard Gaussian noise injection, which entangles magnitude and direction, our approach explicitly separates the perturbation direction from its scalar magnitude. This facilitates precise control over the sampling density near the manifold boundary. Given a state 

q âˆˆ D raw , we generate a query point Ëœq via: 

Ëœq = clip ( q + r Â· u, qmin , qmax ) ,

with u âˆ¼ Unif( SN âˆ’1), r âˆ¼ |N (0 , Ïƒ 2)|. (25) Here, the perturbation direction u is drawn uniformly from the unit hypersphere SN âˆ’1, and the scalar magnitude r is sampled from a half-normal distribution to concentrate sampling den-sity near the manifold. The clip operation ensures Ëœq remains within joint limits [qmin , qmax ]. The supervision label for Ëœq is computed as the distance to its nearest neighbor in the training set, retrieved via FAISS: 

Dnear = {(Ëœ q, d ) | d = min 

> qâˆˆD raw

âˆ¥Ëœq âˆ’ qâˆ¥} . (26) 

Interpolation-based Sampling. To ensure the learned distance field is globally smooth and continuous, we employ linear interpolation. Since the raw data consists of discrete points, we generate intermediate samples between random pairs of poses to bridge gaps in the state space. We randomly sample pairs qa, qb from Draw and construct intermediate states Ëœq =

Î±qa + (1 âˆ’ Î±)qb using a mixing coefficient Î± âˆ¼ Unif(0 , 1) .The interpolation dataset is defined as: 

Dinterp = {(Ëœ q, d ) | d = min 

> qâˆˆD raw

âˆ¥Ëœq âˆ’ qâˆ¥} . (27) The final dataset D comprises 300 million state-distance pairs (q, d ). The data generation process was executed on a server equipped with 10 NVIDIA RTX 3090 GPUs and was completed in approximately 10 hours. 

B. Training Network 

To explicitly embed the kinematic structure of the 29-DoF G1 humanoid into our representation, we design a network architecture that mirrors the robotâ€™s topology. Let q =[q1, . . . , q K ]âŠ¤ denote the input configuration, where qk âˆˆ R

represents the scalar state of the k-th joint. We define a parent mapping Ï„ : {2, . . . , K } â†’ { 1, . . . , K } that returns the index of the parent joint for any given joint k. To capture the spatial dependency of each link on its ancestors, the network encodes features hierarchically along the kinematic chain. Formally, we employ a set of local encoders {f (k) 

> enc

}Kk=1 to propagate latent information from the root to the end-effectors: 

v1 = f (1)  

> enc

(q1),

vk = f (k) 

> enc

(qk, vÏ„ (k)), for k âˆˆ { 2, . . . , K }, (28) where vk âˆˆ R16 denotes the latent feature for joint k. In this formulation, the root feature is derived purely from its local state, while subsequent child encodings are conditioned on both their local joint state qk and the parent feature vÏ„ (k).

C. Riemannian Gradient for 1-DoF Joints 

Let f : SO(3) â†’ R be differentiable, and let âˆ‡f (R) denote the Euclidean (ambient) gradient under the Frobenius inner product âŸ¨A, BâŸ© â‰œ Tr( AâŠ¤B). Projecting âˆ‡f (R) onto the tangent space TRSO(3) yields the Riemannian gradient: 

grad f (R) â‰œ Î R(âˆ‡f (R)) = R skew  RâŠ¤âˆ‡f (R)

= 12 R RâŠ¤âˆ‡f (R) âˆ’ âˆ‡ f (R)âŠ¤R

= 12 âˆ‡f (R) âˆ’ 12 Râˆ‡f (R)âŠ¤R, (29) where Î R : R3Ã—3 â†’ TRSO(3) is the orthogonal projector and 

skew( A) â‰œ (A âˆ’ AâŠ¤)/2.

Restriction to a 1-DoF revolute joint. For a 1-DoF joint with fixed unit axis k âˆˆ R3, the admissible motions form a 1D submanifold whose tangent space is spanned by 

V â‰œ R[k] âˆˆ TRSO(3) . (30) To compare the gradient signal along this 1D direction, we compute the inner product between the second term of (29) and V: 

âˆ’ 12 Râˆ‡f (R)âŠ¤R, R[k]



(31) 

= Tr 



âˆ’ 12 Râˆ‡f (R)âŠ¤R

âŠ¤

R[k]

!

= âˆ’ 12 Tr  RâŠ¤âˆ‡f (R) RâŠ¤R[k]

= âˆ’ 12 Tr  RâŠ¤âˆ‡f (R)[ k] . (32) Using [k]âŠ¤ = âˆ’[k] and the cyclic property of the trace, we can rewrite (32) as 

âˆ’ 12 Tr  RâŠ¤âˆ‡f (R)[ k] = 12 Tr  RâŠ¤âˆ‡f (R)[ k]âŠ¤

= 12 Tr    âˆ‡f (R)âŠ¤R [k]

= 12 Tr  âˆ‡f (R)âŠ¤R[k]

=

 12 âˆ‡f (R), R[k]



. (33) Combining (29) and (33) gives 

âŸ¨grad f (R), R[k]âŸ© = âŸ¨âˆ‡ f (R), R[k]âŸ© . (34) Thus, for a 1-DoF revolute joint, the gradient component along the admissible tangent direction R[k] is identical whether computed from the Riemannian gradient or directly from the Euclidean gradient. Equivalently, the derivative with respect to the scalar joint angle q can be obtained using the Euclidean gradient âˆ‡f (R).

D. Implementation Details of RL-based Tracking 

All RL-based experiments, including our method, AMP [36] and ADD [52], are implemented using MimicKit [34] frame-work. Single-trajectory and style-based tracking are run on NVIDIA RTX 3090 GPUs, whereas the large-scale general motion tracking experiments are run on NVIDIA H20 GPUs. For fairness, we use the same default MimicKit hyperparame-ters for all methods (ours and baselines). For experiments that include our pose-prior reward, the reward weight wP and scale 

Î±P are set to 0.2 and 1, respectively, except for the Sideflip motion in style mimicry, where we set wP = 0 .05 .The retargeting experiments are based on the GMR frame-work [3] using the same configurations. We implement the prior task within the Mink IK solver [48] and solve it jointly with the GMR-defined tasks. 

E. More Experimental Results 

The learning curves for the baselines and our method on the single-trajectory motion tracking, style-based motion mimicry, and general motion tracking tasks are shown in Figure 6, Figure 7, and Figure 8, respectively. Across all motions, our method converges consistently faster than the baselines in both tasks, indicating that PDF-HR substantially improves sample efficiency. Figure 9 further reports the learning curves of ADD [52] trained with GMR [3]-retargeted data versus our     

> Fig. 6: The learning curves of ADD and our method on the single-trajectory motion tracking task.
> Fig. 7: The learning curves of AMP and our method on the style-based motion mimicry task.
> Fig. 8: The learning curves of ADD and our method on general motion tracking task across various episode length.
> Fig. 9: The learning curves of the ADD general motion tracker trained on datasets produced by GMR and our method.

retargeted data, highlighting the superior quality of our retar-geting. Finally, Figure 10 visualizes intermediate qualitative results on style-based motion mimicry after 350M training samples for all policies. Figure 11, Figure 12, and Figure 13 report the learning curves of joint position error, mean reward, and undesired Walk  Run         

> Jump Spinkick
> Cartwheel Sideflip Fig. 10: Qualitative results of our method in style-based motion mimicry task with only 350 Msamples.
> Fig. 11: The learning curves of joint position error for the Beyondmimic policy trained on GMR-retargeted and our retargeted motion.
> Fig. 12: The learning curves of mean reward for the Beyondmimic policy trained on GMR-retargeted and our retargeted motion.
> Fig. 13: The learning curves of undesired contacts for the Beyondmimic policy trained on GMR-retargeted and our retargeted motion.

contacts for the Beyondmimic [23] policy trained on GMR-retargeted motion and our retargeted motion. Under the same training protocol as defined in Beyondmimic, policies trained with our retargeted motions consistently converge faster and reach better plateaus across the nine sequences: the joint position error in Figure 11 drops rapidly to a much lower level, while the GMR-retargeted counterpart typically stalls at a no-ticeably higher tracking error. This improved tracking quality is reflected in Figure 12, where our policy achieves higher mean rewards, indicating that the controller can satisfy task ob-jectives more effectively once trained on physically consistent demonstrations. Moreover, Figure 13 shows that our retargeted data substantially reduces the undesired-contact penalty (often by a large margin) and yields smoother, less noisy curves, suggesting more stable learning dynamics and fewer spurious contacts during execution. Overall, our retargeting gives the policy better training data: the motions stay within kinematic limits and the contact timing is more consistent, which leads Fig. 14: Visual comparisons of classical IK and our HL-IK. We set the right-wrist end-effector to track an âˆž-shaped trajectory. Classical QP-based IK (blue, left) meets the EE targets but can yield less natural whole-arm configurations, whereas our PDF-HR-regularized HL-IK (gray, right) produces more human-like arm postures while tracking the same trajectory. 

to better training and easier deployment. 

F. Other Applications 

Pose Denoising. A critical application of our learned prior is pose denoising, which seeks to reconstruct physically plausible robot configurations from corrupted inputs. We leverage the pre-trained pose distance field as a fixed differentiable function to guide the optimization. Formally, the objective is to project a noisy query pose qinit onto the nearest point on the valid motion manifold. For evaluation, qinit is initialized via uniform sampling within joint limits. The pose is then iteratively refined by following the negative gradient of the distance field (Eq. (14)). As qualitatively demonstrated in Figure 15, the optimization effectively rectifies kinematic violations. The trajectory evolves from an initial state exhibiting severe self-penetration and unnatural artifacts (left) to a collision-free, plausible configuration (right). 0.651  0.341  0.217  0.150  0.014          

> 0.504 0.458 0.217 0.115 0.003
> 0.714 0.561 0.345 0.109 0.006
> Fig. 15: Visualization of the pose denoising process.

Human-Like Inverse Kinematics (HL-IK). HL-IK [7] intro-duces a human-like inverse kinematics setting: given only end-effector (EE) targets at runtime, the solver should track the EE while producing whole-arm configurations that remain human-like, without relying on full-body sensing. Unlike HL-IK [7], which explicitly predicts non-EE joint/landmark targets, our method supports this task implicitly by regularizing IK updates with the learned PDF-HR prior, making the approach more scalable and lightweight. Since the official HL-IK [7] implementation is not publicly available, we provide a qualitative demonstration. We define the right-wrist EE to follow a planar âˆž-shaped trajectory and compare a classical QP-based IK solver against our PDF-HR-regularized variant, as shown in Figure 14. Concretely, we integrate PDF-HR into a quadratic-programming IK solver for the Unitree G1 upper body. At each control step, IK solves for a joint increment âˆ†q that satisfies task-space objectives while remaining close to the learned motion manifold. Given the current configuration qt, we solve: 

min  

> âˆ†q
> 12

wtask J(qt)âˆ† q + r 22 + 12 Î»smooth âˆ†Î¸ 22

+ 12 Î»PDF-HR âˆ‡fÏ•(qt)âŠ¤âˆ†q 22,

(35) optionally subject to joint position and velocity limits. Here, J

and r denote the task Jacobian and residual (e.g., end-effector pose error). The PDF-HR term biases the update direction using the local normal of the learned manifold, âˆ‡fÏ•(qt): by penalizing motion along the normal, the optimizer favors steps within the tangent space, yielding kinematically plausible and human-like joint updates while still meeting the same task-space targets. This soft regularization avoids explicit projection or hard feasibility constraints, yet keeps the solution close to the distribution of demonstrated human-like motions. 

General Motion Tracking with PDF-HR. To test whether PDF-HR generalizes to off-the-shelf motion trackers, we inte-grate our pose-prior reward into the low-level tracking policy of TextOp [43]. TextOp is a real-time, interactive frame-work for text-driven humanoid motion generation and control, featuring a two-layer design: a high-level text-conditioned diffusion autoregressive model generates a kinematic reference trajectory from the current user command, while a low-level universal tracking policy executes the trajectory on the robot to achieve responsive and accurate control. We adopt TextOpâ€™s released low-level tracking policy as our baseline and follow the same training setup. Specifically, we train a general tracking policy on 10K motion clips sampled from AMASS [28] and LaFAN1 [13]. The only change from TextOp is that we additionally include the PDF-HR pose-prior reward (Eqs. (16) and (17)) during training, while keeping the network, observations, and all other reward terms unchanged. As shown in Figure 16a, our method consistently achieves lower joint pose MSE than the baseline throughout training. We further sweep the pose-prior weight w to study its effect on the convergence rate. Figure 16b indicates that increasing 

w noticeably accelerates convergence: the body-pose episode reward rises earlier and reaches a high plateau in fewer steps, while the final performance remains comparable across settings. Overall, these results suggest that PDF-HR serves as a plug-and-play regularizer that improves both sample efficiency and tracking accuracy for general motion tracking. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Step 1e4 

> 0.8
> 1.0
> 1.2
> 1.4
> 1.6
> 1.8
> 2.0
> 2.2 Joint Pos Error
> General Tracking in 10K motion data
> base Ours

(a) Joint pose MSE per step 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Step 1e3  

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8 Body Pose Episode Reward
> General Tracking in 10K motion data
> base w = 0.1 w = 0.5 w = 1.0

(b) Body pose episode reward 

Fig. 16: The key learning curves of training general tracking controller with original reward and our pose-prior integrated reward in 10K motion data. 

Goal-conditioned Task. We further validated PDF-HR on two goal-conditioned tasks defined in AMP [36]: Target Location (navigating to a random target position) and Target Heading (moving along a randomly changing direction at variable speeds). By incorporating our pose-prior reward into the AMP training objective with a weight wP = 0 .1 and scale 

Î±P = 1.0, we evaluated the test episode length and task return. As illustrated in Figure 17, our method accelerates the convergence of the test episode length. This indicates that PDF-HR guides the agent to master basic locomotion skills faster. Consequently, this leads to better completion of high-level goals, resulting in higher task returns compared to the original AMP. 

Fig. 17: The learning curves of AMP and our method on goal-conditioned tasks.
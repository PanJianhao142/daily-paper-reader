---
title: "Event-T2M: Event-level Conditioning for Complex Text-to-Motion Synthesis"
title_zh: Event-T2M：用于复杂文本到动作合成的事件级条件化
authors: "Seong-Eun Hong, JaeYoung Seon, JuYeong Hwang, JongHwan Shin, HyeongYeop Kang"
date: 2026-02-04
pdf: "https://arxiv.org/pdf/2602.04292v1"
tags: ["keyword:MDM", "query:课题"]
score: 9.0
evidence: 基于扩散模型的人体运动合成与时序对齐
tldr: 针对现有文本生成动作模型在处理复杂多动作指令时易出现动作遗漏或顺序混乱的问题，本文提出了 Event-T2M 框架。该框架引入了“事件”定义，将复杂文本分解为语义自洽的动作单元，并通过基于事件的交叉注意力机制进行扩散生成。同时，研究者构建了首个按事件数量分层的基准数据集 HumanML3D-E。实验证明，该方法在处理多事件复杂动作时显著优于现有模型，提升了动作生成的准确性与自然度。
motivation: 现有模型将复杂多动作文本压缩为单一嵌入，导致在生成长序列动作时经常出现动作缺失、顺序错误或过渡不自然的问题。
method: 提出 Event-T2M 框架，通过将文本分解为独立事件并利用运动感知检索模型编码，结合 Conformer 块中的事件级交叉注意力机制实现精确控制。
result: 在 HumanML3D 等标准数据集上达到领先水平，并在新提出的分层基准 HumanML3D-E 上展现出随事件复杂度增加而扩大的性能优势。
conclusion: 事件级条件建模是提升文本生成动作系统处理复杂、多阶段指令泛化能力的有效原则。
---

## 摘要
文本到动作生成在扩散模型的推动下取得了显著进展，但现有系统通常将复杂的多个动作提示词压缩为单个嵌入，导致动作遗漏、顺序错乱或过渡不自然。在这项工作中，我们转变视角，引入了“事件”的原则性定义，即文本提示词中最小的语义自包含动作或状态变化，且能与动作片段在时间上对齐。基于此定义，我们提出了 Event-T2M，这是一个基于扩散的框架，它将提示词分解为事件，使用动作感知检索模型对每个事件进行编码，并通过 Conformer 块中的基于事件的交叉注意力机制将其整合。现有的基准测试混合了简单和多事件提示词，导致不清楚在单动作上成功的模型是否能泛化到多动作情况。为了解决这个问题，我们构建了 HumanML3D-E，这是第一个按事件数量分层的基准测试。在 HumanML3D、KIT-ML 和 HumanML3D-E 上的实验表明，Event-T2M 在标准测试中达到了最先进基准的水平，同时随着事件复杂度的增加，其表现优于这些基准。人类评估验证了我们事件定义的合理性、HumanML3D-E 的可靠性，以及 Event-T2M 在生成保持顺序且自然度接近真实值的多事件动作方面的优越性。这些结果确立了事件级条件化作为一个可泛化的原则，推动文本到动作生成迈向超越单动作提示词的新阶段。

## Abstract
Text-to-motion generation has advanced with diffusion models, yet existing systems often collapse complex multi-action prompts into a single embedding, leading to omissions, reordering, or unnatural transitions. In this work, we shift perspective by introducing a principled definition of an event as the smallest semantically self-contained action or state change in a text prompt that can be temporally aligned with a motion segment. Building on this definition, we propose Event-T2M, a diffusion-based framework that decomposes prompts into events, encodes each with a motion-aware retrieval model, and integrates them through event-based cross-attention in Conformer blocks. Existing benchmarks mix simple and multi-event prompts, making it unclear whether models that succeed on single actions generalize to multi-action cases. To address this, we construct HumanML3D-E, the first benchmark stratified by event count. Experiments on HumanML3D, KIT-ML, and HumanML3D-E show that Event-T2M matches state-of-the-art baselines on standard tests while outperforming them as event complexity increases. Human studies validate the plausibility of our event definition, the reliability of HumanML3D-E, and the superiority of Event-T2M in generating multi-event motions that preserve order and naturalness close to ground-truth. These results establish event-level conditioning as a generalizable principle for advancing text-to-motion generation beyond single-action prompts.

---

## 论文详细总结（自动生成）

这是一份关于论文 **《Event-T2M: Event-level Conditioning for Complex Text-to-Motion Synthesis》** 的深度结构化总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：尽管现有的文本生成动作（Text-to-Motion）模型在标准基准上取得了高分，但在处理**复杂的多动作指令**（例如“向前跑，然后停下，最后挥手”）时经常失败。
*   **核心问题**：现有模型通常将整个文本提示词压缩为单一的全局嵌入（如 CLIP 嵌入），这导致模型在生成长序列动作时容易出现动作遗漏、顺序错误或动作间的过渡极不自然。
*   **研究背景**：目前的评估体系（如 HumanML3D）混合了简单和复杂的指令，掩盖了模型在处理时序逻辑和结构化行为方面的缺陷。

### 2. 论文提出的方法论
*   **核心思想**：引入“**事件（Event）**”的概念，将其定义为文本中最小的语义自包含动作单元。通过将复杂文本分解为事件序列，并对每个事件进行独立编码和细粒度对齐，实现对复杂动作的精确合成。
*   **关键技术细节**：
    1.  **文本到事件分解**：利用大语言模型（LLM，如 Gemini 2.5 Flash）根据预设规则将原始文本拆分为多个事件子句。
    2.  **事件 Token 编码**：使用专门针对运动对齐训练的 **TMR (Text-to-Motion Retrieval)** 编码器，将每个子句转化为事件 Token，同时保留一个全局 Token 以维持整体连贯性。
    3.  **Event-T2M 架构**：基于扩散模型（Diffusion Model），核心组件包括：
        *   **ECA (基于事件的交叉注意力)**：在 Conformer 块中将运动查询（Query）与事件 Token（Key/Value）进行匹配，确保动作序列与文本事件在时序上对齐。
        *   **ATII (自适应文本信息注入器)**：通过通道级门控机制，根据局部运动状态动态过滤全局文本语义。
        *   **LIMM (局部信息建模模块)**：利用 1D 卷积增强运动的局部平滑度和物理接触的稳定性。
*   **算法流程**：输入文本 $\rightarrow$ LLM 分解 $\rightarrow$ TMR 编码 $\rightarrow$ 扩散模型去噪（结合 ECA 和 ATII 引导）$\rightarrow$ 生成运动序列。

### 3. 实验设计
*   **数据集**：使用了 **HumanML3D**、**KIT-ML** 和 **Motion-X** 三个主流数据集。
*   **新基准 (Benchmark)**：构建并发布了 **HumanML3D-E**，这是首个根据事件数量（$\ge 2, \ge 3, \ge 4$ 个事件）对测试集进行分层的基准，专门用于评估模型处理复杂指令的能力。
*   **对比方法**：对比了包括 T2M-GPT、MoMask、AttT2M、GraphMotion、Light-T2M、MoGenTS 和 MARDM 在内的十余种最先进（SOTA）模型。
*   **评估指标**：R-Precision（准确率）、FID（真实度）、MM-Dist（多模态距离）和 MModality（多样性）。

### 4. 资源与算力
*   **硬件环境**：使用了 **2 块 NVIDIA RTX 4090 GPU**。
*   **训练细节**：
    *   在 HumanML3D 上训练 600 个 epoch，在 KIT-ML 上训练 1000 个 epoch。
    *   使用 AdamW 优化器，学习率为 $1 \times 10^{-4}$。
    *   推理阶段采用 UniPC 采样器，仅需 10 个步长即可完成生成。
*   **推理延迟**：模型本身推理极快（约 0.17s），但 LLM 预处理阶段会增加约 1.43s 的额外开销。

### 5. 实验数量与充分性
*   **实验规模**：
    *   **定量实验**：在三个标准数据集和三个分层子集上进行了详尽测试。
    *   **消融实验**：针对 ECA 模块、文本编码器（CLIP vs TMR）、架构选择（Transformer vs Conformer）、采样步数和 CFG 尺度等进行了多组对比。
    *   **用户研究**：开展了两项人类感知实验（共 41 名参与者），分别验证了事件分解的合理性和生成动作的感知质量。
*   **充分性评价**：实验设计非常充分且客观。通过引入分层基准，论文清晰地展示了模型在不同复杂度下的性能增益，避免了被简单样本的平均分所误导。

### 6. 主要结论与发现
*   **性能优势**：Event-T2M 在标准测试中保持 SOTA 水平，而在**复杂指令（4个及以上事件）**下，其性能显著超越所有基准模型（如 FID 显著降低，R-Precision 显著提升）。
*   **事件级控制的重要性**：显式的事件分解能有效防止动作遗漏和顺序混乱，使生成的长序列动作在逻辑上更符合人类直觉。
*   **编码器影响**：相比于通用的 CLIP，专门的运动检索模型（TMR）能提供更丰富的运动语义，是实现高质量合成的关键。

### 7. 优点（亮点）
*   **视角创新**：不再将文本视为单一向量，而是将其视为可对齐的事件流，解决了长序列合成的根本痛点。
*   **评估贡献**：HumanML3D-E 基准填补了复杂动作生成评估的空白，对后续研究具有重要参考价值。
*   **效率平衡**：在保证高性能的同时，通过轻量化设计和快速采样技术，保持了较好的推理效率。

### 8. 不足与局限
*   **LLM 依赖**：推理过程依赖 LLM 进行文本预处理，这增加了系统的复杂性和端到端延迟。
*   **物理约束缺失**：虽然引入了局部建模，但仍未完全解决人体运动中的物理真实性问题（如足部滑动或穿模）。
*   **应用限制**：目前主要关注单人动作，尚未扩展到复杂的人机交互或人与环境交互的场景。

（完）

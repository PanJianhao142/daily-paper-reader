---
title: Discrete Stochastic Localization for Non-autoregressive Generation
title_zh: 面向非自回归生成的离散随机定位
authors: "Yunshu Wu, Jiayi Cheng, Partha Thakuria, Rob Brekelmans, Evangelos E. Papalexakis, Greg Ver Steeg"
date: 2026-02-18
pdf: "https://arxiv.org/pdf/2602.16169v1"
tags: ["keyword:MDM"]
score: 6.0
evidence: 掩码扩散语言模型与迭代优化
tldr: 非自回归（NAR）生成虽能降低延迟，但常受误差累积困扰。本文提出离散随机定位（DSL）方法，通过在连续腐蚀水平上训练单一的信噪比不变去噪器，将中间草图噪声与掩码式端点腐蚀统一在扩散 Transformer 中。DSL 在 OpenWebText 任务上显著提升了采样效率，仅需 1/4 的评估次数即可超越基线，并在高预算下达到自回归生成质量，增强了模型的自纠错与不确定性校准能力。
motivation: 旨在解决非自回归生成在迭代优化过程中因自生成草图导致的误差累积和分布偏移问题，提高采样效率。
method: 提出 DSL 方法，通过在扩散 Transformer 中训练一个跨越连续腐蚀水平的信噪比不变去噪器，连接中间草图与掩码腐蚀。
result: 在 OpenWebText 实验中，DSL 仅需约 1/4 的计算开销即可超越现有基线，且在高步数下能匹配自回归模型的生成质量。
conclusion: DSL 通过改进自纠错和不确定性校准，显著提升了掩码重采样在非自回归生成中的计算效率和生成性能。
---

## 摘要
非自回归 (NAR) 生成通过并行预测多个 token 来降低解码延迟，但迭代细化 (iterative refinement) 往往会受到自生成草稿下的误差累积和分布偏移的影响。掩码扩散语言模型 (MDLMs) 及其重掩码采样器 (如 ReMDM) 可被视为现代 NAR 迭代细化，其中生成过程会反复修正部分观测到的草稿。在这项工作中，我们展示了仅通过训练就能显著提高 MDLM/ReMDM 采样的步数效率。我们提出了 DSL (离散随机定位)，它在连续的损坏水平上训练单个信噪比 (SNR) 不变的去噪器，在单个 Diffusion Transformer 中桥接了中间草稿噪声和掩码式端点损坏。在 OpenWebText 数据集上，DSL 微调在低步数预算下实现了显著的 MAUVE 增益，以约少 4 倍的去噪器评估次数超越了 MDLM+ReMDM 基线，并在高预算下达到了与自回归相当的质量。分析表明，该方法改进了自我修正和不确定性校准，使重掩码过程的计算效率显著提高。

## Abstract
Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \(\sim\)4$\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.
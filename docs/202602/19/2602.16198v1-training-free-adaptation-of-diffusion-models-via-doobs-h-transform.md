---
title: "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform"
title_zh: 基于 Doob $h$-变换的扩散模型免训练适配
authors: "Qijie Zhu, Zeqi Ye, Han Liu, Zhaoran Wang, Minshuo Chen"
date: 2026-02-18
pdf: "https://arxiv.org/pdf/2602.16198v1"
tags: ["keyword:MDM"]
score: 6.0
evidence: 扩散模型的自适应
tldr: "本研究针对预训练扩散模型在适配多样化应用时存在的计算开销大、依赖奖励函数可微性及缺乏理论保证等问题，提出了 DOIT（Doob-Oriented Inference-time Transformation）。这是一种无需训练且计算高效的适配方法，利用 Doob's h-变换在推理阶段动态修正采样过程，将生成分布引导至高奖励目标分布。该方法支持非可微奖励，并提供了高概率收敛的理论保证。实验证明，DOIT 在 D4RL 离线强化学习基准测试中优于现有 SOTA 方法，且保持了极高的采样效率。"
motivation: 现有的扩散模型适配方法通常需要昂贵的重新训练，或者对奖励函数有可微性等严格限制，且缺乏坚实的理论支撑。
method: "提出基于 Doob's h-变换的测度传输框架，通过在推理阶段对采样路径进行动态修正，实现向高奖励分布的无训练适配。"
result: 在 D4RL 离线强化学习基准测试中，该方法在保持采样效率的同时，性能持续超越了现有的最先进基准模型。
conclusion: DOIT 为非可微奖励下的扩散模型适配提供了一个具有理论保障、计算高效且无需训练的通用解决方案。
---

## 摘要
适配方法一直是释放预训练扩散模型在各种应用中变革性力量的主力军。现有方法通常将适配目标抽象为奖励函数，并引导扩散模型生成高奖励样本。然而，这些方法由于需要额外训练而可能产生高昂的计算开销，或者依赖于对奖励函数的严格假设（如可微性）。此外，尽管它们在经验上取得了成功，但很少建立理论依据和保证。在本文中，我们提出了 DOIT（Doob-Oriented Inference-time Transformation，面向 Doob 的推理时变换），这是一种无需训练且计算高效的适配方法，适用于通用的、不可微的奖励。我们方法的核心框架是一种测度传输（measure transport）表述，旨在将预训练的生成分布传输到高奖励的目标分布。我们利用 Doob $h$-变换来实现这种传输，它对扩散采样过程引入了动态修正，并在不修改预训练模型的情况下实现了高效的基于模拟的计算。在理论上，我们通过刻画动态 Doob 修正中的逼近误差，建立了到目标高奖励分布的高概率收敛保证。在经验上，在 D4RL 离线强化学习基准测试中，我们的方法在保持采样效率的同时，始终优于最先进的基准方法。

## Abstract
Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.
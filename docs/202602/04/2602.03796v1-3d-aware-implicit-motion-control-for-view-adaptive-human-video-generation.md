---
title: 3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation
title_zh: 面向视角自适应人体视频生成的3D感知隐式运动控制
authors: "Zhixue Fang, Xu He, Songlin Tang, Haoxian Zhang, Qingfeng Li, Xiaoqiang Liu, Pengfei Wan, Kun Gai"
date: 2026-02-03
pdf: "https://arxiv.org/pdf/2602.03796v1"
tags: ["keyword:MDM", "query:课题"]
score: 7.0
evidence: 讨论了使用隐式3D感知表示的人体运动控制和视频生成。
tldr: 针对现有视频生成中2D姿态受限于视角、3D显式模型（如SMPL）精度不足的问题，本文提出3DiMo。该方法通过隐式、视角无关的运动表示，将驱动帧蒸馏为运动Token并注入预训练生成器。结合多视角监督和渐进式几何引导，实现了高保真度的动作迁移与灵活的相机视角控制，显著提升了生成视频的质量。
motivation: 现有的2D或显式3D运动控制方法难以兼顾视角灵活性与生成器的内在3D先验，导致动作不准或视角受限。
method: 提出一种隐式运动编码器，通过交叉注意力注入视角无关的运动Token，并利用多视角数据和退火式几何监督进行训练。
result: 实验证明3DiMo在保持动作忠实度的同时，支持灵活的文本驱动相机控制，在视觉质量和运动保真度上均优于现有方法。
conclusion: 该研究证明了隐式3D感知运动控制在提升视频生成视角适应性和动作一致性方面的有效性。
---

## 摘要
现有的视频生成人体运动控制方法通常依赖于2D姿态或显式3D参数化模型（如SMPL）作为控制信号。然而，2D姿态将运动与驱动视角硬性绑定，阻碍了新视角合成。显式3D模型虽然具有结构信息，但存在固有的不准确性（如深度歧义和动力学不准确），当作为强约束使用时，会掩盖大规模视频生成器强大的内在3D感知能力。在这项工作中，我们从3D感知的角度重新审视运动控制，提倡一种隐式的、视角无关的运动表示，使其自然地与生成器的空间先验对齐，而不是依赖于外部重建的约束。我们提出了3DiMo，它将运动编码器与预训练的视频生成器联合训练，将驱动帧蒸馏为紧凑的、视角无关的运动token，并通过交叉注意力机制进行语义注入。为了培养3D感知能力，我们使用丰富的视角监督（即单视角、多视角和移动摄像机视频）进行训练，强制要求在不同视角下保持运动一致性。此外，我们使用了辅助几何监督，仅在早期初始化时利用SMPL，并将其逐渐退火至零，使模型能够从外部3D引导过渡到从数据和生成器先验中学习真实的3D空间运动理解。实验证实，3DiMo能够忠实地再现驱动运动，并具有灵活的文本驱动摄像机控制，在运动保真度和视觉质量方面均显著超越了现有方法。

## Abstract
Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.
---
title: Conditional Flow Matching for Visually-Guided Acoustic Highlighting
title_zh: 视觉引导声学突出的条件流匹配
authors: "Hugo Malard, Gael Le Lan, Daniel Wong, David Lou Alon, Yi-Chiao Wu, Sanjeel Parekh"
date: 2026-02-03
pdf: "https://arxiv.org/pdf/2602.03762v1"
tags: ["keyword:FM"]
score: 6.0
evidence: 条件流匹配框架
tldr: 本研究针对视觉引导的音频突出任务，旨在使音频重混缩与视频画面保持一致。针对判别式模型难以处理音频重混歧义性的问题，本文将其转化为生成式任务，提出了条件流匹配（CFM）框架。通过引入“展开损失”来稳定长程流积分并减少轨迹偏移，结合跨模态融合模块实现精准的声源增强。实验证明该方法在性能上显著超越了现有最先进技术，验证了生成式模型在音频重混领域的优越性。
motivation: 现有的判别式模型难以处理音频重混缩中固有的多对一映射歧义，导致视觉与听觉焦点不匹配。
method: 提出一种基于条件流匹配（CFM）的生成式框架，并引入“展开损失”以纠正迭代过程中的轨迹偏移，同时利用跨模态融合模块进行声源选择。
result: 大量定量和定性评估表明，该方法在各项指标上均一致优于之前的判别式最先进方法。
conclusion: 研究结果表明，视觉引导的音频重混缩任务通过生成式建模能够得到更有效的解决。
---

## 摘要
视觉引导的声学突出旨在根据伴随的视频重新平衡音频，从而创造连贯的视听体验。尽管视觉显著性和增强已得到广泛研究，但声学突出仍未得到充分探索，这往往导致视觉和听觉焦点之间的不一致。现有方法使用判别模型，这些模型难以处理音频重混缩中固有的歧义性，即在平衡不佳和平衡良好的音频混音之间不存在自然的一一对应关系。为了解决这一局限性，我们将该任务重新定义为一个生成问题，并引入了一个条件流匹配（CFM）框架。基于流的迭代生成中的一个关键挑战是，在选择要增强的正确声源时的早期预测误差会随步骤累积，并将轨迹推离流形。为了解决这个问题，我们引入了一种展开损失（rollout loss），用于惩罚最终步骤的漂移，从而鼓励自校正轨迹并稳定长程流积分。我们进一步提出了一个条件模块，在向量场回归之前融合音频和视觉线索，从而实现显式的跨模态声源选择。广泛的定量和定性评估表明，我们的方法始终优于之前的最先进判别方法，证明了视觉引导的音频重混缩最好通过生成建模来解决。

## Abstract
Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling.
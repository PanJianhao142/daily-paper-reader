---
title: "PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer"
title_zh: PlanTRansformer：基于目标条件 Transformer 的统一预测与规划
authors: "Constantin Selzer, Fabina B. Flohr"
date: 2026-02-03
pdf: "https://arxiv.org/pdf/2602.03376v1"
tags: ["query:课题"]
score: 6.0
evidence: 轨迹预测与运动预测
tldr: 针对自动驾驶中预测与规划脱节的问题，本文提出PlanTRansformer (PTR)，一个统一的高斯混合Transformer框架。PTR集成了目标条件预测、动态可行性、交互感知和车道拓扑推理，并引入教师-学生训练策略以解决推理时意图缺失的问题。该模型在预测精度和规划误差上均显著优于现有基准，且具有架构无关的通用性，能有效提升复杂场景下的驾驶性能。
motivation: 旨在解决自动驾驶中预测模型因缺乏意图监督而与规划需求脱节，以及现有模型忽视动态可行性和碰撞规避的问题。
method: 提出一种集成了目标条件预测和动态约束的统一Transformer框架，并采用教师-学生策略在训练中逐步掩码智能体指令以对齐推理环境。
result: "PTR在预测指标上优于MTR，且在5秒时界的规划误差上比GameFormer降低了15.5%。"
conclusion: PTR通过统一预测与规划架构，显著提升了自动驾驶系统在交互感知和轨迹规划方面的准确性与可行性。
---

## 摘要
轨迹预测与规划是自动驾驶中基础但相互脱节的组件。预测模型在未知意图下预测周围智能体的运动，产生多模态分布；而规划则假设自车目标已知，并生成确定性轨迹。这种不匹配造成了一个关键瓶颈：预测缺乏对智能体意图的监督，而规划则需要这些信息。现有的预测模型尽管在基准测试中表现强劲，但往往与避障和动力学可行性等规划约束脱节。我们提出了 Plan TRansformer (PTR)，这是一个统一的高斯混合 Transformer 框架，集成了目标条件预测、动力学可行性、交互感知和车道级拓扑推理。一种教师-学生训练策略在训练过程中逐步掩蔽周围智能体的指令，以对齐智能体意图不可用的推理条件。与基准模型 Motion Transformer (MTR) 相比，PTR 在边缘/联合 mAP 上分别实现了 4.3%/3.5% 的提升；与 GameFormer 相比，在 5 秒时界的规划误差降低了 15.5%。这种与架构无关的设计使其能够应用于各种基于 Transformer 的预测模型。项目网站：https://github.com/SelzerConst/PlanTRansformer

## Abstract
Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer
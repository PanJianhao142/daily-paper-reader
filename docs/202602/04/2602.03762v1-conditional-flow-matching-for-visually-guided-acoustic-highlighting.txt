Title: Conditional Flow Matching for Visually-Guided Acoustic Highlighting

URL Source: https://arxiv.org/pdf/2602.03762v1

Published Time: Wed, 04 Feb 2026 02:40:01 GMT

Number of Pages: 15

Markdown Content:
## Conditional Flow Matching for Visually-Guided Acoustic Highlighting 

Hugo Malard 1*, Gael Le Lan 2, Daniel Wong 2, David Lou Alon 2, Yi-Chiao Wu 2, Sanjeel Parekh 21LTCI, T´ el´ ecom Paris, Institut Polytechnique de Paris 

> 2

Meta 

Abstract 

Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio–visual experience. While visual saliency and enhancement have been widely studied, acoustic high-lighting remains underexplored, often leading to misalign-ment between visual and auditory focus. Existing ap-proaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natu-ral one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key chal-lenge in iterative flow-based generation is that early predic-tion errors —in selecting the correct source to enhance— compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajec-tories and stabilizing long-range flow integration. We fur-ther propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative ap-proach, establishing that visually-guided audio remixing is best addressed through generative modeling. 

1. Introduction 

The recent proliferation of video content creation and con-sumption underscores the need for meticulous curation of both visual and audio elements to deliver an engaging user experience. While visual cue manipulation—through techniques such as optimal viewpoint selection or post-editing—has been a long-standing focus in media produc-tion [19, 22, 25], the audio domain has not seen equiva-lent advancements. This disparity frequently results in a noticeable disconnect between visual saliency and acoustic 

> *Work done during an internship at Meta.

Figure 1. We propose to cast visually-guided acoustic highlight-ing in the flow matching setup. Acoustic sources are iteratively highlighted or not based on the visual context. This simplified il-lustration shows that when audio and visual cues are opposed, the source is subdued. However, when they showcase the same event, the source is enhanced. 

emphasis. For instance, when a person speaks on camera, viewers intuitively expect their voice to be clear and promi-nent. However, without deliberate mixing, their speech may be overwhelmed by background noise or music, breaking the perceptual alignment between what is seen and what is heard. Raw daily recordings, in particular, often contain poorly balanced audio due to recording device limitations where microphones attached to video cameras capture all sounds indiscriminately. To bridge this crucial gap, the task of Visually-guided Acoustic Highlighting (VisAH) was introduced [14], aim-ing to automatically transform poorly balanced audio by us-ing the accompanying video stream as guidance, ultimately creating a more harmonious audio-visual experience. This process involves rebalancing loudness of different audio sources, like speech, music, and sound effects, to reflect their relative prominence as implied by the visual context. Previously, this task was approached using a discrimi-native paradigm [14], relying on sound source separation architectures, such as DEMUCS [35]. Thus, training for a one-to-one mapping. However, we observe that given the 

> arXiv:2602.03762v1 [eess.AS] 3 Feb 2026

video stream, there are several ways of creating poor or well-balanced audio mixes. In other words, audio remix-ing (or rebalancing) is fundamentally a task of transporting from one distribution to another. Given its many-to-many nature we argue that a generative approach is better suited than a purely discriminative one. Indeed, prior work [28] shows that in many-to-many settings, where both input and output contain mutual uncertainty, generative models can exploit information from the marginal to capture correla-tions and latent structure that discriminative models ignore. Additionally, recent works have shown that, for multiple audio tasks where there is no clear one-to-one mapping, like speech enhancement or audio source separation, gen-erative approaches significantly outperform their discrimi-native counterparts [2, 13, 26, 36]. Among generative modeling techniques, flow match-ing [23] provides a principled framework for learning a con-tinuous transformation between input and target distribu-tions, making it a natural fit for remixing as shown in Fig 1. In this formulation, a neural network learns the vector field that guides samples from the source distribution towards the target through a sequence of intermediate states. For the vi-sually guided acoustic highlighting task, the main difficulty lies in identifying the audio sources to enhance or suppress. Ideally, this crucial operation must occur from the very first step of the flow matching process, where the model begins transforming the input distribution towards the target one. As this step is inherently difficult, errors, even if small, can easily arise early on. In an iterative framework like flow matching, such early inaccuracies propagate through subse-quent steps, compounding over time and driving the trajec-tory away from the the data manifold. This accumulation of error substantially degrades model performance and mo-tivates our proposal of the rollout loss , an additional loss to stabilize predictions across steps. Furthermore, the source identification problem heavily relies on the conditioning of the model. In prior work, this was based solely on visual cues—either features extracted from an image encoder or textual captions describing the entire visual scene. These image-only features were passed through a small transformer and then fed to the main denois-ing model. Consequently, the latter had to implicitly learn the correspondence between auditory and visual signals—a task that goes beyond its primary goal of regressing the au-dio from a conditioning signal. We argue that this “division of responsibilities” can be made more effective. By inte-grating audio features directly into the visual encoder, we enable cross-modal filtering within the conditioning path-way itself. Such early fusion allows the conditioning net-work to preserve only the visual components relevant to the target sound sources, yielding a compact, audio-aware representation. The main model then receives conditioning features that are already semantically aligned with the audio content, allowing it to “concentrate” entirely on accurate au-dio regression rather than on discovering cross-modal cor-respondences. In summary, our contributions are threefold: • We reformulate the visually guided acoustic highlighting problem as a generative task rather than a discriminative one, and demonstrate the advantages of this formulation both qualitatively and quantitatively. • We introduce a rollout loss that mitigates the error accu-mulation inherent to standard flow matching for such a task, and analyze its impact in detail. • We design an improved conditioning module that enables earlier fusion of audio and visual features before the ve-locity field estimation stage, improving performance and achieving state-of-the-art results. 

2. Proposed Method 

2.1. Task Formulation 

Visually-guided Acoustic Highlighting. The visually-guided acoustic highlighting task was introduced to ad-dress the disconnect often observed between visual and acoustic saliency in video content. Its objective is to uti-lize the accompanying video stream as guidance to trans-form a poorly balanced audio track into a well-balanced one exhibiting appropriate highlighting effects. To facili-tate training models for this task, the Muddy Mix Dataset [14] was curated, leveraging the meticulous audio-visual crafting found in movies, providing a form of ”free supervi-sion”. The input audio, representing poorly mixed content is generated using a pseudo-data generation process that simulates real-world mixing deficiencies. This process sys-tematically disturbs the original high-quality movie audio through three key steps: (1) Separation, decomposing the audio into source components (speech, music, and sound effects); (2) Adjustment, altering the relative levels of these separated sources by applying suppression or emphasis us-ing selected strength levels (high, moderate, low); and (3) Remixing, linearly combining the adjusted sources to form the ill-balanced input audio. 

2.2. Our Approach 

Conditional Flow Matching (CFM). Unlike discrimina-tive models, which learn a point-wise direct one-to-one mapping between inputs and targets, flow matching meth-ods focus on aligning probability distributions. They model a continuous transformation between a source and tar-get distribution, enabling many-to-many correspondences rather than explicit pairwise matches. Given two distribu-tions from which we can sample during training, (x0, x 1) ∼

π0 × π1, where x0 ∈ Rd is typically drawn from a base distribution (e.g., a standard Gaussian) and x1 ∈ Rd repre-sents a sample from the target data distribution, flow match-ing learns a time-dependent velocity field that continuously transports samples from π0 to π1 [23, 42]. An interpolant between the two samples is defined as 

xt = (1 − t)x0 + tx 1, (1) whose evolution follows the conditional ODE 

dx t

dt = x1 − xt

1 − t . (2) In practice, a neural network vθ (xt, t ) is trained to approxi-mate this velocity field field by minimizing 

LCFM (θ) = Et,x 0,x 1

"

x1 − xt

1 − t − vθ (xt, t )

> 22

#

. (3) 

Conditional Flow Matching for Acoustic Highlighting. 

We propose to cast the acoustic highlighting problem as an instance of conditional flow matching, where the model has to learn the flow that goes from a poorly-balanced to a well-balanced audio distribution, while being conditioned on the visual cues. We argue that this formulation is partic-ularly well-suited to the task, as a given video may admit multiple plausible high-quality mixes, as well as diverse degraded versions. Consequently, matching distributions rather than individual instances better captures the under-lying variability. More formally, plugging into the condi-tional flow matching setup mentioned above, we are trying to minimize: 

LCFM (θ) = Et,x 0,x 1 [∥(x1−xt)/(1 −t)−vθ (xt, t, c )∥22] (4) Where x0 and x1 are audios from the poorly-balanced and well-balanced audio distributions, respectively, c is the vi-sual conditioning, and t ∼ U (0 , 1) the sampled timestep. During training, we expose the model to multiple distinct inputs corresponding to the same target across epochs. We do so by randomly generating poorly balanced audio sam-ples on-the-fly as opposed to a fixed dataset used in [14]. Also, motivated by prior work that shows flow matching benefits from pretrained discriminative initialization [20, 47], we adapt the VisAH architecture [14] and training objective to the generative setting while retaining its pre-trained weights. Additional technical details in Sec. 3.1. 

2.2.1. CFM Training with Rollout Loss 

In a standard flow matching setup, each step is trained inde-pendently, and the ground-truth sample is used as input at every time step (i.e., the real x0 is used to obtain xt). However, flow matching can inherently generate drift along the predicted trajectory: small local inaccuracies in the velocity field may compound over successive steps, gradually deviating the model’s trajectory from the target distribution. To mitigate this drift, we propose using back-propagation through the flow, enabling the model not only   

> Figure 2. Illustration of rollout loss. Unlike the regular flow matching loss, which always uses a ground truth trajectory point as input, the rollout loss applies the loss after performing all the steps, forcing the model to have a coherent global trajectory to avoid compounding errors.

to learn the correct instantaneous velocity at each step but also to self-correct so that the overall trajectory converges toward the target endpoint. In the acoustic highlighting task, this effect is particu-larly pronounced. The key challenge lies in determining which sources should be enhanced or attenuated—a deci-sion that must be made during the earliest steps. Later steps primarily refine these earlier decisions. This asymmetry in step difficulty means that early-step prediction errors can have a disproportionate effect, propagating and amplifying throughout the entire trajectory. To make the model more robust to such drift, we intro-duce a rollout loss that supervises the entire generated tra-jectory. Unlike typical flow matching applications that use hundreds of integration steps, acoustic highlighting oper-ates on relatively close input–output distributions, allow-ing us to use only a few steps (four in our experiments, following [4]). This setup makes end-to-end backpropaga-tion through the full flow computationally feasible, enabling the model to jointly optimize intermediate updates and final consistency. Specifically, we perform a full rollout during training and add an auxiliary mean-squared-error (MSE) loss between the final predicted output and the ground-truth target (see Figure 2). This encourages consistency across steps and sta-bility under self-generated predictions, effectively reducing the impact of early-step errors. The overall training objec-tive is: 

L(θ) = LCFM (θ) + λEx0 [|| ˆxT − xT || 22]. (5) Where ˆxT denotes the model prediction after T recurrent applications of the flow operator starting from x0, and xT

is the corresponding ground-truth target. Unless otherwise stated, we set T = 4 steps (matching the total number of inference steps). 

Relation to Consistency Models. Our rollout loss con-ceptually resembles strategies proposed in consistency models [38, 39] for diffusion or flow-based generation, which aim to stabilize iterative predictions by enforcing agreement between nearby timesteps. However, unlike con-sistency models that match predictions across infinitesimal noise scales or time intervals, our rollout loss supervises the final prediction after multiple recurrent steps against the ground truth trajectory. This makes it focus more on the exposure-bias mitigation [29, 34]: the network is explicitly trained to recover from its own intermediate errors rather than only ensuring local consistency. The optimization of this objective is performed via backpropagation through the flow, akin in mechanism to D-Flow [3]. Unlike the latter—which uses this process to optimize the model in-put—we use it to optimize the model weights. Empirically, we found that this end-to-end constraint yields more stable long-range trajectories and avoids the error-amplification is-sues observed when applying standard consistency loss (see Table 3). 

2.2.2. Conditioning Module 

The VisAH model [14] builds upon a DEMUCS [35] ar-chitecture, conditioned in its latent space via a transformer-based conditioning module. Frame-wise features extracted from a CLIP vision encoder [31] or from caption embed-dings (T5 [32]) are processed by a temporal transformer and cross-attended into the U-Net latent space. However, the source to enhance is typically the dominant concept shared between the visual scene and the audio. Since the condi-tioning module in VisAH only accesses visual information, the burden of identifying the target source is largely shifted to the U-Net, which has access to the audio signal. This forces the latter to implicitly perform source selection in addition to the regression task. To better disentangle these roles, we aim for the conditioning module to determine the source to enhance, leaving the U-Net to specialize in vector field estimation. Inspired by recent cross-modal adapter designs [7, 8], we propose an adapter layer that injects audio features from an additional audio encoder into intermediate layers of the CLIP model. The goal of this design is to let the CLIP representations dynamically attend to complementary audio cues, enriching the visual embeddings with audio-specific context. Specifically, we compute cross-attention between a low-dimensional projection of both the CLIP and audio features, before mapping them back to the original embed-ding dimension: m(Fk, E ) = Att( EW E

> down

, F kW F

> down

)W E 

> up

, (6) where Fk denotes the CLIP features at layer k, E the au-dio features, W E 

> down

and W F 

> down

the projection layers, W E

> up

the up-projection layer, and Att denotes cross-attention. This operation effectively acts as a lightweight conditioning mechanism, enabling information exchange across modali-ties without retraining the CLIP backbone. Finally, we per-form a weighted sum of the original CLIP features with the 

> Figure 3. Overview of VisAH-FM architecture: Building upon the VisAH [14] backbone (purple box), VisAH-FM introduces time step conditioning, along with a novel rollout loss to guide flow matching training. Furthermore, we also incorporate an improved multimodal conditioning module (green box) where audio infor-mation is injected into intermediate CLIP representation through cross attention.

audio-aware adapted features using learnable weights and pass this onto the next layer k + 1 . Formally, we define the output of the adapter layer as: adapter (Fk, E, A ) = Fk + λE m(Fk, E ) (7) where λE is a learnable scalar controlling the contribution of the audio-aware features E. The overall conditioning mechanism is illustrated in Figure 3. Note that this con-ditioning can be applied to incorporate text features instead of, or along with, the audio. Importantly, initializing λE to zero ensures that the first forward pass matches the pretrained CLIP encoder, allow-ing us to fully leverage pretrained weights while gradually incorporating cross-modal cues during training. In practice, we use E, the audio features of a CLAP [48] encoder. 

Time (CFM) Conditioning. We encode the flow match-ing timestep using sinusoidal embeddings, relying on the positional encoding scheme as adopted for diffusion timesteps in [12], and append this embedding to the condi-tioning tokens provided to the latent transformer block via cross-attention. We ablate this choice in Appendix. 

3. Experiments 

3.1. Setting 

Implementation Details. To optimally leverage the pre-trained VisAH model, which was trained to directly predict Model Conditioning IB Score ↑ KLD ↓ LDif ↓ Mag ↓ Env ↓ Was ↓

Input - 28.14 20.74 18.36 22.69 6.29 1.96 VisAH CLIP 28.84 11.37 9.66 9.99 3.38 0.84 VisAH T5 28.92 11.71 9.63 10.22 3.44 0.88 VisAH-FM (Ours) CLIP-CLAP 29.12 9.70 7.77 8.28 2.74 0.63  

> Table 1. Main results. Best results are highlighted in bold , and all values except the LDif are multiplied by 100.

well-balanced audio rather than a vector field, we systemati-cally incorporate the source sample ( x0) into the model out-put to define the estimated vector field as vθ (xt, t, c ) = x0 −

uθ (xt, t, c ), where uθ (xt, t, c ) denotes the actual model. We trained the model for 50 epochs ( ∼23,500 iterations) using a learning rate of 1 × 10 −4, a cosine annealing sched-uler, and a batch size of 32. We observed an issue with audio saving quality, and therefore resampled the test set using the official code from [14]. We also trained another model on the exact same dataset as [14], without on-the-fly remixing in Appendix. Following VisAH, the input waveform was sampled at 44 kHz and converted to mono by averaging the stereo channels. Similarly to [4], we dis-cretized the time into four timesteps and sampled them uni-formly during training. Although our model performs sev-eral recurrent passes of the vector-field estimator at infer-ence, the resulting overhead is minor. Most computation is dominated by the conditioning encoders — CLIP ( ∼300 M parameters), CLAP ( ∼31 M), and optionally T5-XXL (11B) or InternVL (8B) - which are run once per video seg-ment, whereas the repeated U-Net vector-field estimator is comparatively lightweight ( ∼60M). Except for the changes mentioned in Section 2, we used the exact VisAH architec-ture. The adapter module is incorporated as the 18 th layer of the CLIP model. All the experiments are performed on the Muddy Mix Dataset. 

Metrics. We evaluate our models using the same met-rics as VisAH, along with the introduction of an additional remixing metric: • Signal metrics: We measure the magnitude dis-tance (Mag) [49], which evaluates audio quality in the time–frequency domain, and envelope distance (Env) [21], which assesses quality in the time domain. We also measure temporal alignment using the Wasser-stein distance (Was). • Semantic alignment: We compute the KL divergence (KLD) between the predicted logits of a PASST [17] model for the predicted and ground-truth audio. Addi-tionally, we report the ImageBind score (IB Score) [11] to quantify the similarity between predicted audio and im-age frames. Unlike VisAH, we report the IB Score di-rectly instead of ∆IB, as we find it more interpretable. • Remixing metric: Since the goal is to adjust the relative loudness across three categories—human speech, music, and sound effects—we employ a sound source separation model [45] to extract stems from both the predicted and ground-truth audio, and measure the loudness difference (LDif) between corresponding sources: 

LDif = 1

K

X

> k

loud( sk(ˆ x)) − loud( sk(x)) , (8) where ˆx and x denote the predicted and ground-truth au-dio, respectively, loud computes the loudness, and sk the separator of the k-th source from the audio. 

3.2. Quantitative Analysis 

3.2.1. Main Results 

Table 1 compares our proposed model VisAH-FM against the VisAH baseline using either image (CLIP) features or text (T5-encoded) features. Repurposing the model for flow matching, combined with the new conditioning mecha-nism, leads to a substantial improvement across all metrics, clearly surpassing the discriminative VisAH models. 

Ablation on Conditioning Module. Table 9 presents an ablation study of the different conditioning modalities and methods for the flow matching model. Without adapters, vision-based conditioning performs slightly better than text-based conditioning. Incorporating additional modalities into the image encoder consistently improves performance across all metrics. Notably, adding audio features en-coded with CLAP [48] yields substantially larger gains than adding textual features. We attribute this improvement to the stronger audio–visual interaction enabled by the condi-tioning module: the module determines which source to en-hance, allowing the velocity-field estimator to focus only on regression. As text captions originate from a VLM, they mainly contain information similar to image features (i.e., visual scene semantics), whereas audio features en-code the auditory scene. Since the target source typically corresponds to elements common to both the visual and au-ditory domains, integrating audio features inside the con-ditioning proves more beneficial than adding the textual counterpart. Additionally, CLAP extracted audio represen-tations are complementary to those from the DEMUCS en-coder, further enriching the model’s understanding of the Adapter Modality IB Score ↑ KLD ↓ LDif ↓                    

> ✓V+A 29.12 9.70 7.77
> ✓V + T 29.08 9.76 7.85
> ✓V+T+A 29.10 9.73 7.90
> ✗V29.09 9.79 7.87
> ✗T28.88 10.50 8.13
> Table 2. Ablation on the conditioning. We compare different con-ditioning methods (with and without adapter) and different modal-ities: audio (A), vision (V), and text (T).

auditory scene. Interestingly, incorporating both textual and audio features performs slightly worse than using only vi-sual and audio modalities, indicating that textual features are not needed when the audio is directly incorporated into the image encoder. This finding holds practical importance for real-world applications as the extraction of text features is very costly, requiring a forward pass through a VLM [5] and a large text encoder [32]. 

Ablation of the Rollout Loss. Table 3 ablates the roll-out loss and compares it with several standard methods de-signed to improve consistency across the flow matching steps. The addition of the rollout loss proves crucial: re-moving it leads to a marked performance drop i.e. approxi-mately +1.2 in KLD and +1.6 in loudness difference. The consistency loss encourages the vector field pre-dicted at time t to remain consistent with the one predicted at a slightly later time t + ∆ , when the input is advanced along the flow using the earlier prediction: 

Lcons = ∥vθ (xt, t )−StopGrad (vθ (xt−∆tvθ (xt, t ), t +∆ t)) ∥

(9) Interestingly, adding this loss substantially degrades perfor-mance. We hypothesize that this occurs because if the first estimate is noisy, the propagated estimate becomes even less accurate, effectively guiding the model in an incorrect direction. In contrast, the rollout loss anchors predictions to the ground-truth trajectory, preventing such drift and stabi-lizing training. Recent works have shown that injecting Gaussian noise into the input trajectories can improve robustness to out-of-distribution trajectories at inference time [1, 4, 24]. The row 

“Bridge Matching” reports the results of a model trained with this strategy (using Gaussian noise centered on zero with standard deviation of 1e-5). While it slightly outper-forms the standard flow matching baseline, it remains no-tably inferior to the version trained with rollout loss. In VAE-based setups, the latent space is assumed Gaussian, hence, added noise can mimic realistic errors. However, in our case, the input space corresponds to raw audio, and the injected noise effectively acts as white noise—significantly different from the model’s own prediction errors. Conse-quently, these perturbations fail to simulate realistic infer-ence conditions, and robustness improvements remain lim-ited. In contrast, the rollout loss directly reuses the model outputs as inputs, accurately reproducing the inference sce-nario and yielding significantly better performance. The row “FM Weighted” corresponds to a flow match-ing model trained with a loss weighted inversely propor-tional to the time variable, emphasizing samples closer to the input distribution. This baseline performs slightly worse than standard flow matching, indicating that simply priori-tizing early steps is insufficient. Instead, enabling the model to iteratively correct its errors across time proves essen-tial—highlighting the effectiveness of the rollout loss. Objective IB Score ↑ KLD ↓ LDif ↓                  

> FM + Rollout 29.09 9.79 7.87 FM 28.92 10.99 9.48 Rollout 28.94 9.92 7.71
> FM + Consistency 28.40 14.58 10.30 Bridge Matching 29.04 10.85 9.62 FM Weighted 28.86 11.16 9.44
> Table 3. Ablation of the rollout component. We compare different standard methods used to stabilize predictions with our rollout pro-posal. All models use standard CLIP conditioning (w/o adapter).

The row “Rollout” corresponds to a model trained with-out the standard flow matching loss, relying solely on the rollout loss. Remarkably, this variant substantially outper-forms vanilla flow matching and only slightly underper-forms the full model that combines flow matching and roll-out loss. This highlights the central role of the rollout mech-anism: even without the intermediate supervision and lin-earity prior imposed by flow matching, the model achieves strong performance. Moreover, this variant significantly surpasses the original VisAH model, suggesting that allow-ing the model to repeatedly refine its predictions through multiple forward passes over the same architecture yields meaningful improvements. This iterative refinement behav-ior is evocative of phenomena observed in large language models, where multi-step reasoning improves output qual-ity (e.g., chain-of-thought prompting [44, 46]). We further analyze the behavior of this model and its learned trajecto-ries in Section 3.2.2. 

Hyperparameter Sensitivity. Table 11 reports the sensi-tivity of our method to the hyperparameters of the rollout loss. The first block of the table varies the rollout coefficient 

λ (Eq. 5). Performance remains stable across a broad range, highlighting the robustness of the method to this hyperpa-rameter. We also vary the rollout horizon, defined as the number of steps before applying the rollout loss. When the horizon is shorter than the full inference trajectory, we ran-domly choose the starting step to ensure coverage of differ-ent temporal segments; otherwise, rollout begins from the λ Horizon Steps IB Score ↑ KLD ↓ LDif ↓                              

> 0.1 4429.12 9.85 8.02 0.3 4429.09 9.79 7.87 0.5 4429.12 9.93 7.80 1.0 4429.04 9.82 7.84 0.1 2429.08 9.68 7.94 0.3 410 29.10 9.79 8.35
> Table 4. Hyperparameter sensitivity. We vary the impact of the rollout loss as well as its horizon and the number of total steps used for inference.

first step. The last two rows correspond to settings where the rollout horizon is smaller than the total number of infer-ence steps. We observe no significant performance degra-dation, suggesting that rollout over only a subset of steps is sufficient to teach the model to recover from its own inter-mediate errors. 

3.2.2. Model Behavior 

In this section, we analyze how the rollout loss shapes the learned trajectories, and how these differ from those ob-tained with standard flow matching. 

Error Accumulation Analysis. Figure 4a reports the co-sine similarity between the predicted and ground-truth vec-tor fields over inference steps on the test set. Consistent with our hypothesis, the model trained without rollout loss progressively diverges from the ground-truth trajectory: ini-tial errors compound and drive the model toward out-of-distribution regions, leading to worsening predictions over time. In contrast, the model trained with both rollout and flow matching losses maintains high alignment throughout the trajectory, demonstrating that the rollout objective ef-fectively constrains error accumulation and stabilizes long-range trajectories. Interestingly, the model trained with roll-out loss alone exhibits small drift during the last steps, al-though it performs well overall (see Table 3), indicating that the learned trajectories differ from the linear one imposed in flow matching. 

Trajectory Linearity. Next, we study how the rollout loss affects the geometry of the learned trajectories. In flow matching, the optimal trajectory is theoretically linear in the data space; however, finite-capacity models and error accu-mulation during inference can induce nonlinear deviations. To quantify this effect, we compute both the variance of the predicted vector field across inference steps and the average discrete curvature of the trajectory on the test set. Formally, we define the discrete curvature at step t as 

Θt = arccos 

 ˆvt+1 · ˆvt

∥ˆvt+1 ∥ ∥ ˆvt∥ + ϵ



, (10) where ˆvt denotes the predicted vector field at time t. Θt

measures the angular deviation between successive vector field directions; higher values indicate stronger trajectory bending. Similarly, higher variance across steps reflects in-stability in the predicted vector fields. Figure 4b reports the average curvature and variance for models trained with (i) flow matching only, (ii) flow match-ing with rollout loss ( λ = 0 .3 and 1.0), and (iii) rollout loss only. The flow matching-only model exhibits strong non-linear deviations after the first step, consistent with error compounding and off-manifold drift. Introducing the roll-out loss markedly stabilizes the trajectory, yielding substan-tially lower curvature and variance. Interestingly, when the rollout term increases, trajectories become less linear again. Consequently, when using the rollout loss alone, the model learns highly non-linear trajectories, explaining the behav-ior observed in Figure 4a: the predicted vector fields are not aligned with the ground truth ones, as the model is taking a completely different path to reach the target. Figure 4c visualizes PCA-projected trajectories in the PASST [17] embedding space for models trained with and without the rollout loss, alongside the ground-truth trajec-tory. Both models follow a similar direction at the first step, but clear differences emerge by the second step: the flow matching-only model quickly deviates and produces noisy trajectories, whereas the rollout-trained model re-mains close to the ground truth. Moreover, even when both models initially move in an incorrect direction, the rollout-trained model partially corrects its path over subsequent steps, showing robustness to early prediction errors. 

3.3. Qualitative Analysis 

Visualization of the Rollout Loss. We display the wave-forms obtained after each step of the flow matching model trained with and without rollout loss, as well as the asso-ciated visual frames in Figure 5. As seen in the accom-panying visuals, the rollout-trained model rightly enhances speech across FM steps (framed in red), while the regular flow matching model does not. 

Visualization of VisAH-FM against VisAH. In Figure 6, top example shows a case where VisAH does not adequately boost the target source, whereas VisAH-FM consistently achieves stronger and more focused enhancement. The bot-tom example illustrates another case where VisAH fails to sufficiently enhance the primary source and instead ampli-fies a secondary event near the end of the clip. In contrast, our model correctly emphasizes the dominant event at the beginning while only mildly enhancing the latter one. Nev-ertheless, the undesired source is not fully suppressed, in-dicating that VisAH-FM still exhibits residual artifacts and leaves room for further improvement. 1 2 3 4

> Step
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Cosine Similarity
> W/o rollout
> W rollout
> Rollout only

(a) Evolution of the cosine distance between pre-dicted and ground truth trajectories across steps. V(v (t)) Curvature 

> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> Value
> 0.20
> 0.69
> 0.03
> 0.25
> 0.06
> 0.33
> 0.60
> 0.70
> Regular FM
> FM + RollOut 0.3
> FM + RollOut 1.0
> RollOut Only

(b) Variance, average curvature, and tortuosity of the predicted vector fields w/ and w/o rollout loss. 4 2 0 2               

> PCA Component 1
> 1.5
> 1.0
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> PCA Component 2
> FM w rollout
> GT
> FM w/o rollout
> 21012
> PCA Component 1
> 1.0
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> PCA Component 2
> FM w rollout
> GT
> FM w/o rollout
> 15 10 505
> PCA Component 1
> 4
> 2
> 0
> 2
> 4
> 6
> PCA Component 2
> FM w rollout
> GT
> FM w/o rollout
> 32101234
> PCA Component 1
> 2
> 1
> 0
> 1
> 2
> PCA Component 2
> FM w rollout
> GT
> FM w/o rollout

(c) Trajectories with and without rollout loss, in the PASST space. 

Figure 4. Three-fold analysis of the impact of the rollout loss. Combined with the flow matching loss, the rollout allows stable trajectory prediction and avoids error accumulation. When used without flow matching loss, it learns nonlinear trajectories. 

Figure 5. Illustration of the behavior of the model trained with and without rollout loss. Waveforms are overlapped for readability. The first step is similar, but the difference appears in the latter steps when the rollout-trained model enhances speech, while the other does not. 

Figure 6. Qualitative comparison between VisAH and VisAH-FM. Main differences are framed in red, and waveforms are overlapped for readability. 

Subjective Test. We conduct a subjective test to compare VisAH and VisAH-FM. Specifically, we ask participants to “choose the audio that best aligns with the visual scene in terms of loudness balance and overall quality”. Five partic-ipants evaluated the models on a set of eight Muddy-Mix and MovieGen [30] generated videos. VisAH-FM achieves a 60% win rate versus 10% for VisAH (30% draw), and 70% versus 10% on the MovieGen subset only. These re-sults underscore our method’s effectiveness. 

4. Related work 

Audio Remixing. Highlighting a mixed audio sig-nal can be viewed as rebalancing its constituent sources—effectively translating one mixing style into another. Prior work in music production has examined this extensively [16, 27, 33, 43], exploring both traditional and creative strategies for shaping a track’s emotional and acoustic character. Reproducing a target mix often requires controlling source energy and applying audio effects to achieve stylistic consistency, whether through expert-driven pipelines [37] or modern learning-based approaches [27, 40]. While most existing efforts emphasize music and instrument stems, these techniques do not generalize naturally to the diverse and highly dynamic sound compositions in real-world media. In contrast, VisAH [14] extends the mixing paradigm beyond music to speech and cinematic sound effects, and leverages visual cues from video to guide source highlighting. While effective, this method relies on a discriminative mapping from input to output mixes, implicitly assuming a one-to-one correspondence between degraded audio mixture and professionally mixed audio—an assumption that may not hold in ambiguous and diverse audio contexts like movies. 

Visually-conditioned Audio Generation. With the rapid progress in audio synthesis, several works have explored conditioning audio generation on video frames [6, 30, 41]. This setting is challenging due to the tight temporal cou-pling between visual events and sounds, as well as the high diversity of real-world audio scenes. Current methods typi-cally specialize in either speech or non-speech sounds (e.g., music, sound effects), limiting their generality. Conse-quently, these models are not directly suited for acoustic highlighting, which requires jointly modeling speech, mu-sic, and effects. 

Audio-Visual Source Separation. Early works in audio-visual source separation [9, 10, 50] demonstrated that visual cues provide powerful guidance for isolating sound sources in videos. More recent studies [13, 15, 18] tackle this task using generative modeling approaches, such as diffusion-based and flow-based frameworks, benefiting from their strong ability to model complex multimodal distributions and generate coherent audio conditioned on visual con-text. Our work builds upon these trends by applying con-ditional flow matching to the audio remixing problem, fo-cusing specifically on maintaining trajectory stability via a rollout loss and enabling early cross-modal fusion through adapter layers, which distinguishes our approach from prior diffusion-based conditioning pipelines. 

5. Conclusion 

We showed that generative modeling offers a more effec-tive solution than discriminative approaches for visually in-formed acoustic highlighting. With a rollout loss and cross-modal conditioning module, our flow matching model out-performs prior works on the Muddy Mix dataset. References 

[1] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying frame-work for flows and diffusions, 2023. URL https://arxiv. org/abs/2303.08797 , 3, 2023. 6 [2] Jean-Eudes Ayilo, Mostafa Sadeghi, Romain Serizel, and Xavier Alameda-Pineda. Diffusion-based unsupervised audio-visual speech enhancement. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 1–5. IEEE, 2025. 2 [3] Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through flows for controlled generation. arXiv preprint arXiv:2402.14017 , 2024. 4 [4] Cl´ ement Chadebec, Onur Tasar, Sanjeev Sreetharan, and Benjamin Aubin. Lbm: Latent bridge match-ing for fast image-to-image translation. arXiv preprint arXiv:2503.07535 , 2025. 3, 5, 6 [5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences , 67(12):220101, 2024. 6 [6] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-to-audio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 28901–28911, 2025. 8[7] Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, and Giuseppe Averta. Samwise: Infusing wisdom in sam2 for text-driven video segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference ,pages 3395–3405, 2025. 4 [8] Sayna Ebrahimi, Sercan O Arik, Tejas Nama, and Tomas Pfister. Crome: cross-modal adapters for efficient multi-modal llm. arXiv preprint arXiv:2408.06610 , 2024. 4 [9] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech sepa-ration. arXiv preprint arXiv:1804.03619 , 2018. 9 [10] Ruohan Gao and Kristen Grauman. Visualvoice: Audio-visual speech separation with cross-modal consistency. In 

2021 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR) , pages 15490–15500. IEEE, 2021. 9[11] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition , pages 15180–15190, 2023. 5 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural information processing systems , 33:6840–6851, 2020. 4 [13] Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Davis: High-quality audio-visual separation with generative diffusion models. 2023. 2, 9 [14] Chao Huang, Ruohan Gao, JMF Tsang, Jan Kurcius, Cagdas Bilen, Chenliang Xu, Anurag Kumar, and Sanjeel Parekh. Learning to highlight audio by watching movies. In Proceed-ings of the Computer Vision and Pattern Recognition Confer-ence , pages 23925–23935, 2025. 1, 2, 3, 4, 5, 8 [15] Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. High-quality sound separation across diverse categories via visually-guided generative modeling. arXiv preprint arXiv:2509.22063 , 2025. 9 [16] Junghyun Koo, Marco A Mart´ ınez-Ram´ ırez, Wei-Hsiang Liao, Stefan Uhlich, Kyogu Lee, and Yuki Mitsufuji. Mu-sic mixing style transfer: A contrastive learning approach to disentangle audio effects. In ICASSP 2023-2023 IEEE Inter-national Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP) , pages 1–5. IEEE, 2023. 8 [17] Khaled Koutini, Jan Schl¨ uter, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069 , 2021. 5, 7, 2[18] Suyeon Lee, Chaeyoung Jung, Youngjoon Jang, Jaehun Kim, and Joon Son Chung. Seeing through the conversation: Audio-visual speech separation based on diffusion model. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 12632–12636. IEEE, 2024. 9 [19] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting mo-ments and highlights in videos via natural language queries. 

Advances in Neural Information Processing Systems , 34: 11846–11858, 2021. 1 [20] Jean-Marie Lemercier, Julius Richter, Simon Welker, and Timo Gerkmann. Storm: A diffusion-based stochastic regen-eration model for speech enhancement and dereverberation. 

IEEE/ACM Transactions on Audio, Speech, and Language Processing , 31:2724–2737, 2023. 3 [21] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Av-nerf: Learning neural fields for real-world audio-visual scene synthesis. Advances in Neural Informa-tion Processing Systems , 36:37472–37490, 2023. 5 [22] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shra-man Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified video-language temporal grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,pages 2794–2804, 2023. 1 [23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-ian Nickel, and Matt Le. Flow matching for generative mod-eling. ICLR , 2022. 2, 3 [24] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and extending diffusion gener-ative models. arXiv preprint arXiv:2208.14699 , 2022. 6 [25] Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In Pro-ceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 3042–3051, 2022. 1 [26] Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, and Emanuele Rodol` a. Multi-source diffusion models for simultaneous music generation and sep-aration. arXiv preprint arXiv:2302.02257 , 2023. 2 [27] Marco A Mart´ ınez Ram´ ırez, Emmanouil Benetos, and Joshua D Reiss. Deep learning for black-box modeling of audio effects. Applied Sciences , 10(2):638, 2020. 8 [28] Andrew Ng and Michael Jordan. On discriminative vs. gen-erative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing sys-tems , 14, 2001. 2 [29] Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models. arXiv preprint arXiv:2308.15321 , 2023. 4 [30] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv preprint arXiv:2410.13720 ,2024. 8 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi-sion. In International conference on machine learning , pages 8748–8763. PmLR, 2021. 4 [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research , 21(140):1–67, 2020. 4, 6 [33] Marco A Mart´ ınez Ram´ ırez and Joshua D Reiss. Model-ing nonlinear audio effects with end-to-end deep neural net-works. In ICASSP 2019-2019 IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP) ,pages 171–175. IEEE, 2019. 8 [34] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In ICLR , 2016. 4 [35] Simon Rouard, Francisco Massa, and Alexandre D´ efossez. Hybrid transformers for music source separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 1–5. IEEE, 2023. 1, 4 [36] Robin Scheibler, John R Hershey, Arnaud Doucet, and Henry Li. Source separation by flow matching. arXiv preprint arXiv:2505.16119 , 2025. 2 [37] Sebastian Schlecht, Julian Parker, Maximilian Sch¨ afer, and Rudolf Rabenstein. Physical modeling using recurrent neu-ral networks with fast convolutional layers. In International Conference on Digital Audio Effects , pages 138–145. DAFx, 2022. 8 [38] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. ICLR , 2024. 3 [39] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. ICLR , 2023. 3 [40] Christian J Steinmetz, Nicholas J Bryan, and Joshua D Reiss. Style transfer of audio effects with differentiable signal pro-cessing. arXiv preprint arXiv:2207.08759 , 2022. 8 [41] Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. Audiox: Diffusion transformer for anything-to-audio generation. arXiv preprint arXiv:2503.10522 , 2025. 8 [42] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482 , 2023. 3 [43] Soumya Sai Vanka, Christian Steinmetz, Jean-Baptiste Rolland, Joshua Reiss, and George Fazekas. Diff-mst: Differentiable mixing style transfer. arXiv preprint arXiv:2407.08889 , 2024. 8 [44] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reason-ing in language models. arXiv preprint arXiv:2203.11171 ,2022. 6 [45] Karn N Watcharasupat, Chih-Wei Wu, Yiwei Ding, Iroro Orife, Aaron J Hipple, Phillip A Williams, Scott Kramer, Alexander Lerch, and William Wolcott. A generalized band-split neural network for cinematic audio source separation. 

IEEE Open Journal of Signal Processing , 5:73–81, 2023. 5 [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large lan-guage models. Advances in neural information processing systems , 35:24824–24837, 2022. 6 [47] Simon Welker, Matthew Le, Ricky TQ Chen, Wei-Ning Hsu, Timo Gerkmann, Alexander Richard, and Yi-Chiao Wu. Flowdec: A flow-based full-band general audio codec with high perceptual quality. arXiv preprint arXiv:2503.01485 ,2025. 3 [48] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale con-trastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 1–5. IEEE, 2023. 4, 5 [49] Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, and Dahua Lin. Visually informed binaural au-dio generation without binaural audios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15485–15494, 2021. 5 [50] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Von-drick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European conference on com-puter vision (ECCV) , pages 570–586, 2018. 9 Conditional Flow Matching for Visually-Guided Acoustic Highlighting 

## Supplementary Material 

6. Experiments on the Fixed Muddy Mix Dataset 

To evaluate the impact of on-the-fly source sampling (i.e., randomly selecting which source to enhance for each audio example at each iteration), we trained the VisAH-FM model on the fixed, pre-generated Muddy Mix dataset, that was originally proposed. Table 5 presents the results obtained on this variant of the dataset. The main observations remain consistent. As expected, the overall performance decreases relative to training with random sampling: flow matching outperforms the discriminative baseline, and the rollout loss provides additional improvements. Model IB score KLD LDiff Env VisAH Text 29.00 11.02 9.23 0.035 FM 29.27 10.91 8.84 0.034 FM + Rollout ( λ = 0 .1) 29.15 10.60 8.82 0.032 

FM + Rollout ( λ = 0 .3) 29.29 10.48 8.83 0.032 

Rollout 29.18 11.38 8.87 0.033 

Table 5. Results on the fixed Muddy Mix dataset. Flow matching setup improves performance, and adding the rollout loss stabilizes inference, as also observed for on-the-fly dataset sampling. 

7. Additional Experiments 

7.1. Ablation of the Time Conditioning 

When the timestep is fed only to the latent transformer, the encoder does not have access to temporal information, which could theoretically lead to suboptimal representa-tions. Table 6 compares a model using timestep condi-tioning exclusively within the latent transformer against a model where the timestep is also injected into each layer of the encoder. The results indicate that adding timestep information to the encoder does not provide any measurable benefit. We hypothesize that the encoder mainly extracts general audio representations, while the actual editing is handled in the latent transformer and decoder. Therefore, the timestep in-formation appears to be unnecessary within the encoder. Model IB Score KLD LDif Mag Env Was Latent Transformer 29.09 9.79 7.87 8.34 2.79 0.65 Encoder + Latent Transformer 29.02 9.87 7.83 8.24 2.70 0.60 

Table 6. Ablation study on timestep encoding. Incorporating the timestep within the encoder does not improve performance com-pared with injecting it directly into the latent space. 

7.2. No Warm Start 

Table 7 reports the performance of models trained with and without warm start (i.e., incorporating the source sample x0

into the estimated vector field as described in Section 3.1). Warm start consistently proves beneficial by allowing bet-ter exploitation of the pretrained model. Performance de-grades significantly when training with flow matching alone and without warm start, highlighting the difficulty of flow matching for this task without pretrained guidance or ex-plicit rollout regularization. Objective Warm start IB Score KLD LDif Mag Env Was FM + Rollout ✓ 29.09 9.79 7.87 8.46 2.79 0.64                      

> FM only ✓28.92 10.99 9.48 11.57 4.68 1.08 FM + Rollout ✗29.28 10.65 8.41 8.71 2.80 0.64
> FM only ✗28.75 16.52 12.73 15.54 4.56 1.37

Table 7. Warm start ablation. Incorporating x0 in the prediction significantly improves the performance, particularly when no roll-out loss is applied. 

7.3. Impact of the Number of Inference Steps for Rollout-only Model 

Table 8 evaluates the model trained only using rollout loss for different number of inference steps. Consistent with the behavior observed in Figure 4a, the first step tends to align well with the ground-truth linear trajectory, leading to good performance when using a single inference step. Perfor-mance decreases as the number of steps increases due to trajectory non-linearity (because it forces interpolation to bigger step), except when using four steps — matching the training setup — which yields the best overall metrics. Number of steps IB Score KLD LDif Mag Env Was 1 29.11 10.09 7.73 8.17 2.84 0.66 2 28.86 10.17 8.36 9.15 2.92 0.70 3 28.78 10.68 9.42 10.34 0.32 0.84 4 28.94 9.92 7.71 8.13 2.78 0.62 

Table 8. Ablation of the number of inference steps using the model trained with rollout loss only. Intermediate steps degrades per-formance, as the model learns non linear trajectory, interpolating from middle steps give noisy result. 

8. Signal Metrics for All Ablations 

In this section, we report the signal metrics for all the abla-tions in the main paper. Most signal-level metrics follow similar trends as the semantic metrics, except for VisAH-FM trained with text-Adapter Modality IB Score ↑ KLD ↓ LDif ↓ Mag ↓ Env ↓ Was ↓                                   

> ✓V+A 29.12 9.70 7.77 8.28 2.74 0.63
> ✓V + T 29.08 9.76 7.85 8.34 2.74 0.63
> ✓V+T+A 29.10 9.73 7.90 8.25 2.74 0.63
> ✗V29.09 9.79 7.87 8.46 2.79 0.64
> ✗T28.88 10.50 8.13 8.22 2.65 0.54

Table 9. Ablation on the conditioning. We compare different con-ditioning methods (with and without adapter) and different modal-ities: audio (A), vision (V), and text (T).                                                               

> λHorizon Steps IB Score ↑KLD ↓LDif ↓Mag ↓Env ↓Was ↓
> 0.1 4429.12 9.85 8.02 8.41 2.78 0.64 0.3 4429.09 9.79 7.87 8.46 2.79 0.64 0.5 4429.12 9.93 7.80 8.43 2.79 0.64 1.0 4429.04 9.82 7.84 8.47 2.81 0.65 0.1 2429.08 9.68 7.94 8.45 2.76 0.63
> 0.3 410 29.10 9.79 8.35 8.58 2.81 0.66

Table 11. Hyperparameter sensitivity. We vary the impact of the rollout loss as well as its horizon and the number of total steps used for inference. 

only inputs, which significantly outperforms other methods in signal metrics while underperforming in semantic met-rics. Model IB Score KLD LDif Mag Env Was FM + Rollout 29.09 9.79 7.87 8.46 2.79 0.64 FM only 28.92 10.99 9.48 11.57 4.68 1.08 Rollout 28.94 9.92 7.71 8.13 2.78 0.62                   

> FM + Consistency 28.40 14.58 10.30 13.54 3.98 1.15 FM Weighted 28.86 11.16 9.44 11.83 3.68 0.99 Bridge Matching 29.04 10.85 9.62 10.98 3.51 0.91

Table 10. Ablation of the rollout component. We compare differ-ent standard methods used to stabilize predictions with our roll-out proposal. All models use standard CLIP conditioning (w/o adapter). 

9. Linearity of Trajectories in Semantic Space 

The analysis in Section 3.2.2 showed that the rollout loss helps linearize the inference trajectory. However, when its contribution becomes too dominant, the trajectories exhibit non-linear behavior again. This analysis was conducted in the time–frequency domain. Here we analyze linearity of trajectories in a semantic representation space, where each inference step can affect how the audio is interpreted. Table 12 reports the variance of the estimated vector fields as well as the average discrete curvature of the model outputs, measured in the PaSST embedding space [17]. The observations mirror those made in the time–frequency domain: adding the rollout loss alongside flow match-ing reduces trajectory curvature, while either increasing its weight or removing the flow matching loss altogether leads to more non-linear trajectories. In a real-world scenario, such behavior is beneficial for models trained with both flow matching and rollout loss. It might allow a user to flexibly control the desired amount of remixing by stopping inference after a chosen number of steps, enabling semantic manipulation along a smooth editing path. Training objective V (vθ (t)) Curvature FM 0.0252 0.9188 FM + RollOut ( λ = 0 .1) 0.0052 0.4390 FM + RollOut ( λ = 0 .3) 0.0052 0.4397 RollOut 0.0218 0.9406 

Table 12. Linearity metrics in the pASST space. Adding the roll-out loss linearized the inference trajectories, but applying it with-out the flow matching loss results in non-linear trajectories. 

10. In-depth Qualitative Analysis 

Figure 7 shows further qualitative samples that highlight the difference of behavior between the flow matching mod-els trained with and without loss. The rollout loss allows more consistent predictions across steps, resulting in more highlighted sources. Figure 8 shows qualitative compari-son between VisAH and our model VisAH-FM. Our model find more accurately the source to enhance and emphasize it more. Finally, audio samples can be found here. 

11. Limitations and Future Work 

VisAH-FM delivers meaningful performance gains com-pared to the discriminative VisAH, but it is computationally more demanding and inherits limitations from the under-lying CLIP/CLAP representations, which may lead to fail-ures when audio–visual cues are weak. Future work should evaluate the model on real-world data once such datasets become available. Additionally, our current VisAH-FM is trained on paired data, requiring artificial mixing of au-dio signals. Training with unpaired data would enable the model to directly leverage real-world inputs, significantly increasing its practical applicability. 0.10 

0.05 

0.00 

0.05 

0.10 

> Amplitude

Step 1 

W RollOut 

W/o RollOut 

0.1 

0.0 

0.1 

> Amplitude

Step 2 

W RollOut 

W/o RollOut 

0.1 

0.0 

0.1 

> Amplitude

Step 3 

W RollOut 

W/o RollOut 

0.2 

0.1 

0.0 

0.1 

> Amplitude

Step 4 

W RollOut 

W/o RollOut 

0 100000 200000 300000 400000 

Sample Index 

0.1 

0.0 

0.1 

> Amplitude

GT Audio 

GT Audio 0.5 

0.0 

0.5 

> Amplitude

Step 1 

W RollOut 

W/o RollOut 

0.5 

0.0 

0.5 

> Amplitude

Step 2 

W RollOut 

W/o RollOut 

0.50 

0.25 

0.00 

0.25 

0.50 

> Amplitude

Step 3 

W RollOut 

W/o RollOut 

0.50 

0.25 

0.00 

0.25 

0.50 

> Amplitude

Step 4 

W RollOut 

W/o RollOut 

0 100000 200000 300000 400000 

Sample Index 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

GT Audio 

GT Audio 0.1 

0.0 

0.1 

> Amplitude

Step 1 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 2 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 3 

W RollOut 

W/o RollOut 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

Step 4 

W RollOut 

W/o RollOut 

0 100000 200000 300000 400000 

Sample Index 

0.2 

0.0 

0.2 

> Amplitude

GT Audio 

GT Audio 0.2 

0.1 

0.0 

0.1 

0.2 

> Amplitude

Step 1 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 2 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 3 

W RollOut 

W/o RollOut 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

Step 4 

W RollOut 

W/o RollOut 

0 100000 200000 300000 400000 

Sample Index 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

GT Audio 

GT Audio 0.2 

0.0 

0.2 

> Amplitude

Step 1 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 2 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 3 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 4 

W RollOut 

W/o RollOut 

0 100000 200000 300000 400000 

Sample Index 

0.2 

0.0 

0.2 

> Amplitude

GT Audio 

GT Audio 0.2 

0.0 

0.2 

> Amplitude

Step 1 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 2 

W RollOut 

W/o RollOut 

0.2 

0.0 

0.2 

> Amplitude

Step 3 

W RollOut 

W/o RollOut 

0.4 

0.2 

0.0 

0.2 

> Amplitude

Step 4 

W RollOut 

W/o RollOut 

0 100000 200000 300000 400000 

Sample Index 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

GT Audio 

GT Audio Figure 7. Qualitative comparison of VisAH-FM models trained with and without rollout loss. 0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

Step 

FM w rollout 

VisAH 

0 100000 200000 300000 400000 

Sample Index 

0.3 

0.2 

0.1 

0.0 

0.1 

0.2 

0.3 

> Amplitude

GT Audio 

GT Audio 0.6 

0.4 

0.2 

0.0 

0.2 

0.4 

0.6 

> Amplitude

Step 

FM w rollout 

VisAH 

0 100000 200000 300000 400000 

Sample Index 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

GT Audio 

GT Audio 0.6 

0.4 

0.2 

0.0 

0.2 

0.4 

0.6 

> Amplitude

Step 

FM w rollout 

VisAH 

0 100000 200000 300000 400000 

Sample Index 

0.6 

0.4 

0.2 

0.0 

0.2 

0.4 

0.6 

> Amplitude

GT Audio 

GT Audio 0.2 

0.1 

0.0 

0.1 

0.2 

> Amplitude

Step 

FM w rollout 

VisAH 

0 100000 200000 300000 400000 

Sample Index 

0.20 

0.15 

0.10 

0.05 

0.00 

0.05 

0.10 

0.15 

> Amplitude

GT Audio 

GT Audio 0.3 

0.2 

0.1 

0.0 

0.1 

0.2 

0.3 

> Amplitude

Step 

FM w rollout 

VisAH 

0 100000 200000 300000 400000 

Sample Index 

0.2 

0.1 

0.0 

0.1 

0.2 

0.3 

> Amplitude

GT Audio 

GT Audio 0.6 

0.4 

0.2 

0.0 

0.2 

0.4 

0.6 

> Amplitude

Step 

FM w rollout 

VisAH 

0 100000 200000 300000 400000 

Sample Index 

0.4 

0.2 

0.0 

0.2 

0.4 

> Amplitude

GT Audio 

GT Audio Figure 8. Qualitative comparison between VisAH-FM and the discriminative VisAH.
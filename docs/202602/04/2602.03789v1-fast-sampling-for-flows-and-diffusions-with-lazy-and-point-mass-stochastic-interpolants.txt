Title: Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants

URL Source: https://arxiv.org/pdf/2602.03789v1

Published Time: Wed, 04 Feb 2026 02:40:03 GMT

Number of Pages: 30

Markdown Content:
# Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Gabriel Damsholt 1 Jes Frellsen 2 Susanne Ditlevsen 1

## Abstract 

Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A pri-mary hyperparameter in these methods is the inter-polation schedule that determines how to bridge a standard Gaussian base measure to an arbi-trary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coeffi-cient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the as-sumption of Gaussian data, we identify lazy sched-ule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statisti-cally optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the useful-ness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating im-ages in fewer steps without retraining the model. 

## 1. Background 

A basic goal of generative AI is to sample from a non-trivial target distribution, potentially in a very high-dimensional space, accessible only through a finite dataset of samples. Specifically, let X(1) , X (2) , . . . , X (N ) âˆˆ Rd be i.i.d. sam-ples from an unknown distribution with density ÏX . We then wish to generate new samples X âˆ¼ ÏX .

> 1

Department of Mathematical Sciences, University of Copen-hagen, Copenhagen, Denmark 2Department of Machine Learn-ing and Signal Processing, Technical University of Denmark, Lyngby, Denmark. Correspondence to: Gabriel Damsholt 

<gnd@math.ku.dk >.

Preprint. February 4, 2026. 

Various techniques for solving this basic generative prob-lem exist. Many modern methods are based on dynamical systems . The idea is to learn the dynamics of a dynamical system with an analytically samplable initial law, typically taken to be standard Gaussian, and with final law ÏX . Sam-ples from ÏX can then be generated by sampling from a standard Gaussian and integrating the dynamics from the initial to the final time. We will use the flexible framework of stochastic interpolants 

(Albergo et al., 2025) which unifies flow and diffusion based generative models. We briefly introduce this framework and then state our contributions. 

1.1. Spatially linear stochastic interpolant 

The stochastic interpolant framework allows one to couple arbitrary distributions, and even arbitrarily many of them (Albergo et al., 2024), possibly in spatially non-linear ways and even using matrix coefficients (Negrel et al., 2025). We will exclusively work with the simplest and most commonly encountered setting, namely the â€œspatially linear one-sided stochastic interpolantâ€ (Albergo et al., 2025, eq. 4.15) which we denote stochastic interpolant or just interpolant . Below we recall its definition and some core results. 

Definition 1.1 (stochastic interpolant) . Given independent random variables Z, X âˆˆ Rd with Z âˆ¼ N (0 , I), X âˆ¼ ÏX

and non-negative scalar functions Î±, Î² : [0 , 1] â†’ [0 , âˆ)

satisfying the below criteria, the stochastic interpolant or simply interpolant is the stochastic process 

It = Î±tZ + Î²tX, t âˆˆ [0 , 1] ,

with Î±, Î² âˆˆ C1([0 , 1]) satisfying the boundary conditions 

Î±0 = Î²1 = 1 , Î± 1 = Î²0 = 0 and the monotonicity condi-tions Ë™Î±t < 0 and Ë™Î²t > 0 for all t âˆˆ (0 , 1) .1

Note that I0 âˆ¼ N (0 , I) and I1 âˆ¼ ÏX for all valid choices of Î±, Î² .One of the primary objects of interest is the following. 

> 1

One can allow for the one-sided derivatives of Î± and Î² to explode at either endpoint as long as the one-sided derivatives exist in the extended reals, see Albergo et al. (2025) for a discussion on this. We require bounded derivatives for simplicity. 

1

> arXiv:2602.03789v1 [stat.ML] 3 Feb 2026

Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants Linear Schedule       

> ğ‘¡
> 00.51
> SDE
> ODE Lazy ODE Schedule
> ğ‘¡
> 00.51
> SDE
> ODE Lazy SDE Schedule
> ğ‘¡
> 00.51
> SDE
> ODE

Figure 1. ODE ( Îµ â‰¡ 0) and statistically optimal SDE ( Îµ = Îµâˆ—) sample path under three different interpolation schedules for ÏX aGaussian mixture density for which the dynamics are analytically known (Albergo et al., 2025, Appendix A). All paths start in the same initial condition which sits in the initial position (cyan circle) for the leftmost two interpolants with density-admitting schedule and in the initial drift for the rightmost point mass schedule. The same Wiener process realization is shared between all three SDE solutions. Leftmost is the linear schedule from Definition 5.1, middle and rightmost is the lazy interpolant for the ODE and statistically optimal SDE, respectively, as in Example 7.4. Since ut = t for all three schedules, all sample paths are equivalent up to a reparameterization of space. Note that all sample paths end in the same position for the ODE and SDE, respectively. See Figure 4 for a plot of the three schedules and their statistically optimal diffusion scales. 

Definition 1.2 (schedule) . The function pair (Î±, Î² ) in Defi-nition 1.1 is the interpolation schedule or simply schedule .The schedule is a hyperparameter that determines how 

N (0 , I) and ÏX are bridged in time. A canonical choice is 

Î±t = 1 âˆ’ t, Î² t = t; this is the choice taken in flow matching. To proceed we define the following functions. 

Definition 1.3. For an interpolant as in Definition 1.1 with density Ï : [0 , 1] Ã— Rd â†’ (0 , âˆ) and Îµ : [0 , 1] â†’ [0 , âˆ)

we define the following functions for (t, x ) âˆˆ [0 , 1] Ã— Rd:

Î·Z (t, x ) := E[Z | It = x], â€œnoise predictorâ€ 

Î·X (t, x ) := E[X | It = x], â€œdata predictorâ€ 

s(t, x ) := âˆ‡ log Ï(t, x ), â€œscoreâ€ 

bÎµ(t, x ) := Ë™ Î±tÎ·Z (t, x ) + Ë™Î²tÎ·X (t, x ) + Îµts(t, x ). â€œdriftâ€ The central idea behind the stochastic interpolant framework is to construct a family of stochastic differential equations 

(SDEs) indexed by an arbitrary time-dependent additive diffusion scale Îµ : [0 , 1] â†’ [0 , âˆ), whose solutions have the same marginal distributions (but different joint distributions) as It, as captured in the following theorem (Albergo et al., 2025, Corollary 18). 

Theorem 1.4 (SDE) . Let Îµt âˆˆ C1([0 , 1]) be any non-negative scalar function. Then the solutions to the SDE 

dXÎµt = bÎµ(t, X Îµt ) d t + âˆš2Îµt dWt, (1) 

solved forward in time t with XÎµ 

> 0

âˆ¼ N (0 , I) independent of 

W , satisfy Law( XÎµt ) = Law( It) for all t âˆˆ [0 , 1] .

With the above result we can pick any non-negative diffusion scale Îµ, sample XÎµ 

> 0

âˆ¼ N (0 , I) and solve the SDE forward in time from t = 0 to t = 1 provided we know the drift 

bÎµ. Then XÎµ 

> 1

âˆ¼ ÏX , as desired. In particular, setting Îµ â‰¡ 0

gives a deterministic probability flow ODE. Each function in Definition 1.3 can be characterized as the minimizer of a simple squared-loss objective, enabling unbi-ased Monte-Carlo estimation from data without simulating the SDE. One then uses the learned estimate of the dynam-ics to generate data, e.g. one learns Ë†bÎµ â‰ˆ bÎµ from data and solves the SDE (1) with bÎµ replaced by Ë†bÎµ to sample 

Ë†X âˆ¼ Ë†ÏX â‰ˆ ÏX .Throughout we assume the following, the validity of which depends on the properties of ÏX , the smoothness of (Î±, Î² )

and the choice of Îµ (see Albergo et al. (2025) for details). 

Assumption 1.5. The data density satisfies ÏX (x) > 0 for all x âˆˆ Rd, and the density Ï : [0 , 1] Ã— Rd â†’ (0 , âˆ) of the interpolant satisfies Ï(Â·, x ) âˆˆ C1([0 , 1]) for all x âˆˆ Rd,

Ï(t, Â·) âˆˆ C2(Rd) for all t âˆˆ [0 , 1] , and similarly for the functions in Definition 1.3. Furthermore, the SDE (1) has a unique strong solution for all initial conditions. 

## 2. Contributions 

1. In Theorem 5.5 we generalize existing results from the ODE to the SDE setting and show that we can uniquely convert any strong solution to the SDE (1) with arbi-trary schedule and diffusion scale into the corresponding strong solution to the SDE (1) with an arbitrary different schedule and diffusion scale. 2. In Definition 6.1 and Theorem 6.3 we extend the interpo-lation schedule to include the point mass boundary con-dition Î±0 = 0 and give sufficient conditions for the SDE to exist in this extended setting with normally distributed initial conditions that sit in the initial drift instead of in the initial position. We stress that this result even holds in the ODE case, i.e. when Îµ â‰¡ 0.3. Assuming that X âˆ¼ N (0 , I) we prove in Proposition 7.1 that the lazy schedule family â€“ the family of schedules such that the dynamics become identically zero â€“ for 2Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

the ODE satisfies the variance-preserving property Î±2 

> t

+

Î²2 

> t

= 1 , while for the SDE with statistically optimal diffusion scale it satisfies Î±2 

> t

+ Î²2 

> t

= Î²t. In particular, 

Î±0 = 0 in the latter case, which motivates our schedule extension to the point mass case. 4. In Propositions 7.5 and 7.6 and Algorithms 1 and 2 we de-rive particularly simple and theoretically well-motivated formulas for reparameterizing space (but not time) in any pretrained flow matching model with linear schedule 

Î±t = 1 âˆ’ t, Î² t = t to yield a lazy ODE and statistically optimal SDE schedule, respectively. 5. In Section 8 we conduct empirical experiments with a state-of-the-art flow matching model for natural image generation and show that using this reparameterization allows for fewer-step numerical sampling in the ODE case and especially so for the statistically optimal SDE, which is otherwise numerically undefined at t = 0 .

## 3. Related work 

The stochastic interpolant framework (Albergo et al., 2025) unifies existing continuous-time generative paradigms based on dynamical systems, namely flow matching (Lipman et al., 2023) and diffusion models 2 (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Song et al., 2021) by disentangling the two primary hyperparameters: the interpolation sched-ule coupling the Gaussian and data distribution in time, and the stochasticity of the stochastic process that samples this coupling. The optimal level of stochasticity has mostly been explored in the stochastic interpolant literature (Albergo et al., 2025; Chen et al., 2024; Ma et al., 2024; Chen et al., 2025), whereas more work exists on tuning the interpolation schedule with the aim of easing numerical integration, albeit often heuristically or empirically based (Nichol & Dhari-wal, 2021; Karras et al., 2022; Song & Ermon, 2020; Xue et al., 2024). Our work is primarily inspired by Chen et al. (2025), but by considering the statistically optimal SDE in-stead of just the ODE we arrive at point mass schedules for which little work exists (Albergo et al., 2025; Chen et al., 2024). Our work also resembles Sabour et al. (2024), but in addition to time reparameterizations we also consider space reparameterizations, even extending to point mass schedules. Changing the interpolation schedule post-training has re-cently been explored for ODE sampling (Liu, 2024; Lai et al., 2025; Chen et al., 2025), but to the best of our knowl-edge pathwise equivalence has never been established for SDE sampling before. More sophisticated approaches for sampling from flow and diffusion models in fewer steps exist; some notable exam-  

> 2Score-based generative models , the continuous-time equiva-lents of denoising diffusion probabilistic models (Ho et al., 2020).

ples are flow rectification (Liu et al., 2023), consistency models (Song et al., 2023; Boffi et al., 2025) and (entropy regularized) optimal transport based approaches (Pooladian et al., 2023; Shi et al., 2023; Bortoli et al., 2021; Poola-dian & Niles-Weed, 2025). These approaches are mostly supplementary to the choice of schedule that we explore. 

## 4. Intra-interpolant conversion formulas 

In this section we fix the schedule (Î±, Î² ) and show how to convert between the functions in Definition 1.3. The proofs can be found in Appendix A. The following diffusion scale plays an important role in our work. We define it now and motivate it later. 

Definition 4.1. Given an interpolation schedule (Î±, Î² ) satis-fying the criteria in Definition 1.1 we denote by Îµâˆ— 

> t

: [0 , 1] â†’

[0 , âˆ] the scalar function 

Îµâˆ— 

> t

= Î±2

> t

Ë™Î²t

Î²t

âˆ’ Î±t Ë™Î±t,

where at t = 0 the equation is interpreted as a right-limit in the extended reals. We start by showing that for a fixed interpolation schedule 

(Î±, Î² ), any one of the functions from Definition 1.3 uniquely defines all remaining functions through affine transforma-tions that simplify using the above definition for Îµâˆ—. This result generalizes the well-known result relating the score to the noise predictor (Albergo et al., 2025, Theorem 8) and Lai et al. (2025, Proposition 6.3.1) for the ODE case Îµ â‰¡ 0.

Proposition 4.2 (intra-interpolant conversion formulas) .

For a fixed interpolation schedule (Î±, Î² ) and non-negative 

Îµ âˆˆ C1([0 , 1]) , each of the functions Î·Z , Î· X , s, b Îµ uniquely defines all of the others on (0 , 1) Ã— Rd. In particular, each function can be expressed in terms of the score as 

Î·Z (t, x ) = âˆ’Î±ts(t, x ),Î·X (t, x ) =  x + Î±2 

> t

s(t, x )/Î² t,bÎµ(t, x ) = ( Îµâˆ— 

> t

+ Îµt)s(t, x ) + ( Ë™Î²t/Î² t)x. 

The next theorem characterizes Îµâˆ— as the statistically opti-mal diffusion scale in a precise sense. This is essentially equation 13 in Chen et al. (2024) but our proof is simpler. 

Theorem 4.3. Assume we are given statistically learnt es-timates Ë†Î·Z , Ë†Î·X , Ë†s or Ë†bÎµ of the true functions from Defini-tion 1.3 with remaining estimates given by Proposition 4.2. Let XÎµt solve the SDE (1) and consider the plug-in SDE 

d Ë†XÎµt = Ë†bÎµ(t, Ë†XÎµt ) d t + âˆš2Îµt dWt,

solved forward in time t with Ë†XÎµ 

> 0

âˆ¼ N (0 , I) independent of 

W . Then the minimizer of the Kullback-Leibler divergence 

3Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

between the path measures of XÎµ and Ë†XÎµ over the diffusion scale Îµ is 

argmin 

> Îµâ‰¥0,Îµ âˆˆC1((0 ,1])

KL 



XÎµ, Ë†XÎµ

= Îµâˆ—,

and the minimum is 

KL 



XÎµâˆ—

, Ë†XÎµâˆ— 

=

Z 10

Îµâˆ— 

> t

Eâˆ¥s(t, I t) âˆ’ Ë†s(t, I t)âˆ¥2 dt. 

Unfortunately, the statistically optimal diffusion scale Îµâˆ—

is ill-behaved numerically in the sense that for any sched-ule (Î±, Î² ) satisfying Definition 1.1 it is infinite and non-integrable at t = 0 (Proposition B.1). Nonetheless, the sta-tistically optimal SDE , i.e., the SDE (1) with Îµ = Îµâˆ—, is well-defined on t âˆˆ (0 , 1] and behaves like a (non-continuous) white-noise process as t â†’ 0+. See Proposition B.5 which proves an equivalence between Îµâˆ—, the Ornstein-Uhlenbeck process, and diffusion models. 

## 5. Inter-interpolant conversion formulas 

We now show how to convert the functions from Defini-tion 1.3 from one schedule to another and how to convert be-tween solutions to the SDEs. The following linear schedule 

used in flow matching simplifies the conversion formulas. 

Definition 5.1 (linear interpolant) . Let Î±t := 1 âˆ’ t, Î² t := t.Then we denote by It := Î±tZ + Î²tX the linear interpolant .We denote by Î·Z , Î· X , s, b Îµ its associated functions as in Definition 1.3 and by XÎµt its associated SDE (1). 

Proposition 5.2. For the linear interpolant It from Defini-tion 5.1 it holds that Îµâˆ— 

> t

= (1 âˆ’ t)/t . Therefore 

Z ts

2Îµâˆ— 

> u

du = 2(log( t) âˆ’ t âˆ’ log( s) + s)

for 0 < s â‰¤ t â‰¤ 1 and the integral is infinite for s = 0 .

Definition 5.3. For t âˆˆ [0 , 1] we define the functions 

ct := Î±t + Î²t, ut := Î²t/c t.

The interpretation is that c is a (generally non-invertible) space-change while u is a proper time change. Indeed, the monotonicity assumptions on the schedule (Î±, Î² ) in Defini-tion 1.1 imply that u âˆˆ C1([0 , 1]) and is strictly increasing on (0 , 1) so that its inverse tu := uâˆ’1(u) exists. The following proposition can be seen as a supplement to Proposition 4.2. It rephrases and generalizes proposition 3.1 from Chen et al. (2025) and proposition 6.3.3 from Lai et al. (2025) which only consider the ODE case ( Îµ â‰¡ 0). 

Proposition 5.4 (inter-conversion formulas) . For a sched-ule (Î±, Î² ) and non-negative Îµ âˆˆ C1([0 , 1]) , the functions 

Î·Z , Î· X , s, b Îµ are uniquely defined from any of the functions 

Î·Z , Î· X , s, b Îµ associated with the linear interpolant. In par-ticular, 

Î·Z (t, x ) = Î·Z (ut, x/c t), Î·X (t, x ) = Î·X (ut, x/c t),s(t, x ) = (1 /c t)s(ut, x/c t),bÎµ(t, x ) = Ë™ct

ct

x + ct Ë™utbÎµ



ut, xct



for Îµut = Î±tÎµt

Î²tÎµâˆ—

> t

.

We finally prove our strongest conversion result, which to the best of our knowledge is new for ÎµÌ¸ â‰¡ 0.

Theorem 5.5 (pathwise conversion) . Let (Î±, Î² ) be a sched-ule satisfying the requirements in Definition 1.1, let z âˆˆ Rd

and let (Wt)tâˆˆ[0 ,1] be a Wiener process realization. Let XÎµt

solve the SDE (1) in physical time t from XÎµ 

> 0

= z driven by W with diffusion scale Îµ. Let XÎµu solve the SDE (1) 

with linear schedule in reparameterized time u = ut from 

XÎµ 

> 0

= z driven by 

W u =

Z tu

> 0

p Ë™us dWs, u âˆˆ [0 , 1] 

with diffusion scale Îµu = ( Î±tu Îµtu )/(Î²tu Îµâˆ— 

> tu

). Then the so-lution XÎµt can be obtained from the solution XÎµut associated with the linear interpolant via the formula 

XÎµt = ctXÎµut , t âˆˆ [0 , 1] .

In particular, XÎµ 

> 1

= XÎµ 

> 1

(since c1 = 1 ). 

With this result, we can sample paths from the interpolant with any schedule and any diffusion scale provided we can sample paths from the linear interpolant (simply apply The-orem 5.5 twice to convert from an arbitrary schedule instead of the linear schedule). This is illustrated in the two leftmost plots in Figure 1 for the ODE and statistically optimal SDE, respectively. 

Remark 5.6 . We shall see that the identity time-transform 

ut = t is of particular interest. Then Theorem 5.5 simplifies significantly, in particular, W = W .

## 6. Point mass schedule 

We use the preceding results to extend Definition 1.1 to transport to the target distribution from a point mass instead of a (non-degenerate) Gaussian. We delay the motivation to the next section. 

Definition 6.1 (point mass schedule and interpolant) . Afunction pair Î±, Î² âˆˆ C1([0 , 1]) satisfying 1. Î±0 = Î±1 = Î²0 = 0 , Î² 1 = 1 ,2. Î±t > 0, Ë™Î²t > 0 for all t âˆˆ (0 , 1) ,4Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

3. Î²t = o(Î±t) as t â†’ 0+ (equivalently u0 = 0 ), 4. Î±2 

> t

= O(Î²t) as t â†’ 0+,5. Ë™u0 := lim tâ†’0+ Ë™ut < âˆ,6. ddt (Î²t/Î± t) > 0 for all t âˆˆ (0 , 1) (equivalently Ë™ut > 0), we call a point mass schedule . The associated interpolant we call a point mass interpolant .

Remark 6.2 . The condition Î±0 = Î²0 = 0 is sufficient and necessary for the density of the interpolant to collapse to a point mass at t = 0 . The condition u0 = 0 is sufficient and necessary for the initial drift to be analytically samplable; if otherwise u0 > 0 we would have to sample the non-trivial law of Iu0Ì¸ = I0 to sample the initial drift, which is impossible in practice. The condition Î±2 

> t

= O(Î²t) as t â†’

0+ is technical and can be relaxed; see the remarks at the end of the proof of Theorem 6.3. The assumption Ë™u0 < âˆ

gives u âˆˆ C1([0 , 1]) , which is not strictly necessary but we impose it for simplicity. The last assumption is equivalent to Ë™ut > 0 for all t âˆˆ (0 , 1) so that t 7 â†’ ut is a proper time change. Remarkably, the initial drift of the point mass interpolant can be defined pointwise from the initial condition of the linear interpolant, even in the ODE case, as we show below. 

Theorem 6.3. Let (Î±, Î² ) be a point mass schedule as in Def-inition 6.1. Then given any non-negative bounded diffusion scale Îµ âˆˆ C1([0 , 1]) , the SDE (1) is well-defined in the clas-sical sense for t âˆˆ (0 , 1] with initial position XÎµ 

> 0

= 0 , and the initial drift is well-defined in distribution. It holds that 

Law( XÎµt ) = Law( It) for all t âˆˆ [0 , 1] and Theorem 5.5 still applies: in particular, the point mass SDE solution and initial drift relates to the solution and initial condition of the linear interpolant as XÎµt = ctXÎµut for t âˆˆ [0 , 1] and 

bÎµ(0 , X Îµ 

> 0

) =   Ë™c0 âˆ’ lim tâ†’0+ (Îµt/Î± t)XÎµ

> 0

.Remark 6.4 . The initial drift in Theorem 6.3 is always bounded for the ODE case Îµ â‰¡ 0, whereas for the SDE case it is bounded if and only if we choose the diffusion scale such that Îµt âˆˆ O(Î±t) as t â†’ 0+.Theorem 6.3 shows that while the normally distributed ini-tial condition sits at the initial position for the density-admitting stochastic interpolant from Definition 1.1, the initial condition instead sits at the initial SDE drift (or ODE velocity when Îµ â‰¡ 0) for the point mass interpolant from Definition 6.1. To the best of our knowledge such a non-standard ODE has not been considered before in the flow or diffusion literature, whereas the SDE has been treated in slightly different settings in Albergo et al. (2025); Chen et al. (2024). A sample path from a point mass interpolant is plotted to-gether with its linear interpolant path in Figure 1 for the ODE and statistically optimal SDE, respectively. We finally extend proposition 2.5 from Chen et al. (2025) to the point mass setting. 

Proposition 6.5. Let (Î±, Î² ) be any valid schedule, possibly a point mass schedule. Then under the assumptions in Theo-rem 4.3, the minimal path-measure KL-divergence attained when Îµ = Îµâˆ— is invariant to the choice of schedule. 

Proposition 6.5 dictates that we must optimize the schedule for some criterion different from statistical optimality, which is what we do in the next section. 

## 7. Lazy schedules under Gaussian data assumption 

In this section, we make the oversimplifying assumption that X âˆ¼ N (0 , I). This allows us to identify families of schedules for which the SDE drift is identically zero, the motivation being that such schedules minimize the en-ergy required to transport from base to target measure. We call these schedules lazy . In the experimental section, we demonstrate empirically that the results we obtain under the Gaussian data assumption are useful even in realistic, high-dimensional and highly non-Gaussian settings. 

7.1. Relation to flows and diffusions 

To simplify the presentation we focus on two natural choices for the diffusion scale: Îµ â‰¡ 0 and Îµ = Îµâˆ—.The former corresponds to ODE sampling and reduces to standard flow matching under the linear interpolant (see Definition 5.1). The latter reduces to sampling a time-reversed Ornstein-Uhlenbeck process as in diffusion model-ing (Proposition B.5) and we will henceforth refer to this as â€œstatistically optimal SDE samplingâ€. Numerical integration is simpler for ODE sampling, but statistically optimal SDE sampling is maximally robust to approximation errors in the learned drift estimate as per Theorem 4.3. Throughout we use the notation b := bÎµâ‰¡0 for the ODE velocity and 

bâˆ— := bÎµ=Îµâˆ—

for the statistically optimal SDE drift. 

7.2. Identifying lazy schedules 

Requiring the velocity and drift to be identically zero un-der the Gaussian data assumption removes one degree of freedom in the schedule. 

Proposition 7.1 (lazy schedule families) . Assume that X âˆ¼N (0 , I). Then for all (t, x ) âˆˆ [0 , 1] Ã— Rd it holds that 1. 

b(t, x ) = 0 â‡â‡’ Î±2 

> t

+ Î²2 

> t

= 1 .

In words, the ODE velocity is identically zero if and only if the schedule is variance preserving. 2. 

bâˆ—(t, x ) = 0 â‡â‡’ Î±2 

> t

+ Î²2 

> t

= Î²t.

5Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

This implies Î±0 = 0 , Î² t = o(Î±t), Î± 2 

> t

= O(Î²t) as t â†’

0+. We have that Ë™u0 < âˆ â‡â‡’ Ë™Î²t = O(âˆšÎ²t) in which case the schedule is a point mass schedule per Definition 6.1. It also holds for all t âˆˆ [0 , 1] that Îµâˆ— 

> t

=Ë™Î²t/2 so that 

Z ts

2Îµâˆ— 

> u

du = Î²t âˆ’ Î²s for 0 â‰¤ s â‰¤ t â‰¤ 1.

Proposition 7.1 curiously shows that requiring the ODE ve-locity to be identically zero under the Gaussian data assump-tion amounts to choosing a variance preserving schedule, which is typically used in diffusion models but not in flow models. On the other hand, for statistically optimal SDE sampling it instead leads to point mass schedule, which is underexplored in the literature. 

Remark 7.2 . From Proposition 7.1 together with the in-evitable boundary conditions Î²0 = 0 , Î² 1 = 1 we see that 

R 10 2Îµâˆ— 

> u

du = 1 , i.e. the quadratic variation of the martingale part of solutions to our SDE from t = 0 to t = 1 are fixed at 1. This is remarkable since for Î±0 = 1 the same integral is fixed at âˆ by Proposition B.1. This result holds even when X is not normally distributed. However, one easily shows that normalizing this quadratic variation by the total variance gives R t 

> 0

(2 Îµâˆ—

> u

)/Î² u du = [log( Î²u)] t 

> 0

= âˆ for all 

t âˆˆ (0 , 1] , resembling the result from Proposition B.1. A natural next question is how to eliminate the last degree of freedom. We give two natural examples below and then discuss why the second example is useful in practice. 

Example 7.3 (Î²t = t). Setting Î²t = t gives, in the ODE case, Î±t = âˆš1 âˆ’ t2. Interestingly, this is not the usual lin-ear schedule used in flow matching models. Also Ë™Î±1 = âˆ’âˆ .Considering instead the SDE case we get Î±t = pt(1 âˆ’ t)

and 2Îµâˆ— 

> t

= 1 . When X âˆ¼ N (0 , I), the solution to the SDE is a standard d-dimensional Brownian motion (Wt)tâˆˆ[0 ,1] 

with W0 = 0 . Also Ë™Î±0 = âˆ and Ë™Î±1 = âˆ’âˆ . The endpoint explosion of Ë™Î± in these schedules is worrisome. 

Example 7.4 (ut = t). Define dt := (1 âˆ’ t)2 + t2. Then for the ODE case requiring ut = t gives the schedule Î±t =(1 âˆ’ t)/âˆšdt, Î² t = t/ âˆšdt. Then Ë™Î±0 = 0 , Ë™Î±1 = âˆ’1 and 

Ë™Î²0 = 1 , Ë™Î²1 = 0 . Considering instead the SDE case gives the schedule Î±t = t(1 âˆ’ t)/d t, Î² t = t2/d t which is a point mass schedule per Proposition 7.1 since Ë™Î²t = O(âˆšÎ²t) as 

t â†’ 0+. We now have Ë™Î±0 = 1 , Ë™Î±1 = âˆ’1 and Ë™Î²0 =Ë™Î²1 = Îµâˆ— 

> 0

= Îµâˆ— 

> 1

= 0 , and also Ë™c0 = 1 = lim tâ†’0+ Îµâˆ— 

> t

/Î± t so that the initial drift is zero per Theorem 6.3. The fact that all functions are bounded everywhere in both schedules is attractive from a numerical perspective. 

We can further characterize and motivate the constraint 

ut = t in Example 7.4 as follows. Consider the log-signal-to-noise ratio Î»t := log( Î²2 

> t

/Î± 2 

> t

). Thus, Î»0 = âˆ’âˆ and 

Î»1 = âˆ.3 It might be natural to require Î»t to be linear 

> 3Even if the schedule is a point mass schedule we still have the

in logit-time . Specifically, define Ï„ (t) := 2 log( t/ (1 âˆ’ t)) ,with inverse t(Ï„ ) = (1 + exp( âˆ’Ï„ / 2)) âˆ’1, i.e. the logistic function with growth rate 1/2. Then requiring Î»t = Ï„ (t) is equivalent to requiring ut = t.

7.3. Lazy schedule transformation of a pretrained flow matching model 

Based on what we have shown so far, we can convert any flow or diffusion model (more generally any stochastic in-terpolant) with an arbitrary schedule to its corresponding lazy schedule variant (under the Gaussian assumption). The motivation is that numerically solving the ODE or SDE is easier when the dynamics are gentle. That the dynamics in practice become gentler under the lazy schedule even when we seriously violate the Gaussian data assumption is strongly suggested by the empirical results in the next sec-tion. To make the schedule conversion particularly simple, we focus on the ut = t case considered in Example 7.4. We focus on converting from a flow matching model ve-locity vflow , which is identical to the linear ODE velocity 

b of the linear interpolant It from Definition 5.1 (see also Appendix B.1). 

Proposition 7.5 (linear velocity to lazy ODE velocity) . De-fine the schedule 

Î±t := (1 âˆ’ t)/pdt, Î²t := t/ pdt, dt := (1 âˆ’ t)2 + t2,

as in Example 7.4 and assume we are given access to the ODE velocity b = vflow of the linear interpolant from Defini-tion 5.1. Then the velocity satisfies 

b(t, x ) = ((1 âˆ’ 2t)/(dt)) x + (1 /pdt)b(t, pdtx),

for all (t, x ) âˆˆ [0 , 1] Ã— Rd. In particular, the initial velocity satisfies 

b(0 , z ) = z + b(0 , z ) = E[X] âˆ€z âˆˆ Rd.

Proposition 7.6 (linear velocity to lazy SDE drift) . Define the schedule 

Î±t := t(1 âˆ’ t)/d t, Î²t := t2/d t, dt := (1 âˆ’ t)2 + t2,

as in Example 7.4 and assume we are given access to the ODE velocity b = vflow of the linear interpolant from Defini-tion 5.1. Then the statistically optimal SDE drift satisfies 

bâˆ—(t, x ) = (2 /d t)((1 âˆ’ 2t)x + tb (t, (dt/t )x)) ,

for all (t, x ) âˆˆ (0 , 1] Ã— Rd, and the initial drift is identically zero, i.e. bâˆ—(0 , 0) = 0 .

We can restate Propositions 7.5 and 7.6 as particularly sim-ple algorithms for how to solve the lazy ODE and SDE using    

> limit lim tâ†’0+Î²t/Î± t= 0 .

6Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants  

> Figure 2. Sample images from the PRX flow model with a varying number of predictor-corrector steps for the ODE and statistically optimal SDE, respectively, sampled using the original linear flow model schedule versus converting to the lazy schedule. The text prompt â€œA car is stopped at a red lightâ€ was used. As expected, the image quality improves as the number of solver steps increases and seems to converge to the â€œground truthâ€ reference image which depends on whether an ODE or SDE is used but less so on which schedule is used. See this Zenodo link for an animation.

the linear ODE velocity. Pseudocode for these algorithms is given in Algorithms 1 and 2. We stress that we can prove results similar to Proposi-tions 7.5 and 7.6 but from a pretrained diffusion model instead of flow model, or more generally from a pretrained stochastic interpolant with any schedule and any training objective. This is omitted for brevity. 

Remark 7.7 . Note that Propositions 7.5 and 7.6 generally do not hold exactly when we only have access to an approx-imate linear flow velocity Ë†vflow â‰ˆ vflow learned from data, as is always the case in practice. 

## 8. Experiments 

To investigate the usefulness of our lazy schedules in realis-tic, high-dimensional settings with highly non-Gaussian data we conduct experiments on the latest, largest Pho-toroom Experimental (PRX) text-to-image classifier-free guided latent-space flow model (Photoroom, 2025; Almaz Â´an et al., 2025; Lipman et al., 2023; Ho & Salimans, 2022) which has around 1.3 billion parameters and is currently in beta 4. We fix the guidance strength at the recommended value of 5 and wish to investigate whether dynamically con-verting to the lazy schedules using Propositions 7.5 and 7.6 as described in detail in Algorithms 1 and 2 enables us to generate good images in fewer steps using the ODE and  

> 4Available on HuggingFace at https://huggingface. co/Photoroom/prx-1024-t2i-beta .

statistically optimal SDE, respectively. We use the predictor-corrector scheme which informally is a mix between the explicit Euler(-Maruyama) scheme and the Heun scheme and which we find is the most efficient of the three. See Appendix E for a precise definition of each scheme and some sample image comparisons. Instead of looking at standard but arbitrary goodness-of-image metrics such as Fr Â´echet Inception distance (Heusel et al., 2017) and Inception score (Salimans et al., 2016), we look at how fast the predictor-corrector solver converges to the idealized â€œground truthâ€ reference image as a function of the total number of numerical solver steps ni. Since the idealized â€œground truthâ€ reference image is unattainable in practice, we define it as the average image after nk =4096 â‰ˆ âˆ solver steps, i.e., 

img ODE reference := ( img ODE,flow  

> 4096

+ img ODE,lazy  

> 4096

)/2,

and similarly for the SDE. This is reasonable since in the idealized setting where Ë†vflow = vflow , i.e. when the flow ve-locity is learned perfectly, then img ODE,flow  

> âˆ

= img ODE,lazy  

> âˆ

,and similarly for the SDE. Results where img ODE reference := 

img ODE,flow  

> 4096

is used instead are shown in Figure 9. As text prompts we use the first 100 captions from the COCO validation dataset (Lin et al., 2014; Chen et al., 2015) as listed in Appendix G. For each experiment we fix a text prompt from our dataset and randomly sample an initial condition and (for SDE generation) a Wiener process real-ization. We let the number of numerical solver steps vary in the range n1 = 4 , n 2 = 8 , . . . , n k = 4096 . This generates 

k = 11 images of increasing quality for each of the four configurations (ODE,SDE) Ã— (flow,lazy) sharing the same prompt, initial condition and Wiener process realiza-tion, as illustrated in Figure 2. We then calculate the root mean square error (RMSE) over pixels 5 between the image produced with ni numerical solver steps and the reference image, i.e. we calculate 

RMSE (img ODE,flow  

> ni

, img ODE reference )

for each i and similarly for img ODE,lazy  

> ni

and the SDE. We repeat this experiment for each of our 100 different prompts, initial conditions and Wiener process realizations and com-pute the average RMSE. This measures numerical conver-gence of the solver under each configuration. 6 The result is plotted in Figure 3 and animations for the generated images for all 100 prompts are accessible at this Zenodo link. Under the linear flow model schedule the statistically op-timal SDE has infinite initial drift and diffusion scale   

> 5This corresponds to a dimension-normalized Euclidean dis-tance in pixel-space.
> 6To be precise, this measures convergence of the latent repre-sentations after decoding, which is more semantically meaningful than convergence in latent space.

7Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 4 8 16 32 64 128 256 512 1024 2048 

#Steps          

> 0
> 20
> 40
> 60
> 80
> Mean RMSE to Reference
> flow ODE lazy ODE flow SDE lazy SDE 4816 32 64 128 256 512 1024 2048

Actual Lazy #Steps    

> 4
> 8
> 16
> 32
> 64
> 128
> 256
> 512
> 1024
> 2048
> Equivalent Flow #Steps
> 710 18 33 65
> 621 24 37 82 171 321 727 1283
> ODE SDE Lazy best ( p<0.05) No difference

Figure 3. Left: Pixel-wise RMSE predictor-corrector convergence plot for ODE (circle markers and solid curves) and statistically optimal SDE (square markers and dashed curves) generation with the original linear flow model schedule versus the converted lazy schedule as in Figure 2. Average and 95% confidence interval (shaded bands) is over 100 prompts, initial conditions and Wiener process realizations. One sees that convergence is (almost) monotone as a function of the number of solver steps for all four configurations. Compare with Figure 8. Right: Visual answer to the question â€œUsing ni numerical solver steps with the lazy schedule, how many solver steps would I on average need to use with the linear flow model schedule to achieve the same RMSE to the reference image?â€. The answer for each ni

on the x-axis is plotted on the y-axis (cyan labels indicate nearest rounded integer). Shaded bands indicate 95% confidence intervals. The lazy schedule almost always performs statistically significantly better (and never worse) for both ODE and SDE generation, but the improvement is much bigger for SDE generation. All confidence intervals are calculated by bootstrapping with 10 ,000 samples. 

(Proposition B.1) so it is numerically undefined. To deal with this we sample the first step under the lazy sched-ule; this is easily shown to be equivalent to setting Xâˆ— 

> âˆ†t

=p(1 âˆ’ âˆ†t)2 + (âˆ† t)2Wâˆ†t where Wâˆ†t is the initial incre-ment of the fixed Wiener process. In Figure 3 (left) we see that each of the four configurations converges more or less monotonically to its reference image, but convergence is significantly faster with ODE generation than with SDE generation, indicating the difficulty in nu-merically solving SDEs. See also Figure 8 showing that the two SDEs converge more slowly to a common reference image. To test whether the effect of converting to the lazy schedule is statistically significantly positive for ODE and SDE gen-eration, respectively, we ask the following question: â€œUsing 

ni numerical solver steps with the lazy schedule, how many solver steps would I on average need to use with the linear flow model schedule to achieve the same RMSE to the ref-erence image?â€. The answer to this question is illustrated in Figure 3 (right). For ODE integration the improvement is statistically significant up until ni = 64 yet quite small; one saves at most three solver steps (which for ni = 4 almost halves the sampling time) using the lazy schedule instead of the original linear flow model schedule. For SDE genera-tion the improvement is much larger; for example, using 128 

predictor-corrector steps with the lazy schedule achieves a performance (on average) similar to using 171 solver steps with the linear flow model schedule. 

## 9. Discussion 

We stress that since the PRX model outputs a learned ap-proximation Ë†vflow â‰ˆ vflow the flow-to-lazy conversion is inexact as noted in Remark 7.7. The fact that the lazy sched-ule performs statistically better than the linear flow schedule that the PRX model was trained with is therefore very con-vincing and leads us to hypothesize that training a model from scratch using e.g. the lazy ODE schedule from Exam-ple 7.4 would lead to significantly bigger improvements in sampling quality. We also stress that the slow convergence of the SDE does 

not allow us to conclude that the statistically optimal SDE generates poorer images than the ODE. The more interest-ing metric, which is harder to measure, is distributional fit to the test dataset and the tradeoff between that and ease of numerical integration for the ODE versus the SDE. We leave this to future work but note that the sample images in Appendix F indicate that using a moderate amount of solver steps (say ni â‰¥ 64 ) for the statistically optimal SDE gener-ates aesthetically pleasing and detailed images, especially with the lazy schedule. On the theoretical side it would be valuable to extend the lazy schedule family beyond the Gaussian data assumption. We hypothesize that for the statistically optimal SDE one will end up with a point mass schedule under reasonable assumptions on the data density ÏX .8Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## Software and Data 

Code will be released upon acceptance. 

## Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 

## References 

Albergo, M., Boffi, N., Lindsey, M., and Vanden-Eijnden, E. Multimarginal generative modeling with stochastic interpolants. In Kim, B., Yue, Y., Chaudhuri, S., Fragki-adaki, K., Khan, M., and Sun, Y. (eds.), International Conference on Learning Representations , volume 2024, pp. 55884â€“55901, 2024. Albergo, M., Boffi, N. M., and Vanden-Eijnden, E. Stochas-tic interpolants: A unifying framework for flows and diffusions. Journal of Machine Learning Research , 26 (209):1â€“80, 2025. Almaz Â´an, J., Bertoin, D., and Frigg, R. Weâ€™re open-sourcing our text-to-image model and the process behind it. https://huggingface.co/blog/ Photoroom/prx-open-source-t2i-model ,November 2025. Anderson, B. D. Reverse-time diffusion equation models. 

Stochastic Processes and their Applications , 12(3):313â€“ 326, 1982. doi: 10.1016/0304-4149(82)90051-5. Boffi, N. M., Albergo, M. S., and Vanden-Eijnden, E. Flow map matching with stochastic interpolants: A mathemati-cal framework for consistency models. Transactions on Machine Learning Research , 2025. ISSN 2835-8856. Bortoli, V. D., Thornton, J., Heng, J., and Doucet, A. Diffu-sion schr Â¨odinger bridge with applications to score-based generative modeling. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems , 2021. Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server, 2015. Chen, Y., Goldstein, M., Hua, M., Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Probabilistic forecast-ing with stochastic interpolants and f Â¨ollmer processes. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Pro-ceedings of the 41st International Conference on Machine Learning , volume 235 of Proceedings of Machine Learn-ing Research , pp. 6728â€“6756. PMLR, 21â€“27 Jul 2024. Chen, Y., Vanden-Eijnden, E., and Xu, J. Lipschitz-guided design of interpolation schedules in generative models, 2025. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish-wanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , volume 30. Curran As-sociates, Inc., 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. 

https://arxiv.org/abs/2207.12598 , 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-bilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 6840â€“ 6851. Curran Associates, Inc., 2020. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Informa-tion Processing Systems , volume 35, pp. 26565â€“26577. Curran Associates, Inc., 2022. Lai, C.-H., Song, Y., Kim, D., Mitsufuji, Y., and Ermon, S. The principles of diffusion models. arXiv preprint arXiv:2510.21890 , 2025. Le Gall, J.-F. Brownian Motion, Martingales, and Stochastic Calculus . Graduate Texts in Mathematics. Springer Cham, 1 edition, 2016. ISBN 978-3-319-31089-3. doi: 10.1007/ 978-3-319-31089-3. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll Â´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T. (eds.), Computer Vi-sion â€“ ECCV 2014 , pp. 740â€“755, Cham, 2014. Springer International Publishing. Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Repre-sentations , 2023. Liu, Q. Let us flow together. https://www.cs. utexas.edu/Ëœlqiang/PDF/flow_book.pdf ,December 2024. Working draft (dated December 24, 2024). Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations , 2023. 9Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., and Xie, S. Sit: Exploring flow and diffusion-based generative models with scalable interpolant trans-formers. In Leonardis, A., Ricci, E., Roth, S., Rus-sakovsky, O., Sattler, T., and Varol, G. (eds.), Computer Vision â€“ ECCV 2024 , pp. 23â€“40, Cham, 2024. Springer Nature Switzerland. ISBN 978-3-031-72980-5. Negrel, H., Coeurdoux, F., Albergo, M. S., and Vanden-Eijnden, E. Multitask learning with stochastic inter-polants. In Advances in Neural Information Processing Systems (NeurIPS 2025) , 2025. Accepted to NeurIPS 2025 as a Spotlight; camera-ready hosted on OpenRe-view. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models, 2021. Photoroom. PRX: Open Text-to-Image Generative Model (prx-1024-t2i-beta). https://huggingface. co/Photoroom/prx-1024-t2i-beta , 2025. Hugging Face model card. Github commit ID: 84772b42065c4ffcfc10a7269e1aebf104fc0ef1. Pooladian, A.-A. and Niles-Weed, J. Plug-in estimation of schr Â¨odinger bridges. SIAM Journal on Mathematics of Data Science , 7(3):1315â€“1336, 2025. doi: 10.1137/ 24M1687340. Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y., and Chen, R. T. Q. Multisam-ple flow matching: Straightening flows with minibatch couplings. In Krause, A., Brunskill, E., Cho, K., Engel-hardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learn-ing , volume 202 of Proceedings of Machine Learning Research , pp. 28100â€“28127. PMLR, 23â€“29 Jul 2023. Sabour, A., Fidler, S., and Kreis, K. Align your steps: optimizing sampling schedules in diffusion models. In 

Proceedings of the 41st International Conference on Ma-chine Learning , ICMLâ€™24. JMLR.org, 2024. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Rad-ford, A., Chen, X., and Chen, X. Improved techniques for training gans. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neu-ral Information Processing Systems , volume 29. Curran Associates, Inc., 2016. Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. Diffu-sion schr Â¨odinger bridge matching. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Sys-tems , volume 36, pp. 62183â€“62223. Curran Associates, Inc., 2023. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequi-librium thermodynamics. In Bach, F. and Blei, D. (eds.), 

Proceedings of the 32nd International Conference on Ma-chine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 2256â€“2265, Lille, France, 07â€“09 Jul 2015. PMLR. Song, Y. and Ermon, S. Generative modeling by estimat-ing gradients of the data distribution. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch Â´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. Song, Y. and Ermon, S. Improved techniques for training score-based generative models. In Larochelle, H., Ran-zato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), 

Advances in Neural Information Processing Systems , vol-ume 33, pp. 12438â€“12448. Curran Associates, Inc., 2020. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-mon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations , 2021. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Con-sistency models. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-ceedings of the 40th International Conference on Ma-chine Learning , volume 202 of Proceedings of Machine Learning Research , pp. 32211â€“32252. PMLR, 23â€“29 Jul 2023. Stein, C. M. Estimation of the Mean of a Multivariate Normal Distribution. The Annals of Statistics , 9(6):1135 â€“ 1151, 1981. doi: 10.1214/aos/1176345632. Xue, S., Liu, Z., Chen, F., Zhang, S., Hu, T., Xie, E., and Li, Z. Accelerating diffusion sampling with optimized time steps. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 8292â€“8301, 2024. doi: 10.1109/CVPR52733.2024.00792. 10 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## A. Proofs 

Proposition 4.2 (intra-interpolant conversion formulas) . For a fixed interpolation schedule (Î±, Î² ) and non-negative 

Îµ âˆˆ C1([0 , 1]) , each of the functions Î·Z , Î· X , s, b Îµ uniquely defines all of the others on (0 , 1) Ã— Rd. In particular, each function can be expressed in terms of the score as 

Î·Z (t, x ) = âˆ’Î±ts(t, x ),Î·X (t, x ) =  x + Î±2 

> t

s(t, x )/Î² t,bÎµ(t, x ) = ( Îµâˆ— 

> t

+ Îµt)s(t, x ) + ( Ë™Î²t/Î² t)x. 

Proof. For t âˆˆ (0 , 1) we can clearly isolate the score s in each of the relations in the proposition. It therefore suffices to prove that each of the relations hold for all (t, x ) âˆˆ (0 , 1) Ã— Rd, as claimed. The first relation is a standard result that can be proved with Steinâ€™s lemma (also known as Gaussian integration by parts )(Stein, 1981) saying that s(t, x ) = âˆ’Î·Z (t, x )/Î± t for (t, x ) âˆˆ [0 , 1) Ã— Rd, see e.g. Theorem 8 in Albergo et al. (2025). The second relation follows easily from the definition of the stochastic interpolant (Definition 1.1) It = Î±tZ + Î²tX and linearity of expectation. Indeed, taking expectations conditional on It = x in the definition for It gives 

Î·X (t, x ) = 1

Î²t

(x âˆ’ Î±tÎ·Z (t, x )) = 1

Î²t

 x + Î±2 

> t

s(t, x ),

where we used the first relation to get the last equality. To get the final relation, plug the preceding relations into the definition of the drift 

bÎµ(t, x ) = Ë™ Î±tÎ·Z (t, x ) + Ë™Î²tÎ·X (t, x ) + Îµts(t, x )= Ë™ Î±t(âˆ’Î±ts(t, x )) + Ë™Î²t

 1

Î²t

 x + Î±2 

> t

s(t, x )

+ Îµts(t, x )= Î±2

> t

Ë™Î²t

Î²t

âˆ’ Î±t Ë™Î±t + Îµt

!

s(t, x ) + Ë™Î²t

Î²t

x. 

Since by Definition 4.1 Îµâˆ— 

> t

= Î±2 

> t

Ë™Î²t/Î² t âˆ’ Î±t Ë™Î±t this concludes the proof of the proposition. 

Theorem 4.3. Assume we are given statistically learnt estimates Ë†Î·Z , Ë†Î·X , Ë†s or Ë†bÎµ of the true functions from Definition 1.3 with remaining estimates given by Proposition 4.2. Let XÎµt solve the SDE (1) and consider the plug-in SDE 

d Ë†XÎµt = Ë†bÎµ(t, Ë†XÎµt ) d t + âˆš2Îµt dWt,

solved forward in time t with Ë†XÎµ 

> 0

âˆ¼ N (0 , I) independent of W . Then the minimizer of the Kullback-Leibler divergence between the path measures of XÎµ and Ë†XÎµ over the diffusion scale Îµ is 

argmin 

> Îµâ‰¥0,Îµ âˆˆC1((0 ,1])

KL 



XÎµ, Ë†XÎµ

= Îµâˆ—,

and the minimum is 

KL 



XÎµâˆ—

, Ë†XÎµâˆ— 

=

Z 10

Îµâˆ— 

> t

Eâˆ¥s(t, I t) âˆ’ Ë†s(t, I t)âˆ¥2 dt. 

Proof. An application of Girsanovâ€™s theorem gives 

KL( XÎµ, Ë†XÎµ) = 14

Z 10

1

Îµt

E

h

bÎµ(t, X Îµt ) âˆ’ Ë†bÎµ(t, X Îµt ) 2i

dt

= 14

Z 10

1

Îµt

E

h

bÎµ(t, I t) âˆ’ Ë†bÎµ(t, I t) 2i

dt

= 14

Z 10

(Îµâˆ— 

> t

+ Îµt)2

Îµt

Eâˆ¥s(t, I t) âˆ’ Ë†s(t, I t)âˆ¥2 dt, 

11 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

where in the second equality we used that Law XÎµt = Law It and in the third equality we used Proposition 4.2 with the assumptions on Ë†b and Ë†s in this theorem. From the last expression we see that minimizing the KL-divergence over the diffusion scale Îµ amounts to minimizing the expression (Îµâˆ— 

> t

+ Îµt)2/Îµ t over Îµt for all t âˆˆ [0 , 1] since the expectation term does not depend on Îµt. Differentiating this expression with respect to Îµt and setting the resulting expression to zero gives the (unique) minimizer Îµt = Îµâˆ— 

> t

for all t âˆˆ [0 , 1] , and the theorem follows. 

Proposition 5.2. For the linear interpolant It from Definition 5.1 it holds that Îµâˆ— 

> t

= (1 âˆ’ t)/t . Therefore 

Z ts

2Îµâˆ— 

> u

du = 2(log( t) âˆ’ t âˆ’ log( s) + s)

for 0 < s â‰¤ t â‰¤ 1 and the integral is infinite for s = 0 .Proof. We have Î±t = 1 âˆ’ t, Î² t = t so that Ë™Î±t = âˆ’1 and Ë™Î²t = 1 . Thus 

Îµâˆ— 

> t

= Î±2

> t

Ë™Î²t

Î²t

âˆ’ Î±t Ë™Î±t = (1 âˆ’ t)2

t + (1 âˆ’ t) = 1 âˆ’ tt .

The rest of the proposition easily follows. Recall Definition 5.3. 

Definition 5.3. For t âˆˆ [0 , 1] we define the functions 

ct := Î±t + Î²t, ut := Î²t/c t.

For the remaining proofs we will often reference the following lemma with some useful algebraic identities. 

Lemma A.1. For t âˆˆ (0 , 1] the following identities hold. 

1 âˆ’ ut = Î±t

ct

, (2) 

Ë™ut

ut

= Ë™Î²t

Î²t

âˆ’ Ë™ct

ct

= Îµâˆ—

> t

Î±tct

. (3) 

Proof. Equation (2) is trivial: 

1 âˆ’ ut = ct âˆ’ Î²t

ct

= Î±t

ct

.

To get the leftmost equality in Equation (3), observe that 

Ë™ut = Ë™Î²tct âˆ’ Î²t Ë™ct

c2

> t

,

so 

Ë™ut

ut

= Ë™Î²tct âˆ’ Î²t Ë™ct

ctÎ²t

= Ë™Î²t

Î²t

âˆ’ Ë™ct

ct

.

Finally, 

ct

Ë™ut

ut

= Î±t

Ë™Î²t

Î²t

+ Ë™Î²t âˆ’ Ë™Î±t âˆ’ Ë™Î²t = Îµâˆ—

> t

Î±t

,

which proves the rightmost equality in Equation (3). 

Proposition 5.4 (inter-conversion formulas) . For a schedule (Î±, Î² ) and non-negative Îµ âˆˆ C1([0 , 1]) , the functions 

Î·Z , Î· X , s, b Îµ are uniquely defined from any of the functions Î·Z , Î· X , s, b Îµ associated with the linear interpolant. In particular, 

Î·Z (t, x ) = Î·Z (ut, x/c t), Î·X (t, x ) = Î·X (ut, x/c t),s(t, x ) = (1 /c t)s(ut, x/c t),bÎµ(t, x ) = Ë™ct

ct

x + ct Ë™utbÎµ



ut, xct



for Îµut = Î±tÎµt

Î²tÎµâˆ—

> t

.

12 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Proof. Using Equation (2) we get 

Î·Z



ut, xct



= E



Z | (1 âˆ’ ut)Z + utX = xct



= E



Z | Î±t

ct

Z + Î²t

ct

X = xct



= E[Z | Î±tZ + Î²tX = x]= Î·Z (t, x ).

An analogous derivation proves Î·X (t, x ) = Î·X (ut, x/c t).From the relation s(t, x ) = âˆ’(1 /Î± t)Î·Z (t, x ) (as in Proposition 4.2) combined with Î·Z (t, x ) = Î·Z (ut, x/c t) we get 

s



ut, xct



= âˆ’ 11 âˆ’ ut

Î·Z



ut, xct



= âˆ’ 11 âˆ’ ut

Î·Z (t, x ).

Observing that 1/(1 âˆ’ ut) = ct/Î± t we get 

1

ct

s



ut, xct



= âˆ’ 1

Î±t

Î·Z (t, x ) = s(t, x ),

as claimed. Getting the final relation requires a bit more work, but the overall strategy is the same as for the above; we first convert the drift to the score, then the score to the linear score, and then the linear score to the linear drift using the particular linear diffusion scale Îµut = ( Î±tÎµt)/(Î²tÎµâˆ— 

> t

). To be precise, we first apply Proposition 4.2, then the relation we just proved, and then Proposition 4.2 again to get 

bÎµ(t, x ) = ( Îµâˆ— 

> t

+ Îµt)s(t, x ) + Ë™Î²t

Î²t

x

= Îµâˆ— 

> t

+ Îµt

ct

s(ut, xct

) + Ë™Î²t

Î²t

x

= Îµâˆ— 

> t

+ Îµt

ct

ï£«ï£­ bÎµ

ut, xct



âˆ’ xutct

Îµâˆ— 

> ut

+ Îµut

ï£¶ï£¸ + Ë™Î²t

Î²t

x. 

We now plug in the linear diffusion scale Îµut = ( Î±tÎµt)/(Î²tÎµâˆ— 

> t

), which we motivate in the proof of Theorem 5.5. By Proposition 5.2 and Equation (2) we get Îµâˆ— 

> ut

= (1 âˆ’ ut)/u t = Î±t/Î² t so that 

Îµâˆ— 

> ut

+ Îµut = Î±t

Î²t

 Îµâˆ— 

> t

+ Îµt

Îµâˆ—

> t



.

Thus 

bÎµ(t, x ) = Î²tÎµâˆ—

> t

Î±tct



bÎµ



ut, xct



âˆ’ xutct



+ Ë™Î²t

Î²t

x

= ct Ë™ut



bÎµ



ut, xct



âˆ’ xutct



+ Ë™Î²t

Î²t

x (using Equation (3)) 

= ct Ë™utbÎµ



ut, xct



+ Ë™Î²t

Î²t

âˆ’ Ë™ut

ut

!

x

= ct Ë™utbÎµ



ut, xct



+ Ë™ct

ct

x. (using Equation (3)) This concludes the proof. 13 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Theorem 5.5 (pathwise conversion) . Let (Î±, Î² ) be a schedule satisfying the requirements in Definition 1.1, let z âˆˆ Rd and let (Wt)tâˆˆ[0 ,1] be a Wiener process realization. Let XÎµt solve the SDE (1) in physical time t from XÎµ 

> 0

= z driven by W with diffusion scale Îµ. Let XÎµu solve the SDE (1) with linear schedule in reparameterized time u = ut from XÎµ 

> 0

= z driven by 

W u =

Z tu

> 0

p Ë™us dWs, u âˆˆ [0 , 1] 

with diffusion scale Îµu = ( Î±tu Îµtu )/(Î²tu Îµâˆ— 

> tu

). Then the solution XÎµt can be obtained from the solution XÎµut associated with the linear interpolant via the formula 

XÎµt = ctXÎµut , t âˆˆ [0 , 1] .

In particular, XÎµ 

> 1

= XÎµ 

> 1

(since c1 = 1 ). Proof. First of all we note that W is indeed a Wiener process in reparameterized time u = ut by L Â´evyâ€™s characterization theorem (see e.g. Theorem 5.12 in Le Gall (2016)) since it has quadratic variation W u = R tu 

> 0

Ë™us ds = R u 

> 0

du = u. Thus both SDEs in the theorem statement are well-defined with unique solutions, so it suffices to prove that their dynamics coincide i.e. that dXÎµt = d 



ctXÎµut



for all t âˆˆ (0 , 1) . By the ItË† o product rule, the latter infinitesimal is 

d



ctXÎµut



= Ë™ ctXÎµut dt + ctbÎµ

ut, X Îµut



dut + ct

p2Îµut dW ut

= Ë™ ctXÎµut dt + ct Ë™utbÎµ

ut, X Îµut



dt + ct

p2Îµut Ë™ut dWt.

Using the linear diffusion scale Îµut = ( Î±tÎµt)/(Î²tÎµâˆ— 

> t

) with Equation (3) gives 

Îµut = ut

Ë™ut

Îµt

Î²tct

= Îµt

c2 

> t

Ë™ut

,

which we plug into the above infinitesimal to arrive at 

d



ctXÎµut



= Ë™ ctXÎµut dt + ct Ë™utbÎµ

ut, X Îµut



dt + âˆš2Îµt dWt.

Using now Proposition 5.4 we see that 

dXÎµt = bÎµ(t, X Îµt ) d t + âˆš2Îµt dWt = Ë™ct

ct

XÎµt dt + ct Ë™utbÎµ



ut, XÎµt

ct



dt + âˆš2Îµt dWt,

which after inserting the theoremâ€™s ansatz XÎµt /c t = XÎµut shows that dXÎµt = d 



ctXÎµut



. This concludes the proof. To prove Theorem 6.3 we first state and prove a useful lemma. 

Lemma A.2. For any point mass schedule (Î±, Î² ) it holds that 

u0 := lim 

> tâ†’0+

ut = 0 â‡â‡’ Î²t = o(Î±t) as t â†’ 0+.

Proof. We have that 

u0 := lim 

> tâ†’0+

ut = lim 

> tâ†’0+

Î²t

Î±t + Î²t

= lim 

> tâ†’0+

11 + Î±t

> Î²t

.

Clearly this limit is zero if and only if the ratio Î±t/Î² t â†’ âˆ as t â†’ 0+, or equivalently if and only if Î²t = o(Î±t) as 

t â†’ 0+.

Theorem 6.3. Let (Î±, Î² ) be a point mass schedule as in Definition 6.1. Then given any non-negative bounded diffusion scale Îµ âˆˆ C1([0 , 1]) , the SDE (1) is well-defined in the classical sense for t âˆˆ (0 , 1] with initial position XÎµ 

> 0

= 0 , and the initial drift is well-defined in distribution. It holds that Law( XÎµt ) = Law( It) for all t âˆˆ [0 , 1] and Theorem 5.5 still applies: in particular, the point mass SDE solution and initial drift relates to the solution and initial condition of the linear interpolant as XÎµt = ctXÎµut for t âˆˆ [0 , 1] and bÎµ(0 , X Îµ 

> 0

) =   Ë™c0 âˆ’ lim tâ†’0+ (Îµt/Î± t)XÎµ

> 0

.

14 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Proof. Since the point mass schedule from Definition 6.1 only differs from the regular schedule from Definition 1.1 at t = 0 

this is the only case we need to consider. In particular, Theorem 5.5 holds on (0 , 1] and also in the limit as t â†’ 0+ since from the boundary conditions Î±0 = Î²0 = 0 we have c0 = u0 = 0 so that XÎµ 

> 0

= c0XÎµ 

> 0

= 0 for all (z, Ï‰ ) âˆˆ Rd Ã— Î© with 

z = XÎµ 

> 0

the initial condition to the linear interpolant and Ï‰ an event in the probability space of the shared Wiener process. Substituting now ctx for x in Proposition 5.4 gives the conversion formula 

bÎµ(t, c tx) = Ë™ ctx + ct Ë™utbÎµ(ut, x ),

for all (t, x ) âˆˆ (0 , 1) Ã— Rd with Îµut = Î±tÎµt

> Î²tÎµâˆ—
> t

. Going to the limit gives us 

lim 

> tâ†’0+

bÎµ(t, c tx) = lim 

> tâ†’0+

Ë™ctx + lim 

> tâ†’0+

ct Ë™utbÎµ(ut, x ),

so it remains to prove that lim tâ†’0+ ct Ë™utbÎµ(ut, x ) = âˆ’ lim tâ†’0+ (Îµt/Î± t)x under our assumptions in Definition 6.1. To this end we first consider the expression for the drift of the linear interpolant, 

bÎµ(ut, x ) =  Îµâˆ— 

> ut

+ Îµut

s(ut, x ) + Ë™Î²ut

Î²ut

x (by Proposition 4.2) 

=

 1 âˆ’ ut

ut

+ Î±tÎµt

Î²tÎµâˆ—

> t

 

âˆ’ Î·Z (ut, x )1 âˆ’ ut



+ 1

ut

x (by Proposition 4.2 and Proposition 5.2) 

= âˆ’ Î±t

Î²t



1 + Îµt

Îµâˆ—

> t

 Î·Z (ut, x )ct

Î±t

+ ct

Î²t

x (by Equation (2)) 

= ct

Î²t



x âˆ’ Î·Z (ut, x ) âˆ’ Îµt

Îµâˆ—

> t

Î·Z (ut, x )



= ct

Î²t

(x âˆ’ Î·Z (ut, x )) âˆ’ utÎµt

Ë™utÎ±tÎ²t

Î·Z (ut, x ) (by Equation (3)) 

= ct

Î²t

(x âˆ’ Î·Z (ut, x )) âˆ’ Îµt

Ë™utÎ±tct

Î·Z (ut, x ).

Multiplying by ct Ë™ut and going to the limit with the assumptions on the point mass schedule (Î±, Î² ) from Definition 6.1 finally gives 

ct Ë™utbÎµ(ut, x ) = c2 

> t

Ë™ut

Î²t

(x âˆ’ Î·Z (ut, x )) âˆ’ Îµt

Î±t

Î·Z (ut, x )

â†’ âˆ’ lim 

> tâ†’0+

Îµt

Î±t

x as t â†’ 0+,

where to get the limit we used that Î·Z (ut, x ) = EZ | Iut = x â†’ x as t â†’ 0+ since u0 = 0 , Ë™u0 < âˆ (and I0 = Z), as well as c2 

> t

Ë™ut/Î² t = Ë™ ut(Î±2 

> t

+ Î²2 

> t

+ 2 Î±tÎ²t)/Î² t â†’ Î±2 

> t

Ë™ut/Î² t â†’ C as t â†’ 0+ for some non-negative bounded constant 

C âˆˆ R+ since Ë™u0 < âˆ, Î± 0 = Î²0 = 0 , and since the point mass schedule is assumed to satisfy Î±2 

> t

= O(Î²t) as t â†’ 0+. This concludes the proof. We note that the point mass schedule assumption Î±2 

> t

= O(Î²t) as t â†’ 0+ can be relaxed, but this comes at the expense of requiring Î·Z (ut, x ) to converge sufficiently fast to x as t â†’ 0+ so that the limiting product lim tâ†’0+ (( Î±2 

> t

Ë™ut)/Î² t)( x âˆ’

Î·Z (ut, x )) still goes to zero. To the best of our knowledge, this requires assuming that the data distribution ÏX has sufficiently light tails; see Assumption B.3 and Theorem B.4 in Chen et al. (2024). We feel like our assumption is simpler, and for what is done in this paper the assumption Î±2 

> t

= O(Î²t) as t â†’ 0+ is not prohibitively restrictive. 

Proposition 6.5. Let (Î±, Î² ) be any valid schedule, possibly a point mass schedule. Then under the assumptions in Theorem 4.3, the minimal path-measure KL-divergence attained when Îµ = Îµâˆ— is invariant to the choice of schedule. Proof. This is essentially already proved in Proposition 2.5 in Chen et al. (2025) whose proof we restate below using the notation of this paper. Recall that under the assumptions in Theorem 4.3, the minimum Kullback-Leibler divergence between the path measures of XÎµ and Ë†XÎµ over the diffusion scale Îµ is 

KL 



XÎµâˆ—

, Ë†XÎµâˆ— 

=

Z 10

Îµâˆ— 

> t

Eâˆ¥s(t, I t) âˆ’ Ë†s(t, I t)âˆ¥2 dt. 

15 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Define now the time change Î· : [0 , 1] â†’ R+ by Î·t := Î±t/Î² t, let ÏÎ· be the probability density function of the random variable Y = X + Î·Z for Î· âˆˆ [0 , âˆ] and denote by s(Î·, y ) the score of Y 7. Since from Definition 1.1 we can write 

It = Î±tZ + Î²tX = Î²t(X + Î·tZ) = Î²tY we see that the scores are related as 

s(t, x ) = âˆ‡ log Ït(x) = 1

Î²t

âˆ‡ log ÏÎ·

 xÎ²t



= 1

Î²t

s



Î·, xÎ²t



,

and similarly we have that the score estimates are related as Ë†s(t, x ) = Ë† s(Î·, x/Î² )/Î² t. Substituting the score of Y for the score of I in the expression for the KL-minimum gives 

KL 



XÎµâˆ—

, Ë†XÎµâˆ— 

=

Z 10

Îµâˆ—

> t

Î²2

> t

E

"

s



Î·t, It

Î²t



âˆ’ Ë†s



Î·t, It

Î²t

 2#

dt

=

Z 10

Îµâˆ—

> t

Î²2

> t

E

h

âˆ¥s(Î·t, Y ) âˆ’ Ë†s(Î·t, Y )âˆ¥2i

dt. 

Noting that 

Îµâˆ—

> t

Î²2

> t

= Î±2

> t

Î²2

> t

Ë™Î²t

Î²t

âˆ’ Ë™Î±t

Î±t

!

= âˆ’ Î±t

Î²t

ddt

 Î±t

Î²t



= âˆ’Î·t Ë™Î·t,

we change the integration variable from t to Î· in the KL-expression to get 

KL 



XÎµâˆ—

, Ë†XÎµâˆ— 

=

Z Î·0

> Î·1

Î·E

h

âˆ¥s(Î·, Y ) âˆ’ Ë†s(Î·, Y )âˆ¥2i

dÎ· =

Z âˆ

> 0

Î·E

h

âˆ¥s(Î·, Y ) âˆ’ Ë†s(Î·, Y )âˆ¥2i

dÎ·. 

Crucially, we substituted Î·1 = 0 but also Î·0 := lim tâ†’0+ Î·t = âˆ in the last expression, as in the original proof. We stress that Î·0 = âˆ holds even for a point mass schedule since in Definition 6.1 we require Î²t = o(Î±t) as t â†’ 0+, which is equivalent to requiring u0 = 0 by Lemma A.2. This requirement cannot be relaxed without making the initial law of the resulting interpolant dependent on ÏX , which is what we are trying to sample in the first place. Since the final expression for the KL-minimum is independent of the schedule (Î±, Î² ), the proof is concluded. 

Proposition 7.1 (lazy schedule families) . Assume that X âˆ¼ N (0 , I). Then for all (t, x ) âˆˆ [0 , 1] Ã— Rd it holds that 1. 

b(t, x ) = 0 â‡â‡’ Î±2 

> t

+ Î²2 

> t

= 1 .

In words, the ODE velocity is identically zero if and only if the schedule is variance preserving. 2. 

bâˆ—(t, x ) = 0 â‡â‡’ Î±2 

> t

+ Î²2 

> t

= Î²t.

This implies Î±0 = 0 , Î² t = o(Î±t), Î± 2 

> t

= O(Î²t) as t â†’ 0+. We have that Ë™u0 < âˆ â‡â‡’ Ë™Î²t = O(âˆšÎ²t) in which case the schedule is a point mass schedule per Definition 6.1. It also holds for all t âˆˆ [0 , 1] that Îµâˆ— 

> t

= Ë™Î²t/2 so that 

Z ts

2Îµâˆ— 

> u

du = Î²t âˆ’ Î²s for 0 â‰¤ s â‰¤ t â‰¤ 1.

Proof. Under the assumption X âˆ¼ N (0 , I) it holds that It âˆ¼ N (0 , (Î±2 

> t

+ Î²2 

> t

) I) so that s(t, x ) = âˆ’x/ (Î±2 

> t

+ Î²2 

> t

). We first consider what this entails for the ODE case and then for the SDE case. It suffices to consider the condition b(t, x ) = 0 for all (t, x ) âˆˆ (0 , 1) Ã— Rd since by continuity the condition b(t, x ) = 0 for all (t, x ) âˆˆ [0 , 1] Ã— Rd is then covered too. From Proposition 4.2 with Îµ â‰¡ 0 we see that requiring b(t, x ) = 0 for all (t, x ) âˆˆ (0 , 1) Ã— Rd is equivalent to requiring 

Îµâˆ— 

> t

s(t, x ) + Ë™Î²t/Î² tx = 0 for all (t, x ) âˆˆ (0 , 1) Ã— Rd. This is equivalent to 

Ë™Î²t

Î²t

âˆ’ Îµâˆ—

> t

Î±2 

> t

+ Î²2

> t

= 0 âˆ€t âˆˆ (0 , 1) ,

> 7

We are abusing notation here, but in this proof it will always be clear from the argument ( t or Î·) whether we are talking about the density of I or Y .

16 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

which by multiplying by Î±2 

> t

+ Î²2 

> t

is equivalent to 

Îµâˆ— 

> t

= Î±2

> t

Ë™Î²t

Î²t

+ Î²t Ë™Î²t âˆ€t âˆˆ (0 , 1) ,

which from Definition 4.1 is again equivalent to 

Î±t Ë™Î±t = âˆ’Î²t Ë™Î²t âˆ€t âˆˆ (0 , 1) .

This is also equivalent to 

ddt

 Î±2 

> t

+ Î²2

> t

 = C âˆ€t âˆˆ (0 , 1) ,

for some constant C. The boundary conditions Î±1 = Î²0 = 0 , Î± 0 = Î²1 = 1 then give Î±2 

> t

+ Î²2 

> t

= 1 , proving the ODE case. Next, we consider the SDE case. Here we use Proposition 4.2 with Îµ = Îµâˆ— to see that requiring bâˆ—(t, x ) = 0 for all 

(t, x ) âˆˆ (0 , 1) Ã— Rd is equivalent to requiring 2Îµâˆ— 

> t

s(t, x ) + Ë™Î²t/Î² tx = 0 for all (t, x ) âˆˆ (0 , 1) Ã— Rd. This is equivalent to 

Ë™Î²t

Î²t

âˆ’ 2Îµâˆ—

> t

Î±2 

> t

+ Î²2

> t

= 0 âˆ€t âˆˆ (0 , 1) ,

which by multiplying by Î±2 

> t

+ Î²2 

> t

is equivalent to 

2Îµâˆ— 

> t

= Î±2

> t

Ë™Î²t

Î²t

+ Î²t Ë™Î²t âˆ€t âˆˆ (0 , 1) ,

which from Definition 4.1 is again equivalent to 

Î±2

> t

Ë™Î²t

Î²t

âˆ’ 2Î±t Ë™Î±t = Î²t Ë™Î²t âˆ€t âˆˆ (0 , 1) .

Denoting a(t) := Î±2 

> t

and observing that Ë™a(t) = 2 Î±t Ë™Î±t this is a first-order linear inhomogeneous ODE of the form 

Ë™a(t) âˆ’ Ë™Î²t

Î²t

a(t) = âˆ’Î²t Ë™Î²t.

Its solutions are of the form a(t) = CÎ² t âˆ’ Î²2 

> t

. Imposing the boundary conditions Î±1 = 0 , Î² 1 = 1 gives C = 1 so that 

Î±t = pÎ²t(1 âˆ’ Î²t), or equivalently Î±2 

> t

+ Î²2 

> t

= Î²t, as claimed. Note that Î²t/Î± t = âˆšÎ²t/âˆš1 âˆ’ Î²t â†’ 0 as t â†’ 0+, or equivalently Î²t = o(Î±t) as t â†’ 0+, as claimed. Similarly, 

Î±2 

> t

/Î² t = 1 âˆ’ Î²t â†’ 1 as t â†’ 0+ so that Î±2 

> t

= O(Î²t) as t â†’ 0+. Differentiating yields 

Ë™ut = ddt

Î²t

pÎ²t(1 âˆ’ Î²t) + Î²t

!

= Ë™Î²t

2pÎ²t(1 âˆ’ Î²t)



1 + 2 pÎ²t(1 âˆ’ Î²t)

 â†’ lim 

> tâ†’0+

Ë™Î²t

2âˆšÎ²t

as t â†’ 0+,

which proves that Ë™u0 < âˆ if and only if Ë™Î²t = O(âˆšÎ²t) as t â†’ 0+, as claimed. In that case we have shown that the schedule is indeed a point mass schedule as per Definition 6.1. Finally, we prove the claimed identity for Îµâˆ— 

> t

. We observe that Ë™Î±t = Ë™Î²t(1 âˆ’ 2Î²t)/(2 Î±t) so that 

Îµâˆ— 

> t

= Î±2

> t

Ë™Î²t

Î²t

âˆ’ Î±t Ë™Î±t = Ë™Î²t(1 âˆ’ Î²t) âˆ’ 12 Ë™Î²t(1 âˆ’ 2Î²t) = 12 Ë™Î²t,

as claimed. It follows that also R ts 2Îµâˆ— 

> u

du = Î²t âˆ’ Î²s for any 0 â‰¤ s â‰¤ t â‰¤ 1.

Proposition 7.5 (linear velocity to lazy ODE velocity) . Define the schedule 

Î±t := (1 âˆ’ t)/pdt, Î²t := t/ pdt, dt := (1 âˆ’ t)2 + t2,

17 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

as in Example 7.4 and assume we are given access to the ODE velocity b = vflow of the linear interpolant from Definition 5.1. Then the velocity satisfies 

b(t, x ) = ((1 âˆ’ 2t)/(dt)) x + (1 /pdt)b(t, pdtx),

for all (t, x ) âˆˆ [0 , 1] Ã— Rd. In particular, the initial velocity satisfies 

b(0 , z ) = z + b(0 , z ) = E[X] âˆ€z âˆˆ Rd.

Proof. Observing that ct = 1 /âˆšdt, Ë™ct = (1 âˆ’ 2t)/(dt

âˆšdt), u t = t, Ë™ut = 1 and applying Proposition 5.4 with Îµ â‰¡ 0 so that also Îµ â‰¡ 0 gives the conversion formula 

b(t, x ) = 1 âˆ’ 2tdt

x + 1

âˆšdt

b(t, pdtx).

In particular, since d0 = 1 and u0 = 0 we get 

b(0 , z ) = z + b(0 , z ) = z + E[X | Z = z] âˆ’ E[Z | Z = z] = E[X].

Note that all equations are well-defined even when t âˆˆ { 0, 1}. This concludes the proof. 

Proposition 7.6 (linear velocity to lazy SDE drift) . Define the schedule 

Î±t := t(1 âˆ’ t)/d t, Î²t := t2/d t, dt := (1 âˆ’ t)2 + t2,

as in Example 7.4 and assume we are given access to the ODE velocity b = vflow of the linear interpolant from Definition 5.1. Then the statistically optimal SDE drift satisfies 

bâˆ—(t, x ) = (2 /d t)((1 âˆ’ 2t)x + tb (t, (dt/t )x)) ,

for all (t, x ) âˆˆ (0 , 1] Ã— Rd, and the initial drift is identically zero, i.e. bâˆ—(0 , 0) = 0 .Proof. Observing that ct = t/d t, Ë™ct = (1 âˆ’ 2t2)/d 2 

> t

, u t = t, Ë™ut = 1 and applying Proposition 5.4 with Îµ = Îµâˆ— so that 

Îµut = Îµt = Î±t

Î²t

= 1 âˆ’ tt = Îµâˆ—

> t

gives the conversion formula 

bâˆ—(t, x ) = 1 âˆ’ 2t2

td t

x + tdt

bâˆ—



t, dt

t x



,

which is valid for t âˆˆ (0 , 1] . With Proposition 4.2 we can convert the statistically optimal SDE drift Îµ = Îµâˆ— to the ODE velocity ( Îµ â‰¡ 0) , which when applied to the linear interpolant evaluated in (t, (dt/t )x) reads 

bâˆ—



t, dt

t x



= 2 b



t, dt

t x



âˆ’

Ë™Î²t

Î²t

dt

t x = 2 b



t, dt

t x



âˆ’ dt

t2 x. 

Inserting this into the expression for bâˆ—(t, x ) gives 

bâˆ—(t, x ) = 1 âˆ’ 2t2 âˆ’ dt

td t

x + 2tdt

b



t, dt

t x



= 2

dt



(1 âˆ’ 2t)x + tb 



t, dt

t x

 

,

which is still only valid for t âˆˆ (0 , 1] . This proves the first part of the proposition. To analyze the case t = 0 we substitute 

(t/d t)x for x to get 

bâˆ—



t, tdt

x



= 2td2

> t

(1 âˆ’ 2t)x + 2tdt

b(t, x ) â†’ 0 as t â†’ 0+,

since d0 = 1 and b(0 , x ) = E[X] âˆ’ x < âˆ for all x âˆˆ Rd. As also (t/d t)x â†’ 0 as t â†’ 0+ we see that the initial drift satisfies bâˆ—(0 , 0) = 0 , as claimed. This concludes the proof. 18 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## B. Relationship to flows and diffusions 

In this section we rigorously describe how the stochastic interpolant from Definition 1.1 relates to the objects studied in flow and diffusion models, respectively. The relationship to diffusion models is analyzed in section 5.1 in (Albergo et al., 2025) but to the best of our knowledge the particular connection to the statistically optimal diffusion scale Îµâˆ— (see Definition 4.1 and Theorem 4.3) as presented in Proposition B.5 has not been pointed out before. First we prove some fundamental properties of the optimal diffusion scale Îµâˆ—.

Proposition B.1. For any schedule (Î±, Î² ) satisfying the criteria in Definition 1.1, Îµâˆ— from Definition 4.1 satisfies 

Îµâˆ— 

> 0

:= lim 

> tâ†’0+

Îµâˆ— 

> t

= âˆ if lim 

> tâ†’0+

Ë™Î²t

Î²t

exists ,Îµâˆ— 

> t

âˆˆ (0 , âˆ) âˆ€t âˆˆ (0 , 1) ,Îµâˆ— 

> 1

= 0 ,

Z t

> 0

Îµâˆ— 

> s

ds = âˆ âˆ€t âˆˆ (0 , 1] .

Proof. We have 

Îµâˆ— 

> 0

:= lim 

> tâ†’0+

Îµâˆ— 

> t

= lim 

> tâ†’0+

Î±2

> t

Ë™Î²t

Î²t

âˆ’ Î±t Ë™Î±t

!

= lim 

> tâ†’0+

Ë™Î²t

Î²t

âˆ’ Ë™Î±0 = lim 

> tâ†’0+

ddt log( Î²t) âˆ’ Ë™Î±0 = âˆ,

if lim tâ†’0+ ( Ë™Î²t/Î² t) exists since Î±0 = 1 , | Ë™Î±0| < âˆ, Î² 0 = 0 as well as Î²t > 0 for t > 0. Strict positivity of Îµâˆ— for 

t âˆˆ (0 , 1) follows from the monotonicity assumptions Ë™Î±t < 0 and Ë™Î²t > 0 for all t âˆˆ (0 , 1) . Îµâˆ— 

> 1

= 0 follows from 

Î±1 = 0 , Î² 1 = 1 , | Ë™Î±1| < âˆ, | Ë™Î²1| < âˆ.To see that Îµâˆ— is not integrable, we use Ë™Î±t < 0 for t âˆˆ (0 , 1) to bound Îµâˆ— 

> t

â‰¥ Î±2  

> tddt

log( Î²t) and write 

Z t

> 0

Îµâˆ— 

> u

du = lim 

> sâ†’0+

Z ts

Îµâˆ— 

> u

du â‰¥ Î±2 

> t

lim 

> sâ†’0+

Z ts

ddu log( Î²u) d u = Î±2 

> t

(log( Î²t) âˆ’ lim 

> sâ†’0+

log( Î²s)) = âˆ âˆ€t âˆˆ (0 , 1] ,

since Î²0 = 0 and Î² âˆˆ C1([0 , 1]) . This concludes the proof. 

B.1. Flow models 

In a flow model (or more precisely a flow matching model ) one considers the time-dependent random variable (1 âˆ’ t)Z + tX 

for Z âˆ¼ N (0 , I) and X âˆ¼ ÏX independent with t âˆˆ [0 , 1] . One then utilizes that the ODE 

ddt Y flow  

> t

= vflow (t, Y flow  

> t

)

with velocity field 

vflow (t, x ) := E[X | (1 âˆ’ t)Z + tX ] âˆ’ E[Z | (1 âˆ’ t)Z + tX ]

for (t, x ) âˆˆ [0 , 1] Ã— Rd generates ÏX , i.e. Y flow  

> 1

âˆ¼ ÏX whenever Y flow  

> 0

âˆ¼ N (0 , I). One sees that this corresponds exactly to the linear interpolant from Definition 5.1, i.e. It = (1 âˆ’ t)Z + tX and 

b(t, x ) = E

h Ë™It | It = x

i

= vflow (t, x )

for all (t, x ) âˆˆ [0 , 1] Ã— Rd, where we used the notation b := bÎµâ‰¡0 for the ODE velocity of the linear interpolant from Definition 5.1. 

B.2. Diffusion models 

In a variance-preserving diffusion model (or more precisely a score-based generative model ) one corrupts data ËœY0 = X âˆ¼ ÏX

into noise ËœYâˆ = Z âˆ¼ N (0 , I) through an Ornstein-Uhlenbeck process run from time 0 to âˆ (or some finite but large T in practice which introduces a bias), 

d ËœYs = âˆ’f (s) ËœYs ds + g(s) d ËœWs,

19 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

with smooth and non-negative f, g satisfying 

Z âˆ

> 0

f (s) d s =

Z âˆ

> 0

g(s) d s = âˆ,

so that the stationary distribution is indeed standard Gaussian, i.e. ËœYâˆ âˆ¼ N (0 , I).One then uses Andersonâ€™s time reversal (Anderson, 1982) to construct the unique reverse-time process run from âˆ (or T ) to 

0 that turns noise into data with the same joint distribution as the forward process. This reverse-time process depends on the score function of the Ornstein-Uhlenbeck process with initial distribution ÏX .To compare diffusion models to flow models and stochastic interpolants one must compactify time via some C1 monotonically increasing bijection Ï• : [0 , âˆ) â†’ [0 , 1) satisfying Ï•(0) = 0 and lim sâ†’âˆ Ï•(s) = 1 , with inverse Ïˆ := Ï•âˆ’1 : [0 , 1) â†’

[0 , âˆ). One can then define YÏ„ := ËœYÏˆ(Ï„ ) for Ï„ âˆˆ [0 , 1) , and by a limiting argument extend to Ï„ âˆˆ [0 , 1] . Using similar standard stochastic calculus techniques as in the proof of Theorem 5.5 one gets that 

dYÏ„ = âˆ’f (Ïˆ(Ï„ )) Ïˆâ€²(Ï„ ) YÏ„ dÏ„ + g(Ïˆ(Ï„ )) pÏˆâ€²(Ï„ ) d WÏ„ ,

with 

WÏ„ := 

Z Ïˆ(Ï„ )0

pÏ•â€²(s) d ËœWs.

In particular, if we take Ï•(s) = 1 âˆ’ exp( âˆ’s) so that Ïˆ(Ï„ ) = âˆ’ log(1 âˆ’ Ï„ ) and Ïˆâ€²(Ï„ ) = 1 /(1 âˆ’ Ï„ ), we get the Ornstein-Uhlenbeck process 

dYÏ„ = âˆ’ f (âˆ’ log(1 âˆ’ Ï„ )) 1 âˆ’ Ï„ YÏ„ dÏ„ + g(âˆ’ log(1 âˆ’ Ï„ )) 

âˆš1 âˆ’ Ï„ dWÏ„ .

As the following Proposition B.5 shows, when one chooses the statistically optimal diffusion scale Îµ = Îµâˆ— in the stochastic interpolant framework, the corresponding reverse-time SDE becomes an Ornstein-Uhlenbeck process in the time range 

[0 , 1] . Before stating this result we need to introduce the backward drift as in (Albergo et al., 2025) as well as the core result analogous to Theorem 1.4 along with an intra-interpolant conversion result similar to Proposition 4.2. 

Definition B.2 (backward drift) . For an interpolant as in Definition 1.1 and Îµ : [0 , 1] â†’ [0 , âˆ], Îµ âˆˆ C1((0 , 1]) we define the backward drift â†

bÎµ(t, x ) := Ë™ Î±tÎ·Z (t, x ) + Ë™Î²tÎ·X (t, x ) âˆ’ Îµts(t, x ).

We refer the reader to Albergo et al. (2025) for a proof of the following result. 

Theorem B.3. Define Ï„ := 1 âˆ’ t and â†

WÏ„ := âˆ’WÏ„ = âˆ’W1âˆ’t. Then the solutions to the family of reverse-time SDEs 

d â†

XÎµÏ„ = âˆ’â†

bÎµ(Ï„, â†

XÎµÏ„ ) d Ï„ + âˆš2ÎµÏ„ d â†

WÏ„ ,

solved forward in reverse-time Ï„ with â†

XÎµ 

> 0

âˆ¼ ÏX independent of â†

W , satisfy Law( â†

XÎµÏ„ ) = Law( I1âˆ’Ï„ ) for all Ï„ âˆˆ [0 , 1] .

The following proposition can be proved analogously to Proposition 4.2. 

Proposition B.4. For a fixed interpolation schedule (Î±, Î² ) and Îµ : [0 , 1] â†’ [0 , âˆ), the backward drift from Definition B.2 can be expressed in terms of the score through the equation 

> â†

bÎµ(t, x ) = ( Îµâˆ— 

> t

âˆ’ Îµt)s(t, x ) + Ë™Î²t

Î²t

x, 

for all (t, x ) âˆˆ (0 , 1) Ã— Rd.

We now state and prove our own result. 20 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Proposition B.5. Let â†

bâˆ— := 

> â†

bÎµâˆ—

be the statistically optimal (in forward-time) reverse-time drift for any (non point mass) stochastic interpolant as per Definition 1.1. Then the associated reverse-time SDE from Theorem B.3 is a time-scaled Ornstein-Uhlenbeck process of the form 

d â†

Xâˆ— 

> Ï„

= âˆ’ Ë™Î²Ï„

Î²Ï„

> â†

Xâˆ— 

> Ï„

dÏ„ + p2Îµâˆ— 

> Ï„

d â†

WÏ„ ,

solved forward in reverse time Ï„ with initial condition â†

Xâˆ— 

> 0

âˆ¼ ÏX . If additionally the interpolant is variance-preserving, i.e. Î±2 

> t

+ Î²2 

> t

= 1 for all t âˆˆ [0 , 1] , then the reverse-time SDE reduces to 

d â†

Xâˆ— 

> Ï„

= âˆ’Îµâˆ—

> Ï„
> â†

Xâˆ— 

> Ï„

dÏ„ + p2Îµâˆ— 

> Ï„

d â†

WÏ„ .

Proof. The SDE from Theorem B.3 reads 

d â†

XÎµÏ„ = âˆ’â†

bÎµ(Ï„, â†

XÎµÏ„ ) d Ï„ + âˆš2ÎµÏ„ d â†

WÏ„ .

Using Proposition B.4 with Îµ = Îµâˆ— gives 

> â†

bâˆ—(Ï„, x ) = Ë™Î²Ï„

Î²Ï„

x, 

so that the SDE from Theorem B.3 reduces to 

d â†

Xâˆ— 

> Ï„

= âˆ’ Ë™Î²Ï„

Î²Ï„

> â†

Xâˆ— 

> Ï„

dÏ„ + p2Îµâˆ— 

> Ï„

d â†

WÏ„ ,

as claimed. Assuming now that Î±2 

> Ï„

+ Î²2 

> Ï„

= 1 , i.e. assuming that the schedule is variance-preserving, gives 

Îµâˆ— 

> Ï„

= (1 âˆ’ Î²2 

> Ï„

) Ë™Î²Ï„

Î²Ï„

âˆ’ Î±Ï„

âˆ’ Ë™Î²Ï„ Î²Ï„

Î±Ï„

!

= Ë™Î²Ï„

Î²Ï„

,

so that the SDE from Theorem B.3 reduces to 

d â†

Xâˆ— 

> Ï„

= âˆ’Îµâˆ—

> Ï„
> â†

Xâˆ— 

> Ï„

dÏ„ + p2Îµâˆ— 

> Ï„

d â†

WÏ„ ,

as claimed. 21 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## C. Schedule visualizations 

The first row of Figure 4 visualizes the linear schedule (see Definition 5.1) used in flow matching models alongside the lazy ODE and SDE schedule, respectively (see Proposition 7.1 and in particular Example 7.4). The second row visualizes the statistically optimal diffusion scale Îµâˆ— (see Definition 4.1 and Theorem 4.3) associated with each schedule. The figure uses the denominator definition dt := (1 âˆ’ t)2 + t2. Note that these are the schedules and diffusion scales used in Figure 1. 

0 0.5 1

0

0.5

1

t

Linear Schedule 

Î±t = 1 âˆ’ t

Î²t = t

0 0.5 1

0

0.5

1

t

Lazy ODE Schedule 

Î±t = (1 âˆ’ t)/âˆšdt

Î²t = t/ âˆšdt

0 0.5 1

0

0.5

1

t

Lazy SDE Schedule 

Î±t = t(1 âˆ’ t)/d t

Î²t = t2/d t

0 0.5 1

0

1

2

3

4

t

Îµâˆ— 

> t

= (1 âˆ’ t)/t 

0 0.5 1

0

1

2

3

4

t

Îµâˆ— 

> t

= (1 âˆ’ t)/(td t)

0 0.5 1

0

1

2

3

4

t

Îµâˆ— 

> t

= Ë™Î²t/2 = t(1 âˆ’ t)/d 2

> t

Figure 4. Schedules (Î±, Î² ) (top row) and corresponding statistically optimal diffusion scale Îµâˆ— (bottom row). These are the schedules and diffusion scales used in Figure 1. 

22 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## D. Pseudocode for lazy ODE and SDE sampling 

Algorithms 1 and 2 give pseudocode for the algorithmic formulation of Propositions 7.5 and 7.6 using the explicit Euler and Euler-Maruyama solver, respectively. We stress that any ODE or SDE solver can be used in practice. 

Algorithm 1 ODE-Sample 

Input: linear velocity b, step size âˆ†tt â† 0

x â† sample N (0 , I )

while t < 1 do 

d â† 1 + 2 t(t âˆ’ 1) 

b â† 1âˆ’2td x + 1âˆšd b



t, âˆšd x 



x â† x + âˆ† t b (explicit Euler step) 

t â† t + âˆ† t

end while Return: x

Algorithm 2 SDE-Sample 

Input: linear velocity b, step size âˆ†tdnext â† 1 + 2âˆ† t(âˆ† t âˆ’ 1) 

Î²next â†

 âˆ†tdnext 

2

âˆ†W â† sample N (0 , Î² next I)

t â† âˆ†tx â† âˆ†W (Xâˆ— 

> 0

= bâˆ—(0 , 0) = 0 )

while t < 1 do 

d â† dnext 

Î² â† Î²next 

bâˆ— â† 2

> d



(1 âˆ’ 2t)x + t b  t, dt x 

tnext â† t + âˆ† tdnext â† 1 + 2 tnext (tnext âˆ’ 1) 

Î²next â†

 tnext 

> dnext

2

âˆ†W â† sample N (0 , (Î²next âˆ’ Î²) I)

x â† x + âˆ† t b âˆ— + âˆ† W (Euler-Maruyama step) 

t â† t + âˆ† t

end while Return: x

23 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## E. ODE and SDE numerical integration schemes 

Below we give numerical schemes for solving the SDE from Theorem 1.4 with drift bÎµ and additive diffusion scale Îµ,

dXÎµt = bÎµ(t, X Îµt ) d t + âˆš2Îµt dWt.

With Îµ â‰¡ 0 the SDE degenerates to an ODE and e.g. the Euler-Maruyama scheme becomes the explicit Euler scheme. Note that the experiments either use Îµ â‰¡ 0 or Îµ = Îµâˆ—, and with the schedules we consider the quadratic variation from time s

to t (0 â‰¤ s â‰¤ t â‰¤ 1) under the latter diffusion scale, R ts 2Îµâˆ— 

> u

du, can be calculated analytically (see Proposition 7.1 and Proposition 5.2) and is therefore used in the below schemes and in our experiments. For a total of N > 1 solver steps we use a fixed step size of âˆ†t = 1 

> N

. We denote the solution after n âˆˆ { 0, 1, . . . , N }

steps by Yn at time tn = nâˆ†t. For n < N the independent Wiener process increments from time tn to tn+1 are denoted 

âˆ†Wn âˆ¼ N (0 , âˆ†t I) and we define Vn := 

R tn+1  

> tn

2Îµt dt



âˆ†Wn/âˆ†t. With the â€œcorrectedâ€ solution at time n denoted by ËœYn

and initially defined as ËœY0 := Y0 the update step from Yn to Yn+1 is then defined as below. 

Euler-Maruyama 

Yn+1 = Yn + âˆ† tb Îµ(tn, Y n) + Vn.

Predictor-Corrector 

Yn+1 = ËœYn + âˆ† tb Îµ(tn, Y n) + Vn,

ËœYn+1 = ËœYn + 12 âˆ†t(bÎµ(tn, Y n) + bÎµ(tn+1 , Y n+1 )) + Vn.

Heun 

Yn+1 = ËœYn + âˆ† tb Îµ(tn, ËœYn) + Vn,

ËœYn+1 = ËœYn + 12 âˆ†t(bÎµ(tn, Y n) + bÎµ(tn+1 , Y n+1 )) + Vn.

We use a variant of the predictor-corrector and Heun scheme in which the final, returned solution is YN so that the corrected final solution ËœYN is never computed. This prevents us from ever having to evaluate the drift bÎµ at t = 1 .8

Note that the â€œcorrectâ€ step for the predictor-corrector scheme is equivalent to the Heun scheme. They both use an Euler-Maruyama â€œpredictâ€ step and only differ in whether the drift in this step is evaluated in Yn or ËœYn. This detail means that the Heun scheme uses two drift evaluations per step while the Euler-Maruyama and predictor-corrector schemes only use one. Since evaluating this drift function in practice amount to doing an expensive forward pass in a neural network the Heun scheme is therefore roughly twice as slow as it requires 2N âˆ’ 1 function evaluations instead of N . To compare the three schemes fairly, we used half the amount of steps for the Heun scheme compared to the predictor-corrector and Euler-Maruyama schemes. For the PRX model and settings used in our experiments we find that the predictor-corrector scheme generally converges faster as a function of drift evaluations than the Euler-Maruyama and Heun scheme, especially for SDE generation. This is indicated by Figure 5 and Figure 6 which consider sample images generated with the standard linear flow model schedule using the ODE generation ( Îµ â‰¡ 0) and statistically optimal SDE generation ( Îµ = Îµâˆ—), respectively, for the same prompt â€œTwo huskyâ€™s hanging out of the car windowsâ€. One sees that the predictor-corrector scheme converges to the reference image at 4096 steps (at which the Heun scheme has used 2048 steps) slightly faster than the Euler-Maruyama and Heun scheme. All schemes more or less converge to the same reference image, as expected. Note that the choice of integration scheme is not important for our experiments since the purpose is not to investigate which scheme leads to better image generation (faster convergence) but rather to assess the effect of transforming to the lazy schedule whilst keeping the integration scheme fixed.      

> 8Evaluating the drift at t= 1 is in fact not a problem for us. The PRX flow model we use uses linear ODE velocity parameterization and no division by zero (which can potentially occur since Î±1= 0 ) is introduced in our conversion formulas from this linear velocity to the ODE velocities and SDE drifts we consider.

24 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

Figure 5. Sample images from the PRX flow model with varying number of steps using different integration schemes for the ODE with original linear flow model schedule. A guidance strength of 5 with the text prompt â€œTwo huskyâ€™s hanging out of the car windowsâ€ was used. 

Figure 6. Same as Figure 5 but for the statistically optimal SDE with original linear flow model schedule. 

25 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## F. Further experimental results 

F.1. Solver convergence animations 

The solver convergence is better visualized as an animation than as a static image. Figure 7 shows sample images for 5

prompts generated with 64 predictor-corrector solver steps. An animation over multiple solver steps is available at this Zenodo link. Heuristically, we find that after 64 steps the SDE solutions are comparable to the ODE solutions in fidelity, but this is of course a subjective matter.          

> Figure 7. Sample images generated using 64 predictor-corrector steps for 5different prompts from our experiment. Each row corresponds to whether an ODE or SDE is used and whether the original linear flow model schedule or the conversion to the lazy schedule is used. An animation over all solver step amounts {4,8, . . . , 4096 }is available at this Zenodo link.

F.2. Equivalence of convergence 

Here we test whether the ODE and SDE solutions converge to the same reference image irrespectively of whether the flow or lazy schedule is used. For each ni amount of solver steps we calculate the RMSE between the image generated with the original linear flow model schedule against the image generated with the conversion to the lazy one using ni steps, i.e. we calculate 

RMSE (img ODE,flow  

> ni

, img ODE,lazy  

> ni

)

for each i and similarly for the SDE. We then average over all 100 prompts and plot this average as a function of ni for the ODE and SDE, respectively. The result is shown in Figure 8. The figure indicates that both the ODE and SDE converge to 26 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

the same reference image irrespectively of which schedule is used, but this convergence is much faster for the ODE than for the SDE. This is probably due to the fact that the flow and lazy schedule are somewhat similar for ODE generation (in particular they both satisfy Î±0 = 1 ), while for SDE generation the lazy schedule is a point mass schedule (see Figure 4). Since the flow model velocity is not learned perfectly, approximation errors make the flow-to-lazy schedule transform inexact which amplifies more for SDE generation relative to ODE generation. 4 8 16 32 64 128 256 512 1024 2048 4096 

#Steps 

> 0
> 10
> 20
> 30
> 40
> 50
> Within-Step Mean RMSE
> flow ODE vs lazy ODE flow SDE vs lazy SDE

Figure 8. Within-step mean RMSE as a function of the number of solver steps using the predictor-corrector scheme. By â€œwithin-step RMSEâ€ we mean the RMSE between the image generated under the original linear flow model schedule versus that generated using the conversion to the lazy schedule for a fixed number of solver steps, for the ODE and SDE, respectively. Shaded area indicates a 95% confidence interval calculated by bootstrapping with 10 ,000 samples. The plot indicates that the RMSE converges to zero as the number of solver steps grows, as one would expect, although the convergence is much faster for the ODE than for the SDE. 

F.3. Convergence results using the flow image as a common reference 4 8 16 32 64 128 256 512 1024 2048 

#Steps          

> 0
> 20
> 40
> 60
> 80
> Mean RMSE to Reference
> flow ODE lazy ODE flow SDE lazy SDE 4816 32 64 128 256 512 1024 2048

Actual Lazy #Steps    

> 4
> 8
> 16
> 32
> 64
> 128
> 256
> 512
> 1024
> 2048
> Equivalent Flow #Steps
> 710 18 33 65
> 621 24 37 81 171 320 704
> ODE SDE Lazy best ( p<0.05) No difference

Figure 9. Same as Figure 3 but with img ODE reference := img ODE,flow  

> 4096

instead of img ODE reference := ( img ODE,flow  

> 4096

+ img ODE,lazy  

> 4096

)/2, and similarly for the SDE. 

Here we recreate Figure 3 but instead of defining 

img ODE reference := ( img ODE,flow  

> 4096

+ img ODE,lazy  

> 4096

)/2,

we instead define 

img ODE reference := img ODE,flow  

> 4096

,

and similarly for the SDE. Clearly this puts the lazy schedule method at a disadvantage. However, Figure 9 shows that even with this disadvantage the lazy schedule converges statistically significantly faster most of the time. As one would 27 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

expect, the results are slightly worse, e.g. the lazy schedule is not statistically significantly better for SDE generation at 1024 

solver steps compared to the linear flow schedule, whereas when using the average reference as common reference it is (see Figure 3). 28 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants 

## G. COCO captions 

The COCO validation dataset consists of images with five human generated captions each. We extract the first caption from the first 100 images and remove any potential trailing period. We use the resulting 100 sentences as text prompts for the PRX flow model. These are shown below sorted alphabetically (ASCII-wise).  

> 1

A bathroom sink with toiletries on the counter  

> 2

A bathroom with a poster of an ugly face above the toilette  

> 3

A bathroom with a sink and shower curtain with a map print  

> 4

A bathroom with a toilet, sink, and shower  

> 5

A bathroom with a traditional toilet next to a floor toilet  

> 6

A beautiful dessert waiting to be shared by two people  

> 7

A bicycle replica with a clock as the front wheel  

> 8

A bike leaning against a sign in Scotland  

> 9

A black Honda motorcycle parked in front of a garage  

> 10

A black and white photo of an older man skiing  

> 11

A black and white toilet with a plunger  

> 12

A black cat is inside a white toilet  

> 13

A boat that looks like a car moves through the water  

> 14

A brown and black horse in the middle of the city eating grass  

> 15

A brown purse is sitting on a green bench  

> 16

A car is stopped at a red light  

> 17

A car that seems to be parked illegally behind a legally parked car  

> 18

A cat eating a bird it has caught  

> 19

A cat in between two cars in a parking lot  

> 20

A couple of birds fly through a blue cloudy sky  

> 21

A cute kitten is sitting in a dish on a table  

> 22

A delivery truck with an advertisement for Entourage  

> 23

A dirt bike rider doing a stunt jump in the air  

> 24

A dog and a goat with their noses touching at fence  

> 25

A dog sitting between its masters feet on a footstool watching tv  

> 26

A door with a sticker of a cat door on it  

> 27

A dual sink vanity with mirrors above the sinks  

> 28

A few items sit on top of a toilet in a bathroom stall  

> 29

A fighter jet is flying at a fast speed  

> 30

A fireplace with a fire built in it  

> 31

A gas stove next to a stainless steel kitchen sink and countertop  

> 32

A green car on display next to a busy street  

> 33

A group of people preparing food in a kitchen  

> 34

A group of people riding mopeds in a busy street  

> 35

A kitchen is shown with wooden cabinets and a wooden celling  

> 36

A large U.S Air Force plain sits on an asphalt ramp  

> 37

A large passenger airplane flying through the air  

> 38

A large passenger jet taking off from an airport  

> 39

A little girl in a public bathroom for kids  

> 40

A long empty, minimal modern skylit home kitchen  

> 41

A man getting a drink from a water fountain that is a toilet  

> 42

A man in a wheelchair and another sitting on a bench that is overlooking the water  

> 43

A man is sitting on a bench next to a bike  

> 44

A man sits with a traditionally decorated cow  

> 45

A man standing in a kitchen while closing a cupboard door  

> 46

A parade of motorcycles is going through a group of tall trees  

> 47

A person holding a skateboard overlooks a dead field of crops  

> 48

A person walking in the rain on the sidewalk 

29 Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants  

> 49

A photograph of a kitchen inside a house  

> 50

A picture advertising Arizona tourism in an airport  

> 51

A picture of a man playing a violin in a kitchen  

> 52

A pinewood and green modern themed kitchen area  

> 53

A public bathroom with a bunch of urinals  

> 54

A random plane in the sky flying alone  

> 55

A room with blue walls and a white sink and door  

> 56

A shot of an elderly man inside a kitchen  

> 57

A small car is parked in front of a scooter  

> 58

A small child climbs atop a large motorcycle  

> 59

A small closed toilet in a cramped space  

> 60

A tiny bathroom with only a toilet and a shelf  

> 61

A toilet sits next to a window and in front of a shower  

> 62

A trio of dogs sitting in their ownerâ€™s lap in a red convertible  

> 63

A variety of pots are stored in a nook by a fireplace  

> 64

A woman is walking a dog in the city  

> 65

An airplane with its landing wheels out landing  

> 66

An all white kitchen with an electric stovetop  

> 67

An office cubicle with four different types of computers  

> 68

An office cubicle with multiple computers in it  

> 69

An old toilet with a hello kitty cover top  

> 70

An old-fashioned green station wagon is parked on a shady driveway  

> 71

Fog is in the air at an intersection with several traffic lights  

> 72

Half of a white cake with coconuts on top  

> 73

Jet liner flying off into the distance on an overcast day  

> 74

Little birds sitting on the shoulder of a giraffe  

> 75

Man in motorcycle leathers standing in front of a group of bikes  

> 76

Office space with office equipment on desk top  

> 77

Pedestrians walking down a sidewalk next to a small street  

> 78

People are waiting for the bus near a bus stop  

> 79

Posted signs point the way through a parking garage  

> 80

Riding a motorcycle down a street that has no one else on it  

> 81

Rows of motor bikes and helmets in a city  

> 82

Several motorcycles riding down the road in formation  

> 83

The airplane is on the runway with two young men standing by  

> 84

The back door with a window in the kitchen  

> 85

The sign of a restaurant in the outside of the store  

> 86

The telephone has a banana where the receiver should be  

> 87

The woman in the kitchen is holding a huge pan  

> 88

The young man is stirring his pot of food with a wooden spoon  

> 89

This is an open box containing four cucumbers  

> 90

Two huskyâ€™s hanging out of the car windows  

> 91

Two women waiting at a bench next to a street  

> 92

Women father around a desk and machinery in a factory  

> 93

a bike leaning on a metal fence next to some flowing water  

> 94

a counter with vegetables, knife and cutting board on it  

> 95

a man sleeping with his cat next to him  

> 96

a man with a bike at a marina  

> 97

a modern flush toilet in a bathroom with tile  

> 98

a small toilet stall with a toilet brush and 3 rolls of toilet paper  

> 99

an airport with one plane flying away and the other sitting on the runway  

> 100

four urinals in a public restroom with a window 

30
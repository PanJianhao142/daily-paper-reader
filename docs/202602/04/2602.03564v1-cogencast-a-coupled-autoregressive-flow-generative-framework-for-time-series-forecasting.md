---
title: "CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting"
title_zh: CoGenCast：一种用于时间序列预测的耦合自回归-流生成框架
authors: "Yaguo Liu, Mingyue Cheng, Daoyu Wang, Xiaoyu Tao, Qi Liu"
date: 2026-02-03
pdf: "https://arxiv.org/pdf/2602.03564v1"
tags: ["keyword:FM", "query:课题"]
score: 7.0
evidence: 用于连续时间动态预测的流匹配机制
tldr: 本研究提出CoGenCast，一种耦合自回归与流匹配的混合生成框架，旨在解决时间序列预测中语义理解与随机建模难以兼顾的问题。该方法通过修改预训练LLM的注意力拓扑结构，构建支持双向编码与因果生成的骨干网络，并集成流匹配机制以捕捉连续的时间演化动态。CoGenCast支持多模态与跨域统一训练，在多个基准测试中表现优于现有模型，为时间序列预测提供了高效的生成式解决方案。
motivation: 现有预测方法难以同时兼顾大语言模型在语义上下文建模上的优势与扩散/流模型在连续概率生成方面的能力。
method: 通过修改预训练解码器LLM的注意力拓扑实现双向编码与因果生成，并结合流匹配机制对自回归表示进行条件随机建模。
result: 在多个基准数据集上的实验证明，CoGenCast的预测性能一致优于现有基准模型，并展现出卓越的跨域与多模态处理能力。
conclusion: 耦合自回归LLM与流匹配机制能有效结合语义理解与随机动力学建模，是提升时间序列预测性能的有效途径。
---

## 摘要
时间序列预测可以被视为一个生成问题，它既需要对上下文条件的语义理解，也需要对连续时间动态的随机建模。现有方法通常要么依赖自回归大语言模型（LLMs）进行语义上下文建模，要么依赖类扩散模型进行连续概率生成。然而，单一方法都无法同时充分建模这两个方面。在这项工作中，我们提出了 CoGenCast，这是一个将预训练 LLMs 与流匹配（flow-matching）机制相结合的混合生成框架，用于有效的时间序列预测。具体而言，我们通过仅修改注意力拓扑结构，将预训练的仅解码器（decoder-only）LLMs 重新配置为原生的预测编码器-解码器骨干网络，从而实现双向上下文编码和因果表示生成。在此基础上，进一步集成了流匹配机制来建模时间演化，在以自回归生成的表示为条件的情况下，捕捉连续的随机动态。值得注意的是，CoGenCast 自然地支持多模态预测和跨域统一训练。在多个基准测试上的广泛实验表明，CoGenCast 一贯优于以往的对比基准。代码可在 https://github.com/liuyaguo/_CoGenCast 获取。

## Abstract
Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at https://github.com/liuyaguo/_CoGenCast.
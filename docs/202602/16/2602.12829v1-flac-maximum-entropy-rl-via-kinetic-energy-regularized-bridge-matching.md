---
title: "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching"
title_zh: FLAC：通过动能正则化桥接匹配实现的最大熵强化学习
authors: "Lei Lv, Yunfei Li, Yu Luo, Fuchun Sun, Xiao Ma"
date: 2026-02-13
pdf: "https://arxiv.org/pdf/2602.12829v1"
tags: ["keyword:FM", "keyword:MDM"]
score: 6.0
evidence: 利用流匹配和扩散模型进行连续控制策略
tldr: 针对扩散模型等迭代生成策略在最大熵强化学习中难以直接计算动作概率密度的问题，本文提出了 FLAC 框架。该方法将策略优化建模为广义薛定谔桥问题，通过惩罚速度场的动能来调节策略随机性，从而无需显式估计动作密度。FLAC 利用拉格朗日对偶机制自动调整动能，在多个高维基准测试中实现了优于或相当的性能，为高表达力策略的正则化提供了新路径。
motivation: 扩散模型等生成式策略虽然表达力强，但其动作对数概率难以直接获取，导致传统的最大熵强化学习方法难以应用。
method: 将策略优化转化为广义薛定谔桥问题，利用速度场的动能作为偏离高熵参考分布的物理代理指标进行正则化。
result: 在多个高维强化学习基准测试中，FLAC 在无需显式密度估计的情况下，取得了优于或与强基准模型相当的性能表现。
conclusion: 本研究证明了通过动能正则化的桥接匹配可以有效实现最大熵强化学习，为复杂生成策略的优化提供了高效的似然无关方案。
---

## 摘要
迭代生成策略（如扩散模型和流匹配）为连续控制提供了卓越的表达能力，但由于其动作对数密度无法直接获取，使得最大熵强化学习（Maximum Entropy Reinforcement Learning）变得复杂。为了解决这一问题，我们提出了场最小能量 Actor-Critic（Field Least-Energy Actor-Critic，简称 FLAC），这是一个无似然（likelihood-free）框架，通过惩罚速度场的动能来调节策略的随机性。我们的核心见解是将策略优化建模为相对于高熵参考过程（如均匀分布）的广义薛定谔桥（Generalized Schrödinger Bridge, GSB）问题。在这种视角下，最大熵原理自然地体现为在优化回报的同时保持与高熵参考过程的接近，而无需显式的动作密度。在该框架中，动能作为衡量与参考过程偏差的物理基础代理：最小化路径空间能量可以限制所诱导的终端动作分布的偏差。基于这一观点，我们推导了一种能量正则化的策略迭代方案，以及一种通过拉格朗日对偶机制自动调整动能的实用离策（off-policy）算法。实验表明，FLAC 在高维基准测试中达到了优于或相当于强基准模型的性能，同时避免了显式的密度估计。

## Abstract
Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.
Title: Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening

URL Source: https://arxiv.org/pdf/2602.12679v1

Published Time: Mon, 16 Feb 2026 01:30:00 GMT

Number of Pages: 19

Markdown Content:
Published as a conference paper at ICLR 2026 

# MOTION PRIOR DISTILLATION IN TIME REVERSAL 

# SAMPLING FOR GENERATIVE INBETWEENING 

Wooseok Jeon 1, Seunghyun Shin 2, Dongmin Shin 1, Hae-Gon Jeon 1âˆ—

> 1

Department of Artificial Intelligence, Yonsei University 

> 2

Department of AI Convergence, GIST Motion Prior     

> Conflict Residual Distilled
> residual
> Motion Prior
> Distillation
> (c) Ours
> Top -down  view (d) Forward (e) Backward (g)  Ours  (f) TRS
> Start
> End
> (a) Ideal case
> End Frame
> Start Frame
> (b) TRS

Figure 1: Overview of the proposed motion prior distillation. (a) Ideal case of generative inbe-tweening task. (b) Motion prior conflict in existing time reversal sampling method. (c) Our proposed motion prior distillation method. (d) A video generated by Stable Video Diffusion model condi-tioned on the start frame, and (e) conditioned on the end frame and temporally flipped. (f) A result from existing time reversal sampling method, showing ghosting artifact and reverse play due to mo-tion prior conflict. (g) A result from our proposed method, showing temporally coherent motion. 

## ABSTRACT 

Recent progress in image-to-video (I2V) diffusion models has significantly ad-vanced the field of generative inbetweening , which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in par-allel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two gener-ated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD) ,a simple yet effective inference-time distillation technique that suppresses bidi-rectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantita-tive evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios. Project page: https://vvsjeon.github.io/MPD/ 

> âˆ—

Corresponding author 

1

> arXiv:2602.12679v1 [cs.CV] 13 Feb 2026

Published as a conference paper at ICLR 2026 

## 1 INTRODUCTION 

Recent advances in diffusion models have significantly improved the performance of image and video generation tasks. In particular, image-to-video (I2V) diffusion models (Blattmann et al., 2023a; Xing et al., 2024b; Bar-Tal et al., 2024; Yang et al., 2025b) demonstrate strong capabil-ities across diverse applications, as they can generate temporally coherent videos from a single conditioning frame. From a generative perspective, this progress has extended video frame interpo-lation to generative inbetweening , which aims to generate natural intermediate frames between two keyframes (See Fig. 1 (a)). However, I2V diffusion models are not directly applicable to bounded generation where both start and end frames serve as a dual-constraint. To address this, recent studies have explored time reversal sampling, which employs temporally forward / backward denoising paths conditioned on the start / end frames during the iterative reverse denoising process (See Fig. 1 (b)). This can be categorized into two approaches, namely parallel and sequential, according to how these two paths are integrated. In the parallel approach (Feng et al., 2024; Wang et al., 2025b; Zhu et al., 2025), samples from the forward and backward paths are denoised simultaneously at each denoising step and then linearly interpolated to form the input for the next denoising step. In contrast, the sequential approach (Yang et al., 2025a) samples two denoising paths sequentially by inserting a single re-noising step between them. However, simply connecting the two temporal paths does not guarantee a single coherent motion during the sampling process because each sample is obtained with the motion prior of its condition-ing frame (See Fig. 1 (d) and (e)). In particular, as shown in Fig. 1 (e), a backward path initialized at the end frame tends to generate forward-looking sequences, instead of faithfully reconstructing his-torical frames. This forward-generation bias commonly arises from I2V models, which are trained to predict consecutive forward frames. As shown in Fig. 1 (f), the generated frames noticeably follow different routes and even disagree on the carâ€™s destination, which we refer to as a motion conflict be-tween two temporal paths. This highlights that the fundamental challenge lies not merely in how to connect the forward and backward paths, but in how to align their conflicting motion priors induced by the forward-generation bias. To this end, we aim to overcome this fundamental misalignment between two temporal paths by proposing a novel inference-time distillation approach, called Motion Prior Distillation (MPD) .Our key intuition is that the residual of the denoised estimates contains motion information induced by a given start frame. Inspired by this, during early denoising steps, our method distills the motion residual induced by the start frame into the backward path (See Fig. 1 (c)). Since our approach delib-erately avoids denoising the end-conditioned path, we can drive the backward path to follow the time reversed motion residual of the forward path, thereby achieving bidirectional path alignment (See Fig. 1 (g)). This single path design effectively removes conflicting motion priors while preserving endpoint consistency, allowing two temporal paths to converge into coherent motions. Through extensive evaluations, we demonstrate that our method consistently outperforms relevant methods, including existing time reversal sampling strategies. In addition, since conventional metrics are not fully capable of evaluating temporal coherence and human preference, we further conduct user studies to validate its robustness under practical scenarios in the presence of complex motion patterns and large temporal displacements. 

## 2 RELATED WORKS 

Video frame interpolation. Video frame interpolation (VFI) aims to synthesize intermediate frames between two input frames while maintaining spatial and temporal coherence (Lyu et al., 2024; Kye et al., 2025). Supervised methods (Bao et al., 2019; Niklaus & Liu, 2018; Park et al., 2020; Lei et al., 2023; Kong et al., 2022; Li et al., 2023; Huang et al., 2022; Lu et al., 2022; Reda et al., 2022) that rely on estimating optical flows have been practically adopted due to their robust performance and interpretable motion trajectories. However, errors in estimated flows often lead to failures, particularly under the scenes with occlusion or non-linear motion (Long et al., 2024). Re-cently, diffusion-based VFI methods (Danier et al., 2024; Voleti et al., 2022) have attempted to take advantage of the generative capabilities of diffusion models to improve the perceptual quality of in-terpolated frames. While these methods improve perceptual fidelity, their performance still degrades under large temporal displacements between two frames. 2Published as a conference paper at ICLR 2026 Fuse     

> flip
> ð’™ ð‘¡
> â€²
> SVD
> ð¼ ð‘’ð‘›ð‘‘
> ð¼ ð‘ ð‘¡ð‘Žð‘Ÿð‘¡
> ð’™ ð‘¡
> SVD
> ð ð‘ð‘¤ð‘‘
> ð ð‘“ð‘¤ð‘‘
> ð’™ ð‘¡ âˆ’1
> Re -noise Ã—K
> ð‘ª ð‘ ð‘¡ð‘Žð‘Ÿð‘¡
> ð¶ ð‘’ð‘›ð‘‘
> Forward path
> â€¦
> Backward path
> â€¦
> â€¦
> âˆ†ð
> âˆ†ð
> Eq. 15
> Eq. 16
> Motion  Prior
> Distillation
> Latent
> Latent
> Residual
> Noise
> Noise
> Encoder
> Motion Prior Distillation

(c) Motion Prior Distillation 

(a) Parallel time reversal sampling        

> ð’™ ð‘¡ âˆ’1,ð’„ start
> Ã—(ðŸ âˆ’ðœ¶ )
> Ã—ðœ¶
> ð’™ ð‘¡
> ð’™ ð‘¡
> â€²ð’™ ð‘¡ âˆ’1,ð’„ end
> â€²ð’™ ð‘¡ âˆ’1,ð’„ end
> â€²â€²
> ð’™ ð‘¡ âˆ’1

(b) Sequential time reversal sampling      

> ð’™ ð‘¡ âˆ’1,ð’„ start ð’™ ð‘¡
> ð’™ ð‘¡
> â€²ð’™ ð‘¡ âˆ’1,ð’„ end
> â€²ð’™ ð‘¡ âˆ’1,ð’„ end
> â€²â€²
> Denoise with ð’„ start
> Denoise with ð’„ end
> Time flip
> Connect

Figure 2: Denoising process of the proposed motion prior distillation. Existing time reversal sampling methods simply connect the two temporal paths either by (a) linearly fusing them or (b) alternatively denoising each path. (c) Our MPD is employed on time reversal sampling framework to distill forward motion prior into the backward path, thereby achieving motion alignment. 

Generative video inbetweening. With the advancement of video diffusion models (Ho et al., 2022b;a; Blattmann et al., 2023b;a), VFI has broadened into generative inbetweening , which is interested in the set of semantically plausible interpolations. Some approaches (Jain et al., 2024; Xing et al., 2024a;b; Wang et al., 2025a; Zhang et al., 2025) train diffusion models to condition on two input frames for interpolation, yielding greater robustness to ambiguous and large motion where traditional methods have struggled. While effective, they typically require substantial train-ing resources. Other approaches leverage pre-trained large-scale I2V diffusion models and achieve remarkable performances by incorporating new sampling techniques. TRF (Feng et al., 2024) pro-poses a time reversal sampling strategy that fuses forward and backward denoising paths in parallel, each conditioned on the start and end frames. Building on this strategy, GI (Wang et al., 2025b) enhances reverse motion fidelity by fine-tuning a diffusion model through rotation of temporal self-attention maps to generate temporally reversed frames. Similarly, FCVG (Zhu et al., 2025) proposes a method that injects line correspondences as frame-wise conditions to alleviate the ambiguity of inbetweening path. Meanwhile, ViBiDSampler (Yang et al., 2025a) introduces a new time reversal strategy that employs sequential sampling along forward and backward paths to achieve on-manifold generation of intermediate frames. However, all of them still operate with two independent motion priors from the start and end frames, so convergence to a single coherent trajectory is not guaranteed. 

## 3 PRELIMINARIES 

3.1 STABLE VIDEO DIFFUSION 

We base our explanation on Stable Video Diffusion (SVD) (Blattmann et al., 2023a), which is widely adopted in time reversal sampling based methods. Specifically, SVD is a UNet-based latent video diffusion model built on EDM framework (Karras et al., 2022). At a reverse denoising step t âˆˆ{T, ..., 1} with the noise level Ïƒt, the denoiser DÎ¸ predicts both the unconditional estimate Ë†x0,âˆ…

and the conditional estimate Ë†x0,c from the current noisy latent xt:

Ë†x0,âˆ… = DÎ¸ (xt; Ïƒt) and Ë†x0,c = DÎ¸ (xt; Ïƒt, c), (1) 3Published as a conference paper at ICLR 2026 where c is the input condition. In EDM framework, the corresponding noise prediction model ÏµÎ¸

and the score prediction model sÎ¸ have the following relationship with the denoiser DÎ¸ :

sÎ¸ (xt; Ïƒt) = âˆ’ ÏµÎ¸ (xt; Ïƒt)

Ïƒt

= DÎ¸ (xt; Ïƒt) âˆ’ xt

Ïƒ2

> t

. (2) To guide the sample toward the condition c, the classifier-free guidance (CFG) (Ho & Salimans, 2021) mixes the unconditional estimate Ë†x0,âˆ… with the conditional estimate Ë†x0,c:

Ë†x0,c â† (1 + w) Ë† x0,c âˆ’ w Ë†x0,âˆ…, (3) where w â‰¥ 0 is a guidance strength. At each iteration, we can denoise the sample with Euler step, progressively denoising from Gaussian noise xT to sample x0:

xtâˆ’1 = Ë† x0,c + Ïƒtâˆ’1

Ïƒt

 xt âˆ’ Ë†x0,c

. (4) In particular, I2V models take the initial starting frame condition as input and generate videos with its motion prior. To reflect both start and end frame conditions, time reversal sampling process involves denoising two temporal paths with each corresponding frame condition. 3.2 TIME REVERSAL SAMPLING 

Parallel method. As illustrated in Fig. 2 (a), parallel time reversal methods denoise the tempo-rally forward/backward path conditioned on the start/end frame, and then fuse them to produce the intermediate frames (Feng et al., 2024; Wang et al., 2025b; Zhu et al., 2025). Letâ€™s denote cstart and 

cend as the encoded latent conditions of the start and end frame, respectively. We can express the denoising step as follows: 

xtâˆ’1 = Î±xtâˆ’1,cstart + (1 âˆ’ Î±)( xâ€² 

> tâˆ’1,cend

)â€² (5) s.t. xtâˆ’1,cstart = Ë† x0,cstart + Ïƒtâˆ’1

Ïƒt

(xt âˆ’ Ë†x0,cstart ) (6) and xâ€² 

> tâˆ’1,cend

= Ë† xâ€² 

> 0,cend

+ Ïƒtâˆ’1

Ïƒt

 xâ€² 

> t

âˆ’ Ë†xâ€²

> 0,cend

 , (7) where (Â·)â€² indicates a temporal flip along the time dimension and Î± âˆˆ [0 , 1] refers to the interpolation weight. However, this method can suffer from off-manifold issues, where samples deviate from the learned data manifold. As a result, their linearly interpolated results often lead to oscillations and undesirable artifacts. Furthermore, they do not resolve the conflicting motion priors induced by the two conditions, so motion fidelity can still degrade. 

Sequential method. An alternative approach adopts the sequential time reversal sampling strat-egy (Yang et al., 2025a). Instead of fusing two temporal paths in parallel, this method sequentially denoises the forward and backward paths as Fig. 2 (b). On-manifold generation can be achieved by inserting a single re-noising step before switching from the forward to the backward path: 

xtâˆ’1,cstart = Ë† x0,cstart + Ïƒtâˆ’1

Ïƒt

(xt âˆ’ Ë†x0,cstart ) , (8) 

xt, cstart = xtâˆ’1,cstart +

q

Ïƒ2 

> t

âˆ’ Ïƒ2 

> tâˆ’1

Îµ, Îµ âˆ¼ N (0 , I), (9) 

xtâˆ’1 = ( Ë† x0,cend + Ïƒtâˆ’1

Ïƒt

(xt, cstart âˆ’ Ë†x0,cend )) â€². (10) Unlike the parallel approach, this sequential structure maintains a more consistent and manifold-aligned path. Nevertheless, alternating two denoised paths results in conflicting motion priors, as each path relies on its own conditioning frame. This highlights the need to align two temporal paths without motion prior conflicts. 

## 4 METHOD 

Given a pair of two frames {Istart , I end }, our goal is to align two temporal paths with both temporal coherence and visual fidelity. Fig. 2 (c) provides an overview of our method. To begin with, we recast the time reversal sampling process as an optimization problem to solve a bidirectional path misalignment problem. In this work, we present a simple yet effective approach called Motion Prior Distillation (MPD) which propagates a motion residual from a forward path into a backward path. 4Published as a conference paper at ICLR 2026 4.1 MOTIVATION 

The existing time reversal sampling methods can be interpreted as a sampling procedure in which each denoising path approximately minimizes the following loss function L:

L(x; Î¸, cstart , cend , Ïƒ ) = âˆ¥ÏµÎ¸ (x; Ïƒ, cstart ) âˆ’ ÏµÎ¸ (xâ€²; Ïƒ, cend )â€²âˆ¥22

= x âˆ’ Ë†x0,cstart 

Ïƒt

âˆ’ (xâ€²)â€² âˆ’ ( Ë† xâ€² 

> 0,cend

)â€²

Ïƒt

> 22

= 1

Ïƒ2

> t

Ë†x0,cstart âˆ’ ( Ë† xâ€² 

> 0,cend

)â€² 22 .

(11) Here, the objective of Eq. (11) is to enforce the consistency between one path and a temporally reversed path in both directions by optimizing the noisy samples x as follows: 

Â¯x = arg min  

> x

L(x; Î¸, cstart , cend , Ïƒ ), (12) where Â¯x denotes the latent that minimizes the discrepancy between the two temporal paths. How-ever, incompatible motion priors induced by two frame conditions introduce the ambiguity between the two denoising paths, especially in early denoising steps. Without resolving this problem, the loss L is optimized to make the misaligned path worse, causing implausible motions in generated videos, as illustrated in Fig. I. When there is a significant gap between two motion priors, we could observe unrealistic motions like reverse play. Note that various types of visual artifacts come from incompatible motion priors, which will be discussed in Sec. 5.2. 4.2 BIDIRECTIONAL PATH ALIGNMENT WITH MOTION PRIOR DISTILLATION 

Since subsequent denoising steps primarily focus on restoring high-frequency details, previous works (Feng et al., 2024; Yang et al., 2025a) often fail to correct this misaligned trajectory. To resolve this issue, we introduce a single path sampling scheme that distills the motion prior induced by the start conditioning frame cstart into the backward path. Here, our key intuition is that the forward motion residual âˆ† of the denoised estimates Ë†x0,cstart contain useful motion information, which can be written as: 

âˆ† Ë† x0,cstart  

> (i)

:= Ë† x(i)0,cstart âˆ’ Ë†x(iâˆ’1) 0,cstart , (13) where i âˆˆ { 2, ...N } denotes the frame index, given N frames. Then, using the relation between DÎ¸

and ÏµÎ¸ in Eq. (2), the residual of noise from the forward path âˆ†Ïµfwd is given as: 

âˆ†Ïµfwd = âˆ†xt âˆ’ âˆ† Ë† x0,cstart 

Ïƒt

, (14) where âˆ†xt = x(i) 

> t

âˆ’ x(iâˆ’1)  

> t

represents the residual of the noisy sample xt. Now, given the encoded latent zend of the end frame Iend , we initialize the first index of the backward denoised estimate 

Ë†xâ€² 

> 0,cend
> (1)

with zend as: 

Ïµ(1)  

> bwd

= (xâ€²

> t

)(1) âˆ’ zend 

Ïƒt

. (15) Next, we reconstruct the backward noise residual Ïµbwd by cumulatively subtracting the forward noise residual from the initial backward noise Ïµ(1) 

> bwd

:

Ïµ(i) 

> bwd

= Ïµ(1)  

> bwd

âˆ’

> i

X

> k=2

âˆ†Ïµ(k)

> fwd

. (16) It is noteworthy that we should ignore the end frame condition cend . Therefore, we reformulate Eq. (2) as follows: 

Ë†xâ€² 

> 0,câˆ—
> start

= xt âˆ’ Ïƒt Ïµbwd . (17) Here, the reconstructed Ïµbwd from the residual of the forward noise âˆ†Ïµfwd provides us with a de-noised estimate Ë†xâ€² 

> 0,câˆ—
> start

from câˆ—

> start

, which implies the flipped motion prior of cstart . To curb off-manifold behaviors, we adopt CFG++ (Chung et al., 2025) following ViBiDSampler (Yang et al., 2025a). 5Published as a conference paper at ICLR 2026 

Algorithm 1 MOTION PRIOR DISTILLATION 

Input: xT âˆ¼ N (0 , Ïƒ 2 

> T

I), zstart , zend , {Ïƒt}Tt=1 , hyperparameters Î», k, Î³

Output: improved inbetweening result x0

1: cstart , cend â† encode( zstart , zend )

2: for t = T : (1 âˆ’ Î³)T do 

3: for j = 1 : k do 

4: Ë†x0,âˆ…, Ë†x0,cstart â† DÎ¸ (xt; Ïƒ, cstart ) â–· denoise forward path with cstart (Eq. (1)) 5: âˆ†x(i) 

> t

â† x(i) 

> t

âˆ’ x(iâˆ’1)  

> t

â–· forward path residuals 6: âˆ†x(i)0,cstart â† Ë†x(i)0,cstart âˆ’ Ë†x(iâˆ’1) 0,cstart â–· forward denoised estimate residuals (Eq. (13)) 7: âˆ†Ïµfwd â† (âˆ† xt âˆ’ âˆ† Ë† x0,cstart )/Ïƒ t â–· forward noise residuals (Eq. (14)) 8: xâ€² 

> t

â† flip( xt) â–· temporal flip 9: Ïµ(1)  

> bwd

â† (( xâ€²

> t

)(1) âˆ’ zend )/Ïƒ t â–· initialize first index of Ïµbwd (Eq. (15)) 10: Ïµ(i) 

> bwd

â† Ïµ(1)  

> bwd

âˆ’ Pik=2 âˆ†Ïµ(k)fwd â–· reconstruct Ïµbwd (Eq. (16)) 11: Ë†xâ€² 

> 0,câˆ—
> start

â† xt âˆ’ ÏƒtÏµbwd â–· reconstruct Ë†xâ€² 

> 0,câˆ—
> start

(Eq. (17)) 12: Ëœx0,cstart â† (1 âˆ’ Î») Ë† x0,cstart + Î»( Ë† xâ€² 

> 0,câˆ—
> start

)â€² â–· fuse two estimates (Eq. (18)) 13: xtâˆ’1 â† Ëœx0,cstart + Ïƒtâˆ’1 

> Ïƒt

(xt âˆ’ Ë†x0,âˆ…) â–· update with Euler step (Eq. (19)) 14: xt = xtâˆ’1 +

q

Ïƒ2 

> t

âˆ’ Ïƒ2

> tâˆ’1

Îµ â–· re-noise 15: end for 

16: end for 

Consequently, the Euler step of SVD in Eq. (4) denoises the sample xt:

Ëœx0,c start = (1 âˆ’ Î») Ë† x0,cstart + Î»( Ë† xâ€² 

> 0,câˆ—
> start

)â€², (18) 

xtâˆ’1 = Ëœ x0,c start + Ïƒtâˆ’1

Ïƒt

(xt âˆ’ Ë†x0,âˆ…), (19) where Î» âˆˆ [0 , 1] serves as the interpolation scale. Note that during this process, we intentionally do not denoise the temporally backward path with the end frame condition cend . This enables the direct transfer of the forward motion prior toward the end-frame constraint without introducing additional sources of misalignment. In addition, the proposed update in Eq. (19) can be seen as satisfying the proposed objective in Eq. (11) in a relaxed form. In our single-path update, the end-conditioned estimate is effectively replaced with the reconstructed estimate Ë†xâ€² 

> 0,câˆ—
> start

distilled from the forward motion prior. Thus, the loss that we define in Eq. (11) is simplified as: 

L(x; Î¸, cstart , câˆ—

> start

, Ïƒ ) = 1

Ïƒ2

> t

Ë†x0,cstart âˆ’ ( Ë† xâ€² 

> 0,câˆ—
> start

)â€² 22

. (20) By replacing the end frame condition cend with câˆ—

> start

, we can reduce the gap between the two temporal paths only with the start frame condition cstart :

Â¯x = arg min  

> x

L(x; Î¸, cstart , câˆ—

> start

, Ïƒ ). (21) This reformulated loss shows that the backward path no longer introduces an independent motion prior; instead, it is aligned through the forward motion prior. This gives the denoiser DÎ¸ the oppor-tunity to reconcile the original denoised path with its reconstructed counterpart within a timestep, producing a more stable trajectory. 4.3 PRACTICAL CONSIDERATIONS 

Specifically, we disable MPD in later denoising steps. This choice follows the observation that dif-fusion sampling proceeds in a coarse-to-fine manner: early denoising steps with large Ïƒ primarily represent global and low-frequency structure, whereas later denoising steps refine high frequency details (Rissanen et al., 2023; Kim et al., 2023; Wu et al., 2025). Complementary studies (Lee et al., 2025b;a; Park et al., 2025) further demonstrate that focusing guidance on these early steps yields better visual quality. Thus, modifying the motion prior is most effective when the trajectory is still being shaped globally. Motivated by these findings, we apply our method during the early denois-ing stage with additional re-noising steps k > 0 to steer the trajectory onto the correct direction. Then, we switch to existing time reversal samplers to enhance endpoint consistencies, which will be discussed in Secs. 5.3 and 5.4. More details of MPD are provided in Algorithm 1. 6Published as a conference paper at ICLR 2026 Table 1: Quantitative comparison results on DAVIS and Pexels dataset. We compare against six baselines. Ours + TRF and Ours + ViBiD refer to our method applied to the parallel and sequential time reversal sampling schemes, respectively. Best results are bold , and second-best are underlined.                                                                                                        

> Method DAVIS Pexels LPIPS â†“FID â†“FVD â†“VB â†‘VB++ â†‘LPIPS â†“FID â†“FVD â†“VB â†‘VB++ â†‘
> FILM 0.2946 55.160 1058.0 0.7978 0.9740 0.1157 43.935 761.60 0.8231 0.9734 DynamiCrafter 0.3158 46.739 678.92 0.7475 0.8735 0.2397 62.598 809.53 0.8211 0.9213 TRF 0.3127 56.894 674.31 0.7618 0.9352 0.2044 59.185 796.48 0.8008 0.9487 GI 0.2432 48.427 654.91 0.7747 0.9320 0.1114 47.990 476.93 0.8211 0.9566 FCVG 0.2347 38.997 621.82 0.7904 0.9353 0.1160 35.269 525.08 0.8245 0.9701 ViBiD 0.2492 39.883 559.49 0.7733 0.9387 0.1447 39.002 641.30 0.8130 0.9488
> Ours + TRF 0.2212 34.910 612.17 0.7992 0.9330 0.1149 34.470 460.99 0.8503 0.9862 Ours + ViBiD 0.2220 37.241 527.05 0.7845 0.9474 0.1028 34.775 412.66 0.8235 0.9605

## 5 EXPERIMENTAL RESULTS 

5.1 EXPERIMENTAL SETTINGS 

Evaluation dataset. Following the previous works (Yang et al., 2025a; Wang et al., 2025b), we compare our method with relevant SOTA methods on two representative datasets. Specifically, we utilize 100 video-keyframe pairs from DAVIS dataset (Pont-Tuset et al., 2017), and 45 from Pex-els 1. To simulate typical inbetweening conditions where long-range temporal reasoning is required between sparsely spaced keyframes, those videos exhibit diverse and large motions such as driving, dancing, and so on. 

Implementation details. We plug our method into both TRF (parallel) and ViBiD (sequential) building on SVD-XT model of SVD (Blattmann et al., 2023a) on a single NVIDIA RTX 4090 GPU. For the sampling process, we use the Euler scheduler with 25 timesteps with the default settings of SVD. Additionally, we configure each TRF and ViBiD with our settings: interpolation scale Î» as 1.0

and 0.5, the number of re-noising steps k as 2 and 3, and the distillation step ratio Î³ as 0.3 and 0.2.5.2 COMPARATIVE RESULTS 

As comparison methods, we choose representative time reversal sampling-based methods: TRF (Feng et al., 2024), GI (Wang et al., 2025b), FCVG (Zhu et al., 2025), and ViBiD (Yang et al., 2025a). We also include a flow-based VFI model, FILM (Reda et al., 2022), and a recent generative VFI model, DynamiCrafter (Xing et al., 2024b), for a broader comparison. 

Quantitative results. For quantitative evaluations, we use metrics for video frame interpolation, including FID (Heusel et al., 2017), FVD (Unterthiner et al., 2019), LPIPS (Zhang et al., 2018). FID and FVD measure the distances of generated frames/videos over ground-truth sequences. LPIPS as-sesses perceptual similarity at frame level. We also evaluate the overall quality of the videos using VBench (Huang et al., 2024a) and VBench++ (Huang et al., 2024b). VBench provides a compre-hensive assessment across multiple dimensions such as subject consistency, background consistency, aesthetic quality, image quality, motion smoothness, and temporal flickering. VBench++ typically evaluates comprehensive performance of videos with a single reference frame. Because inbetween-ing must treat both endpoints symmetically, we compute VBench++ for start and end frame each, then average them. As shown in Tab. 1, our method consistently outperforms the time reversal sampling-based methods, TRF and ViBiD, across all metrics. In particular, our method achieves significant improvements in terms of FID and FVD scores, highlighting its ability to produce temporally coherent sequences with smooth motion. For VBench++, FILM gets slightly better scores than our methods in DAVIS dataset. However, this can be attributed to flow-based warping that preserves local structures near each endpoint. This comes with blurry artifacts and weaker long-range temporal consistency, which yields the lower FVD score. Overall, our method effectively addresses the issue of conflicting motion priors and achieves both fidelity and perceptual quality over SOTA methods. 

> 1

https://www.pexels.com/ 

7Published as a conference paper at ICLR 2026 Ours  + TRF Ours  + ViBiD  ViBiD  TRF FCVG Ours  + ViBiD  Ours  + TRF  GI         

> ð¼ 1ð¼ 8ð¼ 12 ð¼ 17 ð¼ 25
> ð¼ 1ð¼ 4ð¼ 12 ð¼ 20 ð¼ 25

Figure 3: Qualitative baseline comparisons. TRF and ViBiD suffer from back-and-forth motion and intermittent disappearance, while GI and FCVG exhibit noticeable artifacts and ghosting effects. Our method yields more temporally consistent motion than the comparison methods. Additional examples are provided in the project page. 

Qualitative results. As shown in Fig. 3, our method produces a more temporally consistent mo-tion than the comparison methods. In the first group, TRF and ViBiD fail to preserve the childâ€™s forward-walking trajectory. Near the end frames, the child appears to walk backward or partially vanish, indicating the misalignment issue between the two paths. In the second group, GI and FCVG exhibit oscillations and ghosting artifacts. In particular, FCVG, relying on line matching, results in ambiguous artifacts, which is observed with the skier. In common, the comparison methods en-counter difficulties when subjectsâ€™ motion orientations are similar in both the start frame and end frame. In contrast, we validate that injecting forward motion residual into the backward path is enough to represent desirable object motions with fewer artifacts. 

User study. To further evaluate human preference beyond quantitative metrics, we conduct a com-prehensive user study via Amazon Mechanical Turk (Crowston, 2012). Each participant is presented with pairs of the start and end frames, followed by randomly 8 candidate videos generated by differ-ent methods. To avoid ordering bias, the display order is randomized for every sequence. Our user study is designed with three types of questionnaires: (1) Ranking : participants are asked to rank videos in order of overall naturalness and temporal coherence, focusing on how plausibly the generated sequence links the start and end frames. These rankings are converted into scores in recip-rocal order, ranging from 3.5 to -3.5. (2) Artifact detection : participants are asked to select all videos that exhibit noticeable visual artifacts, such as distortions, ghosting, or inconsistent textures. (3) Un-

8Published as a conference paper at ICLR 2026 Table 2: Comparison results of user study. Best results are bold , and second-best are underlined.                                            

> Method Alignment â†‘Artifact â†“Unrealistic â†“Method Alignment â†‘Artifact â†“Unrealistic â†“
> FILM - 0.4060 62.74% 54.76% FCVG 0.0988 20.36% 19.17% DynamiCrafter 0.0190 34.64% 37.14% ViBiD - 0.0678 28.10% 25.24% TRF - 0.3119 28.09% 25.24% Ours + TRF 0.3060 20.36% 22.62% GI 0.1179 22.26% 13.57% Ours + ViBiD 0.2440 8.93% 9.88%

ð¼ 1 ð¼ 9 ð¼ 13  ð¼ 25 

> 0.2 0.4 0.6 0.8 1.0

Figure 4: Ablation study on the effect of distillation ratio Î³. We vary the distillation step ratio 

Î³ âˆˆ { 0.2, 0.4, 0.6, 0.8, 1.0}, where Î³ = 0 .2 corresponds to the default setting and Î³ = 1 applies our method at every denoising step. 

realistic motion identification : participants are asked to choose all videos that contain unrealistic or physically implausible movements, which are closely related to perceptual plausibility. We collect responses from a total of 30 participants across 28 randomly sampled video groups to ensure the statistical reliability of the study. As shown in Tab. 2, our method achieves the highest preference in the ranking task, while being selected least frequently in both the artifact and un-realistic motion categories. These results demonstrate that our approach outperforms competitive baselines in terms of perceptual plausibility and human preference, providing strong evidence of its effectiveness in practical scenarios. 5.3 ABLATION STUDIES 

We conduct ablation studies on DAVIS dataset to evaluate the impact of distillation step ratio Î³,re-noising steps k, and interpolation scale Î». The results are summarized in Table 3. Within the parallel approach, LPIPS and FID are minimized at Î³ = 0 .3 and k = 2 , whereas FVD prefers weaker distillation and fewer re-noising steps. This is because TRF fuses the two conditional paths at every step, and the opposite motion prior is continually re-introduced after MPD, making the process more sensitive. Averaging the two paths partially cancels out the conflict and improves temporal coherence, while stronger MPD process can favor the frame-level fidelity at the cost of temporal smoothness. For the sequential time reversal sampling, we observe a clear and consistent optimum at Î³ = 0 .2,

k = 3 , and Î» = 1 .0, achieving the best scores. This indicates that strong early single-prior distillation with no interpolation is beneficial for the sequential method. Once the backward path is aligned to the forward motion prior in the early phase, subsequent steps rarely introduce conflicting priors, so increasing k steadily helps to improve the temporal and perceptual quality of videos. 9Published as a conference paper at ICLR 2026 Table 3: Ablation results for distillation steps ratio Î³, re-noising steps k, and interpolation scale Î».(a)â€“(c) correspond to Ours + TRF, and (d)â€“(f) correspond to Ours + ViBiD. Best results are bold .                                                                                          

> (a) Distillation step ratio ( Î³)
> Î³LPIPS â†“FID â†“FVD â†“
> 0.3 0.2212 34.910 612.17 0.2 0.2238 35.408 576.01 0.1 0.2236 35.268 573.13
> (b) Re-noising steps ( k)
> kLPIPS â†“FID â†“FVD â†“
> 10.2246 35.149 588.27
> 20.2212 34.910 612.17 30.2248 35.765 662.75 (c) Interpolation scale ( Î»)
> Î»LPIPS â†“FID â†“FVD â†“
> 0.5 0.2212 34.910 612.17
> 1.0 0.2264 35.612 654.72 ----
> (d) Distillation step ratio ( Î³)
> Î³LPIPS â†“FID â†“FVD â†“
> 0.3 0.2421 39.855 574.05 0.2 0.2220 37.241 527.05
> 0.1 0.2374 39.949 545.25 (e) Re-noising steps ( k)
> kLPIPS â†“FID â†“FVD â†“
> 10.2379 39.961 568.06 20.2341 39.368 545.25 30.2220 37.241 527.05
> (f) Interpolation scale ( Î»)
> Î»LPIPS â†“FID â†“FVD â†“
> 0.5 0.2242 37.837 539.99 1.0 0.2220 37.241 527.05
> ----

5.4 THE EFFECT OF DISTILLATION RATIO 

Table 4: Effect of the distillation ratio Î³. Best results are bold .

Î³ LPIPS â†“ FID â†“ FVD â†“

0.2 0.2220 37.241 527.05 

0.4 0.2478 45.636 634.11 0.6 0.2562 55.873 813.08 0.8 0.2679 68.007 973.64 1.0 0.2721 75.544 1086.6 We conduct another ablation study on the variation of the dis-tillation step ratio Î³ up to 1.0. As shown in Tab. 4, increasing 

Î³ consistently leads to worse scores across all metrics. The cropped examples in Fig. 4 further show that applying our method with Î³ > 0.3 does not further improve motion consis-tency, but rather introduces undesirable pixel-level biases and degrades visual fidelity. Both quantitative and qualitative stud-ies support our choice to apply our method in the early stage of sampling, rather than throughout the entire denoising process. 5.5 COMPUTATIONAL EFFICIENCY 

We compare computational cost with other I2V diffusion based methods, as summarized in Tab. 5. DynamiCrafter requires additional training on a I2V diffusion model for the generative inbetween-ing task. Likewise, GI and FCVG rely on fine-tuning SVD models, which also require substantial computational resources. During inference, our method denoises the temporally forward path and then adds a few extra re-noising steps to make the two paths align. Due to these steps, the infer-ence time can be slightly longer than FCVG and ViBiD. However, this small extra cost yields better alignment and fewer artifacts in generated videos. Table 5: Comparisons on computational efficiency.                                                               

> Method Train Inference time (s) VRAM Usage (GB) Resolution DynamiCrafter âœ“26 11.2 16 Ã—512 Ã—320 TRF âœ—429 13.6 25 Ã—1024 Ã—576 GI âœ“663 23.4 25 Ã—1024 Ã—576 FCVG âœ“134 24.1 25 Ã—1024 Ã—576 ViBiD âœ—108 19.2 25 Ã—1024 Ã—576
> Ours + TRF âœ—143 19.2 25 Ã—1024 Ã—576
> Ours + ViBiD âœ—141 19.2 25 Ã—1024 Ã—576

## 6 CONCLUSION 

In this work, we analyze the bidirectional path misalignment problem in existing time reversal sam-pling through the lens of optimization problem. Based on the analysis, we present Motion Prior Distillation (MPD) , a training-free sampling method that resolves motion prior conflict in exist-ing time reversal samplers to enhance generative inbetweening task. MPD replaces two conflicting temporal priors with a single coherent motion prior from the start frame, and distills it through the backward denoising path, yielding a coherent trajectory that satisfies both endpoint constraints. By integrating MPD into time reversal sampling methods, we demonstrate that MPD synergisti-cally reduces temporal discontinuities and visual artifacts, and achieves more appealing results both quantitatively and qualitatively than SOTA methods. 10 Published as a conference paper at ICLR 2026 ACKNOWLEDGMENTS 

This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (RS-2020-II201361 and RS-2025-25441838), by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2024-00338439), and by the Yonsei University Future-Leading Research Initiative of 2025 (2025-22-0409). 

## REFERENCES 

Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement. In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) ,2019. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: A space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers , 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 , 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion mod-els. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023b. Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. CFG++: Manifold-constrained classifier free guidance for diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , 2025. Kevin Crowston. Amazon mechanical turk: A research tool for organizations and information sys-tems scholars. In Shaping the Future of ICT Research. Methods and Approaches: IFIP WG 8.2, Working Conference, Tampa, FL, USA, December 13-14, 2012. Proceedings , 2012. Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffu-sion models. In Proceedings of the AAAI Conference on Artificial Intelligence , 2024. Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael J Black, and Xuaner Zhang. Explorative inbetweening of time and space. In Proceedings of the European Conference on Computer Vision (ECCV) , 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the Neural Information Processing Systems (NeurIPS) , 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In Proceedings of the Neural Information Processing Systems Workshop (NeurIPSW) , 2021. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 , 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Proceedings of the Neural Information Processing Systems (NeurIPS) , 2022b. Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In Proceedings of the European Conference on Computer Vision (ECCV) , 2022. 11 Published as a conference paper at ICLR 2026 Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianx-ing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2024a. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chan-paisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503 , 2024b. Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Proceedings of the Neural Information Processing Systems (NeurIPS) ,2022. Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Taesu Kim, Daehyun Ahn, Hyungjun Kim, et al. Lever-aging early-stage robustness in diffusion models for efficient and high-quality image synthesis. In 

Proceedings of the Neural Information Processing Systems (NeurIPS) , 2023. Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. Ifrnet: Intermediate feature refine network for efficient frame interpola-tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2022. Dahyeon Kye, Changhyun Roh, Sukhun Ko, Chanho Eom, and Jihyong Oh. Acevfi: A comprehen-sive survey of advances in video frame interpolation. arXiv preprint arXiv:2506.01061 , 2025. Dohun Lee, Bryan Sangwoo Kim, Geon Yeong Park, and Jong Chul Ye. Videoguide: Improv-ing video diffusion models without training through a teacherâ€™s guide. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2025a. Haeil Lee, Hansang Lee, Seoyeon Gye, and Junmo Kim. Beta sampling is all you need: Efficient im-age generation strategy for diffusion models using stepwise spectral analysis. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pp. 4215â€“4224. IEEE, 2025b. Pengcheng Lei, Faming Fang, Tieyong Zeng, and Guixu Zhang. Flow guidance deformable compen-sation network for video frame interpolation. IEEE Transactions on Multimedia , 26:1801â€“1812, 2023. Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023. Libo Long, Xiao Hu, and Jochen Lang. Learning to handle large obstructions in video frame inter-polation. In Proceedings of the 32nd ACM International Conference on Multimedia , 2024. Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition (CVPR) , 2022. Zonglin Lyu, Ming Li, Jianbo Jiao, and Chen Chen. Frame interpolation with consecutive brownian bridge diffusion. In Proceedings of the 32nd ACM International Conference on Multimedia , 2024. Simon Niklaus and Feng Liu. Context-aware synthesis for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. Geon Yeong Park, Sang Wan Lee, and Jong Chul Ye. Inference-time diffusion model distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 4049â€“4058, 2025. 12 Published as a conference paper at ICLR 2026 Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation. In Proceedings of the European Conference on Computer Vision (ECCV) , 2020. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision , pp. 4195â€“4205, 2023. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo ArbelÂ´ aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 , 2017. Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Cur-less. Film: Frame interpolation for large motion. In Proceedings of the European Conference on Computer Vision (ECCV) , 2022. Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dis-sipation. In Proceedings of the International Conference on Learning Representations (ICLR) ,2023. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, RaphaÂ¨ el Marinier, Marcin Michalski, and Sylvain Gelly. FVD: A new metric for video generation, 2019. Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. In Proceedings of the Neural Information Processing Systems (NeurIPS) , 2022. Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao OUYANG, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. Framer: Interactive frame interpolation. In Proceedings of the International Conference on Learning Representations (ICLR) , 2025a. Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steve Seitz. Generative inbetweening: Adapting image-to-video models for keyframe inter-polation. In Proceedings of the International Conference on Learning Representations (ICLR) ,2025b. Haoyu Wu, Jingyi Xu, Hieu Le, and Dimitris Samaras. Importance-based token merging for effi-cient image and video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 4983â€“4995, 2025. Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Tooncrafter: Generative cartoon interpolation. ACM Transactions on Graphics (TOG) , 43 (6):1â€“11, 2024a. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In Proceedings of the European Conference on Computer Vision (ECCV) , 2024b. Serin Yang, Taesung Kwon, and Jong Chul Ye. VibiDSampler: Enhancing video interpolation using bidirectional diffusion sampler. In Proceedings of the International Conference on Learning Representations (ICLR) , 2025a. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In Proceedings of the International Conference on Learning Repre-sentations (ICLR) , 2025b. Guozhen Zhang, Yuhan Zhu, Yutao Cui, Xiaotong Zhao, Kai Ma, and Limin Wang. Motion-aware generative frame interpolation. arXiv preprint arXiv:2501.03699 , 2025. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 13 Published as a conference paper at ICLR 2026 Tianyi Zhu, Dongwei Ren, Qilong Wang, Xiaohe Wu, and Wangmeng Zuo. Generative inbetween-ing through frame-wise conditions-driven video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2025. 14 Published as a conference paper at ICLR 2026 

## A ANALYSIS ON DENOISED ESTIMATES w/o  MPD   

> (ð‘¡  = 0.5ð‘‡ )
> forward backward
> w/ MPD
> (ð‘¡  = 0.5ð‘‡ )
> ð¼ 1ð¼ 19 ð¼ 13 ð¼ 7ð¼ 25
> forward backward

Figure I: Analysis on denoised estimates. At the midpoint of time reversal sampling, we take the forward and backward denoised estimate, align the latter to the temporal order, and inspect their difference. For a clearer understanding of our motivation in Sec. 4.1, we conduct an additional experiment in which the forward and backward denoised estimates, Ë†x0,cstart and Ë†xâ€² 

> 0,cend

, are obtained at an inter-mediate time step ( t = 0 .5T ), both with and without applying our method. Note that the backward denoised estimate is temporally flipped to enable direct comparison. As shown in the first two rows of Fig. I, existing methods produce intermediate frames with implausible motion. The forward and backward trajectories attempt to reflect two incompatible motion priors simultaneously, revealing a clear motion conflict. In contrast, as shown in the last two rows of Fig. I, our method maintains a consistent motion trajectory in both temporal paths, generating coherent intermediate frames without such conflicts. 

## B FINE -TUNING METHODS WITH TIME REVERSAL SAMPLING 

In this section, we deeply discuss the fine-tuning methods that incorporate the time reversal sam-pling strategy. Existing fine-tuning approaches (Wang et al., 2025b; Zhu et al., 2025) share two major limitations. First, they still rely on incompatible motion priors at inference time, so mis-matches between forward and backward trajectories are not fundamentally resolved. Second, both methods require additional fine-tuning, which demands substantial computational cost. In contrast, our method aligns the two temporal paths without additional training, and integrates into existing time reversal samplers with only a minor change to the sampling loop. GI (Wang et al., 2025b) improves backward motion fidelity by fine-tuning SVD through rotated temporal self-attention maps. While this design encourages consistency between two paths, the two paths are still driven by different motion priors. Additionally, the backward-motion network is trained only on a small collection of videos, which may not fully capture the diverse backward motion patterns. As shown in Fig. VI, this can lead to motion conflicts such as two surfers being generated, even though there must be one. Instead, our method reconstructs a backward estimate directly from the forward motion residuals, so that the backward path no longer introduces its own motion prior but instead follows the time-reversed motion induced by the start frame, leveraging the faithful forward motion prior of SVD. FCVG (Zhu et al., 2025) adopts frame-wise conditions by extracting matched line segments from the two keyframes. Then, they interpolates the frame-wise correspondences over time, and then feeds them into SVD as the guidance. This works well when scenes contain strong, well-defined structural edges, but it has practical limitations. The method is highly sensitive to the quality of the line extraction and matching pipeline. Thus, failures in detection or matching directly lead to unstable or distorted interpolations. As shown in Fig. IV, such failures appear as noticeable ghosting artifacts on the carrier. 15 Published as a conference paper at ICLR 2026 

## C EXPERIMENTS ON LARGE TEMPORAL GAPS ð¼ 1 ð¼ 11 ð¼ 21  ð¼ 31  ð¼ 41

> TRF

ð¼ 51 

> Ours  + TRF

Figure II: Qualitative results over a 50-frame gap. 

We conduct additional experiments on large temporal gaps, where the two keyframes are separated by 50 frames. In this challenging setting, both the previous time reversal sampling methods and our method exhibit noticeable degradation in generated videos. However, our method still reduces severe ghosting and back-and-forth motion compared to the baselines, thereby producing more plausible intermediate frames. As shown in Fig. II, TRF often yields duplicated or intermittent legs, whereas incorporating our method with TRF generates more coherent inbetweening results. 

## D DISCUSSIONS ON EFFECTIVE AND CHALLENGING SCENARIOS ð¼ 1 ð¼ 9 ð¼ 17  ð¼ 25 

(a) 

(b) 

Figure III: Failure cases. Our method struggles when (a) the end keyframe introduces entirely new objects or (b) the scene undergoes large-scale rearrangements between two keyframes. One of the key components of our method is the endpoint initialization in Eq. (15) that initializes the backward noise Ïµbwd using the latent encoding of the end frame zend . This implicitly assumes that the end frame of the forward path is reasonably consistent with the ground truth end frame. When the forward trajectory ends far from the ground truth end frame, Eq. (15) may become a less informative initialization. In such cases, the shared motion prior can be biased toward an inaccurate end frame, so smalls residual discrepancies in object placement or appearance may remain. We experimentally find that our method is most effective when both keyframes contain the same object and share a semantically coherent motion as presented in Sec. E. In such scenarios, the for-ward motion prior provides the reliable global trajectories, and distilling it into the backward path successfully reduces undesirable artifacts. However, as discussed in previous works (Wang et al., 2025b; Zhu et al., 2025), MPD still struggles with the inevitable problems that arise when the end frame introduces entirely new objects or undergoes large-scale scene rearrangements. Representative failure cases are presented in Fig. III. To better handle these challenging scenarios, we are currently exploring extensions of our approach on large-scale I2V diffusion models such as CogVideoX (Yang et al., 2025b), which adopt a DiT-based architecture (Peebles & Xie, 2023) and can generate over 80 frames. 16 Published as a conference paper at ICLR 2026 

## E ADDITIONAL QUALITATIVE RESULTS TRF GI ViBiD FCVG 

ð¼ 1 ð¼ 7 ð¼ 13  ð¼ 19  ð¼ 25 

> Ours + TRF Ours + ViBiD TRF GI ViBiD FCVG

ð¼ 1 ð¼ 7 ð¼ 13  ð¼ 19  ð¼ 25 

> Ours + TRF Ours + ViBiD

Figure IV: Additional comparison results with baseline models. 17 Published as a conference paper at ICLR 2026 TRF GI ViBiD FCVG 

ð¼ 1 ð¼ 7 ð¼ 13  ð¼ 19  ð¼ 25 

> Ours + TRF Ours + ViBiD TRF GI ViBiD FCVG

ð¼ 1 ð¼ 7 ð¼ 13  ð¼ 19  ð¼ 25 

> Ours + TRF Ours + ViBiD

Figure V: Additional comparison results with baseline models 18 Published as a conference paper at ICLR 2026 TRF GI ViBiD FCVG Ours + TRF Ours + ViBiD 

ð¼ 1 ð¼ 7 ð¼ 13  ð¼ 19  ð¼ 25 TRF GI ViBiD FCVG 

ð¼ 1 ð¼ 7 ð¼ 13  ð¼ 19  ð¼ 25 

> Ours + TRF Ours + ViBiD

Figure VI: Additional comparison results with baseline models 19
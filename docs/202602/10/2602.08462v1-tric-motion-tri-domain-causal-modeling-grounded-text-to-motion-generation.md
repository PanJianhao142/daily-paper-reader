---
title: "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation"
authors: "Yiyang Cao, Yunze Deng, Ziyu Lin, Bin Feng, Xinggang Wang, Wenyu Liu, Dandan Zheng, Jingdong Chen"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.08462v1"
tags: ["keyword:MDM", "query:课题"]
score: 9.0
evidence: 文本到运动生成与时空建模
tldr: "针对文本生成动作任务中空-时-频三域建模不统一以及噪声干扰导致动作失真的问题，本文提出了TriC-Motion框架。该框架基于扩散模型，通过时间运动编码、空间拓扑建模和混合频率分析实现三域联合优化，并引入因果反事实运动解耦器来消除无关噪声。实验证明，该方法在HumanML3D数据集上达到了0.612的R@1，显著提升了生成动作的保真度、连贯性与文本对齐度。"
motivation: 现有方法缺乏空间、时间和频率域的统一优化框架，且容易受到运动无关噪声的干扰导致动作失真。
method: 提出一种集成三域建模与因果干预的扩散框架，利用得分引导的三域融合模块及反事实解耦器来优化生成质量并消除噪声。
result: "在HumanML3D数据集上取得了0.612的R@1评分，在生成质量、多样性和文本对齐度上均超越了现有最先进的方法。"
conclusion: TriC-Motion通过多域联合建模与因果解耦技术，有效解决了动作生成中的域缺失与特征纠缠问题，实现了高保真的动作序列生成。
---

## Abstract
Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

---

## 论文详细总结（自动生成）

## TriC-Motion: 基于三域因果建模的文本驱动动作生成论文总结

### 1. 核心问题与整体含义
*   **研究背景**：文本驱动的人体动作生成（Text-to-Motion）旨在根据文字描述生成逼真、连贯且语义对齐的动作序列。
*   **核心痛点**：
    1.  **多域建模缺失**：现有方法多侧重于时空建模或独立的频域分析，缺乏统一的框架来同时优化空间拓扑、时间连贯性和频率特征（全局趋势与精细动态）。
    2.  **特征纠缠与噪声干扰**：生成过程中，与动作无关的噪声线索（Confounders）常与有效特征纠缠，导致动作失真或语义偏移。
*   **研究动机**：提出一个统一的扩散模型框架，通过集成时-空-频三域建模和因果干预技术，提升生成动作的保真度与文本一致性。

### 2. 方法论
*   **核心思想**：构建一个名为 **TriC-Motion** 的扩散去噪网络，在每个去噪块中并行执行三域特征提取，并引入因果反事实解耦器来消除噪声干扰。
*   **关键技术细节**：
    *   **三域建模模块**：
        *   **TME（时间运动编码）**：使用 Transformer 捕捉动作序列的长短期时间依赖。
        *   **STM（空间拓扑建模）**：利用图卷积网络（GCN）建模人体关节间的空间连接和物理约束。
        *   **HFA（混合频率分析）**：结合离散小波变换（DWT）和快速傅里叶变换（FFT），分别处理低频（粗粒度趋势）和高频（细粒度动态）信号。
    *   **S-Fus（得分引导融合）**：利用全局语义（CLS Token）和运动特征计算权重，动态融合三域信息。
    *   **CCMD（因果反事实运动解耦器）**：基于结构因果模型（SCM），将特征分解为“事实部分”（有效运动特征）和“反事实部分”（无关噪声），通过因果干预操作 $do(\cdot)$ 抑制噪声，强化核心运动特征。
*   **算法流程**：输入文本经 DistilBERT 编码，与带噪动作序列一同进入堆叠的 TriC-Motion 去噪块，经过三域提取、因果解耦、特征融合及文本注入，最终预测清晰动作。

### 3. 实验设计
*   **数据集**：
    *   **HumanML3D**：目前最主流的人体动作-语言数据集。
    *   **SnapMoGen**：包含更复杂、更长文本描述的大规模新数据集。
*   **Benchmark 与对比方法**：
    *   对比了包括 **MDM, T2M-GPT, MoMask, SALAD, MotionPCM, MARDM** 等在内的十余种最先进（SOTA）方法。
*   **评价指标**：R-Precision (Top 1/2/3)、FID（保真度）、MM-Dist（多模态距离）、Diversity（多样性）。

### 4. 资源与算力
*   **硬件环境**：使用 **2 张 NVIDIA RTX 3090 GPU** 进行训练。
*   **训练细节**：
    *   Batch size 为 64，采用 AdamW 优化器。
    *   在 HumanML3D 上训练 650K 次迭代，在 SnapMoGen 上训练 250K 次迭代。
*   **推理效率**：单样本平均推理时间（AIT）为 **3.8 秒**，参数量为 **13.86M**。

### 5. 实验数量与充分性
*   **实验规模**：进行了大规模的定量对比，每个指标均重复 20 次实验并计算 95% 置信区间，确保结果的统计显著性。
*   **消融实验**：极其充分。分别验证了 TME/STM/HFA 各分支的作用、频率分支中 FFT/联合维度的必要性、因果干预（CCMD）在不同域上的增益，以及损失函数权重的敏感性。
*   **主观验证**：组织了 38 人的用户研究，从文本对齐度和整体质量两个维度进行打分。
*   **客观性保障**：额外引入了 **CLaM 评估器** 进行交叉验证，证明性能提升并非源于对特定评估空间的过拟合。

### 6. 主要结论与发现
*   **性能突破**：TriC-Motion 在 HumanML3D 数据集上取得了 **0.612 的 R@1**，显著超越了之前的 SOTA 方法（如 SALAD 的 0.581）。
*   **三域协同效应**：空间建模确保了骨骼不畸形，频率建模增强了动作的细腻程度，两者结合显著提升了 FID。
*   **因果干预的价值**：CCMD 模块能有效分离文本中的核心指令与生成过程中的随机噪声，使模型在处理复杂长指令时更加稳健。

### 7. 优点
*   **理论创新**：首次将因果干预和反事实解耦引入文本转动作任务，提供了处理特征纠缠的新视角。
*   **建模全面**：打破了以往单一域建模的局限，实现了时、空、频三域的有机统一。
*   **模型轻量**：尽管引入了多分支建模，但参数量（13.86M）远低于许多自回归模型（如 MARDM 的 310M）。

### 8. 不足与局限
*   **推理速度瓶颈**：由于基于原始动作空间的扩散模型需要多步迭代（50步），其推理速度（3.8s）虽优于部分扩散模型，但仍慢于基于 VQ 的离散生成模型（如 MoMask 的 0.4s）。
*   **计算复杂度**：三域并行分支导致 FLOPs 较高（388G），在移动端或实时交互场景下可能存在压力。
*   **应用限制**：目前主要针对单人动作生成，尚未扩展到多人交互或复杂环境避障动作生成。

（完）

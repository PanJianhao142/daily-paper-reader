Title: Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity

URL Source: https://arxiv.org/pdf/2602.07413v1

Published Time: Tue, 10 Feb 2026 01:40:13 GMT

Number of Pages: 18

Markdown Content:
# Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity 

Yunhai Han, Linhao Bai †, Ziyu Xiao †, Zhaodong Yang †, Yogita Choudhary, Krishna Jha, Chuizheng Kong, Shreyas Kousik, Harish Ravichandar  

> †

denotes equal contribution Institute for Robotics and Intelligent Machines Georgia Institute of Technology, Atlanta, GA 30332 

Abstract —There has been rapid and dramatic progress in robots’ ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. How-ever, these design choices require significant data and compu-tational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs) , a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment ( visual flow ) and proprioceptive states of the robot (action flow ) co-evolve. By capturing such behavioral dynamics ,UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM , a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner : given an initial condition, it analytically computes the desired robot behavior while simultaneously “imagining” the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning. 

I. I NTRODUCTION 

Reliable dexterous manipulation with multi-fingered hands has the potential to enable robots to seamlessly operate in a world made for and by humans. Unlike simple pick-and-place with parallel-jaw grippers, multi-fingered dexter-ity requires mastering high-dimensional coordinated control under complex contact dynamics, partial observability, and frequent visual occlusion [1–7]. Data-driven approaches like Diffusion Policies [8–10] and Transformers (e.g., ACT [11]) have revolutionized this space by learning directly from visual demonstrations. Fundamentally, these approaches model skills as reactive mappings (ot → at), treating the task as a sequence of independent decisions rather than a continuous physical process. To encourage temporal coherence and prevent jitter, these methods rely heavily on action chunking ( ot → at:t+H )or temporal ensembling [11–14] as heuristics that average predictions over fixed, hard-coded time windows. While ef-fective, these strategies introduce a rigid trade-off between deliberate planning (full-horizon predictions) and reactivity (short chunks), and any resulting coherence is a byproduct of averaging rather than a fundamental property of the learned representation (see Fig. 1, Panel A). An alternative perspective is to view any dexterous skill not as a sequence of decisions, but as being governed by an under-lying dynamical system . Seminal work on Dynamic Movement Primitives (DMPs) [15, 16] and numerous dynamical systems-based approaches [17–23] have long considered skills as solutions to fictitious dynamical systems. However, they tend to only model the dynamics of the robot’s movement, treating the environment as a static boundary condition. While highly effective for goal-reaching movements, this robot-centric view fails to capture the essence of dexterous manipulation: dexter-ity resides in the intricate coupling between the robot hand and the environment. In dexterous tasks, the object’s movement is equally if not more important than that of the robot. In this work, we introduce Unified Behavioral Models (UBMs) , a framework that learns to represent dexterous skills as coupled dynamical systems directly from demonstrations (see Fig. 1, Panel B). UBMs capture the robot-environment interdependence by learning the behavioral dynamics of the entire system that governs how visual features of the envi-ronment ( visual flow ) and proprioceptive states of the robot (action flow ) co-evolve. Specifically, UBMs learn a unified latent space in which the joint state of the robot and object evolves according to a latent dynamical system. During in-ference, UBMs can be seen as “Implicit Planners” that can generate complete “plans” via open-loop latent rollout from any given initial condition. Crucially, UBMs can trivially support dynamic chunking as one can flexibly vary the number of time steps for which the model is rolled out (i.e., atunable planning horizon). However, realizing UBMs presents a significant practical challenge: the joint dynamics of a multi-fingered hand interacting with a deformable or sliding object are highly nonlinear and difficult to learn directly from limited visual data without drifting or diverging. To overcome the challenges of realizing UBMs, we in-troduce Koopman-UBM as the first instantiation of UBMs                    

> arXiv:2602.07413v1 [cs.RO] 7 Feb 2026 Fig. 1: (A) Standard reactive policies (e.g., Diffusion, ACT) map observations to short-horizon action chunks, lacking a consistent internal model of the future or memory beyond the observation window, leading to temporal incoherence and hand-coded chunk lengths. (B) In contrast, Unified Behavioral Models (UBM) model skills as joint behavioral dynamics of the robot and environment governing a continuous flow in a latent space, ensuring coherence and enabling full-horizon “planning” from initial conditions. (C) We propose Koopman-UBM
> as the first instantiation of UBMs, which lifts visual and proprioceptive observations into a latent space governed by a learned Koopman Operator. By enforcing linear spectral dynamics ( zt+1 =Kz t) over a unified “state-inclusive” latent space, K-UBM ensures enables fast inference and predictive monitoring. (D) Our approach enables temporally-coherent predictions (dashed-purple) that are robust to visual occlusion (dashed green and purple trajectories inside gray boxes) and reactivity via an event-triggered replanning strategy that reinitializes the UBM only when the predicted visual features diverge from reality (top orange box).

that leverages Koopman Operator theory [24–28] to ensure tractable learning and efficient computation (see Fig. 1, Panel C). By lifting the nonlinear interaction between the robot and visual features into a latent space where their joint evolution becomes linear ( zt+1 = Kz t), Koopman-UBM transforms the complex nonlinear dynamics learning problem into arepresentation learning problem for linear dynamics. After encoding the initial conditions into the latent space, Koopman-UBM can generate the entire nominal trajectory via linear rollout, ensuring temporal coherence by construction regard-less of the rollout horizon. This allows Koopman-UBM to act as an implicit visuo-motor planner that predicts the future flow of the environment (e.g., object motion) alongside the robot’s actions without the computational cost of continuous sequential decision making via reactive mapping. To ground K-UBM in raw high-dimensional visual data, We fuse proprioceptive history with robust visual representa-tions, and investigate the effectiveness of two complementary representations: (i) motion-centric features derived from point tracking (Object Flow [29]), and (ii) manipulation-centric fea-tures learned via self-supervised (Dynamo [30]). In addition, we leverage a state-inclusive lifting strategy (i.e., visual and proprioceptive features appear as a subvector of z [31]) to form a latent state that explicitly embeds the system’s interpretable physical configuration and enable fast decoder-free inference. By optimizing this representation jointly with the behavioral dynamics, our method learns dynamics-aware embeddings that linearize complex contact behaviors. This helps ensure that Koopman-UBM’s roll-outs are reliable and coherent during nominal execution. Unlike reactive policies that falter and freeze during strong occlusions and frame drops, K-UBM can faithfully propagate the system’s “momentum” without sensory feedback by relying on its internal dynamics model to bridge perceptual gaps (see Fig. 1, Panel D). However, relying solely on open-loop rollout from an initial conditions has a considerable limitation: it cannot account for unanticipated external disturbances during execution. To enable reactivity , we leverage UBM’s unique ability to predict visual features to introduce a framework-level event-triggered replanning strategy. Because Koopman-UBM can explicitly predict the nominal flow of visual features, the model can act as its own runtime monitor (see Fig. 1, Panel D). Our replanning strategy monitors the error between predicted and observed visual features. The system executes the initial nominal plan as long as predictions match reality; when a disturbance causes the error to exceed a threshold, the system triggers a replan (re-initializes zt and computes a new coherent trajectory). Combining this flexible replanning strategy with Koopman-UBM allows the robot to flexibly navigate the spectrum between deliberate planning and reactive decision making without enforcing a rigid trade-off. In summary, our key contributions are: 1) We introduce Unified Behavioral Models (UBM) , a class of sensory-motor skill models that encode dexter-ous skills as coupled dynamical systems, capturing the inter-dependence of robot actions and environmental fea-tures. UBMs ensure temporal coherence by construction and enable full-horizon predictions of unified nominal actions and visual feature flows. 2) We propose Koopman-UBM , a Koopman-based instan-tiation of UBM that acts as an implicit visuo-motor planner. By leveraging state-inclusive lifting and lin-ear spectral dynamics, Koopman-UBM enables reliable open-loop trajectory generation while also supporting closed-loop reactivity via predictive monitoring. 3) We provide a comprehensive evaluation across 7 sim-ulated and 2 real-world dexterous manipulation tasks, demonstrating that our UBM-based approach matches or exceeds the performance of state-of-the-art baselines (e.g., Diffusion policy, ACT, etc.), while offering con-siderably faster inference, smooth execution, robustness to occlusions, and flexible replanning. II. U NIFIED BEHAVIORAL MODELS 

We begin by formulating the problem of learning visuo-motor dexterous skills from demonstrations, and introduce the notion of unified behavioral models (UBMs). Let D = [ {o(1)  

> t

, a(1)  

> t

}T (1)  

> t=1

, · · · , {o(N ) 

> t

, a(N ) 

> t

}T (N ) 

> t=1

] denotes a dataset of N demonstrations of a certain target skill (e.g., opening a box, hammering a nail, etc.), with the i-th demo being a trajectory with T (i) steps. At each time step t, the observation ot consists of the robot joint state and visual observations, while the action at corresponds to the robot joint command. Specifically, we define the observation as 

ot = {qt, It}, where qt ∈ Q ⊆ Rdq denotes the robot joint state (i.e., joint configuration of the arm and the hand) with dq degrees-of-freedom (dof), and It ∈ I ⊆ RH×W ×3

denotes the RGB visual observation. The action is defined as 

at = q target  

> t

∈ Q ⊆ Rdq , representing the robot commands issued to the low-level controller. The most common approach to learning the dexterous skill encoded in D involves learning a reactive policy [8, 11]. A reactive policy can be represented as a map π : ot 7 → at

(or more generally, ot−h:t 7 → at:t+l), where h = H ∈ N and 

l = L ∈ N denote the hard-coded, fixed lengths of history and action chunks, respectively. In contrast to reactive policies, we take the approach of learning a unified behavioral model (UBM) D. Instead of building a map from observations to action (chunks), UBMs learn to represent the target dexterous skill as a dynamical system that encodes behavioral dynamics: laws that govern how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. Formally, a UBM can be defined as zt+1 = fU BM (zt), where 

zt = fenc (ξt) is the latent state of the UBM produced by encoding the unified behavioral state ξt = [ at, o t], containing both action and observation information. In order to extract the predicted actions from the latent flow, UBMs also require an action decoder: at = fdec (zt). As such learning a UBM requires the learning fU BM (the latent behavioral dynamics model), fenc (unified sensory-motor encoder), and fenc (action decoder) simultaneously. While it might be possible to learn these modules separately, co-training them is necessary to learn a temporally-coherent and predictive latent representa-tion over which we could learn dynamics [32, 33]. Although closely related, note that UBMs are different from the increas-ingly popular notion of “robotic world models” [34, 35]. While world models learn to predict the impact of a given action by approximating the underlying natural dynamics of the system (i.e., xt+1 = fW M (xt, a t) where xt is the (latent) state of interest), UBMs capture the coupled sensory-motor behavioral dynamics. Given the above formulation, UBMs offer a number of practical benefits when learning dexterous skills. First, they serve as implicit planners capable of generating “plans” of arbitrary lengths up to the full skill horizon. Specifically, given an initial condition ξ0 = [ a0, o 0], we can initialize the unified latent state z0 = fenc (ξ0) and forward propagate (rollout) the learned dynamics fU BM (·) up to any desired number of time steps Tl ≤ TH (with TH as the full skill-horizon) to generate 

{zt}Tl

> t=1

. Finally, we can leverage the learned action decoder 

fdec (·) to extract the corresponding action trajectory {at}Tl

> t=1

.In addition, to ensure inference-time accessibility of the ini-tial state, we add auxiliary initial state to the action trajectories. Details are provided in the appendix. VII. III. K OOPMAN -BASED UNIFIED BEHAVIOR MODELS 

While the above formulation of UBMs is easy to describe, learning them can present numerous changes. In particular, high-dimensional raw visual observations and action spaces, noisy data collection, non-smooth contact dynamics, temporal drift in latent dynamical systems can introduce considerable challenges and require careful considerations. In this sec-tion, we introduce Koopman-UBM , a practical instantiation of UBMs that leverages Koopman operator theory and motion-centric visual features to overcome these challenges. Koopman-UBM combines representation learning and latent behavioral dynamics learning, and learns from demonstrations in two stages: (i) Visual Feature Extraction , which extracts motion-centric visual features from raw images {It} across all time steps of the demonstrations (See Section. III-A), and ii) Koopman-UBM Learning : learn a Koopman-based UBM that can predicts both visual feature and action flows given initial conditions (See Section. III-B). 

A. Extracting Visual Features 

In this section, we briefly describe the frameworks for extracting and compressing relevant visual features from RGB images. Without the crucial compression step, UBMs would attempt to learn the dynamics of task-irrelevant pixel-level in-formation and potentially find spurious correlations or struggle to converge. In this work, we investigate two visual representa-tions: i) object flow point features [29], and ii) DynaMo visual features [30]. See Appendix. VIII for detailed implementation of these two visual feature extracting frameworks. 

Object Flow Points : We track 256 points using SAM3 and CoTracker, then compress their coordinates into a 128-dimensional latent space using a convolutional autoencoder to provide a compact, motion-centric representation. 

DynaMo: We utilize a ResNet-18 encoder trained with a self-supervised, dynamics-consistency objective that learns com-pact, dynamics-aware embeddings directly from task-specific RGB observations. 

Goal-conditioned Tasks: For tasks involving explicit goals (e.g., moving an object to a target pose), we concatenate the current visual feature at each step with a fixed goal feature. 

B. Koopman Unified Behavioral Model Learning 

In this section, we describe our approach to learning Koop-man UBMs procedure. Specifically, we learning Koopman UBMs from demonstrations by jointly training i) a unified spectral encoder and ii) an approximated Koopman operator governing the linear evolution in unified latent space. 

Defining the unified Behavioral State : We define the unified behavioral state by concatenating robot action at and learned visual feature ϕt:

ξt ≜

at

ϕt



∈ Rdϕ .

where dϕ = dq + dϕ is the size of the unified behavioral state. To balance the magnitudes of the state components, we rescale the visual features ϕt by a constant factor computed based on the average l2 norms of the visual features and the robot actions over all demonstrations. 

Unified Spectral Latent: To approximate the Koopman-invariant subspace, we first define a unified spectral encoder 

gθ : ξt 7 → ψt that acts as a lifting function, with θ as its learn-able parameters. We parametrize this encoder using a multi-layer perceptron (MLP) that computes the lifted behavioral state ψt ∈ Rψ from the unified behavioral state ξt. Then, we define a state-inclusive latent representation of the lifted behavioral state as 

zt ≜

 ξt

ψt



∈ Rdz .

where dz = dξ + dψ denotes the dimension of the Koopman-UBM’s latent space. By explicitly including ξt in the latent representing, we enable the latent Koopman dynamics to be grounded in the behavioral states, avoiding drifts ane en-couranging temporal coherence. Further, state-inclusive latent representation enable fast decoder-free inference. 

Linear Latent Dynamics: With the latent space established, we can define a learnable Koopman matrix K ∈ Rdz ×dz such that the dynamics in the latent space can be approximated as 

zt+1 = K zt.

Co-training the Encoder and Dynamics: To ensure that the learned latent representation of the unified behavioral state is dynamics-aware and will permit a linear structure, we co-train train the linear dynamics jointly with the unified spectral encoder. We minimize a multi-step latent prediction loss over a prediction horizon H to encourage temporal coherence in the latent space and minimize drift. We compute multi-step latent predictions by repeatedly applying the koopman operator on a given latent state: 

ˆzt+l = Kl zt, l = 1 , . . . , H 

We define our linear coherence loss as the prediction error against the ground-truth latent targets: 

LK-coherence = Et

" HX

> l=1

Kl zt − zt+l

> 22

#

.

After comprehensive experimentation, we identified two key factors that influenced training stability: 

Learning rates and gradient clipping: We found that training stability improves significantly when the Koopman matrix is updated at a slower rate than the unified spectral encoder. We believe this is because the Koopman matrix is applied repeatedly during multi-step prediction, and its gradients are more prone to explosion. As such, we recommend using a smaller learning rate for the Koopman matrix, along with gradient clipping. 

Identity initialization of the Koopman matrix: We found that initializing the Koopman matrix K to the identity matrix is crucial for stability, especially in the early stages of training. Intuitively, an identity initialization corresponds to be static 

latent space and updating the matrix (especially at a slower rate as noted above) allows the latent dynamics to gradually grow in a rapidly-evolving latent space (as the encoder learns at a faster rate). IV. S IMULATION EVALUATION 

A. Experimental Design 

Tasks : We consider seven challenging and representative sim-ulated dexterous manipulation tasks (See Fig. 2 for visual illus-trations.). Below, we briefly describe each task, and additional details, including the task state space, objectives, sampling ranges, and success criteria, can be found in Appendix IX. 

Dexart [36]. In the SAPIEN simulator [37], the robot platform consists of a 6-DoF XArm6 arm and a 16-DoF Allegro hand, and is required to perform three challenging articulated object dexterous manipulation tasks:  

> •

Bucket: Lift a randomly positioned bucket using a stable form-closure grasp [38].  

> •

Laptop: Grasp the center of a randomly placed laptop screen and open the laptop lid.  

> •

Toilet: Grasp and open the large lid of a randomly placed toilet. 

Adroit [39]. In the MuJuCo simulator [40], the robot platform consists of an Adroit hand—a 30-DoF system comprising a 24-DoF hand and a 6-DoF floating wrist base, and is required to perform four representative dexterous manipulation tasks:  

> •

Door opening: Given a randomized door position, undo the latch and drag the door open.  

> •

Tool use: Pick up the hammer to drive the nail into the board placed at a randomized height.  

> •

In-hand reorientation: Reorient the blue pen to a random-ized goal orientation (green pen).  

> •

Object relocation: Move the blue ball to a randomized target location (green sphere). Note that Tool use , In-hand reorientation , and Object reloca-tion are goal-conditioned tasks. Door opening 

Bucket Laptop Toilet 

Tool use In-hand reorientation Relocation 

DexArt Tasks 

Adroit Tasks 

Uncover Pot Open Lid 

Real-world Tasks Fig. 2: We evaluate K-UBM on seven simulations tasks and two real-world tasks. 

Demonstrations : For DexArt tasks, we collect 30 demonstra-tions for training and 30 for testing. For Adroit tasks, we collect 50 demonstrations for training and 30 for testing. For both simulation tasks, we use the pre-trained RL experts. Each demonstration consists of visual observations, robot states, and commanded actions. 

Baselines : We choose the following baselines: 

General methods : We consider two commonly used visuomo-tor learning approaches:  

> •

Diffusion policy [8]: Diffusion policy formulates visuo-motor robot policies as Denoising Diffusion Probabilistic Models, which learns to produce robot action trajecto-ries by iteratively refining random noise into a specific motion, conditioned on observations.  

> •

Action Chunking Transformer (ACT) [11]: ACT is trained to predict the sequence of future actions given the current observations. It handles the variability of human demonstrations by leveraging a Conditional Variational Autoencoder (CVAE). 

Existing Koopman-based approaches : We consider two exist-ing Koopman-based methods that support learning from visual features:  

> •

KODex [41]: Uses predefined polynomial lifting func-tions, allowing closed-form least-squares one-step predic-tion, but does not leverage multi-step prediction loss.  

> •

KOROL [42]: Uses only the multi-step robot action prediction loss to enable learning visual features from images, but does not provide supervision for visual fea-ture prediction. 

B. General Task Performance Evaluation 

In this section, we evaluate policy performance on seven simulated tasks and four real-world tasks. We compare all baselines using two types of visual features in terms of task success rate and inference cost. We undertook several precautions to ensure a fair compari-son. First, we designed the robot state and visual features for all methods to be identical. Second, we carefully designed the baselines policies and tuned their hyper-parameters for each baseline method (Appendices XI and XII). Third, we trained each method policy over five random seeds to control for initialization effects [43], except for KODex , which computes an analytical solution. Note that for real-world tasks, due to the burden of physical evaluation, we train and evaluate each policy on a single random seed. In Fig. 3, we report task success rates on unseen test sets. On average, our K-UBM method achieves the highest mean success rate. We also observe that diffusion- and ACT-based policies require task-specific tuning of the action chunk size. This observation is consistent with the practical challenges of architecture tuning for these models. We also evaluate the inference cost of each method for the bucket task 1. We report the inference cost in policy query time, which measures only the time required for policy inference, i.e., the total time the policy spends generating trajectories for task execution. The timing result is averaged over the total number of time steps, as different policies complete the task at different horizons. From the results in Table. I, we can clearly observe the advantages of the flow-based policies. Since they do not rely heavily on real-time feedback, the burden of online visual processing is substantially reduced, leading to significantly faster policy rollouts. 

> 1The inference cost is similar across tasks, so we report results on a single representative task for brevity. Flow DynaMo

Fig. 3: We report the task success rates for each method on the test sets using both visual features (Flow on the top row and DynaMo on the bottom row). For each method, error bars indicate the standard deviation over five random seeds, and average results (last column) are computed over all seeds and all tasks. ACT (10, 20, 40) denotes three ACT variants with different prediction horizons, while Diffusion (2, 4, 8) and (2, 8, 16) denote different execution and prediction horizons.                   

> Method Inference Time (ms, ↓)Flow DynaMo
> Diffusion 29.95( ±1.37) 36.31( ±2.29) ACT 0.52( ±0.06) 0.29( ±0.02) KOROL 0.41( ±0.07) 0.36( ±0.04) KODex 0.40( ±0.04) 0.37( ±0.04) K-UBM 0.42( ±0.05) 0.47( ±0.14)

TABLE I: Average per-step inference time of each method, excluding visual feature extraction time. Koopman-based policies and ACT are orders of magnitude faster than diffusion policies. 

C. Enabling Reliable Evolution of Robot States and Manipulation-Centric Environment Dynamics 

A major benefit of our method is the joint prediction of both robot states and manipulation-centric environment dynamics. The accuracy of robot state prediction has already been demonstrated in the previous section through task per-formance. In this section, we evaluate the prediction accuracy of manipulation-centric environment dynamics, which can be utilized in multiple ways, such as triggering replanning (see Section IV-D), serving as an inference-time verification signal (see result discussion of this section), or acting as an object-centric tracking reward for reinforcement learning [44]. Since Diffusion, ACT, and KOROL do not support explicit object prediction, and KODex performs significantly worse as shown in Section IV-B, we do not include comparisons with these baseline methods. 

Flow Feature Prediction First, in Fig. 4, we present qualita-tive results for three reconstructed flow point trajectories on the test set for the Bucket, Door, and Relocate tasks. These tra-jectories are obtained by decoding the flow features predicted via Koopman rollouts using the flow decoder learned during autoencoder training. We further visualize the reconstructed flow points by overlaying them on the corresponding images as red dots. From these results, we can see that the predicted flow features effectively capture and reflect the manipulation Time 

> Door Relocate  Bucket

Fig. 4: We visualize the predicted flow points by decoding the flow features and overlaying them on the corresponding images. 

objectives throughout the entire task execution. Furthermore, we evaluate the quantitative prediction quality of the reconstructed flow points. In Fig. 6 (top row), we report the root mean square error (RMSE) between the predicted and ground-truth flow points over time for all seven simula-tion tasks on the test sets, averaged across all test rollouts. Since rollout lengths vary across runs, we visualize the error using normalized trajectory percentiles. We further separate the prediction errors for successful and failed rollouts. As shown in the figure, failed rollouts consistently exhibit larger flow prediction errors than successful ones, revealing a strong correlation between degraded object flow prediction and poor robot execution. 

Dynamo Feature Prediction For DynaMo features, following an evaluation procedure similar to that used for flow features, we first present three qualitative results comparing the pre-Bucket Door Relocate Fig. 5: We visualize the predicted DynaMo features by projecting them together with the ground-truth features using t-SNE. 

dicted and ground-truth DynaMo features in the latent space using t-SNE [45] visualization, as shown in Fig. 5. From these results, we observe that the predicted high-dimensional visual features align well with the ground-truth features. We then evaluate the cosine similarity between the predicted and ground-truth features for all seven simulation tasks on the test sets, averaging the results across all test rollouts and visualizing them using trajectory percentiles, as shown in the bottom row of Fig. 6. Overall, we observe that the cosine similarity generally decreases over time, indicating increasing prediction error, which is expected for long-horizon predic-tions. Notably, for the Tool Use and In-Hand Reorientation 

tasks, the feature similarity varies only slightly, resulting in non-monotonic curves. In addition, although the trend is less pronounced than in the flow prediction curves, we still observe a similar pattern: failed rollouts generally exhibit lower cosine similarity than successful ones. 

D. Reactivity Experiments 

One potential concern is that our Koopman-based policy may lack reactivity, as the robot executes actions in a purely open-loop manner. However, in this section, we illustrate how our policy can achieve system-level reactivity, rather than pure policy-level. To demonstrate this, we conduct the reactivity experiments on the door opening and object relocation tasks, while comparing against diffusion- and ACT-based policies. Specifically, during rollouts on the test sets, we change the handle position for the door opening task by sampling from the set of positions that are far away from the previously observed ones. We apply the same procedure to the relocation task: after grasping, we modify the goal position, thereby requiring the robot to transport the object to a different target location. For our policy, we continuously track the visual features and compute the discrepancy between the predicted ones and those observed from real-time images. When this error increases sharply (i.e., changes in the flow centroid position for the flow-based policy, and cosine similarity variations for the DynaMo-based policy), a replanning trigger is activated, prompting the policy to generate a new trajectory for execution, to which the robot can seamlessly switch. See the top row of Fig. 9 in Appendix. XIII for two examples of the flow-based policy for each task, illustrating that when the environment changes, the resulting increase in prediction error is pronounced and readily                     

> Task Method Success Count Flow DynaMo
> Door Opening K-UBM (Open-loop) 1/30 2/30 ACT 10/30 1/30
> K-UBM 15/30 3/30
> Relocation K-UBM (Open-loop) 3/30 1/30 Diffusion 16/30 22/30
> ACT 10/30 13/30
> K-UBM 27/30 21/30
> TABLE II: Success counts under task perturbations for Door Opening and Relocation. We report the number of successful rollouts out of 30 for flow-based and DynaMo-based policies. We omit the Diffusion for the Door Opening since it performs poorly even without changes.

detectable. In the bottom rows, we design the same strategy to the DynaMo features, where significant changes in the latent feature space can similarly serve as replanning triggers. In Table II, we show the results for both flow policies and DynaMo policies. We first report the success rates of pure full-horizon open-loop tracking, i.e., without reacting to task changes, to highlight the severity of the task perturbations as evidenced by the extremely low open-loop success rates. We also report the success rates of the Diffusion and ACT policies under this setting to demonstrate their ability to react to such changes. Finally, for our method, we first report the accuracy of detecting the replanning trigger at the desired reactivity step. Using the flow-based policy, the trigger is correctly detected in 

30/30 test episodes for both tasks. In contrast, due to the subtle visual changes associated with door position variations, the DynaMo-based trigger fails to reliably detect the replanning event, which in turn leads to a lower task success rate. From the results of task success rates, we observe that even under challenging environmental changes, the ability to replan from the beginning provides our framework with strong reactivity, as evidenced by comparable task success rates or only a slight degradation. In contrast, diffusion-and ACT-based policies exhibit reactive behavior only with DynaMo features, while performing extremely poorly with flow features. Additionally, we present two qualitative examples for each task using the flow-based policy to illustrate the replanning procedure in Fig. 7. These results show that once replanning is triggered, the framework can reliably detect the change and seamlessly switch the robot to a replanned trajectory to successfully complete the updated task. 

E. Additional Experiments 

In Appendix. XIV, we conduct systematic experiments evaluating the robustness of reactive policies to camera noise, highlighting the benefits of reliable prediction under tracking failures. Additionally, we report ablation experiments on the Koopman learning recipe in Appendix. XV. V. R EAL -WORLD EVALUATION 

In this section, we demonstration our framework can also be utilized on the real-world tasks. In the real world, the Flow DynaMo       

> Bucket Laptop Toilet Door opening Tool use In-hand reorientation Relocation

Fig. 6: In the top row, we show the reconstruction error (RMSE) between the predicted and ground-truth flow points. In the bottom row, we report the cosine similarity between the predicted DynaMo features and the ground-truth trajectories. Time 

> Door Relocate
> Replanning triggered

Fig. 7: For both tasks, the left column shows the robot executing the original trajectory, where green points denote the predicted flow points and blue points denote the ground-truth flow points. The middle column illustrates the environment change. Specifically, a new handle position for the door-opening task and a new goal position for the relocation task—which leads to a large discrepancy between the predicted (green) and ground-truth (blue) flow points. The right column shows the robot following the replanned trajectory, with red points indicating the new flow points. Time  

> Segmentation Mask (SAM3) Predicted Flow Trajectories
> Prompt: “a small grey cloth”
> Prompt: “a tool box”
> Reveal Pot Open Lid

Fig. 8: Left: we demonstrate the prompts used for SAM3 to generate object-centric masks. Right: we show the flow predictions produced by the trained K-UBM and the flow decoder, overlaid on the manipulation images. 

robot platform consists of a 7-DoF Kinova arm and the 6-DoF PSYONIC hand, and is required to perform two different manipulation tasks: i) Uncover Pot, and ii) Open Lid. See Fig. 2 for an example. For each task, we collect 15 demonstrations for training       

> Method Uncover Pot Open Lid
> ACT 4/10 10/10 K-UBM 4/10 8/10

TABLE III: Evaluation results of ACT and K-UBM on two real tasks. 

and 10 for testing [46]. Each demonstration consists of visual observations, robot states, and commanded actions. We use ACT-based policies as the baseline. First, in Fig. 8, we illustrate the prompts used for SAM3 to generate object masks on the left. On the right, we show the flow predictions produced by the trained Koopman models and the flow decoder, overlaid on the manipulation images. We then report real-world task success rates for our method, K-UBM, and compare them with the best-performing ACT policy. As shown in Table III, K-UBM achieves performance comparable to ACT. VI. C ONCLUSIONS , L IMITATIONS AND FUTURE WORK 

We introduced Unified Behavioral Models (UBMs), aframework treating dexterous manipulation as the coupled evolution of robot and environment dynamics in a shared latent space. We proposed Koopman-UBM, with several crucial design choices to enable effective learning visuo-motor be-havioral dynamics, enabling both temporal coherence and fast inference. Across seven simulated and two real-world tasks, K-UBM bridges dynamical systems theory with modern visual learning, matching state-of-the-art baselines while offering superior occlusion robustness and event-triggered replanning. 

Limitations : While promising, our current instantiation of UBMs has three primary limitations. First, while K-UBM’s linear dynamics capture coarse-grained behavior, they smooth over sharp contact discontinuities; explicit hybrid modeling (e.g., switching Koopman operators) may be needed for high-frequency impact tasks [47]. Second, our “implicit planning” via open-loop rollout might not be sufficient in highly stochas-tic environments with unpredictable object’s dynamics and will likely require online adaptation or residual corrections [48]. Third, our reactivity relies on visual feedback; simultaneous occlusion and disturbance can delay replanning and cause failure, which may benefit from tactile feedback [49]. 

Future Work: The UBM framework opens several exciting directions for future research. Future work could explore more expressive architectures for UBM (e.g., Neural ODEs [50], Transformers, Diffusion), and incorporate active perception for robust predictive monitoring and reward signals. Additionally, integrating UBMs into hierarchical control (in which founda-tion models, such as VLMs, modulate attractors or switch be-tween primitives) could enable language-driven, long-horizon tasks. Ultimately, UBMs provide a promising alternative to reactive policies, flexibly blending forethought with reflex. REFERENCES 

[1] Allison M Okamura, Niels Smaby, and Mark R Cutkosky. An overview of dexterous manipulation. In Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Pro-ceedings (Cat. No. 00CH37065) , volume 1, pages 255– 262, 2000. [2] Lixin Xu, Zixuan Liu, Zhewei Gui, Jingxiang Guo, Zeyu Jiang, Zhixuan Xu, Chongkai Gao, and Lin Shao. Dexs-ingrasp: Learning a unified policy for dexterous object singulation and grasping in cluttered environments. arXiv preprint arXiv:2504.04516 , 2025. [3] Shan An, Ziyu Meng, Chao Tang, Yuning Zhou, Tengyu Liu, Fangqiang Ding, Shufang Zhang, Yao Mu, Ran Song, Wei Zhang, et al. Dexterous manipulation through imitation learning: A survey. arXiv preprint arXiv:2504.03515 , 2025. [4] Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, and Shuran Song. Dexumi: Using human hand as the universal manipulation in-terface for dexterous manipulation. arXiv preprint arXiv:2505.21864 , 2025. [5] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang. Rotating without seeing: Towards in-hand dexterity through touch. arXiv preprint arXiv:2303.10880 , 2023. [6] Zhao-Heng Yin, Changhao Wang, Luis Pineda, Fran-cois Hogan, Krishna Bodduluri, Akash Sharma, Patrick Lancaster, Ishita Prasad, Mrinal Kalakrishnan, Jitendra Malik, et al. Dexteritygen: Foundation controller for un-precedented dexterity. arXiv preprint arXiv:2502.04307 ,2025. [7] Xiaomeng Xu, Dominik Bauer, and Shuran Song. Robopanoptes: The all-seeing robot with whole-body dexterity. arXiv preprint arXiv:2501.05420 , 2025. [8] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research , 44(10-11):1684–1704, 2025. [9] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-Conditioned Imitation Learning us-ing Score-based Diffusion Policies. In Proceedings of Robotics: Science and Systems , Daegu, Republic of Korea, 7 2023. doi: 10.15607/RSS.2023.XIX.028. [10] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. arXiv preprint arXiv:2403.03954 , 2024. [11] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. Robotics: Science and Systems XIX ,2023. [12] Yuejiang Liu, Jubayer Ibn Hamid, Annie Xie, Yoonho Lee, Maximilian Du, and Chelsea Finn. Bidirectional decoding: Improving action chunking via closed-loop resampling. arXiv preprint arXiv:2408.17355 , 2024. [13] Yangcen Liu, Woo Chul Shin, Yunhai Han, Zhenyang Chen, Harish Ravichandar, and Danfei Xu. Immimic: Cross-domain imitation from human videos via mapping and interpolation. arXiv preprint arXiv:2509.10952 ,2025. [14] Nadun Ranawaka Arachchige, Zhenyang Chen, Wonsuhk Jung, Woo Chul Shin, Rohan Bansal, Pierre Barroso, Yu Hang He, Yingyang Celine Lin, Benjamin Joffe, Shreyas Kousik, et al. Sail: Faster-than-demonstration execution of imitation learning policies. arXiv preprint arXiv:2506.11948 , 2025. [15] Auke Jan Ijspeert, Jun Nakanishi, Heiko Hoffmann, Peter Pastor, and Stefan Schaal. Dynamical movement primitives: learning attractor models for motor behaviors. 

Neural computation , 25(2):328–373, 2013. [16] Matteo Saveriano, Fares J Abu-Dakka, Aljaˇ z Kram-berger, and Luka Peternel. Dynamic movement prim-itives in robotics: A tutorial survey. The International Journal of Robotics Research , 42(13):1133–1184, 2023. [17] Harish Ravichandar, Athanasios S Polydoros, Sonia Chernova, and Aude Billard. Recent advances in robot learning from demonstration. Annual review of control, robotics, and autonomous systems , 3:297–330, 2020. [18] S. Mohammad Khansari-Zadeh and Aude Billard. Learn-ing Stable Nonlinear Dynamical Systems With Gaussian Mixture Models. IEEE Transactions on Robotics , 27 (5):943–957, October 2011. ISSN 1941-0468. doi: 10.1109/TRO.2011.2159412. Conference Name: IEEE Transactions on Robotics. [19] Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, and Deepak Pathak. Neural dynamic policies for end-to-end sensorimotor learning. Advances in Neural Information Processing Systems , 33:5058–5069, 2020. [20] Karl Van Wyk, Ankur Handa, Viktor Makoviychuk, Yijie Guo, Arthur Allshire, and Nathan D Ratliff. Geometric fabrics: a safe guiding medium for policy learning. In 

2024 IEEE International Conference on Robotics and Automation (ICRA) , pages 6537–6543. IEEE, 2024. [21] Mandy Xie, Karl Van Wyk, Ankur Handa, Stephen Tyree, Dieter Fox, Harish Ravichandar, and Nathan Ratliff. Neural Geometric Fabrics: Efficiently Learning High-Dimensional Policies from Demonstration. In Conference on Robot Learning (CoRL) , 2022. [22] Hanyao Guo, Yunhai Han, and Harish Ravichandar. On the surprising effectiveness of spectrum clipping in learning stable linear dynamics. arXiv preprint arXiv:2412.01168 , 2024. [23] Tianyu Li, Sunan Sun, Shubhodeep Shiv Aditya, and Nadia Figueroa. Elastic motion policy: An adaptive dy-namical system for robust and efficient one-shot imitation learning. arXiv preprint arXiv:2503.08029 , 2025. [24] B. O. Koopman. Hamiltonian Systems and Transfor-mation in Hilbert Space. Proceedings of the National Academy of Sciences , 17(5):315–318, 1931. doi: 10. 1073/pnas.17.5.315. [25] Alexandre Mauroy, Y Susuki, and Igor Mezi´ c. Koopman operator in systems and control . Springer, 2020. [26] Ian Abraham, Gerardo De La Torre, and Todd D Mur-phey. Model-based control using koopman operators. 

arXiv preprint arXiv:1709.01568 , 2017. [27] Daniel Bruder, Xun Fu, R Brent Gillespie, C David Remy, and Ram Vasudevan. Koopman-based control of a soft continuum manipulator under variable loading conditions. IEEE robotics and automation letters , 6(4): 6852–6859, 2021. [28] Lu Shi, Masih Haseli, Giorgos Mamakoukas, Daniel Bruder, Ian Abraham, Todd Murphey, Jorge Cort´ es, and Konstantinos Karydis. Koopman operators in robot learning. IEEE Transactions on Robotics , 2026. [29] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European conference on computer vision , pages 18–35. Springer, 2024. [30] Zichen Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Hal-dar, and Lerrel Pinto. Dynamo: In-domain dynamics pretraining for visuo-motor control. Advances in Neural Information Processing Systems , 37:33933–33961, 2024. [31] Milan Korda and Igor Mezi´ c. Linear predictors for nonlinear dynamical systems: Koopman operator meets model predictive control. Automatica , 93:149–160, July 2018. ISSN 0005-1098. doi: 10.1016/j.automatica.2018. 03.046. URL https://www.sciencedirect.com/science/ article/pii/S000510981830133X. [32] Mido Assran, Adrien Bardes, David Fan, Quentin Gar-rido, Russell Howes, Mojtaba, Komeili, Matthew Muck-ley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Fran-cois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning, June 2025. URL http://arxiv.org/abs/2506. 09985. arXiv:2506.09985 [cs]. [33] Quentin Garrido, Tushar Nagarajan, Basile Terver, Nico-las Ballas, Yann LeCun, and Michael Rabbat. Learning Latent Action World Models In The Wild, January 2026. URL http://arxiv.org/abs/2601.05230. arXiv:2601.05230 [cs]. [34] Bo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, To-bias Pfaff, Cheston Tan, Henrik I. Christensen, Hao Su, Jiajun Wu, and Yunzhu Li. A review of learning-based dynamics models for robotic manipulation. Sci-ence Robotics , 10(106):eadt1497, September 2025. doi: 10.1126/scirobotics.adt1497. URL https://www.science. org/doi/10.1126/scirobotics.adt1497. [35] Chenhao Li, Andreas Krause, and Marco Hutter. Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics, April 2025. URL http: //arxiv.org/abs/2501.10100. arXiv:2501.10100 [cs]. [36] Chen Bao, Helin Xu, Yuzhe Qin, and Xiaolong Wang. Dexart: Benchmarking generalizable dexterous manipu-lation with articulated objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 21190–21200, 2023. [37] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,pages 11097–11107, 2020. [38] Antonio Bicchi and Vijay Kumar. Robotic grasping and contact: A review. In Proceedings 2000 ICRA. Millennium conference. IEEE international conference on robotics and automation. Symposia proceedings (Cat. No. 00CH37065) , volume 1, pages 348–353. IEEE, 2000. [39] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipula-tion with deep reinforcement learning and demonstra-tions. arXiv preprint arXiv:1709.10087 , 2017. [40] Todorov, Emanuel and Erez, Tom and Tassa, Yuval. Mujoco: A physics engine for model-based control. In 

2012 IEEE/RSJ International Conference on Intelligent Robots and Systems , pages 5026–5033, 2012. [41] Yunhai Han, Mandy Xie, Ye Zhao, and Harish Ravichan-dar. On the utility of koopman operator theory in learning dexterous manipulation skills. In Conference on Robot Learning , pages 106–126. PMLR, 2023. [42] Hongyi Chen, Abulikemu Abuduweili, Aviral Agrawal, Yunhai Han, Harish Ravichandar, Changliu Liu, and Jef-frey Ichnowski. KOROL: Learning Visualizable Object Feature with Koopman Operator Rollout for Manipu-lation, September 2024. URL http://arxiv.org/abs/2407. 00548. arXiv:2407.00548 [cs]. [43] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforce-ment learning that matters. In Proceedings of the AAAI conference on artificial intelligence , 2018. [44] Yunhai Han, Zhenyang Chen, Kyle A Williams, and Harish Ravichandar. Learning prehensile dexterity by imitating and emulating state-only observations. IEEE Robotics and Automation Letters , 2024. [45] Laurens van der Maaten and Geoffrey Hinton. Visu-alizing data using t-sne. Journal of machine learning research , 9(Nov):2579–2605, 2008. [46] Chuizheng Kong, Yunho Cho, Wonsuhk Jung, Idris Wibowo, Parth Shinde, Sundhar Vinodh-Sangeetha, Long Kiu Chung, Zhenyang Chen, Andrew Mattei, Ad-vaith Nidumukkala, Alexander Elias, Danfei Xu, Taylor Higgins, and Shreyas Kousik. A closed-form geometric retargeting solver for upper body humanoid robot tele-operation. arXiv preprint arXiv:2602.01632 , 2026. [47] Craig Bakker, Arnab Bhattacharya, Samrat Chatterjee, Casey J Perkins, and Matthew R Oster. Learning koop-man representations for hybrid systems. arXiv preprint arXiv:2006.12427 , 2020. [48] Minghao Han, Jacob Euler-Rolle, and Robert KKatzschmann. Desko: Stability-assured robust control with a deep stochastic koopman operator. In Interna-tional conference on learning representations (ICLR) ,2022. [49] Kelin Yu, Yunhai Han, Qixian Wang, Vaibhav Saxena, Danfei Xu, and Ye Zhao. Mimictouch: Leveraging multi-modal human tactile demonstrations for contact-rich manipulation. In 8th Annual Conference on Robot Learning , 2024. URL https://openreview.net/forum?id= 7yMZAUkXa4. [50] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems , 31, 2018. [51] Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoub-hik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, et al. Sam 3: Segment anything with concepts. 

arXiv preprint arXiv:2511.16719 , 2025. [52] Lu Shi and Konstantinos Karydis. Acd-edmd: Analyt-ical construction for dictionaries of lifting functions in koopman operator-based nonlinear robotic systems. IEEE Robotics and Automation Letters , 7(2):906–913, 2021. [53] Igor Mezic. Koopman operator, geometry, and learning. 

arXiv preprint arXiv:2010.05377 , 2020. APPENDICES 

VII. D ATASET MODIFICATION 

In typical imitation learning problems, the first action a(i)1

varies across demonstrations and is therefore unavailable at in-ference time. To address this issue, UBMs utilize an auxiliary initial action a(i)0 = q (i)1 , where q(i)1 denotes the initial robot joint configuration. In practice, it is reasonable to initialize the robot to the same configuration across demonstrations, so 

q(i)1 (and thus a(i)0 ) is identical for all i. To ensure consistent trajectory lengths during learning, we also duplicate the initial image by setting I(i)0 = I (i)1 . This process results in a new dataset ˜D =

h

{I(1)  

> t

, a(1)  

> t

}T (1)  

> t=0

, . . . , {I(N ) 

> t

, a(N ) 

> t

}T (N )

> t=0

i

. Our goal is to learn a UBM from the dataset ˜D that will enable the robot to autonomously perform the associated dexterous skill. VIII. V ISUAL FEATURE LEARNING 

In this section, we describe key design choices to extract the relevant visual features from images and compress . Without this crucial compression step, UBMs would attempt to learn the dynamics of task-irrelevant pixel-level information and po-tentially find spurious correlations or struggle to converge. In this work, we investigate two visual representations: i) object flow point features [29], and ii) DynaMo visual features [30]. 

Object Flow Points : Before extracting and tracking flow points, we first leverage SAM3 [51] with a language prompt describing the objects of interest to generate an object-centric segmentation mask. Based on this mask, we sample initial object flow points and track them over time using CoTracker [29], yielding N sequences of object flow points 

{p(i) 

> t

}T (i) 

> t=0

for i = 1 , . . . , N . Each p(i) 

> t

∈ R256 ×2 represents the image coordinates of 256 tracked pixel points at time t.To obtain a compact representation suitable for Koopman-based dynamics learning, we train a convolutional autoencoder on the extracted object flow trajectories. 

Representation: At each time step t of the i-th demonstration, we represent the object flow as a set of 256 2D points 

p(i) 

> t

= {(u(i) 

> t,j

, v (i) 

> t,j

)}256  

> j=1

in image coordinates. We reshape these points into a spatial grid of size 16 × 16 and treat it as a two-channel image: x(i) 

> t

∈ R2×16 ×16 , where the two channels correspond to the horizontal and vertical coordinates, respectively. 

Encoder: The encoder E consists of a stack of strided convo-lutional layers with stride 2, which progressively downsample the spatial resolution while increasing the channel capacity. Starting from an input resolution of 16 × 16 , the encoder applies a single strided convolution to downsample the spatial resolution to 8 × 8 while expanding the channel dimension from 2 to 8, followed by a 1 × 1 convolution that projects the features back to a two-channel latent flow representation: 

z(i) 

> t

= E(x (i) 

> t

) ∈ R2×8×8, resulting in a 128-dimensional flow feature obtained by flattening z(i) 

> t

.

Decoder: The decoder F mirrors the encoder structure using transposed convolutions to upsample the latent feature map back to the original spatial resolution, followed by a ReLU activation: ˆx(i) 

> t

= F(z (i) 

> t

) ∈ R2×16 ×16 .

Training Objective: We train the flow autoencoder by min-imizing a reconstruction loss between the reconstructed and ground-truth flow points: 

Lflow = Ei,t 



ˆx(i) 

> t

− x(i)

> t
> 22



.

We aggregate all flow frames x(i) 

> t

across demonstrations and treat each time step independently during training. The model is optimized using Adam with an exponential learning-rate decay schedule. 

DynaMo: As an alternative to flow-based representations, we adopt DynaMo visual features [30], which provide compact and dynamics-aware representations directly from raw RGB observations. DynaMo is a self-supervised, in-domain visual representation learning method that exploits the temporal structure of expert demonstrations by modeling latent visual dynamics, without requiring action annotations, data augmen-tations, or out-of-domain pretraining data. 

Encoder: Given a set of expert demonstrations consisting of image sequences {o1, . . . , o T }, DynaMo jointly trains a visual encoder and latent dynamics models directly on task-specific observations. Each RGB image ot ∈ R3×224 ×224 is mapped to a latent embedding 

st = fθ (ot) ∈ Rd, (1) where fθ is a ResNet-18–based encoder. The encoder is pretrained entirely in-domain using only demonstration images from the target task. 

Latent Dynamics Modeling: To capture the underlying visual dynamics, DynaMo introduces two auxiliary models oper-ating in latent space: (i) a latent inverse dynamics model 

q(zt:t+h−1 | st:t+h), and (ii) a forward dynamics model 

p(ˆ st+1: t+h | st:t+h−1, z t:t+h−1). Here, zt ∈ Rm represents a low-dimensional latent transition variable between consecutive embeddings, with m ≪ d, preventing the latent from trivially encoding the next state. Both the inverse and forward dynamics models are implemented as causally masked Transformer encoders. 

Training Objective: The encoder and dynamics models are trained end-to-end using a latent-space dynamics consistency objective. Specifically, the forward model predicts the next-step embeddings ˆst+1: t+h, which are compared against the target embeddings s∗ 

> t+1: t+h

using a cosine similarity loss: 

Ldyn (ˆ st, s ∗ 

> t

) = 1 − ⟨ˆst, s ∗ 

> t

⟩∥ˆst∥2 · ∥ s∗ 

> t

∥2

. (2) To avoid representational collapse, the target embedding is stop-gradient detached, i.e., s∗ 

> t

= sg (st). In addition, acovariance regularization loss is applied over a minibatch of embeddings S to encourage feature decorrelation: 

Lcov (S) = 1

d

X

> i̸=j

Cov( S)2 

> i,j

. (3) The final training objective is given by 

L = Ldyn + λLcov , λ = 0 .04 . (4) IX. T ASK DESCRIPTION 

In this section, we describe the task state space for both simulated and real-world tasks, along with the task objectives, sampling ranges, and success criteria for each. 

A. Dexart Benchmarks 

We follow the original state space design used in prior RL benchmark work for these three articulated object dexterous manipulation tasks [36] in Sapien simulator [37], where the robot proprioception space is the same across tasks and is defined by the 6-DoF XArm6 arm joint positions and velocities together with the 16-DoF Allegro hand finger joints (28 dimen-sions in total). We use the provided RL expert checkpoints to generate rollouts and collect successful trajectories for imitation learning, consisting of visual observations, observed robot states, and target robot commands. We also adopt the original sampling strategies during data collection and the success criteria for imitation learning policy evaluation, which are detailed below. 

Bucket This task requires the robot to lift a bucket. To ensure stable lifting, the robot must extend its hand beneath the bucket handle and grasp it to achieve form closure. This task introduces randomness in the bucket assets, including variations in size and geometry, as well as random initial object positions by adding independent uniform noise of up to 5 cm along each axis relative to the annotated positions. The task is considered successful if the bucket is lifted by at least 30 cm within a specified time duration. 

Laptop This task requires the robot to grasp the middle of the screen and then open the laptop lid. This task introduces randomness in the laptop assets, including variations in size and geometry, as well as random initial object poses by adding independent uniform noise of up to 10 cm along each axis relative to the annotated positions and applying a uniformly sampled angular perturbation within ±60 degrees around the annotated orientation. The task is considered successful if the laptop is opened to at least 95% of its full opening angle within a specified time duration. 

Toilet This task requires the robot to open a toilet lid. This task introduces randomness in the toilet assets, including variations in size and geometry and diverse lid shapes, as well as random initial object poses by adding independent uniform noise of up to 20 cm along each axis relative to the annotated positions and applying a uniformly sampled angular perturbation within 

±45 degrees around the annotated orientation. The task is considered successful if the toilet lid is opened to at least 95% of its full opening angle within a specified time duration. 

B. Adroit Benchmarks 

We follow the original state space design used in prior RL benchmark work for these four representative dexterous manipulation tasks [39] in MuJoCO simulator [40], where the Adroit robot hand’s proprioception space varies across tasks and is described separately below. Also, since the original benchmark uses joint torques as the action space, which is less commonly adopted in modern imitation learning approaches, we instead adopt another work that uses target joint positions as the action space, which achieve comparable task performance across all tasks [44]. We use the provided expert checkpoints to generate rollouts and collect successful trajectories, consisting of visual observations, observed robot joints, and target robot joints. We also adopt the original sampling strategies during data collection and the success criteria for imitation learning policy evaluation, which are detailed below. 

Door opening This task requires the robot to undo the latch and drag the door open. The floating robot hand base can translate along the direction perpendicular to the door plane and rotate freely, and all 24 hand joints are fully actuated, resulting in a 28-dimensional robot proprioception space. This task introduces randomness in the initial door positions by uniformly sampling the Cartesian coordinates as x ∈ X ∼ U(−0.3, 0) , y ∈ Y ∼ U(0 .2, 0.35) , and 

z ∈ Z ∼ U (0 .252 , 0.402) (unit: m). The task is considered successful if the door opening angle is larger than 1.35 rad within a specified time duration. 

Tool use This task requires the robot to pick up the hammer to drive the nail into the board. The floating robot hand base can rotate along the x and y axis, and all 24 hand joints are fully actuated, resulting in a 26-dimensional robot proprioception space. This task introduces randomness in the target nail heights by uniformly sampling the nail height as 

h ∈ H ∼ U(0 .1, 0.25) (unit: m). The task is considered successful if the Euclidean distance between the nail position and the goal nail position is smaller than 0.01 within aspecified time duration. 

In-hand reorientation This task requires the robot to reorient a pen to a goal orientation. The floating robot hand base is fixed, and only the 24 hand joints are actuated, result-ing in a 24-dimensional robot proprioception space. This task introduces randomness in the goal pen orientation by uniformly sampling the pitch ( α) and yaw ( β) angles as 

α ∈ A ∼ U (−1, 1) and β ∈ B ∼ U (−1, 1) (unit: rad). At each time step t, if ogoal · open  

> t

> 0.90 (ogoal · open  

> t

measures orientation similarity), then we have ρ(t) = 1 . The task is considered successful if PTt=1 ρ(t) > 10 (T = 100 ). 

Object relocation This task requires the robot to move a ball to a target position. The floating robot hand base can move freely, and all 24 hand joints are fully actuated, resulting in a 30-dimensional robot proprioception space. This task intro-duces randomness in the target positions by uniformly sam-pling the Cartesian coordinates as x ∈ X ∼ U (−0.15 , 0.15) ,

y ∈ Y ∼ U(−0.15 , 0.15) , and z ∈ Z ∼ U(0 .15 , 0.35) 

(unit: m). At each time step t, if p|ptarget − pball  

> t

|2 < 0.10 ,then we have ρ(t) = 1 . The task is considered successful if PTt=1 ρ(t) > 10 (T = 100 ). C. Real-world Tasks 

We also propose our own real-world dexterous manipulation tasks, including Uncover pot , and Open lid . The robot propri-oception space is the same across both tasks and is defined by the 7-DoF Kinova arm joint positions together with the 6-DoF PSYONIC hand finger joints (13 dimensions in total). For both tasks, we use a state-of-the-art vision-based teleop-eration system [46] to collect demonstrations. We further post-process the data to align the initial robot joint configurations across trajectories. The tasks are then performed under varying object configurations by reloading the object at different initial poses. The success criteria are defined as: i) Uncover pot : the robot grasps the cloth and moves it away to uncover the pot; and ii) Open lid : the robot grasps the lid and opens it. X. H YPER -PARAMETER FOR KOOPMAN LEARNING 

As shown in Table. IV, we use the same training hyper-parameters for our method across all seven simulation tasks and four real-world tasks, with the only modification being an increased prediction horizon of 50 steps for real-world tasks, which typically involve trajectories longer than 150 steps. XI. B ASELINE POLICY DESIGN 

We show the detailed baseline policy design in this section. 

Diffusion : Following the design choices of [8], we implement a CNN-based diffusion policy that represents the visuomotor mapping as a conditional denoising diffusion process. A 1D temporal Convolutional Neural Network with a U-Net-like structure serves as the noise-prediction network, which itera-tively refines a sequence of actions from Gaussian noise during inference. To incorporate visual context, the model integrates pre-extracted flow or DynaMo features as global conditioning via Feature-wise Linear Modulation (FiLM) layers. Finally, the policy employs a receding horizon control strategy to ensure temporal action consistency and provide robustness against execution latency. We implement the diffusion policy using the AdamW opti-mizer with a learning rate of 10 −4, weight decay of 10 −6,and betas of [0 .95 , 0.999] . The model is trained for 1000 epochs with a batch size of 256, incorporating a linear learning rate warmup of 500 steps followed by a cosine decay. The architecture utilizes an observation horizon ( To) of 2. For the action prediction horizon ( Tp) and the action execution horizon ( Ta), we evaluate two configurations— (Ta, T p) ∈{(4 , 8) , (8 , 16) }—to determine the optimal settings for each task (see Table VI). For the diffusion process, we employ a DDPMScheduler with 100 training timesteps and a squared cosine noise schedule. The noise-prediction network features a 1D temporal U-Net with a kernel size of 5 and down-sampling dimensions of 256, 512, and 1024. 

ACT : We follow the policy architecture design as [11]. During training, we first sample tuples of flow (or DynaMo) features and joint angles, together with the corresponding action se-quence as prediction target. We then infer style variable z,modeled as a diagonal Gaussian, using CVAE encoder. This variable is then fed into a decoder alongside visual features and current joint states. The decoder, structured as a Transformer with cross-attention, learns to reconstruct the action sequence from these inputs. At test time, the CVAE encoder is discarded, and the CVAE decoder is used as the policy. The latent variable 

z is fixed to zero—the mean of the prior distribution. This allows the decoder to act as a deterministic policy that maps multi-modal observations directly to robot actions. We train the ACT model with a learning rate of 10 −5

and a batch size of 8. The architecture comprises a 4-layer Transformer encoder and a 7-layer decoder, featuring 8 attention heads, a hidden dimension of 512, and a feedforward dimension of 3200. We apply a dropout rate of 0.1 and set the KL divergence weight to 10. To optimize performance, we evaluated three chunk sizes—10, 20, and 40—and selected the optimal value for each task based on the results in Table VII. 

KODex : In the literature, Koopman-based methods commonly employ polynomial and sinusoidal (cosine/sine) lifting func-tions [28, 41, 52]. As a result, the mapping between the original state and the lifted state is predefined, allowing the one-step prediction objective to be solved directly via least squares. However, this formulation typically does not support optimization with multi-step prediction losses. To optimize the predefined lifting functions, we design three variants of polynomial bases and report the corresponding dimension-alities in Table. V. Since the flow features and DynaMo features share the same input dimensionality, the resulting lifted dimensionalities are identical for both. The optimized Koopman matrix is a square matrix whose dimensionality matches the lifted state dimension for each task. Let xh ∈ Rnh denote the original robot state and xo ∈ Rno

denote the original visual states. All lifting variants include the original states and their element-wise second-order terms for both the hand and visual states, i.e., 

Φbase (xh, xo) = xh, x⊙2 

> h

, xo, x⊙2

> o

,

where ⊙ denotes element-wise exponentiation. 

V1 : We further augments the base lifting with third-order polynomial terms of the visual states, pairwise interaction terms between robot states, and third-order polynomial terms of the robot states: 

Φv1 = Φ base ∪ x⊙3 

> o

, {xh,i xh,j }i<j , x⊙3

> h

.

V2 : We remove higher-order polynomial terms of the visual states and retains only nonlinear expansions of the robot states: 

Φv2 = Φ base ∪ {xh,i xh,j }i<j , x⊙3

> h

.

V3 : We extend v2 by additionally incorporating trigonometric features for both robot and visual states: 

Φv3 = Φ v2 ∪  cos( xh), sin( xh), cos( xo), sin( xo).

KOROL : In the vanilla implementation of KOROL, a pre-trained ResNet-18 visual encoder is fine-tuned every epoch, while the Koopman operator is updated at fixed intervals. Robot states and 8-dimensional visual object features extracted TABLE IV: Hyper-parameters on seven simulator tasks             

> Hidden Layer Lifting Dimension Activation Encoder lr Koopman lr Multi-step Length Identity Koopman Matrix Initialization (128, 256) 256 ReLU 5e-4 5e-5 15 True

TABLE V: Lifting dimensionalities for three lifting variants across seven simulation tasks                      

> Variant Task Group Bucket / Laptop / Toilet / Door Tool Reorientation Relocation
> nh=28 , n o=128 nh= 26 , n o=256 nh= 24 , n o=256 nh= 30 , n o=256
> V1 846 1171 1116 1293 V2 718 915 860 1037 V3 1030 1479 1420 1609 The Tool, Reorientation, and Relocation tasks use goal-conditioned policies, so their visual state dimensionality is 256.

from images are first lifted using hand-crafted polynomial basis functions, and these lifted representations are used to alternately update the feature extractor and re-estimate the Koopman matrix. However, for our implementation of KOROL, we replace the ResNet-18 model with Flow and DynaMo feature extractors and keeps the visual backbone frozen. Instead, we introduce a learnable MLP projection head to process the visual features, which is updated every epoch to adapt features for Koopman learning. We retain an output feature dimension of 8, use a learning rate of 1e-4 for Flow features, and tune the learning rate to 0.025 for DynaMo features for stable convergence. The pre-diction horizon is fixed to 15, consistent with other baselines. We design three MLP variants with varying hidden dimensions for model optimization as shown in Table IX. XII. O PTIMIZING BASELINE POLICY ARCHITECTURE 

For the baseline models, we make substantial efforts to op-timize their architectures to ensure a fair comparison. Specif-ically, for diffusion-based models, we tune the combination of execution horizon and prediction horizon, which is critical to their performance. For ACT models, we tune the prediction horizon. For KODex models, we select appropriate polynomial lifting functions, and for KOROL models, we tune the MLP used to ingest visual features. We generated five random seeds for parameter initialization per architecture choice, per baseline, and per task, except for the KODex method. For each baseline policy, we report the mean and standard deviation of the task success rate on the test set over five random seeds. We then select the best-performing configuration based on average performance for comparison with our method in Section IV-B. For diffusion-based models, Tables VI report results using flow features and DynaMo features, respectively. Similarly, for ACT models, refer to Tables VII; for KODex models, refer to Tables VIII; and for KOROL models, refer to Tables IX. XIII. R EPLAN TRIGGER DETECTION RESULTS 

As described in Section IV-D, we can observe the sharp discrepancies between the predicted visual states and the actual visual observations when the environment changes. The results in Fig. 9 illustrate such sharp changes that K-UBM can precisely detect as replanning triggers, enabled by its accurate object motion prediction. Door Relocate 

> Flow
> Relocate
> DynaMo

Fig. 9: For both visual features, we demonstrate how replanning triggers can be detected. Left: For the flow-based K-UBM policy, we show a sharp change in flow centroid distance. Right: Similarly, for DynaMo features, we visualize the corresponding changes in cosine similarity. Due to the subtle visual changes for the Door Opening task, the DynaMo-based policy fails to accurately detect the replanning event. 

XIV. R OBUSTNESS EXPERIMENTS 

In this section, we conduct experiments to compare reactive policies with K-UBM policies. The key distinction between the two paradigms lies in the trade-off between robustness and 

reactivity . Reactive policies rely on real-time sensory feedback at a fixed control frequency, which increases the likelihood of observing inaccurate sensory inputs, while enabling adapta-tion to environmental changes. In contrast, K-UBM policies support open-loop execution and are therefore highly robust to online sensor noise, but typically lack reactivity. However, as described in the Section. IV-D, the ability to predict en-vironment dynamics combined with a replan-from-beginning strategy endows K-UBM with framework-level reactivity. In this section, we focus on robustness. Specifically, we evaluate the robustness of diffusion- and ACT-based policies across all simulation tasks under increasing levels of visual corruption, modeled as blacked-out images to mimic potential camera failures in real-world manipulation scenarios. For each task, visual feature, and model, we select the best-performing architecture and seed. We omit analogous experiments for our policy, as it support open-loop rollout that does not rely on online feedback during execution. As shown in Fig. 10, the performance of both policies drops significantly across all tasks under corrupted visual inputs. This strongly supports a key advantage of the UBM frame-works: robustness to inaccurate online feedback compared to reactive policies. XV. R ECIPES FOR THE KOOPMAN MODEL LEARNING 

In this section, we systematically examine the key de-sign choices that enable effective Koopman model learning. TABLE VI: Policy Architecture Optimization for Diffusion Models                                                                                                                          

> Architecture Success Rate (%, ↑)Bucket Laptop Toilet Door Hammer Pen Relocation Average
> (To, T a, T p)Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo (2, 4, 8) 90.7 ±3.9 75.3 ±5.0 78.0 ±4.5 78.7 ±6.5 69.0 ±3.1 68.0 ±8.8 0.0 ±0.0 0.0 ±0.0 88.3 ±2.9 65.3 ±8.6 80.7 ±2.5 32.7 ±3.8 89.2 ±6.0 50.0 ±2.1 70.8 ±30.5 52.7 ±25.5
> (2, 8, 16) 92.7 ±2.5 76.0 ±3.3 60.0 ±19.1 82.7 ±4.9 67.6 ±4.6 76.7 ±4.7 0.0 ±0.0 0.0 ±0.0 92.7 ±1.3 78.7 ±6.2 81.3 ±4.5 3.3 ±2.9 86.7 ±2.9 55.3 ±3.4 68.9 ±30.6 53.2 ±25.2

TABLE VII: Policy Architecture Optimization for ACT Models                                                                                                                                                                          

> Model Architecture Success Rate (%, ↑)Bucket Laptop Toilet Door Tool Reorientation Relocation Average (chunk size) Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo 10 80.7 ±5.7 72.7 ±8.5 90.0 ±3.7 78.7 ±2.7 74.5 ±6.4 82.7 ±3.9 0.0 ±0.0 0.0 ±0.0 52.0 ±18.3 88.7 ±8.4 80.0 ±7.7 28.0 ±8.7 82.0 ±9.8 72.0 ±3.8 65.6 ±30.3 60.4 ±27.0 20 77.3 ±5.5 83.3 ±7.0 86.7 ±4.1 92.0 ±3.4 69.0 ±8.1 78.7 ±3.4 23.2 ±9.3 41.3 ±5.1 79.2 ±11.6 96.7 ±4.1 85.0 ±5.8 25.3 ±4.5 89.4 ±7.8 72.0 ±3.0 73.4 ±25.7 69.9 ±24.7
> 40 74.7 ±6.9 74.7 ±2.7 83.3 ±5.8 76.0 ±6.5 68.3 ±4.5 82.0 ±5.4 44.2 ±8.0 44.7 ±13.9 67.2 ±13.9 92.0 ±1.8 78.8 ±5.2 26.0 ±8.0 84.6 ±3.9 81.3 ±1.8 71.6 ±18.7 68.1 ±22.7

TABLE VIII: Policy Architecture Optimization for KODex Models 

Model Architecture Success Rate (%, ↑)Bucket Laptop Toilet Door Tool Reorientation Relocation Average Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo V1 10.0 80.0 6.7 33.3 3.4 16.7 3.3 50.0 36.7 20.0 73.3 70.0 26.7 23.3 22.9 ±23.7 41.9 ±23.4 V2 13.3 80.0 10.0 33.3 10.3 6.7 10.0 53.3 30.0 33.3 76.7 73.3 23.3 46.7 24.8 ±22.4 46.7 ±23.3 

V3 20.0 86.7 23.3 13.3 3.4 3.3 13.3 53.3 30.0 50.0 63.3 76.7 33.3 20.0 26.7 ±17.6 43.3 ±29.7 

TABLE IX: Policy Architecture Optimization for KOROL Models                                                                                                                                 

> Model Architecture Success Rate (%, ↑)Bucket Laptop Toilet Door Tool Reorientation Relocation Average (hidden dim) Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo Flow DynaMo
> [64] 50.7 ±16.2 56.7 ±27.6 20.7 ±3.9 72.0 ±3.4 33.8 ±8.6 42.8 ±10.6 10.7 ±6.8 11.33 ±2.67 32.0 ±17.2 13.3 ±12.11 32.0 ±20.1 52.0 ±9.1 8.0 ±9.1 15.33 ±4.52 26.8 ±13.8 37.6 ±22.5
> [2 ·input dim ,128] 46.7 ±7.6 81.3 ±4.5 19.3 ±11.0 71.3 ±1.6 27.6 ±3.1 43.4 ±6.8 9.3 ±3.9 9.33 ±2.49 32.7 ±17.2 4.7 ±6.2 36.7 ±11.2 46.0 ±6.11 8.0 ±2.7 15.33 ±3.4 25.8 ±13.3 44.9 ±24.4
> [512 ,256 ,128] 35.3 ±20.9 78.7 ±4.5 16.9 ±5.7 71.3 ±6.2 16.6 ±13.7 53.1 ±8.3 14.0 ±4.4 11.33 ±1.63 13.0 ±2.7 5.33 ±9.09 34.0 ±17.6 37.33 ±5.33 6.7 ±2.1 17.33 ±3.89 19.5 ±10.1 46.1 ±23.6

Through extensive experiments, we identify two critical fac-tors: i) identity matrix initialization of the Koopman matrix, and ii) gradient clipping and the use of smaller learning rates for the Koopman matrix. Intuitively, temporally neighboring states should be similar. Based on this, we initialize the Koopman matrix as the identity matrix. Through experiments, we find that this choice is crucial, as multi-step predictions from a randomly initialized Koopman matrix can produce large prediction errors, leading to unstable gradient descent. Additionally, we find that enabling gradient clipping and using separate learning rates for the encoder and the Koopman matrix are beneficial, as multi-step matrix rollouts accumulate gradient signals during prediction, which can otherwise lead to unstable updates. We use the flow-based K-UBM policy as an example and present an ablation study across seven simulation tasks and two real-world tasks by training models using the same best-performing random seed. In Fig. 11, Identity init + separate LR denotes the training configuration used throughout the main paper, which incorporates both key factors. Separate LR only 

corresponds to using separate learning rates for the lifting network and Koopman matrix without identity initialization. 

Identity init only initializes the Koopman matrix as the identity but uses a shared learning rate, while Neither disables both. From the results, we first observe that without separate learning rates, training becomes highly unstable across all nine tasks, as evidenced by frequent spikes in both the training loss and the spectral radius of the Koopman matrix. Next, when only separate learning rates are used without identity initialization, the training remains stable but is sig-nificantly less efficient (note that the training loss is plotted on a logarithmic scale), and we observe a sharp increase of the spectral radius from its initial value early in training. These results indicate that stable and accurate long-horizon prediction requires the Koopman matrix to maintain sufficient expressivity while avoiding excessive energy dissipation dur-ing rollout. In practice, this corresponds to keeping the spectral radius close to one [53], which is effectively achieved by initializing the Koopman matrix as the identity. Fig. 10: For both visual features, we observe that performance of reactive policies often drops significantly under corrupted visual inputs, with earlier corruption leading to more severe degradation. The only exception is the Door task, where performance improves under noise. However, we find this is due to the policy exploiting unrealistic behaviors in simulation. Training Loss Matrix Spectral Radius 

Buckct Laptop Toilet Door opening 

Tool use Reorientation Relocation Uncover pot Open lid 

> Training Loss Matrix Spectral Radius

Fig. 11: We conduct ablation studies to evaluate two key factors that enable efficient Koopman learning. Identity init + separate LR denotes the training configuration used throughout the main paper, which incorporates both key factors. Separate LR only corresponds to using separate learning rates for the lifting network and Koopman matrix without identity initialization. Identity init only initializes the Koopman matrix as the identity but uses a shared learning rate, while Neither disables both. From the results, it can observed that without separate learning rates, training becomes highly unstable, and without identity initialization, training is less efficient.
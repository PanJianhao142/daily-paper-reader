---
title: "Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity"
title_zh: 顺流而行：作为视觉运动灵巧性隐式规划器的 Koopman 行为模型
authors: "Yunhai Han, Linhao Bai, Ziyu Xiao, Zhaodong Yang, Yogita Choudhary, Krishna Jha, Chuizheng Kong, Shreyas Kousik, Harish Ravichandar"
date: 2026-02-07
pdf: "https://arxiv.org/pdf/2602.07413v1"
tags: ["keyword:MDM", "query:课题"]
score: 6.0
evidence: 基于扩散的行为模型，用于运动的时间连贯性
tldr: 针对多指灵巧操作中现有模型对数据依赖大且难以平衡动作平滑性与反应性的问题，本文提出统一行为模型（UBM）。该模型利用Koopman算子理论将视觉流与动作流建模为耦合的线性动力系统，构建了一个能够同时预测机器人行为和视觉演化的隐式规划器。实验表明，该方法在保持高性能的同时，显著提升了推理速度、执行平滑度及对环境变化的适应能力。
motivation: 现有的反应式策略依赖固定时界的动作分块来缓解抖动，导致在复杂灵巧操作中难以兼顾时序连贯性与实时反应性。
method: 提出Koopman-UBM框架，通过Koopman算子将潜在视觉和本体特征的联合流建模为线性动力系统，并引入基于视觉流偏差的在线重规划机制。
result: 在九项仿真与真实任务中，Koopman-UBM在性能上比肩或超越了SOTA基准，同时展现出更快的推理速度和更强的遮挡鲁棒性。
conclusion: 将机器人技能建模为耦合动力系统，为实现高效、平滑且具备自监控能力的视觉运动灵巧操作提供了有效的路径。
---

## 摘要
由于采用了基于扩散（diffusion）和 Transformer 骨干网络的高表达力策略类，机器人从演示中学习复杂视觉运动操纵技能的能力取得了快速而显著的进展。然而，这些设计选择需要大量的数据和计算资源，且仍远未达到可靠的水平，特别是在多指灵巧操纵的背景下。从根本上说，它们将技能建模为反应式映射，并依赖固定时界的动作分块（action chunking）来减轻抖动，从而在时间连贯性与反应性之间造成了僵化的权衡。在这项工作中，我们引入了统一行为模型（Unified Behavioral Models, UBMs），这是一个将灵巧技能表示为耦合动力系统的框架，该系统捕捉了环境视觉特征（视觉流）与机器人本体感受状态（动作流）如何共同演化。通过捕捉这种行为动力学，UBM 能够通过结构设计而非启发式平均来确保时间连贯性。为了使这些模型可操作化，我们提出了 Koopman-UBM，这是 UBM 的第一个实例化，它利用 Koopman 算子理论有效地学习一种统一表示，其中潜在视觉和本体感受特征的联合流由结构化线性系统支配。我们证明了 Koopman-UBM 可以被视为一个隐式规划器：给定初始条件，它能解析地计算出所需的机器人行为，同时“想象”出在整个技能时界内产生的视觉特征流。为了实现反应性和适应性，我们引入了一种在线重规划策略，其中模型充当其自身的运行时监控器，当预测的视觉流与观察到的视觉流之间的偏差超过阈值时，会自动触发重规划。在七个模拟任务和两个现实世界任务中，我们证明了 K-UBM 达到或超过了最先进基准模型的性能，同时提供了显著更快的推理速度、平滑的执行、对遮挡的鲁棒性以及灵活的重规划。

## Abstract
There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.
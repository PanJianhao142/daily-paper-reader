Title: IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation

URL Source: https://arxiv.org/pdf/2602.07498v1

Published Time: Tue, 10 Feb 2026 01:42:52 GMT

Number of Pages: 19

Markdown Content:
# IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation 

## Zhufeng Xu 1,2 , Xuan Gao 1,2 , Feng-Lin Liu 1,2 , Haoxian Zhang 3, Zhixue Fang 3,Yu-Kun Lai 4, Xiaoqiang Liu 3, Pengfei Wan 3, Lin Gao 1,2* 1Institute of Computing Technology, Chinese Academy of Sciences 

> 2

## University of Chinese Academy of Sciences 

> 3

## Kling Team, Kuaishou Technology 4Cardiff University 

Figure 1. IM-Animation introduces an impressive implicit motion representation and retargeting method. Our model supports implicit video model motion control in cases with significant scale differences or substantial variations in posture and body shape. 

## Abstract 

Recent progress in video diffusion models has markedly ad-vanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement be-tween motion and appearance. To address the above chal-lenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inher-ent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retar-geting module that enforces a temporal training bottleneck, mitigating interference from the source image’s motion and improving retargeting consistency. Our methodology em-ploys a three-stage training strategy to enhance the train-ing efficiency and ensure high fidelity. Extensive experi-ments demonstrate that our implicit motion representation and the proposed IM-Animation achieve superior or com-petitive performance compared with state-of-the-art meth-ods. 

## 1. Introduction 

Character image animation is a popular research topic with a wide range of applications in gaming, film production, and virtual reality. This task aims to precisely transfer motion and expressions from a driving video to a new identity pro-

> arXiv:2602.07498v1 [cs.CV] 7 Feb 2026

vided by a source image. Early works [44, 54] built upon GAN-based frameworks [16], but suffered from low resolu-tion, poor generalization, and unrealistic, fuzzy details. Recently, diffusion models have emerged as a powerful solution for video generation tasks [3, 8, 9, 18, 21, 45], including character animation. Large video foundational models [2–5, 18, 20, 21, 32, 34, 45, 50, 52, 57, 76, 79] have achieved remarkable progress in text- or image-to-video synthesis, serving as versatile backbones for down-stream tasks. However, a major challenge remains: how to precisely control content and style to satisfy person-alized requirements across varied contexts. Recent ef-forts [19, 33] integrate control signals via modules such as ControlNet [69], ControlNet++ [30], and T2IAdapter [39]. With the development of Diffusion Transformer (DiT) [41], how to effectively inject condition signals while avoiding cumbersome network becomes an interesting problem. For video diffusion-based character animation, besides control sign injection, another core challenge lies in effec-tively decoupling the identity and motion information. Ide-ally, the target character’s body shape, spatial layout, and appearance should remain unaffected by those in the driv-ing video. Similarly, the final retargeting pose should not be influenced by the motion in the source image. This issue is mild in self-reenactment tasks [11, 49, 74, 81], such as vir-tual try-on, because the characters in the driving video and edited image have similar body shape and scale. In con-trast, cross-reenactment—e.g., animating a tall adult from a child’s motion, or vice versa—poses substantial difficulty due to drastic mismatches in body shape and spatial layout. Existing cross reenactment works can be classified into explicit and implicit methods. Explicit approaches [23, 36, 47, 53, 56, 72, 77, 78, 80] use representation such as skele-ton, DWPose [66] or SMPL [35, 40] to capture motion signals, but cannot handle the mentioned identity conflict between the driving video and source image. Follow-up work [55] rescales the joint lengths of the source identity to match the driving person, but fails in complex move-ments. Other approach [10] utilizes an image model to change the source image into a standard pose for shape alignment, while introducing additional cumulative errors. Implicit works go beyond learning basic dynamic patterns to capture deeper and more semantic understandings of mo-tion rather than merely replicating spatial shifts. However, most of them [31, 37, 48, 58, 70] rely on per-video fine-tuning, making them time-consuming and restricting practi-cal application scope. More recent generalized method [46] directly uses 2D character tokens to represent identity, but carries the risk of leaking motion information that affects the retargeting pose accuracy. We propose IM-Animation, a diffusion-based framework that achieves robust character animation even when the source and driving identities differ significantly in body shape or spatial layout (e.g., full-body driving half-body), as illustrated in Figure 1. To extract motion while prevent-ing identity leakage, we design a compact motion repre-sentation that compresses per-frame dynamics into 1D mo-tion tokens via a transformer-based encoder–decoder with a quantized codebook. This spatially invariant representa-tion, trained with keypoint supervision, relaxes 2D grid con-straints to remove identity information in the driving video. To merge the driving motion with the source identity while preventing motion leakage, we design a novel mask token based retargeting module. By incorporating learnable mask tokens as a training bottleneck within self-attention, our module removes pose information from the source image to ensure coherent motion transfer and pose consistency. The resulting retargeted latent features and expression cues are then fed into a video diffusion model to generate realistic animation sequences. Finally, a three-stage training strategy progressively optimizes motion representation, retargeting module, and video generation, leading to stable training and high-quality outputs. Extensive experiments show that IM-Animation achieves high-quality results with limited computational resources. Our main contributions are: • We present an innovative compact motion representation that compresses each frame’s motion into spatially invari-ant 1D tokens, effectively preventing character informa-tion leakage while preserving animation integrity. • We introduce a mask token based retargeting module that leverages mask tokens as a latent bottleneck to remove motion information from the source image, ensuring re-targeting pose coherence and consistency. • We develop a three-stage training pipeline that progres-sively trains motion representation, retargeting module, and video diffusion model. It reduces training costs, achieves effective disentanglement and produces realistic animation results. 

## 2. Related Work 

2.1. Diffusion Model for Video Generation 

Recently, diffusion models have become a key player in vi-sual generation, transforming noise into high-fidelity con-tent through iterative denoising. Research has increas-ingly focused on enhancing their efficiency, leading to mile-stones like Latent Diffusion Models (LDMs) [43]. In this evolving field, video generation has emerged as a signif-icant challenge, emphasizing temporal coherence and dy-namic scene modeling. Early explorations extended text-to-image (T2I) models to video generation. Some stud-ies [13, 21, 22, 26, 42, 45, 59, 65] applied inter-frame at-tention mechanisms, enabling models to capture dynamic dependencies. Other research introduced dedicated tempo-ral layers [4] or developed injectable motion modules [18]. These modules were trained on large video datasets to inte-grate temporal dynamics into T2I backbones without sacri-ficing pre-learned spatial knowledge. The video generation paradigm significantly changes with the rise of Diffusion Transformers (DiTs) [41] as a mainstream architecture. Unlike previous structure, DiTs leverage self-attention mechanisms to model global spatio-temporal dependencies. This allows them to capture long-range temporal coherence and fine-grained spatial details. Building on this, recent state-of-the-art works [28, 51, 67, 75] have further advanced DiT-based video generation, achieving remarkable results in high-fidelity content cre-ation and temporal consistency by scaling model capacity and training on massive text-video datasets. 

2.2. Video-driven Character Animation 

Video models excel at generating detail-rich results and serve as a foundation for complex controllable tasks. In character animation, they are widely used to create high-quality dynamic representations. Based on the form of con-trol signal representation, existing methods can generally be categorized into two main types: explicit control signal and implicit control signal representations. 

2.2.1. Explicit Motion Representation 

Explicit methods extract pose sequences from the driving video, which serve as explicit guides for animating the tar-get character. For instance, numerous works leverage skele-tal keypoints as conditional signals [23, 36, 47, 55, 56, 72], while others adopt more detailed representations such as DensePose maps [63], SMPL renderings [53, 77, 78, 80], or specially designed motion cues [14, 15, 17, 36, 62] to better capture desired movements. However, a critical limitation of these methods lies in their sensitivity to spatial discrepancies: when there exists a significant shape gap between the target character and the subject of the driving video, the generated animations often suffer from distortions like misaligned limbs or un-natural postures. To mitigate this issue, researchers have proposed some interesting solutions. UniAnimate-DiT [56] and RealisDance-DiT [78] introduce pose alignment as a preprocessing step to align the driving motion with the body of the target character, while Animate-X [47] learns an addi-tional implicit pose indicator by merging CLIP features and DWPose keypoints from the driving video. However, these methods still struggle to handle complex motion scenarios, which can result in failures in the driving process. 

2.2.2. Implicit Motion Representation 

Instead of strict spatial constraints, another line of works learn implicit motion representations directly from driving videos. They enable flexible cross-subject motion transfer by capturing dynamic patterns and temporal correlations in a data-driven manner. Firstly, this strategy succeeded in fa-cial animation works [12, 27, 60, 61, 64, 73], which excel at capturing subtle expression changes in the latent space. However, extending implicit motion representation to full-body digital humans poses greater challenges, as it demands coherent modeling of complex, multi-part body movements and fine-grained details across both space and time. To solve the above issue, two categories of approaches have emerged: finetuning-based methods and training-free methods. Finetuning-based approaches [31, 37, 48, 58, 70] adapt the model to each specific driving video through fine-tuning, encoding motion information into the model weights to guide animation. This per-video fine-tuning lim-its efficiency for full-body scenarios. In contrast, training-free methods [7, 38] avoid the costly per-video tuning by designing attention-based operations to directly extract mo-tion features from the driving video during inference, but often struggle with preserving motion precision. To address these limitations, recent works have explored end-to-end frameworks to enhance efficiency and transfer-ability for full-body applications. EfficientMT [6] reuses a pretrained video model for reference feature extraction, enabling end-to-end motion transfer. However, it lacks sup-port for driving specific characters and struggles with com-plex motion sequences. X-UniMotion [46] uses dedicated image encoders to extract motion tokens for the full body, hands, and face separately, retargeting them to the refer-ence identity via a ViT decoder for fine-grained, identity-preserving motion transfer. However, it directly models patchified token sequence of the character image, which can lead to shortcut learning in motion representations. 

## 3. Methods 

Given a identity image and a motion-driven video of any character, our method generates a retargeted video that matches the body shape and perspective of the character identity following the given driving motion. To achieve ef-fective decoupling, we design a novel motion representation extracted from a video clip while preventing the leakage of identity information from the motion sequence. Further-more, to integrate identity information into the motion fea-ture, we retarget the motion representation to the given char-acter image, which provides information about the char-acter’s identity, pose and spatial location. We establish a learning bottleneck for the model by injecting a mask token to prevent it from learning the original motion representa-tions of the identity image, which could lead to the leak-age of motion information contained in the identity image. Then we use a video model to generate the video based on the compact retargeted identity representation. 

3.1. Motion Representation Figure 2. We propose IM-Animation, an implicit portrait animation solution. Given a identity image and a motion video, we employ a three-stage training strategy. In the first stage, we train a compact motion encoder based on a 1D tokenizer. In the subsequent second and third stages, we train a temporal retargeting module based on mask tokens, utilizing a lightweight heatmap decoder for intermediate supervision. This approach ensures that we can encode precise retargeted information without disclosing the ID information of the driven video or the pose information of the source image. Ultimately, we achieve end-to-end training of the entire model. 

We do not require the motion video to provide detailed spa-tial correspondences. In other words, we hope that the mo-tion tokens can transcend the original 2D grid areas after patchification, achieving more flexible, high-level, and se-mantically rich compact representations that are not limited to 2D space. Inspired by the work of TiTok [68], which has achieved remarkable results in image reconstruction and generation, we design a motion representation encoder to resemble a 1D tokenizer which can obtain a more compact one-dimensional representation of tokens that is indepen-dent of patches. This design enables the encoding of each frame of the original driving video into a compact and high-quality motion representation comprising tokens. Specifically, we concatenate a set of learnable tokens of length Nm with the patchified motion video frames, and then feed them into a vision transformer (ViT) encoder. In our settings, Nm = 32 . Subsequently, the corresponding components of the learnable tokens output by motion en-coder Emotion are sent to a codebook for vectorization, re-sulting in the formation of 1D tokens, which can be defined as: 

Mimg = Quant (Emotion (Timg ⊕ Tlatent )) ∈ RNm×Cm

(1) Here, Mimg represents the final encoded motion represen-tation, with a length of Nm and dimension of Cm. Timg and 

Tlatent refer to patchified image tokens and learnable latent tokens, respectively. Quant is the quantizer with a code-book that vectorizes the output of the motion encoder. The following operations are performed during this process: 

Quant (z1d ) = ci, where i = argmin 

> j∈{ 1,2,...,K }

∥z1d − cj ∥2.

(2) Here, z1d denotes the retained latent tokens output by the Transformer encoder, which are generated based on two sets of image tokens and learnable tokens during this process. The vector quantizer Quant maps it to the nearest code ci ∈

RCm in the learnable codebook C ∈ RK×Cm .Unlike reconstruction or generation tasks, the motion en-coder should not retain any character shape or appearance Figure 3. Method of Control Signal Injection in Video Model. 

texture features apart from motion. We model the first train-ing stage as a task which regresses the joint map from the motion video. We carefully design a training process with the help of an additional motion decoder, which is only used in training Stage 1 rather than inference. We repeat the mask token J times corresponding to the number of joints, denoted as Tmask ∈ RJ×D .Similar to the design of the motion encoder, in order to supervise the extraction of motion features in the first train-ing stage, Mimg and Tmask are concatenated and fed into the transformer decoder. In Stage 1, the parts of the output other than Tmask are dropped, and a joint decoder Djoint are applied to adjust the output to match the size of the corresponding mid-level supervised joint heatmap. This process is super-vised by the Mean Squared Error (MSE) loss between the output joint heatmap and the real joint heatmap. 

ˆHjoint = Djoint (Dmotion (Mimg ⊕ Tmask )) . (3) 

3.2. Temporal Retargeting 

We hope that the motion representation extracted by the motion encoder can provide feature information, while the character image offers identity and spatial layout informa-tion. We design a retargeting module based on mask tokens. To prevent the character image from leaking motion infor-mation, we set learnable mask tokens, which are concate-nated with image latent tokens and temporal motion tokens before being fed into the retargeting module for training. In order to effectively distinguish between the three dif-ferent types of input tokens, we additionally apply a set of learnable positional embeddings for each type of tokens, 

Mret = Retarget (Mvideo ⊕ Tref ⊕ Tmask ) ∈ R Wf × Hf ×D

(4) Here, Mvideo is the motion encoding representation of the entire sequence obtained by concatenating Mimg along the temporal dimension. Tmask is the learnable tokens aligned in dimension and Tref is the features of the reference image that have been encoded through VAE and patchification, re-spectively. In Stage 2, we use Djoint to train both motion representation and retargeting module at the same time. 

3.3. Control Signal Injection 

We believe that incorporating the video model as a flow matching decoder can obtain more advanced semantic fea-tures during the training phase. Therefore, in the end-to-end process, we ultimately release all the aforementioned train-able parameters for further fine-tuning. After obtaining the retargeted motion tokens, we stack them with the noisy latent representations along the chan-nel dimension and perform patch embedding to ensure their size is supported by the video model. For facial expres-sions, we aim for our method to transfer the expression se-quences from the original video. We use the encoder from X-NeMo [73] as our expression encoder and calculate cross attention frame by frame to inject the expression signals. The final output is then connected via a residual connection to the corresponding DiT Block output. 

3.4. Training Strategy 

Most of existing implicit video-driven methods require sub-stantial training costs. In contrast, we aim to achieve more efficient convergence under the constraint of limited train-ing resources. Overall, we train IM-Animation in three stages. In-spired by X-Unimotion [46], we implement mid-level joint heatmap supervision to ensure that the model can learn ef-fective motion representations. • Stage 1. We first train the motion encoder, as introduced in Sec. 3.1. The joint-aligned decoder is only trained in this phase. During this stage, we fine-tune the pretrained encoder model and quantizer from TiTok using the self-reconstructed joint map for supervision. • Stage 2. During the joint training of the motion encoder and the retargeting module, we perform color augmen-tation, random cropping, and padding on the video data collected from the internet. Additionally, we construct a batch of action-consistent but identity-inconsistent paired datasets using Unreal Engine (UE). To ensure that the re-targeting module can predict the actions of the reference image identity, We also apply the joint decoder here to decode joint map for supervision. • Stage 3. Finally, we inject the motion and expression control signals into the DiT model to achieve end-to-end Figure 4. Qualitative Results. We compare our method with several state-of-the-art approaches, among which UniAnimate-DiT and Wan-Animate are trained using a 14B base model. In contrast, our method is faster while also achieving competitive results. Furthermore, our approach demonstrates impressive performance in maintaining character identity and retargeting significant character differences. 

training in the final stage. During this process, we also maintain the supervision from the joint decoder. 

## 4. Experiments 

4.1. Implementation Details 

Datasets. We collected over 50k video clips for our train-ing set. To enable the model to effectively retarget charac-ters of varying body types, we generated and selected 8k data pairs with identical motions but different identities us-ing Unreal Engine 5 and other generation models. In the comparative experiments, we use 50 samples from the data we collected, which excludes in the training set, to evaluate three key aspects: cross-identity driving accuracy, gener-ated video quality, and motion precision. To further demon-strate the robustness of our method, we test the performance of the Self-Reenactment task on a publicly available TikTok dataset [24, 25] and compare with existing work. 

Training Settings. All of our experiments are conducted in an environment with 16 GPUs. For the motion encoder in   

> Figure 5. Qualitative Results of Ablation Experiment. In each set, the images are arranged from left to right as follows: driving frame, character image, full model performance, and performance of the ablation experiment without the specified module.

Stage 1, we train the self-reconstruction model for 5k steps. In Stage 2, we train the retargeting module for 10k steps. Fi-nally, in Stage 3, we use Wan2.2 5B [51] as our base model and perform end-to-end training of the DiT model for 30k steps. Due to computational resource limitations, we pro-Figure 6. Qualitative Results of IM-Animation. The top row of each image set represents the driving video, while the bottom row displays the generated results. 

cess all training data into videos with 81 frames. In Stage 1 and Stage 2, all models were trained using AdamW with a learning rate of 10 −4 . In Stage 3, the DiT model was trained with a learning rate of 10 −5 , while the remaining components were trained with a learning rate of 5 × 10 −5 .

4.2. Evaluations and Comparisons 

We conduct a comparison with several models, including Animate-X [47], AnimateAnyone [23], MimicMotion [71], UniAnimate-DiT [56], Champ [80] and Wan-Animate [10]. We conduct experiments on cross reenactment and self reenactment to evaluate the performance of our model. For cross reenactment, we calculate several metrics on the con-structed paired dataset, including PSNR, SSIM, LPIPS, FID and FVD . For self reenactment, we assess the genera-tion quality on the first 50 entries of TikTok dataset to ex-amine the model’s performance in practical applications. As shown in Table 1 and Figure 4, IM-Animation model demonstrates significant advantages in both cross reenact-ment and self reenactment tasks. In the cross reenact-ment task, IM-Animation achieves the highest scores in PSNR, LPIPS, and FID, indicating superior visual clarity and appeal. In SSIM and FVD, IM-Animation remains close in performance to Wan-Animate, which uses a larger base model and costs much more inference time. Since X-UniMotion has not yet been open-sourced, we provide a visual comparison of the cases available on its official website, as shown in Figure 7. In the the self-reenactment task, IM-Animation shows strong competitiveness in all major metrics. These results confirm that IM-Animation can maintain high performance and competitiveness. To demonstrate the robustness of IM-Animation, we present additional results on in-the-wild data in Figure 6. 

> Figure 7. Qualitative Results Compared with X-Unimotion. Since X-Unimotion has not released its source code, we conducted a vi-sual comparison using the sample examples provided on the X-Unimotion homepage.

4.3. Ablation Study 

We perform ablation experiments to systematically evalu-ate the contributions of different components of our model. This approach helps us understand the impact of each com-ponent on the overall performance and effectiveness of our method. We validate the effectiveness of several key mod-ules in our model, including the 1D tokenizer-based mo-tion encoder, which is compared to a direct convolutional module for encoding. In addition, we investigate the contri-butions of the mask token-based motion encoder, heatmap supervision, and the expression adapter. As shown in the Figure 5 and Table 2, exp.A demon-strates that our motion encoder extracts high-quality mo-Table 1. Quantitative evaluation of video generation on bench-mark and TikTok datasets. Values in bold represent the best per-formance, while underlined values indicate the second.                                                                                           

> Cross Reenactment
> Methods PSNR ↑SSIM ↑LPIPS ↓FID ↓FVD ↓
> Champ [80] 20.42 0.89 0.28 88.09 785.54 MimicMotion [71] 20.01 0.88 0.30 97.86 901.55 AnimateAnyone [23] 19.24 0.86 0.31 85.24 690.15 UniAnimate-DiT [56] 21.97 0.90 0.25 71.61 436.80 Animate-X [47] 22.16 0.90 0.27 75.38 624.24 Wan-Animate [10] 22.72 0.92 0.24 59.10 267.71
> IM-Animation (ours) 22.87 0.91 0.24 51.19 270.42
> Self Reenactment
> Methods PSNR ↑SSIM ↑LPIPS ↓FID ↓FVD ↓
> Champ [80] 17.05 0.70 0.37 94.29 959.92 MimicMotion [71] 16.81 0.71 0.39 92.04 927.46 AnimateAnyone [23] 18.13 0.71 0.34 89.37 793.61 UniAnimate-DiT [56] 19.76 0.75 0.29 71.32 504.19 Animate-X [47] 20.16 0.76 0.30 76.24 483.60 Wan-Animate [10] 21.12 0.78 0.21 36.78 382.86 IM-Animation (ours) 21.85 0.76 0.20 35.98 351.70

Table 2. Ablation study of different modules. We evaluated the effectiveness of the motion encoder design, retargeting module, and expression adapter through experiments .                                            

> Method PSNR* ↑SSIM ↑LPIPS ↓FVD ↓FID ↓
> exp A. w/o 1d tokenizer-based motion encoder 19.67 0.84 0.29 81.51 543.66 exp B. w/o expression adapter 22.01 0.89 0.26 69.03 374.90 exp C. w/o mask token-based retargeting module 21.03 0.88 0.27 78.32 561.89 exp D. w/o middle supervision 21.54 0.89 0.25 72.80 443.37 exp E. full model 22.87 0.91 0.24 51.19 270.42

Table 3. Ablation study of different training strategy. We evaluated the performance of different granularity levels of training division strategies alongside the strategy we ultimately adopted.                          

> Method PSNR* ↑SSIM ↑LPIPS ↓FVD ↓FID ↓
> End-to-End training 19.40 0.73 0.37 78.89 566.04 2 Stages training 20.65 0.79 0.30 67.11 334.96 3 Stages training 22.97 0.91 0.24 51.19 270.42

tion representations. Compared to lightweight motion en-coders, our model can effectively suppress the leakage of identity information and positional information of the orig-inal images. Exp.B shows that the expression adapter better preserves the facial expression details of characters. Exp.C indicates that our designed retargeting module can more ef-fectively transfer motion sequences of characters with dif-ferent postures to new characters. Finally, exp.D reveals that not adding extra middle supervision can lead to the gen-eration of additional limbs or poor motion correspondences. In addition, we evaluate the effectiveness of our three-stage training approach. This evaluation involves compar-ing the current three-stage training setup with a joint train-ing method that retains Stage 2 and Stage 3, as well as an end-to-end training approach. All comparisons are con-ducted using the DiT training framework for 30k steps. As shown in Table 3, our model can quickly converge on the metrics of the three-stage training with a shorter number of DiT training steps. We believe that the motion represen-tation and retargeting module, after initial training, can pro-

Table 4. User study results.                                     

> User Study
> Methods MQ ↑IP ↑VR ↑OA ↑
> Animate-X 3.95 3.35 3.25 3.60 MimicMotion 3.20 3.35 3.20 3.30 AnimateAnyone 2.55 3.15 2.9 2.70 UniAnimate-DiT 3.90 3.85 3.90 3.75 Champ 2.75 2.80 2.05 2.10 Wan-Animate 4.35 4.40 4.35 4.35 IM-Animation (ours) 4.55 4.45 4.35 4.40

vide better guidance to the DiT compared to randomly ini-tialized modules. Additionally, during the end-to-end train-ing process, the flow matching decoder can offer improved supervision for the motion representation module. 

4.4. User Study 

We conducted a user study to assess participants’ subjec-tive preferences regarding the generated videos. We se-lected 8 benchmark videos from the cross-ID task and in-vited 20 participants to evaluate them. Participants rate the videos based on several criteria: motion and expression quality (MQ), identity preservation (IP), video reality (VA), and overall video quality (OA). Each participant watched several video clips, one of them generated by our method and others by baseline methods.This allowed participants to evaluate multiple pairs of videos, leading to a comprehen-sive comparison. The results in Table 4 indicate a strong preference for our method among participants. Our experiments are consistent with both quantitative and qualitative evaluations, further demonstrating the effectiveness of our approach in meeting user expectations for high-quality human video generation. 

## 5. Conclusion 

We introduce IM-Animation, a diffusion-based character animation framework built upon a compact implicit mo-tion representation and a mask token based retargeting mod-ule. By encoding per-frame dynamics into 1D motion to-kens, our method mitigates identity leakage from the driv-ing video. The mask-token bottleneck further separates identity information from the source image, leading to co-herent pose retargeting even under significant mismatches in body shape, scale, and spatial layout. Extensive experi-ments demonstrate that IM-Animation achieves competitive or superior motion fidelity, identity preservation, and visual quality compared with existing approaches. However, we acknowledge that our current strategy does not fully guarantee that facial expression features are en-tirely independent of the original identity. Designing spe-cific facial retargeting module may be a potential solution. Additionally, allowing users to control the camera perspec-tive is an exciting direction for future work. Integrating our implicit motion control with explicit 3D scene point cloud representation possibly solve this issue. References 

[1] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lian-rui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from a single video. arXiv preprint arXiv:2503.11647 , 2025. 2 [2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-rmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: A space-time diffu-sion model for video generation. In SIGGRAPH Asia 2024 Conference Papers , pages 1–11, 2024. 2 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 , 2023. 2 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with la-tent diffusion models. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition , pages 22563–22575, 2023. 2 [5] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems ,35:31769–31781, 2022. 2 [6] Yufei Cai, Hu Han, Yuxiang Wei, Shiguang Shan, and Xilin Chen. Efficientmt: Efficient temporal adaptation for motion transfer in text-to-video diffusion models. arXiv preprint arXiv:2503.19369 , 2025. 3 [7] Changgu Chen, Xiaoyan Yang, Junwei Shu, Changbo Wang, and Yang Li. Lmp: Leveraging motion prior in zero-shot video generation with diffusion transformer. arXiv preprint arXiv:2505.14167 , 2025. 3 [8] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 , 2023. 2 [9] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffu-sion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7310– 7320, 2024. 2 [10] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055 , 2025. 2, 7, 8, 3 [11] Zheng Chong, Wenqing Zhang, Shiyue Zhang, Jun Zheng, Xiao Dong, Haoxiang Li, Yiling Wu, Dongmei Jiang, and Xiaodan Liang. Catv2ton: Taming diffusion transformers for vision-based virtual try-on with temporal concatenation. 

arXiv preprint arXiv:2501.11325 , 2025. 2 [12] Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pan-tic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8498– 8507, 2024. 3 [13] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision , pages 7346–7356, 2023. 2 [14] Jiayi Gao, Zijin Yin, Changcheng Hua, Yuxin Peng, Kong-ming Liang, Zhanyu Ma, Jun Guo, and Yang Liu. Conmo: Controllable motion disentanglement and recomposition for zero-shot motion transfer. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 7191– 7200, 2025. 3 [15] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Yusuf Aytar, Michael Rubinstein, Chen Sun, et al. Motion prompting: Controlling video generation with motion trajec-tories. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 1–12, 2025. 3 [16] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems , 27, 2014. 2 [17] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for ver-satile video generation control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Tech-niques Conference Conference Papers , pages 1–12, 2025. 3 [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 , 2023. 2 [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision , pages 330–348. Springer, 2024. 2 [20] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jos´ e Lezama. Photorealistic video generation with diffusion models. In 

European Conference on Computer Vision , pages 393–411. Springer, 2024. 2 [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video dif-fusion models. Advances in neural information processing systems , 35:8633–8646, 2022. 2 [22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868 , 2022. 2 [23] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8153–8163, 2024. 2, 3, 7, 8 [24] Y. Jafarian and H. Park. Self-supervised 3d representa-tion learning of dressed humans from social media videos. 

IEEE Transactions on Pattern Analysis Machine Intelli-gence , 2022. 6 [25] Yasamin Jafarian and Hyun Soo Park. Learning high fi-delity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 12753–12762, 2021. 6 [26] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-vosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In 

Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 15954–15964, 2023. 2 [27] Taekyung Ki, Dongchan Min, and Gyeongsu Chae. Float: Generative motion latent flow matching for audio-driven talking portrait. arXiv preprint arXiv:2412.01064 , 2024. 3 [28] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 , 2024. 3[29] Kwai. Keling, 2025. 2 [30] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Con-trolnet++: Improving conditional controls with efficient consistency feedback: Project page: liming-ai. github. io/controlnet plus plus. In European Conference on Com-puter Vision , pages 129–147. Springer, 2024. 2 [31] Xiaomin Li, Xu Jia, Qinghe Wang, Haiwen Diao, Meng-meng Ge, Pengxiang Li, You He, and Huchuan Lu. Motrans: Customized motion transfer with text-driven video diffusion models. In Proceedings of the 32nd ACM International Con-ference on Multimedia , pages 3421–3430, 2024. 2, 3 [32] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In Proceedings of the AAAI conference on artificial intelligence , 2018. 2 [33] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapt-ing diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967 , 2024. 2 [34] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316 ,2025. 2 [35] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 34(6):248:1–248:16, 2015. 2 [36] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724 , 2025. 2, 3[37] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207 , 2025. 2, 3 [38] Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, and Pinar Yanardag. Motionflow: Attention-driven mo-tion transfer in video diffusion models. arXiv preprint arXiv:2412.05275 , 2024. 3 [39] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence , pages 4296–4304, 2024. 2 [40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019. 2[41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF inter-national conference on computer vision , pages 4195–4205, 2023. 2, 3 [42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-ing attentions for zero-shot text-based video editing. In 

Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 15932–15942, 2023. 2 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨ orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 10684–10695, 2022. 2 [44] Aliaksandr Siarohin, St´ ephane Lathuili` ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems , 32, 2019. 2 [45] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 ,2022. 2 [46] Guoxian Song, Hongyi Xu, Xiaochen Zhao, You Xie, Tian-pei Gu, Zenan Li, Chenxu Zhang, and Linjie Luo. X-unimotion: Animating human images with expressive, uni-fied and identity-agnostic motion latents. arXiv preprint arXiv:2508.09383 , 2025. 2, 3, 5, 1 [47] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image ani-mation with enhanced motion representation. arXiv preprint arXiv:2410.10306 , 2024. 2, 3, 7, 8 [48] Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, et al. Synmotion: Semantic-visual adaptation for motion customized video generation. arXiv preprint arXiv:2506.23690 , 2025. 2, 3 [49] TUAL TRY. Magictryon: Harnessing diffusion trans-former for garment-preserving video vir. 2 [50] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399 , 2022. 2[51] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video gen-erative models. arXiv preprint arXiv:2503.20314 , 2025. 3, 6, 1 [52] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571 , 2023. 2 [53] Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. 

arXiv preprint arXiv:2405.18156 , 2024. 2, 3 [54] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-shot video-to-video synthesis. arXiv preprint arXiv:1910.12713 , 2019. 2 [55] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion mod-els for consistent human image animation. arXiv preprint arXiv:2406.01188 , 2024. 2, 3 [56] Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya Zhang, Changxin Gao, Yuehuan Wang, and Nong Sang. Unianimate-dit: Human image animation with large-scale video diffusion transformer. arXiv preprint arXiv:2504.11289 , 2025. 2, 3, 7, 8 [57] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. Imaginator: Conditional spatio-temporal gan for video generation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision , pages 1160– 1169, 2020. 2 [58] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhi-heng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hong-ming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6537–6549, 2024. 2, 3 [59] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In 

Proceedings of the IEEE/CVF international conference on computer vision , pages 7623–7633, 2023. 2 [60] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait anima-tion with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers , pages 1–11, 2024. 3 [61] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Pro-cessing Systems , 37:660–684, 2024. 3 [62] Zhongcong Xu, Chaoyue Song, Guoxian Song, Jianfeng Zhang, Jun Hao Liew, Hongyi Xu, You Xie, Linjie Luo, Gu-osheng Lin, Jiashi Feng, et al. High quality human image animation using regional supervision and motion blur condi-tion. arXiv preprint arXiv:2409.19580 , 2024. 3 [63] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human im-age animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1481–1490, 2024. 3 [64] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition con-trol for enhanced portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 15909–15919, 2025. 3 [65] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers ,pages 1–11, 2023. 2 [66] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effec-tive whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 4210–4220, 2023. 2, 1 [67] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-han Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 , 2024. 3 [68] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arxiv: 2406.07550 , 2024. 4, 1 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In 

Proceedings of the IEEE/CVF international conference on computer vision , pages 3836–3847, 2023. 2 [70] Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, and Yansong Tang. Flexiact: Towards flexible action control in heterogeneous scenarios. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Tech-niques Conference Conference Papers , pages 1–11, 2025. 2, 3[71] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mim-icmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680 , 2024. 7, 8, 3 [72] Haoyu Zhao, Zhongang Qi, Cong Wang, Qingping Zheng, Guansong Lu, Fei Chen, Hang Xu, and Zuxuan Wu. Dy-namictrl: Rethinking the basic structure and the role of text for high-quality human image animation. arXiv preprint arXiv:2503.21246 , 2025. 2, 3 [73] Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, and Yebin Liu. X-nemo: Expressive neural motion reenactment via disen-tangled latent attention. arXiv preprint arXiv:2507.23143 ,2025. 3, 5 [74] Jun Zheng, Jing Wang, Fuwei Zhao, Xujie Zhang, and Xiaodan Liang. Dynamic try-on: Taming video virtual try-on with dynamic attention mechanism. arXiv preprint arXiv:2412.09822 , 2024. 2 [75] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404 , 2024. 3 [76] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 , 2022. 2 [77] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. Realisdance: Equip controllable character anima-tion with realistic hands. arXiv preprint arXiv:2409.06202 ,2024. 2, 3 [78] Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Wei-hua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Sim-ple yet strong baseline towards controllable character anima-tion in the wild. arXiv preprint arXiv:2504.14977 , 2025. 2, 3[79] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Al-legro: Open the black box of commercial-level video gener-ation model. arXiv preprint arXiv:2410.15458 , 2024. 2 [80] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image an-imation with 3d parametric guidance. In European Confer-ence on Computer Vision , pages 145–162. Springer, 2024. 2, 3, 7, 8 [81] Tongchun Zuo, Zaiyu Huang, Shuliang Ning, Ente Lin, Chao Liang, Zerong Zheng, Jianwen Jiang, Yuan Zhang, Mingyuan Gao, and Xin Dong. Dreamvvt: Mastering realis-tic video virtual try-on in the wild via a stage-wise diffusion transformer framework. arXiv preprint arXiv:2508.02807 ,2025. 2 IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation 

# Supplementary Material 

## 6. Implementation Details 

In the following section, we outline the training strate-gies for IM-Animation, designed to achieve efficient con-vergence with limited resources. Unlike existing im-plicit video-driven methods, our approach consists of three stages. In Stage 1, we train the motion encoder, fine-tuning the pretrained encoder and quantizer from TiTok [68] us-ing self-reconstructed joint maps for supervision. Stage 2 involves joint training of the motion encoder and the retar-geting module, incorporating data augmentation techniques and constructing action-consistent but identity-inconsistent paired datasets. Finally, in Stage 3, we inject motion and expression control signals into the DiT model for end-to-end training, while maintaining supervision from the joint decoder. This structured approach aims to enhance the model’s performance in complex scenarios and ensure ef-fective learning of motion representations. 

Stage 1: Motion encoder training stage. As mentioned in the main text, our subsequent experiments are based on the pretrained checkpoints provided by TiTok. TiTok’s original design is based on image reconstruction and generation tasks, and the features extracted by its en-coder tend to capture more detailed image semantics than desired. During the motion representation phase, we aim to minimize identity detail leakage to the downstream DiT model. To achieve this, we have meticulously designed a decoder that transforms the compressed tokens, of which we mentioned there are 32 in the main text, into a joint map. In this process, we restructure the motion decoder. We align the number of mask tokens with the quantity of joint supervision. Specifically, we select the 20 body joints from DWPose [66] along with all hand joints as supervi-sion points. After obtaining the corresponding joint tokens, we employ a series of convolutional layers to upsample the mask tokens to the scale of the ground truth joint map. 

Loss motion = 1

T

> T

X

> t=1

(Hmotion,t − Hgt ,t )2 (5) Here, T represents the total number of frames in the video sequence, Hmotion,t is the heatmap generated by the model for the t-th frame, and Hgt ,t is the corresponding ground truth heatmap. At this stage, we do not design data augmen-tation or similar self-supervised retargeting training con-structs like X-UniMotion [46]. In other words, this stage is more akin to training a keypoint prediction process. 

Stage 2: Retargeting training stage. In the imple-mentation of the retargeting module, we first compress the source image into latent space using the VAE from Wan2.2 [51] before performing patchification. In our de-sign, the number of mask tokens is aligned with the number of patches from the source image. This approach allows us to overcome the limitations of the patch grid while facilitat-ing channel concatenation during the subsequent process of controlling condition injection. In this phase, unlike the first stage where we directly regress the heatmap, we employ random data augmentation to enable the model to learn retargeting across different IDs and at larger scales. After applying random color transfor-mations, we randomly crop or scale the original video, and then supervise the model using the original video. The loss function of this stage is defined as follow, 

Lretarget = 1

T

> T

X

> t=1

(Hretarget,t − Hgt,t )2 (6) where T denotes the total number of time steps, Hretarget,t is the generated retargeting heatmap, and Hgt,t is the ground truth heatmap at time step t. By averaging the squared differences between the predicted heatmap and the corre-sponding ground truth heatmap at each time step, Lretarget 

aims to minimize the disparity between the generated and true heatmaps, thereby improving the model’s performance in the retargeting task. 

Stage 3: End-to-End training stage. In the final stage, we conduct end-to-end training. In this part, we use addi-tional synthetic data to enhance the data scale. Our method for injecting control signals has been thoroughly described in the main text. To avoid disrupting the pre-trained ca-pabilities of DiT, we reuse the weights from the original checkpoint for the embedding part corresponding to the channel-wise concatenation, while initializing the region part to zero. The original Wan2.2 5B model contains 30 DiT blocks. We insert an expression block after every 6 DiT blocks, employing a skip connection technique to pre-vent disrupting the original capabilities of the DiT. At this stage, we also employ intermediate supervision for training. For the diffusion loss function, 

LDiT = E t,x 1: N  

> 0,y,ϵ

ϵ − ϵθ (√¯αtx1: N 

> 0

+ √1 − ¯αtϵ, t, y ) 2 ,

(7) where t is sampled from the range [1 , T ] (denoting the de-noising steps), ϵ represents the random noise, y denotes the text prompts, and x1: N 

> 0

refers to the video data comprising 

N frames. Figure 8. Samples of synthesized data . The first row is the reference video and the second row is the target video. 

> Figure 9. Samples of UE data . The first row is the reference video and the second row is the target video.
> Figure 10. Visualization of Joint Heatmap.

We also employ the aforementioned intermediate super-vision to accelerate convergence. The loss function for the entire Stage Three is defined as 

Ltotal = LDiT + α · Lretarget (8) During our training process, α is set to 10. 

## 7. Dataset Details 

We utilize Kling [29] to synthesize data that maintains con-sistent motion across diverse identities. Throughout the synthesis process, we engineered data featuring a range of spatial characteristics and body types, followed by a metic-ulous manual selection of the outputs. Sample outputs of the generated data are presented in Figure 8. For the UE data, similar to the approach taken in Re-CameraMaster [1], we synthesized data from various cam-era positions and an array of scenes. This data synthesis strategy enhances our model’s ability to adapt to different viewpoints and environmental variations, thereby improv-ing both the quality and diversity of the generated character animations. Table 5. Motion Encoder Comparsion .                     

> Method PSNR* ↑SSIM ↑LPIPS ↓FVD ↓FID ↓
> Wan + VAE motion encoder 17.14 0.76 0.37 94.44 653.30 full model 22.87 0.91 0.24 51.19 270.42

Table 6. Retargeting Module Comparsion .                     

> Method PSNR* ↑SSIM ↑LPIPS ↓FVD ↓FID ↓
> Wan + SA retargeting module 19.89 0.82 0.26 68.99 477.63 full model 22.87 0.91 0.24 51.19 270.42

## 8. Visualization of Joint Map 

To validate the effectiveness of our intermediate supervi-sion, we present visual results of the joint decoder outputs following the redirection process, as illustrated in Figure 10. The heatmaps generated by our decoder demonstrate a strong correspondence with the ground truth, indicating a high level of accuracy in the predictions. This alignment not only underscores the robustness of our model, but also highlights the efficacy of the intermediate supervision strat-egy in enhancing the learning process. The visual comparison reveals that the decoder success-fully captures the intricate details of the target output, sug-gesting that our approach effectively mitigates discrepan-cies that typically arise during the generation process. Such results provide compelling evidence that our method con-tributes to improved performance in generating high-fidelity character animations. 

## 9. More Ablation Study 

For the motion encoder, we conduct more fine-grained ab-lation experiments. We compare our motion encoder with the results of directly inputting latent variables compressed by VAE into the downstream model for retargeting. This comparison allows us to more clearly assess the advantages of the motion encoder in terms of generation quality and performance. As shown in the Figure 11 and Table 5, directly using VAE for encoding results in background information being directly encoded into the motion tokens, leading to a decline in generation quality. In contrast, our method effectively prevents the leakage of semantic information from the mo-tion tokens. In addition, we conduct a fine-grained validation of the effectiveness of the mask token-based retargeting module. In fact, we experiment with the retargeting implementation proposed by X-UniMotion, which utilizes self-attention to combine two sources of tokens. Although this implementa-tion is not open-sourced, we develop a similar version and conduct comparative experiments. As shown in the table, we find that in some cases, this type of retargeting leads to an increased probability of implicit representation control failure, resulting in generated videos that tend to maintain 

Figure 11. Ablation Study. 

static motions which can be found in Figure 11. 

## 10. More Visualization results. 

Here, as shown in the Figure 12, we provide some generated results of IM-Animation on the TikTok dataset. Additionally, as shown in Figure 13, we present more qualitative results here compared to Animate-X [47], MimicMotion [71], AnimateAnyone [23], UniAnimate-DiT [56], Champ [80] and Wan-Animate [10], highlight-ing that our model better preserves certain details compared to those in the main text. We also demonstrate that our model performs well in cases where explicit driven retar-geting fails. In Figure 14, we also present the generation quality on the synthetic UE dataset. In the Figure 15, we also present additional visualization results. Figure 12. More visualization results on TikTok dataset. Figure 13. More visualization results of Comparsion Results. Figure 14. More visualization results on UE dataset. Figure 15. More visualization results on in the wild data
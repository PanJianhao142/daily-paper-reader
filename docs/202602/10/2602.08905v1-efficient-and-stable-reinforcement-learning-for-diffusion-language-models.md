---
title: Efficient and Stable Reinforcement Learning for Diffusion Language Models
title_zh: 扩散语言模型的高效稳定强化学习
authors: "Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.08905v1"
tags: ["keyword:MDM", "query:课题"]
score: 6.0
evidence: 扩散模型中的时空剪枝
tldr: 针对扩散语言模型（dLLMs）在强化学习中面临的效率与稳定性挑战，本文提出了时空剪枝（STP）框架。该框架通过空间剪枝约束探索空间，并利用时间剪枝跳过冗余的后期细化步骤，从而压缩生成过程中的冗余。理论证明STP能降低对数似然估计的方差，实验表明其在效率和准确性上均优于现有技术。
motivation: 扩散语言模型在应用强化学习时存在生成过程冗余、训练效率低下且策略更新不稳定的核心问题。
method: 提出STP框架，通过空间剪枝（利用静态先验约束探索）和时间剪枝（跳过后期冗余细化步骤）来优化RL过程。
result: 实验证明STP在显著提升训练效率的同时，其任务准确性也超越了目前最先进的基准模型。
conclusion: STP通过减少生成过程中的时空冗余，为扩散语言模型提供了一种更高效、更稳定的强化学习训练方案。
---

## 摘要
强化学习 (RL) 对于释放基于扩散的大语言模型 (dLLMs) 的复杂推理能力至关重要。然而，将强化学习应用于 dLLMs 在效率和稳定性方面面临着独特的挑战。为了应对这些挑战，我们提出了时空剪枝 (Spatio-Temporal Pruning, STP)，这是一个旨在同时提高 dLLMs 强化学习效率和稳定性的框架。STP 通过以下方式压缩生成过程中的冗余：(1) 空间剪枝 (spatial pruning)，利用静态先验约束探索空间；(2) 时间剪枝 (temporal pruning)，跳过冗余的后期细化步骤。我们的理论分析表明，STP 严格降低了对数似然估计的方差，从而确保了更稳定的策略更新。广泛的实验表明，STP 在效率和准确性方面均优于最先进的基准模型。我们的代码可在 https://github.com/Lolo1222/STP 获取。

## Abstract
Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.
---
title: Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction
title_zh: 通过 4D 手-物轨迹重建从 RGB 人体视频中学习灵巧操作策略
authors: "Hongyi Chen, Tony Dong, Tiancheng Wu, Liquan Wang, Yash Jangir, Yaru Niu, Yufei Ye, Homanga Bharadhwaj, Zackory Erickson, Jeffrey Ichnowski"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.09013v1"
tags: ["query:课题"]
score: 6.0
evidence: 重建4D人体运动用于机器人操作学习
tldr: 本研究提出 VIDEOMANIP 框架，旨在解决多指机器人灵巧操作中数据获取难的问题。该框架无需穿戴设备，直接从单目 RGB 人类视频中重建 4D 机器人-物体轨迹。通过手-物接触优化和演示合成策略，将人类动作重定向至机器人手，从而训练出泛化性强的操作策略。实验表明，该方法在仿真和现实世界中均取得了显著的成功率，优于现有的重定向方法。
motivation: 传统的灵巧操作数据获取依赖昂贵的遥操作或穿戴设备，限制了大规模数据的采集和扩展。
method: 通过从单目视频中估计人手姿态和物体网格，结合接触优化与交互中心抓取建模，将重建的 4D 轨迹重定向至机器人并合成多样化训练数据。
result: "在仿真中实现了 70.25% 的抓取成功率，在现实世界 7 项任务中达到 62.86% 的平均成功率，显著优于基准方法。"
conclusion: VIDEOMANIP 证明了直接从普通 RGB 视频中学习复杂灵巧操作策略的可行性，为机器人技能获取提供了一种高效且可扩展的途径。
---

## 摘要
由于高维动作空间以及大规模训练数据获取的困难，多指机器人手的操作和抓取极具挑战性。现有方法在很大程度上依赖于使用可穿戴设备或专门传感设备的人工遥操作来捕获手-物交互，这限制了其可扩展性。在这项工作中，我们提出了 VIDEOMANIP，这是一个无需设备的框架，可以直接从 RGB 人体视频中学习灵巧操作。利用计算机视觉的最新进展，VIDEOMANIP 通过估计人手姿态和物体网格，从单目视频中重建显式的 4D 机器人-物体轨迹，并将重建的人体运动重定向到机器人手以进行操作学习。为了使重建的机器人数据适用于灵巧操作训练，我们引入了结合以交互为中心的抓取建模的手-物接触优化，以及一种能从单个视频生成多样化训练轨迹的演示合成策略，从而在无需额外机器人演示的情况下实现可泛化的策略学习。在仿真中，所学习的抓取模型在使用 Inspire Hand 对 20 种不同物体进行抓取时达到了 70.25% 的成功率。在现实世界中，从 RGB 视频训练的操作策略在使用 LEAP Hand 的七项任务中达到了 62.86% 的平均成功率，比基于重定向的方法高出 15.87%。项目视频可在 videomanip.github.io 查看。

## Abstract
Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.
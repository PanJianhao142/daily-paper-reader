---
title: Language-Guided Transformer Tokenizer for Human Motion Generation
title_zh: 用于人体动作生成的语言引导 Transformer 分词器
authors: "Sheng Yan, Yong Wang, Xin Du, Junsong Yuan, Mengyuan Liu"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.08337v1"
tags: ["query:课题"]
score: 7.0
evidence: 用于高效运动生成的运动离散标记化
tldr: 本研究针对人体动作生成中的离散分词问题，提出了语言引导的Transformer分词器（LG-Tok）。通过在分词阶段引入自然语言对齐，LG-Tok实现了高语义、紧凑的动作表示。该方法利用Transformer的全局注意力机制克服了传统CNN的局限，并在减少分词数量的同时显著提升了动作生成的质量和效率，在多个主流基准测试上达到SOTA水平。
motivation: 传统的动作分词器通常通过增加分词数量来提高重建质量，但这增加了生成模型的学习难度且缺乏高层语义引导。
method: 提出基于Transformer的LG-Tok架构，通过语言与动作的语义对齐以及随机语言丢弃方案，实现高效的语义分词与灵活的解码。
result: 在HumanML3D和Motion-X基准上，LG-Tok在Top-1准确率和FID指标上均显著优于现有SOTA方法，且在分词数减半时仍保持极高竞争力。
conclusion: 语言引导的语义表示能有效降低动作生成的复杂度并提升重建质量，证明了Transformer架构在动作分词任务中的优越性。
---

## 摘要
在本文中，我们专注于动作离散分词，即将原始动作转换为紧凑的离散标记（tokens）——这一过程已被证明对高效的动作生成至关重要。在这一范式中，增加标记数量是提高动作重建质量的常用方法，但更多的标记会增加生成模型的学习难度。为了在保持高重建质量的同时降低生成复杂度，我们提出利用语言来实现高效的动作分词，并将其命名为语言引导分词（Language-Guided Tokenization，简称 LG-Tok）。LG-Tok 在分词阶段将自然语言与动作对齐，从而产生紧凑的高层语义表示。这种方法不仅增强了分词和反分词过程，还简化了生成模型的学习。此外，现有的分词器主要采用卷积架构，其局部感受野难以支持全局语言引导。为此，我们提出了一种基于 Transformer 的分词器，利用注意力机制实现语言与动作之间的有效对齐。此外，我们设计了一种语言丢弃（language-drop）方案，在训练过程中随机移除语言条件，使反分词器在生成阶段能够支持无语言引导。在 HumanML3D 和 Motion-X 生成基准测试中，LG-Tok 的 Top-1 分数分别达到 0.542 和 0.582，优于现有最先进的方法（MARDM：0.500 和 0.528），FID 分数分别为 0.057 和 0.088，而对比方法分别为 0.114 和 0.147。LG-Tok-mini 仅使用一半的标记，同时保持了具有竞争力的性能（Top-1：0.521/0.588，FID：0.085/0.071），验证了我们语义表示的高效性。

## Abstract
In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.
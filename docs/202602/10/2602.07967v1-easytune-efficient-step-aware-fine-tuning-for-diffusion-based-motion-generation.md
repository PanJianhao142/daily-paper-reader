---
title: "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation"
title_zh: EasyTune：基于扩散的运动生成的高效步感知微调
authors: "Xiaofeng Tan, Wanjiang Weng, Haodong Lei, Hongsong Wang"
date: 2026-02-08
pdf: "https://arxiv.org/pdf/2602.07967v1"
tags: ["keyword:MDM", "query:课题"]
score: 8.0
evidence: 基于扩散的运动生成与步进感知微调
tldr: EasyTune 是一种针对扩散运动生成模型的高效微调框架。针对现有基于奖励的对齐方法中存在的递归依赖导致的内存消耗大、效率低等问题，EasyTune 通过解耦去噪轨迹，实现步感知（Step-Aware）的细粒度优化。同时引入自细化偏好学习机制解决数据稀缺问题。实验表明，该方法在提升对齐性能的同时，显著降低了内存开销并大幅提高了训练速度。
motivation: 现有的扩散模型对齐方法受限于去噪轨迹中各步骤间的递归依赖，导致优化效率低且内存消耗巨大。
method: 提出 EasyTune 框架，通过在每个去噪步骤独立进行微调来解耦递归依赖，并结合自细化偏好学习（SPL）动态识别偏好对。
result: "相比 DRaFT-50，EasyTune 在对齐性能上提升了 8.2%，同时仅需约 31.16% 的额外内存开销，训练速度提升了 7.3 倍。"
conclusion: EasyTune 为扩散运动生成模型提供了一种高效、细粒度且内存友好的微调方案，有效解决了偏好对齐中的性能与资源瓶颈。
---

## 摘要
近年来，运动生成模型取得了显著进展，但在与下游目标对齐方面仍面临挑战。最近的研究表明，使用可微奖励直接对齐扩散模型的偏好已取得显著成效。然而，这些方法存在以下问题：（1）优化效率低且粒度粗糙，（2）显存消耗高。在这项工作中，我们首先从理论和实证上确定了这些局限性的关键原因：去噪轨迹中不同步骤之间的递归依赖关系。受此启发，我们提出了 EasyTune，它在每个去噪步骤而非整个轨迹上对扩散模型进行微调。这解耦了递归依赖，使我们能够进行（1）密集且细粒度的优化，以及（2）显存高效的优化。此外，偏好运动对的稀缺限制了运动奖励模型训练的可行性。为此，我们进一步引入了一种自我细化偏好学习（SPL）机制，该机制能够动态识别偏好对并进行偏好学习。大量实验表明，EasyTune 在对齐（MM-Dist）提升方面比 DRaFT-50 高出 8.2%，同时仅需其 31.16% 的额外显存开销，并实现了 7.3 倍的训练加速。项目页面见此链接：{https://xiaofeng-tan.github.io/projects/EasyTune/index.html}。

## Abstract
In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

---

## 论文详细总结（自动生成）

### EasyTune：基于扩散的运动生成的高效步感知微调论文总结

#### 1. 核心问题与整体含义（研究动机和背景）
文本驱动的运动生成（Text-to-Motion）目前主要依赖扩散模型，但这些模型在预训练后往往难以与下游目标（如语义一致性、物理真实感和用户偏好）完全对齐。现有的对齐方法（如可微奖励微调）面临两大瓶颈：
*   **递归依赖导致的资源浪费**：由于去噪轨迹中各步骤相互依赖，计算梯度需要存储整个去噪过程的计算图，导致显存消耗巨大（$O(T)$ 复杂度）。
*   **优化效率低下**：梯度在长链条传递中容易消失，导致早期去噪步骤（通常决定全局结构）无法得到有效优化，且优化信号稀疏。
*   **数据稀缺**：运动领域缺乏大规模的高质量人类偏好标注对，难以训练有效的奖励模型。

#### 2. 论文提出的方法论
EasyTune 的核心思想是**将去噪轨迹解耦，实现步感知（Step-Aware）的独立优化**。
*   **步感知微调（Step-Aware Fine-Tuning）**：不再对整个 $T$ 步轨迹进行反向传播，而是在每个去噪步骤 $t$ 独立计算奖励并更新参数。通过在输入端引入停止梯度（Stop-gradient）操作，打破了步骤间的递归依赖，使显存开销保持为常数（$O(1)$）。
*   **自细化偏好学习（SPL）**：为了在无人工标注的情况下训练奖励模型，SPL 利用预训练的检索模型动态构建偏好对。它将地面真值（Ground-truth）视为“偏好运动”，将检索模型错误匹配的 Top-1 结果视为“非偏好运动”，通过 KL 散度优化奖励模型。
*   **噪声感知奖励感知**：针对中间去噪步骤的模糊状态，对于 ODE 模型采用单步预测（One-step prediction）还原清晰运动再评分，对于 SDE 模型则采用噪声感知奖励模型直接评分。

#### 3. 实验设计
*   **数据集**：HumanML3D 和 KIT-ML（主流的文本-运动基准数据集）。
*   **Benchmark 与对比方法**：
    *   **微调基准**：对比了 DRaFT-10/50、ReFL、AlignProp、DRTune 等最先进的可微奖励微调方法。
    *   **生成基准**：对比了 MLD、MDM、T2M-GPT、MotionDiffuse 等十余种主流运动生成模型。
*   **评估指标**：R-Precision（语义对齐）、FID（生成质量）、MM Dist（多模态距离）、Diversity（多样性）以及显存占用（Memory）。

#### 4. 资源与算力
*   **硬件环境**：实验在一台配备 **NVIDIA RTX A6000 (48GB)** 显存的服务器上完成。
*   **训练效率**：
    *   **显存**：EasyTune 仅需约 22.10 GB 显存，而 DRaFT-50 需要 37.32 GB。
    *   **速度**：在达到相同奖励水平时，EasyTune 比 DRaFT 实现了 **7.3 倍的训练加速**。
    *   **推理**：微调后的模型在推理阶段不增加额外开销。

#### 5. 实验数量与充分性
论文实验设计非常详尽且具有高度的客观性：
*   **广泛的通用性测试**：在 6 种不同的预训练扩散模型架构上验证了有效性。
*   **消融实验**：深入探讨了步骤重加权策略（证明了优化早期步骤的重要性）、K 值敏感性、学习率影响以及噪声感知能力的有效性。
*   **用户研究**：组织了针对 100 个 Prompt 的人类评估，从对齐性、物理保真度和连贯性三个维度对比，证明了方法未发生“奖励作弊（Reward Hacking）”。
*   **附录补充**：提供了长达 18 页的附录，包含收敛性证明、梯度消失的理论推导及更多可视化结果。

#### 6. 主要结论与发现
*   **性能领先**：EasyTune 在 HumanML3D 上的 FID 达到 0.132，比基准模型 MLD 提升了 72.1%，在语义对齐指标上全面超越现有微调方法。
*   **效率突破**：成功将扩散模型微调的显存复杂度从随步数线性增长降低为常数级，解决了长轨迹优化的痛点。
*   **早期步骤至关重要**：实验发现，对去噪初期的步骤进行细粒度优化对最终生成质量的提升远大于后期步骤。

#### 7. 优点
*   **理论与实证结合**：不仅提出了方法，还通过数学推导揭示了现有方法梯度消失的本质原因。
*   **极高的实用性**：大幅降低了微调扩散模型的硬件门槛，且训练速度极快。
*   **无需人工标注**：SPL 机制巧妙地利用检索失败案例进行自我进化，解决了运动数据标注难的问题。

#### 8. 不足与局限
*   **物理约束缺失**：目前的奖励模型主要基于语义对齐，虽然实验显示其具有一定的物理感知力，但并未显式引入物理定律（如重力、碰撞检测）的约束。
*   **偏好对构建的噪声**：SPL 依赖检索模型，如果检索模型本身存在严重偏差，构建的偏好对可能会引入噪声。
*   **应用限制**：主要针对文本驱动的运动生成，在其他模态（如视频生成）上的迁移效果尚待验证。

（完）

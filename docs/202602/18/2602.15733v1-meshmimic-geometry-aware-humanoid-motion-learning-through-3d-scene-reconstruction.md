---
title: "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction"
title_zh: MeshMimic：通过 3D 场景重建实现的几何感知类人运动学习
authors: "Qiang Zhang, Jiahao Ma, Peiran Liu, Shuai Shi, Zeran Su, Zifan Wang, Jingkai Sun, Wei Cui, Jialin Yu, Gang Han, Wen Zhao, Pihai Sun, Kangning Yin, Jiaxu Wang, Jiahang Cao, Lingfeng Zhang, Hao Cheng, Xiaoshuai Hao, Yiding Ji, Junwei Liang, Jian Tang, Renjing Xu, Yijie Guo"
date: 2026-02-17
pdf: "https://arxiv.org/pdf/2602.15733v1"
tags: ["query:课题"]
score: 6.0
evidence: 类人运动学习与合成
tldr: MeshMimic是一个创新的类人机器人运动学习框架，旨在解决传统动捕数据缺乏环境几何信息导致的物理不一致问题。该框架通过单目视频进行3D场景重建，提取人体轨迹与地形几何，并利用运动学一致性优化和接触不变重定向技术，使机器人能从视频中学习复杂的“运动-地形”交互。实验证明，该方法仅需消费级传感器即可在非结构化环境中实现高动态、鲁棒的物理交互，为机器人自主进化提供了低成本路径。
motivation: 传统的类人机器人运动学习依赖昂贵的动捕数据，且因缺乏环境几何上下文，常导致机器人与地形交互时出现穿模或打滑等物理不一致现象。
method: 结合3D视觉模型重建场景与运动，通过运动学一致性优化提取高质量数据，并采用接触不变重定向方法将人机交互特征迁移至机器人。
result: 实验表明，MeshMimic在多种复杂地形上实现了鲁棒且高动态的运动表现，证明了仅利用单目视频即可训练复杂的物理交互行为。
conclusion: 该研究为类人机器人在非结构化环境中的自主进化提供了一种低成本、可扩展且具备几何感知能力的视觉驱动学习方案。
---

## 摘要
近年来，类人机器人运动控制取得了重大突破，深度强化学习（RL）已成为实现复杂类人行为的主要催化剂。然而，类人机器人的高维数和复杂的动力学特性使得手动运动设计变得不切实际，导致其严重依赖昂贵的运动捕捉（MoCap）数据。这些数据集不仅获取成本高昂，而且往往缺乏周围物理环境必要的几何上下文。因此，现有的运动合成框架通常面临运动与场景解耦的问题，导致在地形感知任务中出现接触打滑或网格穿透等物理不一致现象。在这项工作中，我们提出了 MeshMimic，这是一个连接 3D 场景重建与具身智能的创新框架，使类人机器人能够直接从视频中学习耦合的“运动-地形”交互。通过利用最先进的 3D 视觉模型，我们的框架能够精确地分割并重建人体轨迹以及地形和物体的底层 3D 几何结构。我们引入了一种基于运动学一致性的优化算法，从噪声视觉重建中提取高质量的运动数据，并结合一种接触不变的重定向方法，将人与环境的交互特征迁移到类人智能体上。实验结果表明，MeshMimic 在各种具有挑战性的地形中均实现了鲁棒且高动态的性能。我们的方法证明，仅利用消费级单目传感器的低成本流水线即可促进复杂物理交互的训练，为非结构化环境下类人机器人的自主进化提供了一条可扩展的路径。

## Abstract
Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled "motion-terrain" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.
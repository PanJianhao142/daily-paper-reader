Title: Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr\"odinger Bridge Matching

URL Source: https://arxiv.org/pdf/2602.15396v1

Published Time: Wed, 18 Feb 2026 01:28:21 GMT

Number of Pages: 15

Markdown Content:
# Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

Jeongwoo Shin 1 Jinhwan Sul 2 Joonseok Lee † 1 Jaewoong Choi † 3 Jaemoo Choi † 2

# Abstract 

Diffusion models often yield highly curved trajec-tories and noisy score targets due to an uninfor-mative, memoryless forward process that induces independent data-noise coupling. We propose Ad-joint Schr ¨odinger Bridge Matching (ASBM) , a generative modeling framework that recovers opti-mal trajectories in high dimensions via two stages. First, we view the Schr ¨odinger Bridge (SB) for-ward dynamic as a coupling construction problem and learn it through a data-to-energy sampling per-spective that transports data to an energy-defined prior. Then, we learn the backward generative dy-namic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces sig-nificantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fi-delity with fewer sampling steps. We further show-case the effectiveness of our optimal trajectory via distillation to a one-step generator. 

# 1. Introduction 

Generative modeling aims to sample from a data distribution 

pdata by transforming a simple prior distribution pprior (e.g. ,Gaussian). Diffusion models achieve this by learning con-tinuous dynamic based on Stochastic Differential Equation (SDE) that connect pprior and pdata (Song et al., 2020b; Ho et al., 2020; Song et al., 2020a). While highly successful, these methods face two significant limitations (Song et al., 2020b; Karras et al., 2022). First, the learned trajectories are often highly curved, requiring a large Number of Func-tion Evaluations (NFEs) at sample generation. Second, their 

> 1

Seoul National University 2Georgia Institute of Technol-ogy 3Sungkyunkwan University. Correspondence to: Jaemoo Choi <jaemoo.choi@gatech.edu >, Jaewoong Choi <jaewoong-choi@skku.edu >, Joonseok Lee <joonseok@snu.ac.kr >.

Preprint. February 18, 2026. -10 -5 0 5 10 -10          

> -5 0510
> -10
> -5
> 0
> 5
> 10 -10 -5 0510 -10
> -5 0510
> -10
> -5
> 0
> 5
> 10

Figure 1. Generation trajectory of score matching and ASBM. 

Top : Backward drift accumulated over time in pixel level. Bottom :Denoising path in image level. ASBM shows significantly smaller transport cost with straighter path, leading to efficient generation. 

training objectives typically rely on independent endpoint pairing (X0, X 1) ∼ pdata ×pprior , which yields noisy training targets and slow convergence. This independent coupling is induced by the memoryless forward process (Domingo-Enrich et al., 2024) (cf. Eq. (8)). Optimal Transport (OT) (Villani et al., 2008; Peyr ´e et al., 2019) provides a powerful alternative by seeking an optimal coupling between X0 ∼ pdata and X1 ∼ pprior that min-imizes a transport cost. Among various OT formulations, the Schr ¨odinger Bridge (SB) problem (Schr ¨odinger, 1931; L ´eonard, 2013; Chen et al., 2016; 2021b; De Bortoli et al., 2021) is particularly relevant to diffusion models, because the optimal coupling is represented by a pair of consistent forward-backward SDE dynamics. Unlike the trivial inde-pendent coupling in standard diffusion, the cost-minimizing principle of SB induces the shortest path between pdata and 

pprior . By generalizing the memoryless forward process into the non-memoryless regime, SB can achieve optimal trajec-tories that are straighter than those obtained from indepen-dent endpoint pairing, leading to less NFEs for generation. However, realizing the non-memoryless SB in high-dimensional settings, such as images, remains challeng-1

> arXiv:2602.15396v1 [cs.CV] 17 Feb 2026 Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching

ing. Existing SB-inspired generative methods (Chen et al., 2021a; Deng et al., 2024) often resort to independent pairing 

(X0, X 1) ∼ pdata × pprior , or perform auxiliary pretraining with empirical bridge matching (Shi et al., 2023), limiting primary benefits of the optimal trajectories. Furthermore, they typically use forward–backward alternating training, requiring bidirectional trajectory rollouts to supervise each other. In practice, this bidirectional supervision is noisy and often leads to inconsistent dynamics that do not corre-spond to a single optimal path measure, which weakens the intended OT property and reduces sampling efficiency. In this paper, we introduce Adjoint Schr ¨odinger Bridge Matching (ASBM) , an SB-based generative modeling framework that efficiently learns organized trajectories un-der the informative endpoint couplings with highly sta-ble convergence. We decompose generative modeling via Schr ¨odinger Bridges into two simple subproblems: (i) con-structing the endpoint optimal coupling using data-to-prior forward dynamic, and (ii) optimizing the backward dynamic with a simple matching loss supervised by the resulting op-timal coupling. At stage (i), we view the SB problem as a Stochastic Optimal Control (SOC)-based sampling prob-lem (Zhang & Chen, 2021; Vargas et al., 2023; Havens et al., 2025; Liu et al., 2025), which learns the optimal control that transports initial data pdata to desired Boltzmann distribution, 

i.e. , unnormalized density with a known energy function. Since forward dynamic in SB is the transport from sam-plable distribution, i.e. , pdata , to energy-known pprior , e.g. ,Gaussian, the problem can be reformulated as a data-to-energy sampling problem and efficiently solved. At stage (ii), we use the learned forward process to obtain the opti-mal SB coupling (X0, X 1) and train the backward dynamic with a simple matching loss. Interestingly, our approach also recovers the standard diffusion model as a special case with a specific forward dynamic which yields independent endpoint coupling without training (See Sec. 3.1). Our design brings three key advantages. First, ASBM re-quires only the forward simulation at training. Since it transports from data to a simple prior, it yields stable and fast optimization, requiring dramatically fewer NFEs ( e.g. ,20 vs. 100–200 in prior work) to construct endpoint cou-plings, sufficiently with a lighter network. This stable for-ward training produces higher-quality optimal couplings, enabling more informative supervision for backward dy-namic. Second, since optimal coupling is induced by learned forward dynamic, we can optimize backward dynamic via simple matching loss which converges fast with high stabil-ity. Third, ASBM’s straighter trajectory results in improved generative performance with lower NFE compared to prior SB-based generative models and diffusion model. We fur-ther showcase the effectiveness of our efficient trajectory via distillation to one-step generator, with better performance and mode coverage. Our contributions are three-fold: • We propose Adjoint Schr ¨odinger Bridge Matching (ASBM) , which learns optimal trajectory in significantly 

efficient and stable manner through our novel perspec-tive on Schr¨ odinger Bridge optimization. • ASBM achieves superior performance over diffusion model and prior SB methods on image generation with faster sampling (low NFE) and better fidelity. • Leveraging the efficient trajectory of ASBM, we im-prove the sample quality and mode coverage in distilla-tion task, compared to score-based distillation. 

# 2. Background 

Diffusion Models (DMs) (Song et al., 2020b; Ho et al., 2020) learn to sample from a target data distribution pdata 

by reversing a fixed forward noising process. This forward process is designed to describe the stochastic dynamic from the data distribution pdata to the prior distribution pprior , typi-cally a Gaussian. Specifically, consider a forward Stochastic Differential Equation (SDE) for Xt ∈ Rd:

dXt = f DM  

> t

(Xt) d t + σDM  

> t

dWt, X0 ∼ pdata , (1) where f DM : [0 , 1] × Rd → Rd is the base drift and σDM :[0 , 1] → R>0 is the noise schedule. This forward SDE has a corresponding reverse-time SDE (Song et al., 2020b) that follows the same stochastic dynamic backward in time: 

dXt = f DM  

> t

(Xt) − σDM 

> t
> 2

∇ log pt(Xt)dt + σDM  

> t

dWt,

for X1 ∼ p1, where ∇x log pt(x) is a marginal score and 

p1 ≈ pprior under a proper noise schedule in Eq. (1). DMs estimate this marginal score via conditional score match-ing (Song et al., 2020b; Ho et al., 2020) and generate sam-ples by simulating the reverse SDE with an approximate score st starting from pprior :

min  

> s

E pt|0, X 0∼pdata 

h

st(Xt) − ∇ xt log p(Xt | X0) 2i

, (2) where s : [0 , 1] × Rd → Rd is a score function to approxi-mate the marginal score. 

Schr¨ odinger Bridge. Consider a controlled SDE as 

dXt = ft(Xt) + σtuθt (Xt) dt + σt dWt, X 0 ∼ pdata , (3) where uθ : [0 , 1] × Rd → Rd is a parameterized forward control. pu is the path measure induced by controlled SDE in Eq. (3), and the base path measure pbase is induced by the uncontrolled base SDE, i.e. , by setting u ≡ 0 in Eq. (3). Schr ¨odinger Bridge (SB) problem finds a path measure pu

that matches both endpoint marginals, pdata and pprior , while minimizing the KL divergence relative to a reference base 2Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

process pbase (Schr¨ odinger, 1931; L´ eonard, 2013): 

min  

> u

DKL 

 pu ∥ pbase  =E pu

Z 10

12 ∥uθt (Xt)∥2 dt 



, (4) subject to Eq. (3) and X1 ∼ pprior . The optimal bridge admits a reverse-time SDE representation with the same boundary constraints: 

dXt = ft(Xt)−σtvϕt (Xt) dt+σt dWt, X 1 ∼ pprior , (5) where vϕt (x) : [0 , 1] × Rd → Rd is a parameterized back-ward control. Optimal path measure p⋆ can be induced by both direction with optimal controls u⋆t or v⋆t .

Reciprocal Property of SB. Optimal path measure p⋆ fol-lows a reciprocal process (L´ eonard et al., 2014): 

p⋆(Xt) = pbase (Xt|X0, X 1) p⋆(X0, X 1). (6) This representation indicates that the optimal path measure is characterized by the optimal coupling p⋆(X0, X 1). Given the coupling of two terminal distributions, the intermediate distribution can be constructed from the base process. 

# 3. Method 

We introduce a generative model that generalizes the stan-dard Diffusion Models (DMs) into a non-memoryless regime by leveraging the Schr ¨odinger Bridge (SB) problem. Our approach addresses the fundamental inefficiencies of diffusion models, i.e. , curved trajectories and noisy training targets, by explicitly recovering optimal transport couplings. 

3.1. Diffusion Models as Memoryless SB Bridge Matching. As shown in Shi et al. (2023), given the optimal coupling (X0, X 1) ∼ p⋆

> 0,1

, we could obtain the optimal backward control v⋆t by solving 

min  

> ϕ

E pbase  

> t|0,1p⋆
> 0,1

h

vϕt (Xt) − σt∇ log pbase  

> t|0

(Xt | X0) 2i

. (7) This property implies that if the optimal joint distribution 

p⋆(X0, X 1) is accessible, we can optimize the backward control vϕt via bridge matching (Liu et al., 2023b). 

Connection between DMs and SB. In DMs, we set the base drift and diffusion term (f DM , σ DM ) so that the X1 sampled from pbase  

> 1|0

(·| X0) is almost independent to X0, i.e. ,

pbase  

> 0,1

(X0, X 1) memoryless 

:= pbase  

> 0

(X0) pbase  

> 1

(X1). (8) We denote this condition as a memoryless condition , and underlying dynamics as a memoryless dynamics . Then, the following theorem holds: 

Proposition 3.1. If the base path measure pbase is memory-less, then the optimal path measure p⋆ of the SB problem is also memoryless, i.e., 

p⋆(X0, X 1) memoryless 

= pdata (X0) pprior (X1). (9) Consequently, under the memoryless condition, the expecta-tion in Eq. (7) reduces to 

pbase  

> t|0,1

· p⋆ 

> 0,1

⇔ pbase  

> t|0,1

· pprior · pdata ⇔ pbase  

> t|0

· pdata , (10) which is exactly the same form as the score matching objec-tive in Eq. (2). This indicates that the diffusion model is a special case of SB, where the forward dynamic in Eq. (1) is a fixed memoryless base dynamic. 

Limitation of Memoryless Base SDE. This interpretation shows the fundamental inefficiency of the score matching. In the memoryless regime, the injection of massive noise makes the matching target ∇xt log pbase (Xt | X0) highly stochastic. This leads to a slow convergence and highly curved backward path, leading to numerous function evalua-tions for generating high-quality samples (Lipman et al., 2022; Karras et al., 2022). In other words, as endpoint couplings are independent from each other, it is not infor-mative to learn effective generation path. Therefore, we propose a method to obtain an informative optimal cou-pling p⋆(X0, X 1) to more efficiently supervise our back-ward bridge matching to learn straighter generation path. 

3.2. Adjoint Schr¨ odinger Bridge Matching 

To break through the limitations of the memoryless dy-namics, we adopt a non-memoryless base SDE to induce informative optimal couplings, and thereby learn efficient generation trajectory. Our core contribution is a decoupled optimization of forward-backward dynamics in a two-stage process: 1) Optimal Coupling Construction for the for-ward process by a novel interpretation of it as a data-to-energy sampling problem, and 2) Backward Dynamic Opti-mization via simple matching loss using reciprocal process 

under optimal coupling p⋆(X0, X 1).

Optimal Coupling Construction. Since non-memoryless base SDE no longer transports X0 to pprior on its own, we need to optimize additional forward control uθt in Eq. (3) to enforce the terminal marginal pprior . Stochastic Optimal Control (SOC)-based sampling problem (Zhang & Chen, 2021; Vargas et al., 2023; Havens et al., 2025; Liu et al., 2025) finds the optimal control which tilts the base SDE to transport a samplable distribution to a target Boltzmann distribution, which is known up to its energy function. Our novel perspective is that, within the generative model-ing framework, the forward dynamic in the SB problem (3) can be seen as a data-to-energy sampling problem. In this viewpoint, the forward process transports from an empiri-cal distribution pdata to an energy-known distribution pprior ,3Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

e.g. , Gaussian. This is highly beneficial because the energy gradient provides a dense, point-wise characterization of 

pprior , whereas empirical supervision relies on finite sam-ples and can only specify the target distribution through sparse Monte Carlo estimates. This reformulation allows us to isolate the coupling construction part from the unstable alternating optimization of forward-backward system. Then, our first goal is finding the optimal control u⋆t of sam-pling problem. Under the SB optimality (Pavon & Wakol-binger, 1991; Chen et al., 2021c; Caluya & Halder, 2021), the optimal controls can be characterized by 

u⋆t (x) = σt∇x log φt(x), v⋆t (x) = σt∇x log ˆ φt(x), (11) where φt, ˆφt ∈ C1,2([0 , 1] , Rd) are SB potentials satisfying 

φt(x) = 

Z

pbase  

> 1|t

(y | x) φ1(y) dy, φ0(x) ˆ φ0(x) = pprior (x),

ˆφt(x) = 

Z

pbase  

> t|0

(x | y) ˆ φ0(y) dy, φ1(x) ˆ φ1(x) = pdata (x).

Optimal controls in Eq. (11) are typically obtained by al-ternating optimization of φt and ˆφt (Fortet, 1940; Kull-back, 1968; Cuturi, 2013; Shi et al., 2023). However, since we focus on data-to-energy sampling framework, we only 

need the forward optimal control u⋆t , which can be learned by alternating Adjoint Matching (AM) (12) and Corrector Matching (CM) (13) (Liu et al., 2025): 

min  

> θ

E pbase 

> t|0,1, p ¯uθ
> 0,1



uθt (Xt) +  σt∇E + ¯ vϕ

> 1

(X1) 2

, (12) 

min  

> ϕ

E p¯uθ

> 0,1



vϕ 

> 1

(X1) − σ1∇x1 log pbase (X1 | X0) 2

, (13) where ¯u = stopgrad( u) and ¯v = stopgrad( v), and we define the energy E by pprior ∝ exp ( −E(x)) . Note that CM is same as bridge matching only at t = 1 . See Appendix B for more details. By training forward dynamic via SOC framework without any reliance on backward dynamic, we can optimize the forward control to approximate the optimal coupling with 

high stability . This also allows us to simulate endpoint pairs via only the forward dynamic, which has various advantages. Since forward control learns dynamic from complicated data space to simple prior, it is much easier to learn compared to backward dynamic, leading to lighter model capacity 

(Appendix D) with fast convergence (Sec. 4.5). Most impor-tantly, this high stability of our optimization scheme allows us to adopt non-memoryless base SDE which requires more complicated training compared to the memoryless one. Upon convergence, our forward dynamic (3) approximates optimal couplings p⋆(X0, X 1). It is important to note that our p⋆(X0, X 1) is optimal coupling , since our framework minimizes the transport cost (4) while considering the non-trivial correlation between X0 and X1 via the non-memoryless condition. Intuitively, since non-memoryless 

Algorithm 1 ASBM optimization with VP base SDE 

Require: X0 ∼ pdata ; pprior (x) ∝ e−E(x); forward control 

uθt (·), backward control vϕt (·); steps N1, N 2.1: Base SDE: ft(x) := − 12 βtx, σ t := √βt,

with βt := (1 − t)βmax + tβ min .2: Reciprocal sampler: Given 

κt := exp   − 12

R 1 

> t

βτ dτ , ¯κt := exp   − 12

R t 

> 0

βτ dτ ,

Xt ∼ pbase 

> t|0,1

(· | X0, X 1) = N (μt, ΣtI) where, 

μt = ¯κt(1 − κ2 

> t

)1 − ¯κ21

X0 + κt(1 − ¯κ2 

> t

)1 − ¯κ21

X1,

Σt = (1 − κ2 

> t

)(1 − ¯κ2 

> t

)1 − ¯κ21

.

3: Stage 1: optimize uθt via SOC. 4: for i = 1 to N1 do 

5: Update θ via Eq. (12). 6: Update ϕ at t=1 via Eq. (13). 7: end for 

8: Stage 2: optimize vϕt via BM with reciprocal sampler, under p¯uθ

> 0,1

(X0, X 1) ≈ p⋆

> 0,1

(X0, X 1).

9: for i = 1 to N2 do 

10: Update ϕ via Eq. (14). 11: end for 

base SDE injects smaller noise compared to the forward process in standard diffusion, i.e. , σt ≪ σDM  

> t

, transport cost minimization leads to significantly straighter trajectories, allowing considerably low NFEs (Sec. 4.4) for simulating endpoint pair. This property is fundamentally unattainable under the standard memoryless forward dynamic in diffu-sion models, relying on independent endpoint pairings. 

Backward Dynamic Optimization. Given the optimal joint puθ

(X0, X 1) induced by the optimized forward dy-namic (3) , we supervise our backward dynamic by bridge matching in Eq. (7) under these couplings: 

min  

> ϕ

E pbase 

> t|0,1, p ¯uθ
> 0,1

h

vϕt (Xt) − σt∇xt log pbase (Xt | X0) 2i

. (14) With direct supervision under an optimal coupling, back-ward training converges much faster than memoryless dif-fusion baselines. Moreover, the two-stage design enables principled use of the reciprocal process , which is exact only when the optimal endpoint coupling p⋆(X0, X 1) is avail-able. In contrast, prior methods typically lack in optimal coupling and thus rely on alternating forward-backward op-timization with reciprocal process of imperfect endpoints. Our full algorithm is described in Algorithm 1. 

Advantages of ASBM. Previous methods (De Bortoli et al., 2021; Chen et al., 2021a; Shi et al., 2023; Liu et al., 2022a; 2023a; Chen et al., 2023) address the SB problem by al-ternating the optimization of forward and backward dy-namics, repeatedly generating trajectories using the current forward (resp. backward) model to provide supervision for updating the backward (resp. forward) dynamic. Despite extensive prior work, scaling such alternating schemes to high-dimensional settings remains challenging. These ap-4Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

proaches implicitly assume that trajectories generated by one direction provide sufficiently informative supervision for optimizing the opposite direction. However, in genera-tive modeling, learning the backward dynamic, from a sim-ple prior to a complex data distribution, is particularly diffi-cult. As a result, inaccurate trajectories learned at training can destabilize the optimization of the counterpart dynamic and lead to error accumulation, ultimately yielding mis-matched forward and backward processes and non-optimal couplings (see Sec. 4.2). To mitigate this, prior methods either resort to memoryless base dynamics, which dimin-ishes the benefits of SBs, or rely on pretraining stages using independent couplings, which do not fully align with the theoretical formulation and may lead to practical instability. In summary, our two-stage optimization completely resolves these issues. By isolating the forward optimization as data-to-energy sampling problem, we obtain optimal coupling with (i) high stability, (ii) low NFEs, and (iii) light model capacity under (iv) non-memoryless condition. Our back-ward optimization also becomes considerably simpler and more stable through the bridge matching loss with exact reciprocal process under optimal coupling. 

3.3. Distillation to One-Step Generator 

To further verify the effectiveness of the optimal trajectory of our method, we introduce a data-free distillation method for one-step generator within the SB framework. This part demonstrates the inherent strengths of our approach: since the learned generative paths are significantly more organized than those in standard diffusion models, ASBM provides a more suitable and efficient foundation for distillation. 

Distillation in Control Space. We aim to distill our learned backward control vϕt to a one-step generator Gψ : Rd → Rd.Upon successful training, we are given the learned back-ward controlled SDE (5) , which approximately reaches the data marginal pdata at t = 0 . We denote the path measure induced from the backward dynamic as pϕ. Let 

pψ 

> 0,1

:= Law  (Gψ (X1), X 1) (15) denote the joint distribution induced by X1 ∼ pprior and 

z ∼ N (0 , I ), where Gψ is a one-step generator mapping the prior sample X1 to a data sample X0 = Gψ (X1).We further extend the notation pψ 

> 0,1

to a full path measure 

pψ on [0 , 1] × Rd, which represents an (implicit) stochastic process whose endpoint coupling is given by pψ

> 0,1

. Our goal is to match this path measure with a target path measure pϕ,by minimizing the path-space KL divergence DKL (pψ ∥ pϕ).In particular, achieving pψ ≈ pϕ ensures that the induced marginal satisfies pψ 

> 0

= pdata , i.e. , Gψ successfully trans-ports the prior distribution to the data distribution. Since the path measure pψ induced by Gψ is not explic- 

> Figure 2. Generated samples from ASBM on pixel space (CIFAR-10) and on latent space (FFHQ).

itly tractable, we introduce a control-based parametrization to approximate it. Specifically, we consider a controlled diffusion whose path measure pξ is induced by 

dXt = ft(Xt) − σtvξt (Xt) dt + σtdWt, X1 ∼ pprior , (16) where vξ : [0 , 1] × Rd → Rd is a parameterized control. The role of vξ is to parameterize pξ that approximates the implicit path measure pψ . As a result, we need to alternately optimize Gψ and vξ to reflect continuously changing pψ

> 0,1

.Given the current one-step generator Gψ , we aim to learn control vξ , such that the corresponding path measure pξ

correctly estimates pψ . Given endpoint pairs (X0, X 1) ∼

pψ

> 0,1

, we utilize bridge matching to update vξ :

min  

> ξ

E pbase 

> t|0,1, p ψ
> 0,1

h

vξt (Xt) − σt∇ log pbase  

> t|0

(Xt | X0) 2i

. (17) This objective encourages vξ to approximate the score of the base bridge conditioned on X0, thereby matching the induced path measure pξ to pψ .Finally, we update the generator Gψ by minimizing the discrepancy between the learned control vξ and the target control vϕ. Concretely, we solve 

min  

> ψ

Epbase 

> t|0,1, p ψ
> 0,1

 ¯vξt (Xt) − ¯vϕt (Xt) 2, (18) which can be derived from a KL minimization that aligns the pξ to pψ by Girsanov’s theorem (S ¨arkk ¨a & Solin, 2019) (See Appendix C for detailed derivation). With memoryless forward base process, our SB distillation recovers the score distillation (Poole et al., 2022), following Eq. (10). 

Initialization of Gψ . Following the score distillation (Yin et al., 2024b), we initialize Gψ from pretrained backward control vϕt via Tweedie’s formula (Efron, 2011) at t = 1 :

Gψ (x) = x + (1 − ¯κ21)vϕ 

> 1

(x)/σ 1

¯κ1

, (19) with notation as in Algorithm 1. For diffusion score models, this one-step estimate often produces noisy images due to its 5Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching                                    

> Method OC No PT FB FID ↓
> Refinement method
> DOT (Tanaka, 2019) ✓✗–15.78 DGFLOW (Ansari et al., 2020) ✓✗–9.63
> Memoryless SDE
> Score SDE (Song et al., 2020b) ✗✓✓4.61 SB-FBSDE (Chen et al., 2021a) ✗✗✗5.26 VSDM (Deng et al., 2024) ✗✗✗4.24
> Non-Memoryless SDE
> DSBM (Shi et al., 2023) ✓✗✗9.68
> ASBM (Ours) ✓✓✓3.16
> Table 1. FID evaluation on CIFAR-10. ASBM shows superior generative performance by achieving all three key advantages: optimal coupling (OC), no reliance on pre-training (No PT), and consistent forward–backward dynamics (FB).

highly stochastic trajectory, typically mitigated by timestep shifting (Yin et al., 2023), which introduces bias. In contrast, ASBM’s straighter path makes this initialization reliable without timestep shifting. See Appendix C for illustration. 

Advantages of ASBM Distillation. ASBM not only yields a straighter path, but also constructs highly organized trajec-tories connecting adjacent pairs (X0, X 1). This is a special characteristic of non-memoryless SB originated from its 

non-memoryless optimal coupling (Proposition 3.1). Specif-ically, covering the entire prior space with non-memoryless trajectories leads to a more efficient connection from the spe-cific mode in data space to the specific nearby area in pprior .In other words, both puθ 

> 1

(X1 | X0) and pvϕ 

> 0

(X0 | X1) have lower variance than the case of memoryless process (See Sec. 4.2 for demonstrations). Together with straightness, it has strong benefits to distillation in the aspect of learning difficulty and mode coverage compared to the memoryless trajectory of diffusion models. 

# 4. Experiments 

We validate the generative performance and efficiency of ASBM in Sec. 4.1, and analyze its optimal trajectory in Sec. 4.2. Then, we verify effective distillation of ASBM in Sec. 4.3. Finally, we explore the main hyperparameters through the ablation study in Sec. 4.4 and compare the training cost to score matching in Sec. 4.5. 

Dataset & Baselines. We compare our ASBM against Score SDE (Song et al., 2020b) as a representative instantiation of memoryless diffusion and prior SB-based frameworks on the pixel space of CIFAR-10 (Krizhevsky et al., 2009) with FID (Heusel et al., 2017) metric. Then, we further verify ASBM in the LDM (Rombach et al., 2022) framework using the Stable Diffusion 3 autoencoder (Esser et al., 2024) on FFHQ (Karras et al., 2019). For distillation task, we report recall and precision metrics (Kynk ¨a ¨anniemi et al., 2019) to evaluate mode coverage, together with FID.                      

> Figure 3. FID comparison along the NFE. We use M and NM to denote the memoryless and non-memoryless condition, respec-tively. BM denotes empirical bridge-matching pretraining. NFE 25 50 100 250 500 1000 Score SDE 52.08 19.02 9.84 7.79 6.88 6.63
> ASBM (Ours) 8.85 7.64 6.85 6.47 6.38 6.27
> Table 2. FID evaluation on latent space of FFHQ.

Experimental Setup. We adopt the Variance Preserving (VP) path (Song et al., 2020b) with non-memoryless setting as the base SDE for ASBM. For coupling generation at training, we use the Euler-Maruyama solver (Kloeden, 2011) with 20 NFE for CIFAR-10 and 50 NFE for FFHQ. More details can be found in Appendix D. 

4.1. Image Generation Performance Generation on Pixel Space. Tab. 1 reports FID on CIFAR-10 at 100 NFE using each method’s reported solver. ASBM significantly outperforms all baselines by achieving the ef-ficient and straight trajectory under optimal coupling. As discussed in Sec. 3.1, memoryless base SDEs (Score SDE, SB-FBSDE and VSDM) induce independent coupling at optimal state and thus fail to yield an informative optimal coupling. While DSBM relaxes this via a non-memoryless condition, it remains difficult to scale to high-dimensional data even with an additional pretraining stage. We ablate SB-FBSDE with non-memoryless process and DSBM without pretraining to evidently show these limita-tions in Fig. 3. (i) SB-FBSDE performs well only in the memoryless regime but substantially degrades under the non-memoryless setting, suggesting it cannot yield optimal coupling in high dimensions. (ii) DSBM works well only with empirical bridge matching pretraining, incorrectly as-suming independent endpoint coupling as optimal one with the non-memoryless settings. Even with pretraining, DSBM shows unstable and inferior FID scores across various NFEs. In contrast, our ASBM achieves significantly lower FID with low NFE (20-100), implying its efficient trajectory. 

Generation in LDM Framework. We verify generalizabil-ity of ASBM on latent space of FFHQ in Tab. 2. Consistent with the pixel-space results, ASBM achieves substantially lower FID at small NFE and maintains superior FID across the entire NFE range compared to Score SDE. 6Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching             

> Figure 4. Trajectory efficiency. Left : Ours shows significantly straighter trajectory, leading to low NFE at generation. Right : Ours has lower trajectory variance, implying its better organized path. Method Score SDE SB-FBSDE DSBM ASBM
> FID 6.72 285.77 39.84 3.74
> Table 3. FID at 25 steps with Heun solver on CIFAR-10.

4.2. Optimal Trajectory of ASBM 

We further analyze strength of our trajectory through path straightness/variance, and forward-backward consistency. 

Trajectory Straightness. We measure the path straightness via the trajectory functional 

S(X0:1 ) := 

PT −1 

> i=0

∥X(i+1) /T − Xi/T ∥22

∥X1 − X0∥22

, (20) from a T -step trajectory {Xi/T }Ti=0 . We use 10K trajec-tories generated with T = 100 steps. As shown in Fig. 4 (left), ASBM yields substantially smaller S than Score SDE, confirming that our non-memoryless coupling produces straighter generation trajectories. 

Trajectory Variance. As discussed in Sec. 3.3, non-memoryless SB is expected to have localized coupling ,which can be demonstrated by low trajectory variance where most mass of pvϕ 

> 0

(X0|X1 = x1) concentrates more on its centroid. To verify this property, for each of 10K initial noises, we generate 10 images and compute the average 

ℓ2 distance of these images to their centroid. As shown in Fig. 4 (right), ASBM exhibits a notably stronger concentra-tion, indicating its strongly organized trajectory. We verify this property by inversion test for further intuition. Starting from a fixed image X0, we sample X1 ∼ puθ

(· | 

X0) using our forward SDE in Eq. (3), then reconstruct bX0

by running the backward SDE in Eq. (5) from X1. As shown in Fig. 5, ASBM recovers bX0 that remains highly similar to original X0, whereas Score SDE produces a random recon-struction, consistent with its memoryless dynamic. These results clearly indicate that ASBM’s trajectory is not only straighter but also highly organized under non-memoryless process, which in turn simplifies distillation. 

Forward-Backward Consistency. Finally, we evaluate the consistency of forward-backward dynamics. For an exact                   

> Figure 5. Localized prior-data coupling. ASBM trajectories pre-serve information: reversing from a noised image produces samples similar to the original. In contrast, memoryless dynamics yield completely random samples due to highly noisy trajectories. Method FID ↓Recall ↑Precision ↑
> SDS (Poole et al., 2022) 9.36 0.504 0.706 DMD (Yin et al., 2024b) 8.25 0.513 0.715 Ours 6.68 0.542 0.702
> Table 4. Result of distillation to one-step generator on CIFAR-10.

SB solution, the optimal bridge is unique, implying compat-ible forward-backward dynamics that share the same time marginals. We quantitatively evaluate this property through generation via the Heun’s method (Ascher & Petzold, 1998; Karras et al., 2022), based on probability flow ODE (Song et al., 2020b) requiring precisely coupled forward-backward dynamics. Tab. 3 verifies the limitation of prior works by showing notable degradation of these baselines with Heun’s method, whereas ASBM remains robust with a few steps, highlighting the benefit of our two-stage optimiza-tion. This is because, in practice, alternating optimization in SB-FBSDE and DSBM can be unstable in high dimensions and each direction provides inconsistent trajectory supervi-sion to its counterpart optimization, eventually producing mismatched forward-backward systems. 

4.3. Distillation to One-Step Generator 

We showcase the ASBM’s efficient trajectory via distillation to one-step generator (Sec. 3.3) on CIFAR-10, comparing it to score-based distillation baselines: Score Distillation Sampling (SDS) (Poole et al., 2022) and Diffusion Matching Distillation (DMD) (Yin et al., 2024b), which augments SDS with a regression loss to mitigate mode collapse. As shown in Tab. 4, ASBM outperforms prior score-distillation models. Notably, the improved recall indicates substantially reduced mode collapse , avoiding the need of costly regression loss employed in DMD. This regression loss requires a large number of noise-image pairs gener-ated from the original score model ( e.g. , 500K pairs). We attribute the superiority of ASBM’s distillation to the local-ized prior–data coupling, i.e. , efficiently organized trajectory induced by our optimal coupling, which provides more in-formative guidance and better covers diverse data modes. 7Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching  

> Figure 6. Ablation on different degree of memorylessness.

4.4. Ablation Study 

We investigate the degree of memorylessness and the effects of forward NFE on CIFAR-10. 

Memorylessness. For the VP base process (Algorithm 1), a larger βmax injects more noise, and pushes the base SDE toward the memoryless process; for example, βmax = 20 

recovers the standard memoryless diffusion setting. We ab-late memorylessness by varying βmax . As shown in Fig. 6, 

βmax controls the trade-off between training efficiency and prior coverage. When βmax is too small ( e.g. , βmax = 1 ), the path becomes straighter and training is efficient, but the terminal distribution puθ 

> 1

under-covers pprior even after suc-cessful optimization, because pbase  

> 1

is too different from pprior .This eventually leaves low-density holes in puθ 

> 1

, resulting in worse FID with fine-grained NFE. Conversely, a larger 

βmax (e.g. , βmax = 8 ) improves coverage of the prior space, but the increased noise injection leads to more curved paths, making accurate simulation difficult with limited NFEs. We therefore use βmax = 4 by default as a favorable trade-off between informative coupling and robust prior matching. 

Forward NFE. Forward NFE largely determines the train-ing cost. Prior SB methods (Chen et al., 2021a; Shi et al., 2023) typically require 100–200 NFEs to alternately sim-ulate forward-backward dynamics, which dominates run-time, while ASBM only uses lightweight forward simula-tion, achieving strong performance with just 20 NFEs. As shown in Tab. 5, ASBM remains strong even with 10 NFEs, and the negligible gap between 20 and 50 NFEs supports our hypothesis that the data-to-energy forward optimization is easier, enabling accurate couplings with fewer NFEs. 

4.5. Training Efficiency 

Our backward optimization converges significantly faster, 

i.e. , 600 epochs for ASBM and 3300 epochs for Score SDE, due to its supervision under optimal coupling . As discussed in Sec. 3.2, our forward dynamic is also lightweight due to its data-to-energy direction, converging with the cost same as 150 backward epochs. Considering the cost of coupling construction, the total training cost of ASBM is equivalent to 2100 Score SDE epochs, yielding a 0.64 × reduced com-putation relative to Score SDE.                           

> Forward Backward NFE NFE 25 50 100 250 500 1000 10 23.62 8.03 3.58 3.40 3.28 3.05 20 20.83 5.39 3.05 2.91 2.87 2.74
> 50 20.73 5.87 3.16 3.01 2.94 2.77
> Table 5. Ablation on different forward NFE.

# 5. Related Work 

Generative Models. Dynamic-based generative models have become a major paradigm in generative tasks (Ho et al., 2020; Song et al., 2020a;b; Lipman et al., 2022; Liu et al., 2022b; Albergo et al., 2023; De Bortoli et al., 2021). In order to improve the efficiency of these models, research efforts devoted to reducing the NFE of these models, e.g. ,different dynamic solvers (Lu et al., 2022; 2025; Zhang & Chen, 2022; Karras et al., 2022), or rectifying the trajec-tories (Liu et al., 2022b; 2024). However, these works are based on post-refinement on the top of the noisy diffusion path. To obtain the optimal trajectory, study on dynamic optimal transport has been urged. 

Dynamic Optimal Transport (OT). OT-based genera-tive models range from Wasserstein objectives (Arjovsky et al., 2017) to entropy-regularized OT, i.e. , Schr ¨odinger Bridges (SB) (L ´eonard, 2013; Chen et al., 2016; 2021b; Caluya & Halder, 2021). Scalable SB solvers typically al-ternate forward-backward updates, e.g. , Iterative Propor-tional Fitting/Sinkhorn-style methods (Fortet, 1940; Kull-back, 1968; R ¨uschendorf, 1995; De Bortoli et al., 2021; Chen et al., 2021a) and recent matching-based variants im-proving practicality (Shi et al., 2023; Chen et al., 2023; Peluchetti, 2023). SB has been applied to image genera-tion (De Bortoli et al., 2021; Chen et al., 2021a; Deng et al., 2024) and image-to-image translation (Liu et al., 2023b; Theodoropoulos et al., 2024; Gushchin et al., 2024). Our approach is fundamentally different, as we first formulate generative modeling as finding optimal coupling and then supervise the generation path under this coupling to learn efficient trajectory. 

# 6. Conclusion 

We present ASBM, a two-stage SB framework that learns 

informative optimal couplings. The key idea is to decou-ple forward and backward optimization: we first learn the forward dynamic as a controlled sampling problem under a non-memoryless base process, then train the backward dynamic using the optimal couplings induced by the learned forward transport. This avoids unstable bidirectional alter-nating training, yielding an efficient trajectory. While our ex-tensive experiments thoroughly validate ASBM on standard benchmarks (pixel space, LDM framework, distillation), we leave large-scale high-resolution and conditional generation evaluations to future work, expecting the natural transfer. 8Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

# Impact Statement 

Our work focuses on developing computational methods for image generation with optimal trajectory. The techniques are purely theoretical and computational, relying exclusively on public image dataset. No personal data, or sensitive con-tents are involved. We therefore identify no ethical concerns arising from this research. For this framework, there are many possible societal impacts, none of which need specific highlighting. 

# References 

Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797 , 2023. Ansari, A. F., Ang, M. L., and Soh, H. Refining deep gen-erative models via discriminator gradient flow. arXiv preprint arXiv:2012.00780 , 2020. Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-erative adversarial networks. In International conference on machine learning , pp. 214–223. PMLR, 2017. Ascher, U. M. and Petzold, L. R. Computer methods for ordinary differential equations and differential-algebraic equations . SIAM, 1998. Bellman, R. The theory of dynamic programming. Bulletin of the American Mathematical Society , 60(6):503–515, 1954. Caluya, K. F. and Halder, A. Wasserstein proximal algo-rithms for the schr ¨odinger bridge problem: Density con-trol with nonlinear drift. IEEE Transactions on Automatic Control , 67(3):1163–1178, 2021. Chen, T., Liu, G.-H., and Theodorou, E. A. Likelihood training of schr ¨odinger bridge using forward-backward sdes theory. arXiv preprint arXiv:2110.11291 , 2021a. Chen, T., Liu, G.-H., Tao, M., and Theodorou, E. Deep momentum multi-marginal schr ¨odinger bridge. Advances in Neural Information Processing Systems , 36:57058– 57086, 2023. Chen, Y., Georgiou, T. T., and Pavon, M. On the relation between optimal transport and schr ¨odinger bridges: A stochastic control viewpoint. Journal of Optimization Theory and Applications , 169(2):671–691, 2016. Chen, Y., Georgiou, T. T., and Pavon, M. Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schr ¨odinger bridge. Siam Review , 63(2):249–313, 2021b. Chen, Y., Georgiou, T. T., and Pavon, M. Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schr ¨odinger bridge. Siam Review , 63(2):249–313, 2021c. Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems , 26, 2013. De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. Diffu-sion schr ¨odinger bridge with applications to score-based generative modeling. Advances in neural information processing systems , 34:17695–17709, 2021. Deng, W., Luo, W., Tan, Y., Bilo ˇs, M., Chen, Y., Nevmyvaka, Y., and Chen, R. T. Variational schr ¨odinger diffusion models. arXiv preprint arXiv:2405.04795 , 2024. Domingo-Enrich, C., Drozdzal, M., Karrer, B., and Chen, R. T. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861 , 2024. Efron, B. Tweedie’s formula and selection bias. Journal of the American Statistical Association , 106(496):1602– 1614, 2011. Esser, P., Kulal, S., Blattmann, A., Entezari, R., M ¨uller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning , 2024. Fortet, R. R ´esolution d’un syst `eme d’ ´equations de m. schr ¨odinger. Journal de math ´ematiques pures et ap-pliqu´ ees , 19(1-4):83–105, 1940. Gushchin, N., Kholkin, S., Burnaev, E., and Korotin, A. Light and optimal schr ¨odinger bridge matching. In 

Forty-first International Conference on Machine Learn-ing , 2024. Havens, A., Miller, B. K., Yan, B., Domingo-Enrich, C., Sriram, A., Wood, B., Levine, D., Hu, B., Amos, B., Karrer, B., et al. Adjoint sampling: Highly scalable dif-fusion samplers via adjoint matching. arXiv preprint arXiv:2504.11713 , 2025. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems , 30, 2017. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-bilistic models. Advances in neural information process-ing systems , 33:6840–6851, 2020. Karras, T., Laine, S., and Aila, T. A style-based generator architecture for generative adversarial networks. In Pro-ceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 4401–4410, 2019. 9Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. 

Advances in neural information processing systems , 35: 26565–26577, 2022. Kloeden, P. E. Stochastic differential equations. In Interna-tional Encyclopedia of Statistical Science , pp. 1520–1521. Springer, 2011. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Kullback, S. Probability densities with given marginals. 

The Annals of Mathematical Statistics , 39(4):1236–1243, 1968. Kynk ¨a ¨anniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assess-ing generative models. Advances in neural information processing systems , 32, 2019. L ´eonard, C. A survey of the schr ¨odinger problem and some of its connections with optimal transport. arXiv preprint arXiv:1308.0215 , 2013. L ´eonard, C., Rœlly, S., and Zambrini, J.-C. Reciprocal processes. a measure-theoretical point of view. 2014. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022. Liu, G.-H., Chen, T., So, O., and Theodorou, E. Deep generalized schr ¨odinger bridge. Advances in Neural In-formation Processing Systems , 35:9374–9388, 2022a. Liu, G.-H., Lipman, Y., Nickel, M., Karrer, B., Theodorou, E. A., and Chen, R. T. Generalized schr ¨odinger bridge matching. arXiv preprint arXiv:2310.02233 , 2023a. Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., Nie, W., and Anandkumar, A. I2SB: Image-to-image schr¨ odinger bridge. arXiv:2302.05872 , 2023b. Liu, G.-H., Choi, J., Chen, Y., Miller, B. K., and Chen, R. T. Adjoint schr ¨odinger bridge sampler. arXiv preprint arXiv:2506.22565 , 2025. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learn-ing to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003 , 2022b. Liu, X., Zhang, X., Ma, J., Peng, J., et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations , 2024. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural infor-mation processing systems , 35:5775–5787, 2022. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research , pp. 1–22, 2025. Pavon, M. and Wakolbinger, A. On free energy, stochastic control, and schr ¨odinger processes. In Modeling, Estima-tion and Control of Systems with Uncertainty: Proceed-ings of a Conference held in Sopron, Hungary, September 1990 , pp. 334–348. Springer, 1991. Peluchetti, S. Diffusion bridge mixture transports, schr ¨odinger bridge problems and generative modeling. 

Journal of Machine Learning Research , 24(374):1–51, 2023. Peyr ´e, G., Cuturi, M., et al. Computational optimal trans-port: With applications to data science. Foundations and Trends® in Machine Learning , 11(5-6):355–607, 2019. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dream-fusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 , 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition , pp. 10684–10695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolu-tional networks for biomedical image segmentation. In In-ternational Conference on Medical image computing and computer-assisted intervention , pp. 234–241. Springer, 2015. R ¨uschendorf, L. Convergence of the iterative proportional fitting procedure. The Annals of Statistics , pp. 1160–1174, 1995. S ¨arkk ¨a, S. and Solin, A. Applied stochastic differential equations , volume 10. Cambridge University Press, 2019. Schr ¨odinger, E. ¨Uber die umkehrung der naturgesetze . Ver-lag der Akademie der Wissenschaften in Kommission bei Walter De Gruyter u . . . , 1931. Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. Diffu-sion schr ¨odinger bridge matching. Advances in Neural Information Processing Systems , 36:62183–62223, 2023. Song, J., Meng, C., and Ermon, S. Denoising diffusion im-plicit models. arXiv preprint arXiv:2010.02502 , 2020a. 10 Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-mon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 , 2020b. Tanaka, A. Discriminator optimal transport. Advances in Neural Information Processing Systems , 32, 2019. Theodoropoulos, P., Komianos, N., Pacelli, V., Liu, G.-H., and Theodorou, E. A. Feedback schr ¨odinger bridge matching. arXiv preprint arXiv:2410.14055 , 2024. Tong, A., Fatras, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with mini-batch optimal transport. arXiv preprint arXiv:2302.00482 ,2023. Vargas, F., Grathwohl, W., and Doucet, A. Denoising diffu-sion samplers. arXiv preprint arXiv:2302.13834 , 2023. Villani, C. et al. Optimal transport: old and new , volume 338. Springer, 2008. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828 , 2023. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Du-rand, F., and Freeman, B. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems , 37:47455–47487, 2024a. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6613–6623, 2024b. Zhang, Q. and Chen, Y. Path integral sampler: a stochas-tic control approach for sampling. arXiv preprint arXiv:2111.15141 , 2021. Zhang, Q. and Chen, Y. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902 , 2022. 11 Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

# Appendix A. Proofs 

Stochastic Optimal Control (SOC). In our framework (in general, SOC transports from samplable distribution to Boltzmann distribution), SOC-based sampling problem finds a control that transports pdata to a prescribed prior pprior ∼ exp( −E(x)) 

under cost minimization: 

min  

> u

EX∼pu

Z 10

12 ∥ut(Xt)∥2 dt + g(X1)



(21) s.t. dXt = ft(Xt) + σtut(Xt) dt + σt dWt, X0 ∼ pdata , (22) where g(x) : Rd → R is a terminal cost and SDE is constrained only by data distribution X0 ∼ pdata . Note that the terminal constraint on pprior is implicitly enforced by the terminal cost g(x).Under the SOC optimality, optimal control can be analytically derived through Hamilton-Jacobi-Bellman (HJB) equa-tion (Bellman, 1954). 

Theorem A.1. (SOC optimality) Under the SOC optimality, the optimal control is −→u ⋆t (x) = −σt∇Vt(x), where Vt(x) : [0 , 1] × Rd → Rd is a value function, 

Vt(x) = − log EX∼pbase [exp( −g(X1)) | Xt = x] . (23) 

The optimal joint distribution can also be characterized as 

p⋆(X0, X 1) = pbase (X0, X 1) exp (−g(X1) + V0(X0)) . (24) 

Proof of Proposition 3.1. 

Proof. Consider the memoryless condition 

pbase  

> 0,1

(X0, X 1) memoryless 

:= pbase  

> 0

(X0) pbase  

> 1

(X1). (25) Under the memoryless condition (8), the initial value function (23) becomes: 

V0(X0) memoryless 

= − log 

Z

pbase (X1) exp( −g(X1)) d X1. (26) Note that right-hand side is constant to X0. Substituting (26) and (25) into (24) yields the factorization 

p⋆(X0, X 1) = pbase (X0) pbase (X1) exp( −g(X1)) 

R pbase (X1) exp( −g(X1)) d X1

. (27) As a result, X0 and X1 are independent under p⋆, directly indicating that non-independent optimal couplings cannot be recovered under the memoryless condition. 

# B. Terminal Cost in SOC-based Sampling Problem 

Case of Memoryless Base SDE. To remove the bias introduced by V0(X0) in Eq. (24), a common approach is the adoption of memoryless condition (25). Starting from Eq. (24), 

p∗(X1) = 

Z

p∗(X0, X 1) d X0 =

Z

pbase (X0, X 1) exp (−g(X1) + V0(X0)) dX 0 (28) 

=

Z

pbase (X0) pbase (X1) exp (−g(X1) + V0(X0)) d X0 (29) 

∝ pbase (X1) exp (−g(X1)) = pprior (X1), (30) iEfficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

where second equality holds for memoryless condition (25). As a result, we can set terminal cost as 

g(x) = log pbase  

> 1

(x)

pprior (x) (31) for SOC-based sampling problem with memoryless base SDE (Zhang & Chen, 2021; Peluchetti, 2023; Havens et al., 2025). 

Case of Non-Memoryless Base SDE. ASBS (Liu et al., 2025) generalize the sampling problem to non-memoryless dynamic. To resolve the bias by initial value function V0(X0) in Eq. (28) without reliance on memoryless condition Eq. (25), they further deploy the SB optimality. 

Theorem B.1. (Optimal control in SB problem) Under the SB optimality (Pavon & Wakolbinger, 1991; Chen et al., 2021c; Caluya & Halder, 2021), the optimal control is 

u⋆t (x) = σt∇x log φt(x), v⋆t (x) = σt∇x log ˆ φt(x) (32) 

where φt, ˆφt ∈ C1,2([0 , 1] , Rd) are SB potentials satisfying 

φt(x) = 

Z

pbase 1|t (y | x) φ1(y) dy, φ0(x) ˆ φ0(x) = pprior (x),

ˆφt(x) = 

Z

pbase  

> t|0

(x | y) ˆ φ0(y) dy, φ1(x) ˆ φ1(x) = pdata (x).

Under the SOC optimality Theorem A.1 and SB optimality Theorem B.1, we can obtain the transform 

φt(x) = exp (−Vt(x)) , ˆφt(x) = exp (Vt(x)) p⋆t (x), (33) which connects between SOC value function and SB potentials. This leads to setting terminal cost as 

g(x) = log ˆφ1(x)

pprior (x) . (34) Applying Adjoint Matching (AM) (Domingo-Enrich et al., 2024) to SOC objective (21) with this terminal cost (34) under VP base SDE yields 

u∗ = arg min  

> u

Ep¯u

∥ut(Xt) + κtσt(∇E(X1) + ∇ log ˆ φ1(X1)) ∥2 , (35) where we use the notation in Algorithm 1. However, we need an access to v∇ log ˆ φ1(x)) to make this objective feasible. To resolve this problem, Corrector Matching (CM) is introduced, which can be derived by variational form of ∇ log ˆ φ1(x):

∇ log ˆ φ1 = arg min  

> h

Ep∗

> 0,1

h

h(X1) − ∇ x1 log pbase (X1|X0) 2i

. (36) Finally, following the adoption of reciprocal projection (Havens et al., 2025), we can alternately train these two objectives with parameterized models as 

min  

> θ

E pbase 

> t|0,1, p ¯uθ
> 0,1



uθt (Xt) +  σt∇E + ¯ vϕ

> 1

(X1) 2

, (37) 

min  

> ϕ

E p¯uθ

> 0,1



vϕ 

> 1

(X1) − σ1∇x1 log pbase (X1 | X0) 2

. (38) 

# C. Distillation 

Derivation for SB Distillation Framework. Upon successful training, we are given the pretrained backward dynamic, 

dXt = ft(Xt) − σtvϕt (Xt) dt + σt dWt, X1 ∼ pprior , (39) which approximately transports prior distribution to data distribution pdata . Denote the path measure induced by backward dynamic (39) as pϕ.ii Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching 

Figure I. Initialization of one-step generator (Uncurated generation). As discussed in Sec. 3.3, ASBM shows clear initialization for one-step generator, indicating its straighter and more organized trajectory. On the other hand, score-based initialization gives much more noisy initialization even with timestep shifting (Yin et al., 2024b). 

Now consider the one-step generator Gψ : Rd × Rd → Rd, which defines the output distribution, 

pψ 

> 0

:= Law  Gψ (X1, z ), (40) where X1 ∼ pprior , z ∼ N (0 , I ). Although we define the one-step generator as Gψ : Rd → Rd which only takes X1 in main paper for simplicity, it is more general to define it as Eq. (40) to consider the stochasticity via noise z.The most straightforward way is to minimize the KL divergence between pϕ 

> 0

and pψ 

> 0

, which is infeasible. Instead, we distill our learned backward control vϕt in continuous-time control space. Assume an optimal backward control vξ : [0 , 1] × Rd → Rd such that its corresponding controlled SDE 

dXt = (ft(Xt) − σtvξt (Xt)) d t + σt dWt, X1 ∼ pprior , (41) induces the terminal marginal X0 ∼ pξ 

> 0

≡ pψ 

> 0

. Let pξ denote its path measure. Then, by data processing inequality, KL divergence between terminal distributions is bounded by 

DKL 

 pξ 

> 0

∥ pϕ

> 0

 ≤ DKL 

 pξ ∥ pϕ. (42) Girsanov’s theorem (S ¨arkk ¨a & Solin, 2019) turns the path-space KL divergence (42) into a tractable drift-matching loss that can be estimated from sampled trajectories: 

12 Epξ

Z 10

vξt (Xt) − vϕt (Xt) 2 dt 



. (43) Since we assume vξt to be optimal, pξ follows reciprocal process, leading to, 

min  

> ψ

Epbase 

> t|0,1, X 0, X 1

 ¯vξt (Xt) − ¯vϕt (Xt) 2, (44) where X0 ∼ Gψ (X1, z ), X 1 ∼ pprior .Then, inspired by the distillation frameworks in DMs (Poole et al., 2022; Yin et al., 2024b), we initialize vξt from the pretrained vϕt and dynamically update it with bridge matching loss to reflect the continuously changing pψ 

> 0

:

min  

> ξ

Epbase 

> t|0,1, X 0, X 1

 vξt (Xt) − σt∇xt log pbase (Xt | X0) 2, (45) where X0 ∼ Gψ (X1, z ), X 1 ∼ pprior . As a result, we mainly optimize Gψ via Eq. (44) while dynamically updating vξt via Eq. (45). iii Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schr¨ odinger Bridge Matching   

> Figure II. Mode collapse in distillation (Uncurated generation). ASBM shows strong mode coverage in distillation task while the score distillation models still suffer from mode collapse even with costly regression loss.

With memoryless condition (25) , reciprocal process in Eq. (44) and Eq. (45) collapses to conditional path in standard diffusion model, and our distillation framework becomes exactly same as score distillation framework (Poole et al., 2022; Yin et al., 2024b). 

Initialization for One-Step Generator. As discussed in Sec. 3.3, ASBM provides strong initialization for one-step generator 

Gψ with Tweedie’s formula (19) due to its significantly straighter and efficiently organized trajectory. As shown in Fig. I, ASBM yields clearly better one-step estimate than score-based initialization, which makes the distillation easier than that from memoryless diffusion. 

Better Mode Coverage of ASBM. Score-distillation models suffer from mode collapse problem (Yin et al., 2024b) which inevitably requires further refinement through regression loss (Yin et al., 2024b) or adversarial loss (Yin et al., 2024a). Especially, regression loss requires generation of huge amount of noise-image pairs from original score model which is highly costly. As shown in Fig. II, pure score distillation (SDS) (Poole et al., 2022) exhibits severe mode collapse. Even with an additional heavy regression loss, DMD (Yin et al., 2024b) can still collapse. In contrast, ASBM substantially mitigates mode collapse, which we attribute to its organized and efficient trajectories, as demonstrated in Sec. 4.3. 

# D. Experiment Settings 

Model Architecture. We use UNet (Ronneberger et al., 2015) architecture following the hyperparameters in (Tong et al., 2023). For backward dynamic, we use 4 residual blocks for each channel following the Score SDE (Song et al., 2020b) and 2 residual blocks for our forward dynamic which significantly reduces the computation cost. For LDM experiment, we employ Stable Diffusion 3 (Esser et al., 2024) autoencoder. We set batch size as 128. 

Training Environment. We conduct all the experiments with a single NVIDIA A100 40 GB. 

Training Time. It takes about 4 days for ASBM (2100 epochs) and 6 days for Score SDE (3300 epochs) on single A100. iv
Title: PolyNODE: Variable-dimension Neural ODEs on M-polyfolds

URL Source: https://arxiv.org/pdf/2602.15128v1

Published Time: Wed, 18 Feb 2026 01:07:23 GMT

Number of Pages: 13

Markdown Content:
# PolyNODE: Variable-dimension Neural ODEs on M-polyfolds 

# Per Åhag Alexander Friedrich Fredrik Ohlsson Viktor Vigren Näslund 

Department of Mathematics and Mathematical Statistics, Umeå University, Umeå, Sweden 

per.ahag@umu.se, alexander.friedrich@umu.se, fredrik.ohlsson@umu.se, viktor.vigren.naslund@umu.se 

Authors are listed alphabetically according to their surname. 

Abstract 

Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold’s dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at 

https://github.com/turbotage/PolyNODE .

1 Introduction 

Many modern neural networks change the dimension of their hidden representation across layers. They expand features to increase expressive power or compress them to extract information-dense latent representations. The ability to accommodate variable feature space dimen-sions is central in encoder-decoder models, convolutional neural networks and many other ubiquitous feed-forward machine learning architectures. A natural question is what happens if we take the continuous depth viewpoint in this setting? What is a continuous depth model whose state dimension is allowed to change over time? Neural ordinary differential equations (NODEs) pro-vide the continuous depth limit of residual type net-works with constant width layers by replacing discrete layers with the flow of an ordinary differential equation (ODE) [Chen et al., 2018]. In its basic form, the hidden state evolves as 

ddt ϕ(t, p ) = X(ϕ(t, p )) , ϕ(0 , p ) = p , (1) where X : Rn → Rn is the vector field generating the flow ϕ : R × Rn → Rn and h(p) = ϕ(1 , p ) is the diffeo-morphism defining the map between the input and the output of the NODE model. This construction extends to manifold valued states by replacing Rn with a mani-fold M and generalising to flows ϕ : R × M → M gener-ated by vector fields X on M [Falorsi and Forré, 2020; Lou et al., 2020; Mathieu and Nickel, 2020]. NODEs are widely used in flow based learning and generative modelling, and features attractive properties such as uni-versality [Andersdotter et al., 2025; Zhang et al., 2020] and scalable training using flow matching [Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023; Tong et al., 2024]. In (1) , the state ϕ(t, p ) always lies in a space of fixed dimension, either Rn or a manifold M . This is not a limitation of parametrisation but an intrinsic geometric restriction. A smooth manifold does not allow trajectories whose number of local degrees of freedom drop or increase at an intermediate time. For this reason, NODEs do not provide an intrinsic continuous depth analogue of, e.g. encoder-decoder architectures [Baldi and Hornik, 1989; Bourlard and Kamp, 1988; Hinton, 1989; LeCun, 1987] where a dimensional bottleneck is used to extract a latent representation from which the original data can be efficiently reconstructed. More generally, NODEs cannot capture variable width feed forward network dynamics in continuous time [Ruiz-Balet and Zuazua, 2023]. If we insist on changing feature dimensions, then the correct state space is no longer a manifold. The contin-uous time analogue is instead a stratified space, where smooth pieces of different dimensions meet along singu-lar loci. A simple example of such a space correspond-ing to the dimensional bottleneck of the autoencoder 1

> arXiv:2602.15128v1 [cs.LG] 16 Feb 2026

is Ωmn := (( −∞ , τ 1) × Rn × Rm) ∪ ([ τ1, τ 2] × Rn ×{ 0}) ∪

(( τ2, ∞) × Rn × Rm). Outside [τ1, τ 2] trajectories evolve in (R \[τ1, τ 2]) × Rn × Rm, but on [τ1, τ 2] they are con-strained to [τ1, τ 2] × Rn ×{ 0}. At τ1 and τ2 the local dimension changes, and at these these points there are no manifold charts since the space is not locally home-omorphic to Rn. Consequently, the standard manifold calculus that underlies NODE theory breaks down. A natural approach to extend NODEs would be to embed Ωmn into an ambient Euclidean space and define an ordinary NODE there. However, this does not resolve the main issue which is the calculus at the singular set. If we require the ambient vector field to be smooth and tangent to the stratified set, then its flow cannot probe a genuine bottleneck in finite time. Finite time collapse of the transverse coordinates forces the dynamics to become singular or at least non-Lipschitz near the tran-sition, and then the usual sensitivity calculus used for training NODEs is no longer justified. What we need is an intrinsic notion of smoothness on the stratified state space that keeps a chain rule and a well defined tangent map while still allowing controlled collapse. M-polyfold theory provides exactly this [Åhag et al., 2025; Hofer et al., 2007, 2009a,b, 2021; Weber, 2019]. It was developed to give a workable differential calculus on spaces that arise from gluing and degeneration, where effective dimension changes are part of the geometry. In our setting, the key point is that an M-polyfold equips a stratified space with a global notion of differentiability that records how the collapsing coordinates vanish as one approaches the bottleneck. This replaces the missing manifold structure at τ1 and τ2 and restores the tools needed for flow based learning, including a chain rule and a tangent construction that are compatible with the bottleneck. In this paper we use the M-polyfold framework to extend NODEs beyond manifolds. We introduce PolyN-ODEs, continuous depth models driven by parametrised vector fields on explicit M-polyfolds. We explicitly con-struct M-polyfolds that realise dimensional bottlenecks and PolyNODE autoencoders that encode and decode by traversing these bottlenecks. We then demonstrate empirically that these models can be trained for recon-struction tasks and that the resulting latent representa-tions support downstream classification. Our main contributions are as follows. 

• We introduce PolyNODEs, a class of continuous depth models on M-polyfolds that allows variable dimension dynamics. 

• We construct explicit M-polyfold bottleneck spaces and PolyNODE autoencoders by parametrising vector fields whose flows traverse the bottlenecks. 

• We provide proof of concept experiments showing that PolyNODE autoencoders can be trained for reconstruction and that the learned latent codes support classification. 

2 Neural ODEs on M-polyfolds 

The description of the stratified spaces required to ac-commodate NODE state vectors that can change their dimension – its regular components, their boundaries and intersections – is quite cumbersome even for sim-ple configurations like the space Ωmn introduced above and illustrated in Figure 1. Generalising NODEs to stratified spaces, the regular components correspond to regions of fixed dimension dynamics, and the boundaries to discontinuous changes in the dimension of the state vector ϕ(t, p ), as illustrated in Figure 2. The concept of 

Figure 1: Illustration (grey shading) of the stratified topological space Ω11 ⊂ R3 with τ1 = −2 and τ2 = 2 .a M-polyfold allows a consistent description of stratified spaces like Ωmn . More importantly, polyfold theory en-dows these spaces with a global smooth structure which gives us access to much the same analytical tools that are available on manifolds. In particular, we have well-defined notions of vector fields and can construct the flows required to extend NODEs to these spaces. Below we present a very brief introduction to the theory of M-polyfolds based on [Hofer et al., 2021]. Subsequently, we give an explicit M-polyfold structure for the stratified space Ωmn , construct flows and ODEs on M-polyfolds, and define the PolyNODE model which is our first main contribution. We use Ωmn as a recurring example, as it features in our construction of PolyNODE autoen-coders in Section 3, and denote a point in this space as 

p = ( τ, x, y ) with τ ∈ R, x ∈ Rn, and y ∈ Rm. 

> 2.1 Differential Geometry of M-polyfolds sc-Banach Spaces

A scale Banach space , or sc-Banach space, is a Banach space E, together with a filtration 

{Ek}k∈N called a scale structure , or sc-structure, where 

E0 = E and Ek+1 ⊂ Ek for all k ∈ N, all inclusions 2Figure 2: Illustration of a semi-flow in Ω11 ⊂ R3 with 

τ1 = −2 and τ2 = 2 , polyfold structure indicated as grey planes. 

ιk : Ek+1 ,→ Ek are compact, and the space E∞ := T 

> k∈N

Ek is dense in every scale Ek.Each Ek can be turned into a sc-Banach space of its own, denoted by Ek, where (Ek)i := Ek+i for all i ∈ N.Direct sums of sc-Banach spaces are again sc-Banach spaces with component-wise scales. Note that finite dimensional Banach spaces admit only the constant scale structure, i.e. Ek = E for all k ∈ N, since the only dense vector subspace of E is E itself. Infinite dimensional Banach spaces on the other hand cannot be equipped with the constant scale structure. In this article we are mainly interested in the case of L2(R) with a weighted Sobolev sc-structure. Let 

{δk}∞ 

> k=0

be a non-negative, strictly increasing sequence with δ0 = 0 , let α : R → [−1, 1] be a smooth odd function with α(0) = 0 and α(s) = 1 for s > 1. The weighted Sobolev norms are then given by 

∥f ∥2 

> k

:= 

Z

> R
> k

X

> i=0

|dif (s)|2eδk sα (s) ds , 

and the scales of L2(R) are defined by 

L2(R)k := f ∈ L2(R) ∥f ∥k < ∞ . (2) The inclusion ι : L2(R)k+1 ,→ L2(R)k is compact by em-bedding theorems for weighted Sobolev spaces, compare Lemma 4.10 in [Fabert et al., 2016], and E∞ contains 

C∞ 

> c

(R) which is dense in any Sobolev space. In this example it is clear that points in higher scales have higher regularity and decay rates at infinity since the higher norms incorporate more derivatives and the exponential weights are increasing. For two sc-Banach spaces E and F a map ψ : E → F

is called scale continuous , or sc 0, if ψ|Ek : Ek → Fk is continuous for all k ∈ N. Furthermore, a sc-continuous map ψ is called scale differentiable , or sc 1, if there exists a sc-continuous map Dψ : E1 ⊕ E → F , such that 

Dψ ξ : E → F is a bounded linear operator and for 

ξ, h ∈ E1

lim 

> ∥h∥1→0

∥ψ(ξ + h) − ψ(ξ) − Dψ ξ (h)∥0

∥h∥1

= 0 .

The tangent space of E at a point ξ ∈ E1 is defined as 

Tξ E := E.Note that sc-continuity is a stronger condition than ordinary continuity. Sc-differentiability on the other hand is weaker than ordinary differentiability, assuming sc-continuity is established, since we only require Dψ 

to exist on E1, and use the stronger convergence for h

in the E1 norm. A map ψ : E → F is called sc k, for k ∈ N, if the above construction can be iterated k times for the respective differentials and scale smooth , or sc ∞, if it is sc k for any k. 

> Retracts

In the theory of M-polyfolds, retracts play a similar role as open sets of Rn do for manifolds. A sc -smooth retraction on a sc-Banach space E is a sc -smooth map r : E → E that satisfies the projection property r ◦ r = r. The image of a sc -smooth retraction is called a sc -smooth retract . If r is a retraction and 

O := Im r the associated retract, the tangent space of 

O at ξ ∈ O ∩ E1 is defined as Im Dr ξ .A manifold-like polyfold , or M-polyfold, is a paracom-pact Hausdorff space that admits an atlas with charts constructed out of retracts, giving rise to a rich structure in the same way as on manifolds. For the purposes of this paper we are content with a single retract. This is analogous to a manifold that can be covered by a single chart.   

> Ω11as an M-polyfold

We now return to the space Ω11

and show that it can be described by a single retract. Let τ1, τ 2 ∈ R, with τ1 < τ 2, set J := [ τ1, τ 2], and define 

β : R \J → R as 

β(τ ) = exp (1 /(τ1 − τ ) + 1 /(τ − τ2)) .

Let L2(R) be the scale Banach space with weighted Sobolev scales in (2) , fix a γ ∈ C∞ 

> c

(R) ⊂ E∞ with 

∥γ∥L2(R) = 1 , define γτ (s) := γ(s + β(τ )) , and abbrevi-ate ρτ := d/dτ γ τ . We then define a sc -Banach space 

E := R ⊕ R ⊕L2(R), denote a point in this space as 

ξ = ( τ, x, f ) with τ ∈ R, x ∈ R, and f ∈ L2(R), and define the map r : E → E,

r(τ, x, f ) = 

(

(τ, x, ⟨f, γ τ ⟩γτ ), τ ∈ R \J

(τ, x, 0) , τ ∈ J , (3) 3where ⟨· , ·⟩ denotes the standard inner product on L2(R).Explicit calculations show that r is sc ∞, with differential 

Dr (τ,x,f )(σ, y, g ) = 



(σ, y, σ ⟨f, γ τ ⟩ρτ

+σ⟨f, ρ τ ⟩γτ

+⟨g, γ τ ⟩γτ ) , τ ∈ R \J

(σ, y, 0) , τ ∈ J,

and it clearly satisfies the projection property making it a retraction on E.In the above construction the function β has singu-larities at τ1 and τ2. This means the support of γτ is shifted to infinity at these points. This in turn yields strong decay estimates for the term ⟨f, γ τ ⟩ on every scale, which leads to the apparent jump in (3) not only being continuous but sc-smooth. We can think of the curves τ 7 → r(τ, f ) decaying to 0 in the last component as τ ↗ τ1 and τ ↘ τ2 for f in any scale. The retract O = Im r is homeomorphic to Ω11 =(R \J × R × R)∪(J × R ×{ 0}), with the subspace topol-ogy of R3, via the map η : O → Ω11

η(τ, x, f ) = 

(

(τ, x, ⟨f, γ τ ⟩), τ ∈ R \J

(τ, x, 0) , τ ∈ J . (4) At τ ∈ R \J, the tangent space of O is 

T(τ,x,f )O = span {(1 , 0, ⟨f, γ τ ⟩ρτ ) , (0 , 1, 0) , (0 , 0, γ τ )} ,

whereas the tangent space of Ω11 is simply R3,

Tη(τ,x,f )Ω11 = Dη (τ,x,f )(T(τ,x,f )O)= span {(1 , 0, 0) , (0 , 1, 0) , (0 , 0, 1) } ,

due to the fact that γτ and ρτ are L2(R)-orthogonal 1.The constructions of this section generalise in a straightforward manner. The second coordinate, de-noted by x ∈ R above, can be replaced by a coordinate in Rn since the construction does not depend on it. More importantly, we may increase the jump in dimensions to m by choosing an L2-orthonormal family {γi}mi=1 

of smooth compactly supported functions instead of a single γ. The retraction r is then given by r(τ, x, f ) = (τ, x, π τ (f )) , for τ ∈ R \J, where πτ is the L2-projection to span {(γi)τ }mi=1 . In this way we can construct M-polyfolds Ωmn ∼= ( R \J × Rn × Rm) ∪ (J × Rn ×{ 0}) for arbitrary n, m ∈ N.

> 2.2 Flows and ODEs on M-polyfolds

In order to extend NODE models to M-polyfolds we need vector fields compatible with the sc-smooth structure. A sc k vector field on a sc-Banach space E is a sc k

map X : E1 → E, and we denote the space of such vector fields on E by Xk(E). On a retract O ⊂ E,     

> 1Consider the τderivative of 1 = ∥γτ∥2
> L2(R).

with retraction r, we can view the set of vector fields as 

Xk(O) = {X ∈ Xk(E) | Xξ ∈ Tξ O, ∀ξ ∈ O ∩ E1}.To construct the flow generated by a vector field X

we need to solve ODEs on E. If a vector field X is actu-ally in sc 0(E1, E 1) then, since E is a filtration of Banach spaces, ODE solution theory applies independently on every scale Ek. Thus ODEs on E can be solved on every scale. If additionally X is Lipschitz continuous on every scale then standard uniqueness arguments imply the solutions for different scales agree when restricted to the same scale. This generalises to the theory of flows by similar arguments for the dependence on the initial conditions. We discuss ODEs and flows on sc-Bannach spaces and M-polyfolds in greater detail in a forthcom-ing article. For ODEs on Banach spaces see for instance Chapter 16 in [Pata, 2019]. Given X ∈ Xk(O) on a retract O with retraction 

r, we can consider a lift, i.e. ˜X ∈ Xk(E) such that 

Dr ξ ˜X(ξ) = X(r(ξ)) . If ˜X can be integrated to a flow ˜ϕ

on E1, then it gives rise to a flow r ◦ ˜ϕ on O ∩ E1 when restricted to O ∩ E1.However, in the case of the retraction (3) for the Ω11

model this construction is not applicable, since at τ1

and τ2 any lift of a vector field whose flow traverses the bottlenecks will be singular as a map from E1 to E1.This is expected because at these points any flow going into the dimensional bottleneck has to loose injectivity and thus ceases to be a flow. This is a fundamental property of stratified spaces like Ω11 and indeed the main feature we use for the autoencoder construction. Thus, what we are really interested in for the autoencoder application are semi-flows that traverse the dimensional bottleneck. A semi-flow ϕ that is sc 0 at τ1 and τ2 has a component ϕ3 which converges to 0 at these points in any scale by definition of scale continuity. In coordinate charts η (4) this corresponds to super-exponential decay. For example, suppose lim t→t1 ϕ(t)1 = τ1 from below, then 

lim 

> t→t1

η(ϕ(t)) 3 exp ( δiβ(ϕ(t)1)) = 0 ∀i ∈ N . (5) In general, vector fields on the retract O = Im r have the form 

X(τ,x, ⟨f,γ τ ⟩γτ ) = ( X1, X 2, X 1⟨f, γ τ ⟩ρτ + X3γτ ) ,

for functions Xi : O → R, i ∈ { 1, 2, 3}. This means that for τ ∈ R \J we can view vector fields on Ω11 ⊂ R3 as classical vector fields Y(τ,x,y ) = ( Y1, Y 2, Y 3), which lift to sc 0 vector fields 

X(τ,x,yγ τ ) = Dη −1(Y (η(τ, x, yγ τ ))) = ( Y1, Y 2, Y 1yρ τ + Y3γτ ) . (6) Using techniques like separation of variables it is straight forward to find vector fields Y = ( Y1, Y 2, Y 3) whose semi-flows satisfy (5) . Note that condition (5) is fulfilled if 4there is a t0 < t 1 such that η(ϕ(t)) 2 = 0 for all t ∈ [t0, t 1].In Section 3.1 we give an example of a vector field with an associated semi-flow with this property. With slight abuse of notation, we let ϕ denote both the semi-flow on Ω11 generated by the vector field Y and its image under η−1. See Figure 2 for an illustration of a semi-flow in Ω11. The generalization to vector fields and semi-flows on Ωmn is straightforward, as indicated in Section 2.1. 

> 2.3 PolyNODEs: Neural ODEs on M-polyfolds

Having defined the necessary machinery from polyfold theory, we can now extend the definition of a neural ODE to stratified spaces. Let E be a sc-Banach space, 

r : E → E a retraction, and O = Im r the corresponding retract. A PolyNODE parametrised by the vector field 

X ∈ Xk(O) is the map h : O → O, h(ξ) = ϕ(1 , ξ ), where 

ϕ : R ×O → O is the semi-flow generated by X,

ddt ϕ(t, ξ ) = X(ϕ(t, ξ )) , ϕ(0 , ξ ) = ξ . (7) The generalisation to M-polyfolds covered by several retracts is straightforward, but not required for the case 

Ωmn we consider below. The semi-flow ϕ generated by X can traverse the stratification points at the intersection of the regular components of the underlying space, meaning that the dimension of the state vector ϕ(t, ξ ) can change in a sc -differentiable way. This construction of PolyNODEs is our first major contribution; a flow-based machine learn-ing model capable of accommodating variable-dimension dynamics. 

3 A PolyNODE Autoencoder Model on Ωmn

Having defined PolyNODEs in the previous section, we proceed to construct explicit examples of such models. The purpose is to demonstrate the viability of PolyN-ODEs in machine learning by establishing; 1) that M-polyfolds corresponding to specific machine learning problems can be constructed; and 2) that these spaces admit vector fields which probe their M-polyfold struc-ture. In the subsequent section we then present nu-merical experiments that further demonstrate; 3) that 

sc -NODE models defined by these vector fields can be trained to solve common types of machine learning tasks. 

> 3.1 Model Construction

As a proof-of-concept, we consider the problem of re-constructing geometric objects using a PolyNODE with a dimensional bottleneck similar to that of a traditional autoencoder, for which the appropriate geometry is the space Ωmn with J = [ τ1, τ 2] described in detail above. We can then interpret the region τ < τ 1 as the input space, the stratification at τ = τ1 as the encoding, τ ∈ (τ1, τ 2)

as the latent space, the stratification at τ = τ2 as decod-ing, and τ > τ 2 as the output space of the autoencoder structure, see Figure 3 for an illustration. For an index set I, we sample a submanifold S ⊂

R1+ n+m of dimension dim S ≤ n + 1 , {zi}i∈I , zi ∈ S.We choose two embeddings ι1 for the input data and 

ι2 for the target data into Ωmn , such that ι1(zi)1 < τ 1

and ι2(zi)1 > τ 2 for all i ∈ I. An easy way to achieve this is to embed the samples zi ∈ S ⊂ Rn+m into Ωmn as 

X = {(τX , z i)}i∈I for a fixed τX < τ 1 to obtain the input data and Y = {(τY , z i)}i∈I for some fixed τY > τ 2 to obtain the target data. We use this type of embedding in our main experiment, the autoencoding of the spiral, and assume it in the following. For the reconstruction task, we restrict ourselves to vector fields whose semi-flows actually traverse the bottleneck. Furthermore, we regard the τ coordinate in Ωmn as artificial, corresponding to the depth in a traditional autoencoder, and consequently prescribe a constant velocity in this direction by enforcing X1 =

τY − τX . A latent representation of a point p ∈ X is given by ϕ(t, p ) for any t such that ϕ(t, p )1 ∈ [τ1, τ 2].We may always choose Ωmn and the embeddings of the data such that this is realised at t = 1 /2. 

> 3.2 Parameterisation of Compressing Vector Fields

In order to construct vector fields X for the PolyNODE model that can accomplish the dimensional reduction in the encoding phase, we need to ensure convergence of the semi-flow to the stratification point τ = τ1 in finite time. This is accomplished by choosing the parametrisa-tion in (6) and imposing conditions on the components 

Y1+ n+j , j = 1 , . . . , m compressing the semi-flow in the directions transverse to the latent space. Let τ0 < τ 1 define the compression region, let kj ∈

C0,1(R1+ n+m, R+), with 0 < K j < k j for constants Kj ,and let a ∈ (0 , 1) . For p ∈ (τ0, τ 1) × Rn+m we then restrict to vector fields 

Y1+ n+j (p) = −kj (p) sign (yj )|yj |a , (8) which guarantees convergence of the semi-flow to yj = 0 

in finite time [Bhat and Bernstein, 2000]. Specifically, for a constant rate function kj (p) = Kj the time to con-vergence is given by T (y0 

> j

) = |y0 

> j

|1−a/(Kj (1 − a)) , where 

y0

> j

̸ = 0 is the initial value. This means, by choosing the lower rate bounds Ki appropriately, we can guaran-tee that the semi-flow ϕ maps a bounded subset of the region [τ0, τ 1) × Rn+m, containing the slice at τ = τ0

of all flow lines originating on the input data X , to 

(τ0, τ 1) × Rn ×{ 0} before it arrives at the stratification 

τ = τ1. Thus ϕ restricted to that bounded set satisfies condition (5) and is therefore an sc 0 semi-flow. We then obtain a family of compressing vector fields by parametrising the functions kj using neural networks. The components Y1+ i, i = 1 , . . . , n corresponding to the 5xi directions parallel to latent space are unrestricted and parametrised directly by neural networks. The same is true for Y1+ n+j for τ < τ 0 and τ > τ 2.Note that away from yj = 0 , the Y1+ n+j in (8) are smooth but at yj = 0 they are only Hölder continuous. ODE theory still guarantees the existence of a semi-flow but flow lines are no longer unique after reaching yj = 0 ,in particular flow lines may merge. When employing gradient based methods to train a PolyNODE with these compressing vector fields, the singularity at yj = 0 needs to be handled explicitly. See Appendix B for details of our implementation. 

Figure 3: Illustration of a flow line entering the bottle-neck at τ1 and exiting at τ2. Explicit compression starts at τ0.

4 Experiments 

In this section we conduct several numerical experiments based on the PolyNODE autoencoder model constructed above 2. All experiments are based on parametrised fam-ilies of the compressing vector fields in (8) . Complete experimental details are provided in Appendix A the im-plementation is found in [Friedrich and Vigren Näslund, 2026]. In all experiments described below, we train our PolyNODEs using the adjoint sensitivity method [Chen et al., 2018] with a custom implementation of the back-ward pass for the compressing vector fields (see Ap-pendix B for further details).  

> 4.1 Reconstruction of Geometrical Objects Spirals

Our goal is to autoencode a family of spirals, where the number of turns ranges from N = 1 /2 to 

N = 5 , and extract a latent representation. To this end we choose Ω21 with τ1 = 0 and τ2 = 1 as a M-polyfold, sample the spiral and embed to obtain the input data set X = {pi}i∈I = {(τX , z i)}i∈I . The target data set is given by Y = {(τY , z i)}i∈I . For the training of the model we employ a loss function L = L1 + λ(L2 + L3)

with three loss terms, with weight λ = 20 for the last 

> 2In this section, we use the term flow to refer to all collections of integer curves that appear, dropping the explicit semi-flow qualification.

two, where 

L1 = 1

|I|

X

> i∈I

|(ϕ(1 , p i) − (τY , z i)) |2 ,L2 = 1

|I|

X

> i∈I

| (ϕ(t0, p i)3, ϕ (t0, p i)4) − (1 , 1) |2 ,L3 = 1

|I|

X

> i∈I

dS (pi, q pi ) − cd|ϕ(t0, p i)2 − ϕ(t0, q pi )2| .

Here cd is the square root of the intrinsic diameter of the spiral; chosen this way since the intrinsic distance scales with the square of the angular parameter.Additionally, 

qp is a random element of X , such that qp̸ = p, ∀p ∈ X 

and qp̸ = qp′ if p̸ = p′.The term L1 is the mean squared error of the recon-structed spiral. The second term L2 forces the flow to explore the fourth dimension and ensures that the y1

and y2 coordinates are in a range to be mapped to 0

by the compressing vector fields by the time the flow reaches τ = 0 . The last term L3 enforces the flow to be approximately isometric at time t0 = 1 /4, which aids in the unwinding of the spiral to a line. Here τX and 

τY are chosen such that at time t0 the flow is at τ = τ0,i.e. the onset of the compressing vector fields. For the spiral we know the distance function dS explicitly. In general the approximate distance function may be re-constructed from the data, see for instance [Mémoli and Sapiro, 2005]. This last loss term is rather strong and problem specific, however it does not contain the entire information of the spiral parametrisation. Moreover, we stress that the goal of this example is to show that a vector field for an unwinding and autoencoding flow can be learned. In the next example, we show that a round sphere can be autoencoded using only the mean squared reconstruction error. Figure 4 illustrates the 

(a) (b) Figure 4: (a) Flow lines (green) for individual input samples for N = 1 . Input set X (blue) and reconstructed output ˆY (orange). The y2 component is projected out for visualisation. (b) Time slices of the flow for a spiral with N = 4 . Colour scale corresponds to the angular parameter. unwinding and encoding of the spirals to the line as well as the subsequent reconstruction. Note that the flow 6Figure 5: Relative monotonicity error during training for N = 4 .for τ < τ 1 is 4 dimensional, thus allowing for apparent crossing of flow lines. To determine how well the spirals are encoded onto the line we may simply investigate the monotonicity of the map s 7 → ϕ(1 /2, f (s)) 2, where 

f is the parametrisation of the spiral and s ∈ [0 , 2π].Figure 5 shows the monotonicity error relative to the number of sample points during training for a spiral with 

N = 4 . At the start of the training more than 40% of the points are misaligned but as the training progresses the number of misaligned points goes to zero; shown by omission of points in the log scaled plot. This indicates that in this case the spiral is encoded perfectly onto the line, within the sample accuracy of 5000 equidistant points. All of the experiments show very good monotonicity on sampled data points, with less than 0.02% misaligned points. See Section A.1 for plots of the other experi-ments, including the reconstruction.  

> Sphere

As a further example, consider the two dimen-sional round sphere of radius 1 in R4 around a point x0

given by {(z1, z 2, 0, z 3) | z ∈ S21 (x0) ⊂ R3}. We choose 

Ω12 with τ1 = 0 and τ2 = 3 as a M-polyfold, sample it and, with e1 = (1 , 0, 0, 0) , embedded the samples as 

X = {pi}i∈I = {zi + τX e1}i∈I and Y = {zi + τY e1}i∈I .Here we break the embedding convention of Section 3.1 for an easier visualisation. The loss function is the mean squared reconstruction error, 

L = 1

I

X

> i∈I

|(ϕ(1 , p i) − (zi + τY e1)) |2 .

Figure 6 shows the encoding and decoding of the 

2-sphere embedded in R4, where the fourth dimension is indicated by colour. We see that, starting from the monochrome sphere where the fourth coordinate is zero, the flow explores the extra dimension while the height is compressed. At the end of the encoding the sphere is contained in R3. At 0.06% , the relative mean recon-struction error of the decoded sphere is very low. 

(a) (b) Figure 6: Time slices of the encoding (a) and decoding (b) of the 2-sphere in R4. The colour represents the coordinate in the fourth dimension. 

> 4.2 Latent Space Classification Tasks

Encoding–decoding architectures are often employed to enable downstream tasks based on the latent represen-tation. We illustrate this capability using the latent representation learned in the spiral experiment, Sec-tion 4.1, to solve radial and angular classification tasks. In both cases, we define three labels {l1, l 2, l 3}, described in detail in the following paragraphs, and extract latent states at t = 1 /2 to produce labelled samples for the classifications. In principle, any model can be used for the classifi-cation task, but we opt to stay in the NODE framework. We designate target points in latent space corresponding to each label and learn a vector field whose flow maps the sample points to their corresponding label target. The latent representation of the spiral is a one-dimen-sional curve lying in a two-dimensional plane. For a general ordering of labels along the latent curve, we cannot guarantee the existence of a vector field making points flow to their target point if the integral curves are restricted to lie in the plane. This is because crossings of integral curves would be necessary for all but the simplest monotone orderings. Thus, we augment the NODE to three dimensions. The loss used is MSE between the last points on the integral curves and their corresponding target points. In addition, an attractor term for each target point is added to the vector field to promote flow towards a target point, giving the NODE 

dx (t)

dt = Yθ (x, t ) + C X

> i

yi − x(t)

|yi − x(t)| e−k(yi−x(t)) 2

, (9) where yi indicates the target point for label li and 

Yθ (x, t ) is the learnable vector field. We choose k so that there is a clear separation between the target point attractors. For details on the experimental setup and the neural networks used see Section A.2.  

> Radial Classification

We divide the interval [0 , R max )

into three equal parts, where Rmax is the maximal radius 7of the spiral. Each point in the latent space corresponds to a point on the spiral. A point in the latent space is given the label li, i ∈ { 1, 2, 3} if its corresponding point on the spiral has radius in [Rmax (i − 1) /3, R max i/ 3) .Due to the unwinding nature of the flow for the spi-ral auto-encoder, the labelling in latent space becomes monotone for the radial classification problem. Conse-quently, a simple flow is expected and experimentally observed, see Figure 7a. The model reaches a peak accuracy of 100% in this task, and consistently stays above 80% accuracy after a single epoch. 

(a) (b) Figure 7: Classifier flow lines of the radial (a) and angu-lar (b) classification problems. Colours indicate which label the trajectory belongs to and what label the target points (squares) correspond to. The latent representa-tions constituting the classifier input correspond to the line where the integral curves start.  

> Angular Classification

Classes are set by the angle in the plane of the spiral. The interval [0 , 2π) is equally di-vided into [2 π(i − 1) /3, 2πi/ 3) , i ∈ { 1, 2, 3} and a latent point is assigned a label li according to its corresponding point on the spiral. For angular classification, the unwinding nature of the autoencoder flow causes the labels to switch sev-eral times along the latent line. Therefore, no simple classification flow is possible and the augmentation is necessary. The experiment is qualitatively consistent with this expectation, see Figure 7b. The model reaches a peak accuracy of 98% in this task and consistently exceeds 80% accuracy after 150 epochs. With sufficiently many parameters and an expressive enough architecture, any desirable accuracy is achievable for both classification problems. Therefore, the accuracy presented should not be interpreted as an indication of superior model structure. Rather it demonstrates the ability of PolyNODEs to extract latent representations of sufficient accuracy for downstream tasks. 

5 Conclusion 

In this work, we have demonstrated that it is possi-ble to extend flow-based NODE models to M-polyfolds to accommodate variable-dimension dynamics. The construction hinges on the use of infinite dimensional Banach spaces as the underlying spaces where the flow dynamics plays out, even in the case where the tangent spaces of the retracts and polyfolds we consider are finite dimensional. The reason is the fact that the non-trivial scale structures required to relax the differentiability condition and accomplish dimensional jumps cannot be defined in Euclidean spaces. We believe there are several interesting consequences of the construction of our PolyNODE models. The gen-erality and flexibility of NODE models are increased by allowing varying dimension. We use the encoder-decoder structure as an example, but more generally PolyNODEs can be defined to represent the continuum limit of feed forward networks with arbitrary layer widths. Similar architectural flexibility is often used in practice to de-sign accurate and efficient neural networks. Somewhat conversely, the PolyNODEs could also be leveraged to gain a better understanding of well-posedness, stability and convergence for general architectures using ODE theory (cf. [Haber and Ruthotto, 2018; Thorpe and van Gennip, 2023]). We construct a M-polyfold mirroring an autoencoder setup, together with a family of parametrised vector fields that traverse the dimensional bottleneck. Our experiments with this class of models demonstrate the ability to train PolyNODE models and extract mean-ingful latent representations in an autoencoder setting, but contain no quantitative evaluation on more realistic problems or comparisons with other model architectures. Nor do we explore the vast range of possible geometries and architectures that could be constructed from flows on M-polyfolds. Both these aspects indicate interesting future directions of research. Unlike the situation for Euclidean spaces (or even manifolds), there is currently no general theory for ODEs on M-polyfolds, meaning that the theoretical foundations for our PolyNODEs are not yet established. Furthermore, an important question is whether the con-struction of PolyNODEs enables us to learn dynamics – relevant for machine learning – which cannot be de-scribed in terms of flows in the ambient space where finite-dimensional polyfolds can be embedded. For ex-ample, the stratified spaces we consider in this paper can always be embedded in Euclidean space Ωmn ⊂ R1+ n+m.In a forthcoming publication, we address both of these concerns by developing a theory of (semi-)flows on M-polyfolds generated by sc-smooth vector fields, and constructing examples of flows that traverse the bottleneck in Ωmn but cannot be extended to ambient space. We hope that this will establish the foundation for a new exciting research direction in variable-dimension flow-based machine learning. 

Acknowledgements 

The authors gratefully thank Aron Persson for many fruitful and insightful discussions. 8The work of PÅ and FO was partially funded by the Swedish Research Council under grant agreement no. 2025-05053. The work of PÅ, FO, and VVN was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The work of AF was supported by a postdoctoral fellowship funded by Kempestiftelserna under grant number JCSMK24-0043. The computations were enabled by resources pro-vided by the National Academic Infrastructure for Su-percomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725. 

References 

Per Åhag, Rafał Czyż, Håkan Samuelsson Kalm, and Aron Persson. On manifold-like polyfolds as differen-tial geometrical objects with applications in complex geometry. arxiv:2401.09875 [math.DG] , 2025. Michael S. Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In Pro-ceedings of the International Conference on Learning Representations (ICLR) , 2023. Emma Andersdotter, Daniel Persson, and Fredrik Ohls-son. Equivariant manifold neural ODEs and differen-tial invariants. Journal of Machine Learning Research ,26:1–33, 2025. P. Baldi and K. Hornik. Neural networks and principal components analysis: Learning from examples with-out local minima. Neural Networks , 2:53–58, 1989. Sanjay P. Bhat and Dennis S. Bernstein. Finite-time stability of continuous autonomous systems. SIAM Journal on Control and Optimization , 38(3):751–766, 2000. H. Bourlard and Y. Kamp. Auto-association by multi-layer perceptrons and singular value decomposition. 

Biological Cybernetics , 59:291–294, 1988. Ricky T. Q. Chen. torchdiffeq, 2018. URL https: //github.com/rtqichen/torchdiffeq .Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Pro-cessing Systems , volume 31. Curran Associates, Inc., 2018. Oliver Fabert, Joel W. Fish, Roman Golovko, and Ka-trin Wehrheim. Polyfolds: A first and second look. 

EMS Surveys in Mathematical Sciences , 3(2):131–208, 2016. Luca Falorsi and Patrick Forré. Neural ordinary differ-ential equations on manifolds. In Proceedings of the INNF+ Workshop of the International Conference on Machine Learning (ICML) , 2020. Alexander Friedrich and Viktor Vigren Näslund. polyn-ode, 2026. URL https://github.com/turbotage/P olyNODE .Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Confer-ence on Artificial Intelligence and Statistics , volume 9, pages 249–256. PMLR, 2010. Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems , 34:014004, 2018. G. E. Hinton. Connectionist learning procedures. Arti-ficial Intelligence , 40:185–234, 1989. Helmut Hofer, Krzysztof Wysocki, and Eduard Zehn-der. A general Fredholm theory. I. A splicing-based differential geometry. J. Eur. Math. Soc. (JEMS) , 9 (4):841–876, 2007. Helmut Hofer, Krzysztof Wysocki, and Eduard Zehn-der. A general Fredholm theory. II. Implicit function theorems. Geom. Funct. Anal. , 19(1):206–293, 2009a. Helmut Hofer, Krzysztof Wysocki, and Eduard Zehnder. A general Fredholm theory. III. Fredholm functors and polyfolds. Geom. Topol. , 13(4):2279–2387, 2009b. Helmut Hofer, Krzysztof Wysocki, and Eduard Zehnder. Polyfold and Fredholm theory. In Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge. A Series of Modern Surveys in Mathematics , volume 72, pages xxii+1001. Springer, Cham, 2021. Yan LeCun. Modèles connexionistes de l’apprentissage .PhD thesis, Université de Paris VI, 1987. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In Proceedings of the 11th International Conference on Learning Representations (ICLR) , 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In Proceedings of the 11th International Conference on Learning Representations (ICLR) , 2023. Aaron Lou, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser Nam Lim, and Christopher M De Sa. Neural manifold ordinary differential equa-tions. In Advances in Neural Information Processing Systems , volume 33, pages 17548–17558. Curran As-sociates, Inc., 2020. 9Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. In Advances in Neural Information Processing Systems , volume 33, pages 2503–2515. Curran Associates, Inc., 2020. Facundo Mémoli and Guillermo Sapiro. Distance func-tions and geodesics on submanifolds of Rd and point clouds. SIAM Journal on Applied Mathematics , 65 (4):1227–1260, 2005. Vittorino Pata. Fixed Point Theorems and Applications .Springer International Publishing, 2019. Domènec Ruiz-Balet and Enrique Zuazua. Neural ODE control for classification, approximation, and trans-port. SIAM Review , 65:735–773, 2023. Matthew Thorpe and Yves van Gennip. Deep limits of residual neural networks. Research in the Mathemati-cal Sciences , 10(6), 2023. Alexander Tong, Kilian Fatras, Nikolay Malkin, Guil-laume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and gener-alizing flow-based generative models with minibatch optimal transport. Transactions of Machine Learning Research , 03, 2024. Joa Weber. Scale calculus and M-polyfolds—an introduc-tion . 32 o Colóquio Brasileiro de Matemática. Instituto Nacional de Matemática Pura e Aplicada (IMPA), Rio de Janeiro, 2019. Han Zhang, Xi Gao, Jacob Unterman, and Tom Ar-odz. Approximation capabilities of neural ODEs and invertible residual networks. In Proceedings of the 37th International Conference on Machine Learning ,volume 119, pages 11086–11095. PMLR, 2020. 10 A Experimental Details 

The implementation for pytorch for the experiments below can be found in [Friedrich and Vigren Näslund, 2026] at 

https://github.com/turbotage/PolyNODE .

> A.1 Reconstruction Experiments

In all reconstruction experiments we employ a reduced setup for the vector fields detailed in Table 1. Recall that we keep the speed of Y1 constant to ensure a total time T = 1 and a latent time of 1/2. The rates of the compressing vector fields are constant as well, ki = 25 , and a = 1 /2.Table 1: Structure of vector fields for reconstruction experiments 

Region Shape of Vector Field Neural Net-works Input 

{τ < τ 0} (C, Y 2, ..., Y n+m+1 ) {Y }n+m+1 2 (τ, x, y )

{τ0 < τ < τ 1} (C, 0, ..., 0, Y n+2 , ...Y n+m+1 ),

{Yi}n+m+1  

> i=n+2

as in (8), - (τ, x, y )

{τ1 < τ < τ 2} (C, 0, ..., 0) - -

{τ > τ 2} (C, Y 2, ..., Y n+m+1 ) {Y }n+m+1 2 (τ, x, y )

All neural networks are sequential with four fully connected layers of width 200 and tanh as activation function. The first three layers have bias, the last one does not. The bias are initialised as 0 and linear layers are initialised according to a normal distribution. Using the Euler ODE solver implemented in the torchdiffeq python package with 500 time steps, along with the adjoint method for the backpropagation of the same package, [Chen, 2018], we solve the flow equation in two stages, each integrating for half the total time. Thus, after the first stage, the flow maps the input data into the latent space. There, we project to the latent space to eliminate the remaining numerical error in the latent components, which we find to be of order 10 −8, then we continue the flow. The backwards pass for the compressing vector fields is customised as described in Appendix B. The models are trained using RMSprop, with momentum 

0.3.The experiments were run on a cluster, using the CUDA backend of PyTorch with NVIDIA Tesla T4 GPUs, 

16 GB VRAM, and Intel(R) Xeon(R) Gold 6226R CPUs ( 2.90 GHz).  

> Spiral

For the spiral experiment, consider the following parametrisation where v ∈ R, e2 = (0 , 1, 0, 0) .

f : [0 , 2π] → R3 , s 7 → (1 + 0 .5s)(cos( vs ), sin( vs ), 0) + 6 e2.

We vary the number of turns from N = 0 .5 to N = 5 by varying the speed v. The input training data is equidistantly sampled according to intrinsic distance dS of the spiral, which can be computed analytically. The number of sample points is 20 times the length of the spiral, rounded down, and capped at 5000 due to memory constraints during training. For the validation data 0.3 times the number of training samples are drawn randomly in the angle domain 

(0 , 2π).We choose Ω21 with τ1 = 0 and τ2 = 1 as an M-polyfold, and τ0 = −3 for the onset of the compressing vector fields. For the embedding we choose τX = −7 and τY = 8 , hence the speed is C = 15 . The models are trained for about 27000 epochs unless the loss does not improve significantly. This happened only for N = 0 .5 and N = 1 ,which stopped at about 8000 and 18000 epochs respectively. Figure 8 shows time slices of the unwinding and encoding of spirals with N = 2 to N = 5 and Figure 9 shows the corresponding projected reconstructions. Up to N = 3 the encoding and reconstruction is excellent, but these plots also illustrate that our autoencoding NODE achieves very good encoding, even if the reconstruction suffers slightly. This observation is supported by the mean reconstruction error plot in Figure 10 and the fact the monotonicity error is below 0.02% in all experiments. This is likely due to the rather strong approximate isometry loss L3 for the unwinding and the explicit compressing vector field. Hence the encoder and decoder are not symmetric.  

> Sphere

Consider a two dimensional round sphere of radius 1 in R4 around the point x0 = (0 , 0, 0, 3) given by 

{(z1, z 2, 0, z 3) | (z1, z 2, z 3) ∈ S21 (x0) ⊂ R3}. The z ∈ S21 in turn are parametrised by spherical coordinates. The training data is sampled from the sphere, equidistantly on a grid in the angle domain (0 , 2π) × (−π, π ) with 6000 

11 Figure 8: Unwinding and encoding of spirals with number of turns ranging from N = 2 to N = 5 

Figure 9: Reconstruction (2d projection) of spirals with number of turns ranging from N = 2 to N = 5 

Figure 10: Mean euclidean reconstruction error relative to extrinsic spiral diameter points in total. The 1800 validation data points are sampled randomly from the angle domain. We choose Ω12 with 

τ1 = 0 and τ2 = 3 as an M-polyfold with τ0 = −3 for the onset of the compressing vector fields. For the embeddings of the samples X = {pi}i∈I = {zi + τX e1}i∈I and Y = {zi + τY e1}i∈I we choose τX = −7 as well as τY = 10 . This leaves us with a speed of C = 17 . The model was trained for about 10000 epochs. 

> A.2 Classification Experiments

Both the radial and angular classification experiments use the same neural network architecture. The layers consist of a linear input layer with input size 4 (spatial + time) and output size 512 , followed by two hidden layers with input and output size 512 . A skip connection is used between the hidden layers. The final output layer is a linear layer with input size 512 and output size 3. All layers are initialised using Xavier uniform initialisation [Glorot and Bengio, 2010], and the hidden-layer biases are zero-initialised. ReLU is used as the nonlinearity between layers. The models are trained using the Adam optimiser. A reduce-on-plateau learning rate scheduler is used with a patience of 20 epochs and a factor of 0.8. Both experiments run for 300 epochs, and the batch size is 32 .The target points are placed equidistantly to the latent line at the points (−3, 4, 0) for l1, (0 , 5, 0) for l2, and 12 (3 , 4, 0) for l3. For the attractor terms in (9) we use C = 50 and k = 64 /d min , where dmin is the minimum distance between any two target points ( √2 in this setup). Experiments were conducted on the CUDA backend of PyTorch with an NVIDIA RTX 4090 GPU, with CUDNN set to deterministic mode and a fixed random seed of 42 , and Intel(R) Core i9-14900K CPU. 

B Changes to the Backpropagation 

The derivative of the compressing vector field (8) has a singularity of the form |yi|a−1 at yi = 0 . However, we need to calculate the derivative during training with gradient descent methods. Thus, we introduce a cut-off for the vector field for small values of yi, and implement a corresponding derivative which is used in the custom backward method of the vector field. For simplicity we use a cut-off function φ based on a polynomial, 

φ(x) = 



0 for x < 0(3 − 2x)x2 for 0 ≤ x ≤ 11 for x > 1

,

which has derivative 

φ′(x) = 



0 for x < 06(1 − x)x for 0 ≤ x ≤ 10 for x > 1

.

Scaling φ gives a cut-off function φb,c := φ(( x − b)/(c − b)) in the interval (b, c ), b ≥ 0. In the forward pass the vector field then reads 

Y1+ n+j (p) = −kj (p) sign (yj )|yj |aφb,c (|yj |) .

In the backwards pass the derivative is given as 

∇Y1+ n+j (p) = −∇ kj (p) sign (yj )|yj |aφb,c (|yj |)

− kj (p)a|yj |a−1φb,c (|yj |)ej

− kj (p)|yj |a φ′

> b,c

(|yj |)

c − b ej ,

which does not have a singularity at 0.In the experiments of Section 4.1 we use b = 10 −7, and c = 10 −6. This introduces numerical errors in the latent state in the compressed directions y which are at most of the order b. In these experiments we find errors of the order 10 −8 and use a projection to the latent plane to eliminate them as described in Appendix A. 13
Title: Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching

URL Source: https://arxiv.org/pdf/2602.15827v1

Published Time: Wed, 18 Feb 2026 02:08:40 GMT

Number of Pages: 12

Markdown Content:
# Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching 

Zhen Wu* 1, Xiaoyu Huang* 1,2, Lujie Yang* 1, Yuanhang Zhang 1,3, Koushil Sreenath 1,2, Xi Chen 1,Pieter Abbeel â€ 1,2, Rocky Duan â€ 1, Angjoo Kanazawa â€ 1,2, Carmelo Sferrazza â€ 1, Guanya Shi â€ 1,3, C. Karen Liu â€ 1,41Amazon FAR, 2UC Berkeley, 3CMU, 4Stanford University, â€ Amazon FAR team co-lead Page: https://php-parkour.github.io/ (a) ï½ž3 m/s 

60 seconds 

1. 25 m

(b) 

(d) 

60 seconds 

(c) ï½ž3 m/s 

Fig. 1: Perceptive Humanoid Parkour (PHP) enables a Unitree G1 humanoid robot to execute highly dynamic, long-horizon parkour behaviors using onboard perception. By composing various agile human skills via motion matching and a teacher-student training pipeline, we train a single multi-skill visuomotor policy capable of complex contact-rich maneuvers including (a) cat-vaulting over a short obstacle followed by dash-vaulting over a higher obstacle at approximately 3 m/s, (b) climbing onto a 1.25 m (96% of robot height) wall, and rolling down, (c) speed-vaulting over an obstacle at approximately 3 m/s, and (d) a 60-second continuous traversal of a complex parkour course with autonomous skill selection and seamless transitions. 

Abstract â€”While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environ-ments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only 

* Equal contribution. 

onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25 m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations. 

I. I NTRODUCTION 

Achieving the agility and adaptivity of human motion in traversing complex terrains remains a central challenge for humanoid robotics. Humans traverse challenging terrains of drastically different dimensions by rapidly selecting and chaining dynamic whole-body skills based on perceived envi-ronmental context. Our goal is to endow humanoids with the same capability. In this work, we study parkour as a concrete, self-contained testbed for this broader objective. Parkour highlights several core challenges. First, the robot must perform highly dynamic and contact-rich skills, such as climbing walls around or above its body height or vaulting 

> arXiv:2602.15827v1 [cs.RO] 17 Feb 2026

over obstacles within fractions of a second. This requires effective control in the humanoidâ€™s vast, high-dimensional action space. Second, these skills must be tightly coupled with exteroception, such as vision, to enable adaptation to environmental variation and rapid reaction to unexpected per-turbations. Furthermore, to generalize beyond isolated ma-neuvers and traverse complex obstacle courses, the robot must consolidate many highly dynamic skills into a single visuomotor policy, which becomes increasingly difficult as the number and diversity of required skills grow. Human motion data has become essential for learning highly dynamic humanoid behaviors. Prior work [20, 43] has used hu-man motion data to successfully demonstrate highly dynamic skills such as jumping, rolling, and flipping. However, highly dynamic motion data is inherently scarce: capturing fast, contact-rich maneuvers typically requires specialized setups and careful curation, so datasets often include only one or two demonstrations per skill, each lasting just a few seconds. This scarcity is not unique to parkour but applies broadly to all dynamic human skills. Yet long-horizon tasks such as parkour require both rich within-skill variation that adapts to how the robot approaches an obstacle, and smooth, natural transitions between multiple skills across complex courses. To address this challenge, we adopt motion matching [5, 9] as a simple yet powerful mechanism. Motion matching synthe-sizes long-horizon motion by retrieving and stitching motion fragments via nearest-neighbor search in a designed feature space. Crucially, this process densifies a sparse motion library by producing diverse transitions across approach distances, headings, and timing, while preserving the realism of captured motions. In our framework, motion matching enables the gen-eration of a large set of obstacle-adaptive, long-horizon kine-matic reference trajectories for downstream policy learning. Learning a visuomotor policy that executes dozens of highly dynamic skills requires perceptive inputs that can be efficiently simulated and reliably transferred to the real world. To improve training efficiency, prior work typically trains privileged state-based experts in simulation and distills them into vision-based students using DAgger [32]. However, for humanoid parkour, pure imitation loss is limited: compounding errors can quickly derail highly dynamic skills such as climbing and vaulting. To address this, we augment distillation with an RL objective that provides task-level corrective feedback, steering the student towards successful traversal and yielding a scalable recipe across many skills. To this end, we present Perceptive Humanoid Parkour (PHP), a modular framework that integrates human motion priors, long-horizon skill composition, and perceptive control. We first retarget human motion data into a library of robot-compatible atomic skills using OmniRetarget [43]. We then employ motion matching to compose these skills into a diverse set of long-horizon kinematic trajectories. These composed trajectories preserve agility and smooth transitions, while providing sufficient variation to learn adaptive long-horizon behaviors. We then train motion-tracking expert policies and distill them into a single depth-conditioned, multi-skill policy that enables the robot to autonomously select and transition among behaviors, such as stepping, climbing, and vaulting, using onboard depth sensing. Our contributions are threefold: 1) An efficient kinematic skill composition pipeline that chains retargeted human motions into diverse long-horizon trajectories via motion matching. 2) A scalable training framework that distills multiple ex-perts into a single visuomotor policy, enabling seamless transitions across diverse parkour skills. 3) Successful zero-shot sim-to-real transfer of depth-based policies on a physical humanoid robot, achieving highly dynamic parkour over various obstacles. II. R ELATED WORKS 

The goal of parkour is to traverse challenging terrains agilely by perceiving, reacting, and chaining skills for different obstacles. We review related work in these areas. 

A. Perceptive Terrain Traversal for Legged Robots 

While blind locomotion has achieved strong robustness on moderately structured terrains such as slopes and stairs on quadrupedal robots [19, 28, 21, 18], perception enables traversal of substantially more challenging terrains [26]. In par-ticular, perception is critical for handling sparse footholds [1, 44, 46] and discontinuous terrain such as gaps and tall ob-stacles [1, 45]. Building on these capabilities, prior work has enabled quadrupeds to traverse parkour-style terrain courses with consecutive gap jumps and obstacle climbs [8, 23, 13, 33]. However, translating the success on quadrupeds to hu-manoids remains challenging. While quadrupedal parkour skills can often be trained from scratch via reward shaping, this approach scales poorly to humanoids due to high-dimensional whole-body control. As a result, prior perceptive humanoid locomotion has primarily focused on lower-dynamic terrain traversal, including stair climbing [22], walking on sparse terrain [37, 12, 2], and stepping onto low platforms [52]. Moreover, to reduce exploration difficulty in RL when training from scratch, most works adopt a teacher-student pipeline where an expert is trained with privileged states and a vision-based student is distilled via DAgger [8, 33]. We follow this paradigm but find pure DAgger insufficient for highly dynamic humanoid skills, and therefore augment it with RL to improve distillation performance. Note that this differs from the fine-tuning stage in [33], which primarily focuses on adapting an already performant DAgger-distilled policy to unseen terrains. 

B. Humanoid Skill Chaining with Human Motion Data 

Using human motion references effectively reduces reward engineering and produces agile, natural humanoid behav-iors [20, 43, 48, 29, 40, 7], but comes at the cost of more challenging skill chaining. With reward shaping, quadrupeds can learn transitions either implicitly by a single policy [8, 23, 12, 2], or through specialist switching or distillation using a shared locomotion state [13, 51, 6, 33]. In contrast, human motion data spans heterogeneous styles that can lie in disjoint Teacher Training 

> Kinematic
> Parkour
> Trajectories
> RL Motion Tracking
> Teacher
> ðœ‹ !

Student Training      

> Student
> Multiple Teachers
> ðœ‹ "ðœ‹ $ðœ‹ %â€¦ðœ‹ &
> DAgger + RL

Zero -Shot Sim2Real Skill Composition    

> Locomotion Atomic Parkour Skills
> Proprioception
> Heightmap
> Reference Motion
> Proprioception
> Velocity Command
> Depth Image
> Long -Horizon Parkour Trajectories
> Skill Composition
> Nearest Neighbor
> Search
> Skill 1
> Skill 2
> Search Window
> Motion Matching

Fig. 2: Perceptive Humanoid Parkour overview. Atomic parkour skills are composed into long-horizon kinematic reference trajectories via motion matching. Single-skill teacher policies are trained with privileged information using RL-based motion tracking. Multiple teachers are distilled into a single depth-based student policy using a hybrid DAgger and RL objective. This scalable recipe enables zero-shot sim-to-real transfer onto a physical humanoid robot that adaptively traverses through complex terrains by autonomously executing highly agile parkour skills using onboard perception. regions of the state space, making long-horizon composition a fundamental challenge. AMP [31] addresses this challenge by training a single policy to learn a distribution of skills, allowing transitions to implicitly emerge from RL exploration, but replaces hand-crafted rewards with a learned style reward from motion data. While promising in animation and quadrupeds [42, 39], humanoid hardware demonstrations have so far been limited to less agile skills such as walking, stepping, and box lift-ing [50, 35, 38]. To address the transition problem more explicitly, another line of work generates intermediate kinematic trajectories us-ing learned kinematics models (e.g., MDM [36]) and executes them with tracking controllers (e.g., DeepMimic [30]). These kinematics models can provide smooth transition references at test time [41, 24, 10, 47] or training time [16], but their tra-jectory quality degrades significantly in the low-data regimes common in parkour. This often requires either costly iterative co-training [41] to recover usable motion or receding-horizon replanning [15], which is costly with perception in real time. In contrast, we adopt motion matching [5, 14] as a sim-ple yet highly effective source of kinematic references for humanoid skill chaining. Motion matching has been widely adopted in video games and character animation for its simplicity and practical controllability, while still producing high-quality motion [5, 11]. While a mature technique in animation [3], it has so far been applied in robotics only to relatively simple quadruped behaviors [17]. In this work, we show that it is a powerful tool for chaining dynamic and expressive human skills over difficult terrain courses for humanoid robots, substantially improving both success rate and transition smoothness. III. A DAPTIVE AND AGILE LONG -H ORIZON PARKOUR 

A. Overview 

The objective of this work is to enable a humanoid robot to execute agile parkour behaviors over multiple obstacles autonomously using onboard perception. We first generate long-horizon kinematic reference motions via motion match-ing by composing locomotion with atomic parkour skills. We then train motion-tracking expert policies with privileged observations in simulation, and finally distill them into a depth-based student policy using DAgger in combination with a PPO objective, enabling zero-shot sim-to-real deployment. An overview of the system is shown in Fig. 2. 

B. Skill Composition via Motion Matching 

Motion matching [5, 9] is a technique originally developed in the video game industry for interactive character control, where motion is generated online by selecting, at each frame or transition point, the animation frame from a large database whose motion features best match the current pose and desired future behavior. In this work, we adopt motion matching as an offline motion synthesis module for composing scarce atomic parkour skills with locomotion into long-horizon references. 

1) Basic motion matching: We briefly summarize the stan-dard motion matching formulation; implementation details are provided in Appx. A. Let a motion database consist of N

frames, where each frame i is associated with a kinematic pose qi and a matching feature vector xi derived from 

qi. Following [14], xi concatenates (i) short-horizon future trajectory positions and facing directions, (ii) local foot joint positions and velocities, and (iii) root velocity, all expressed in the characterâ€™s local coordinate frame. When generating transitions, given the current character state and a desired 2D velocity command, we first convert the command into a desired future trajectory and facing directions, and then concatenate these with foot positions, velocities and root velocities from the current state to form the query feature 

Ë†xt. The best matching frame is then retrieved via 

iâ‹†t = arg min 

> iâˆˆC t

âˆ¥ Ë†xt âˆ’ xiâˆ¥2, (1) where Ct denotes the search window of the user-specified upcoming skill. This nearest-neighbor search is performed pe-riodically (every M frames) or when the commanded velocity changes significantly. After selecting iâ‹†t , the system transitions playback to frame iâ‹†t and plays forward from that frame until the next search is performed. A short blending window is applied around transitions to avoid discontinuities. 

2) Long-Horizon Parkour Trajectory Synthesis: Since lo-comotion (walking and running) serves as a ubiquitous and naturally reusable connector between more challenging park-our skills, we generate long-horizon parkour trajectories by composing locomotion segments with short parkour skill clips in the form of Locomotion â†’ Parkour Skill â†’

Locomotion . By routing all skills through a shared locomo-tion manifold, this formulation enables consistent transitions across heterogeneous skills without requiring specific, hand-captured transitions between every possible skill pair, and supports scalable composition of long-horizon behaviors. We maintain (i) a locomotion database Dloco =

{(xloco  

> i

, qloco  

> i

)} and (ii) a set of skill databases {D k}, one for each parkour skill k. Each skill clip is paired with a corresponding terrain asset. For every atomic skill clip in Dk,we manually annotate the skill start and end frame indices 

(sk, e k). We additionally define a pre-skill entry window of skill-dependent length Hk:

Ek := [ sk âˆ’ Hk, s k ], (2) which corresponds to the approach phase right before the main contact-rich maneuver, where transitioning into the clip is meaningful. For example, for a vault clip, Ek captures the final approach steps before takeoff, avoiding transitions outside the intended approach phase. 

Locomotion mode. During locomotion, we run standard motion matching via Eq.(1) with Ct = Dloco , and advance playback sequentially as described in Sec. III-B1. 

Locomotion â†’ Skill transition. When a transition into skill k is required, we restrict the search window Ct to the pre-skill entry window Ek, and transition to the matched entry frame through Eq.(1). After the transition, the skill clip is replayed sequentially until the annotated end frame ek. At the switch, we place the paired terrain by applying the terrain-to-root offset at the matched entry frame in the reference clip to the robotâ€™s current root pose. During skill execution, we disable further motion matching and simply advance the playback index to preserve the contact-rich human motion. 

Skill â†’ Locomotion transition. After reaching ek, we return to locomotion by resuming motion matching via Eq.(1) with Ct = Dloco , and continue sequential playback. (c)    

> 0Â°, 0.95 m 20 Â°, 1.84 m
> 1m/s
> 2 m/s
> 4.8 m
> right le g
> left le g
> 3.9 m
> (b)
> (a)

Fig. 3: Diverse variations of composed parkour skills synthesized via motion matching. (a) Different approach distances trigger varying stride phases and entry poses. (b) Diverse locomotion speeds, directions, and durations. (c) Ran-domized terrain poses and shapes. 

Dataset construction. We synthesize long-horizon refer-ence trajectories by rolling out the motion-matching compo-sition procedure as follows. As visualized in Fig. 3(b), each trajectory starts from a standing state and enters a locomotion phase driven by 2D velocity commands sampled from two speed levels ( low (1m/s), high (2m/s)) and five turning direc-tions ( âˆ’90 â—¦, âˆ’45 â—¦, 0â—¦, 45 â—¦, 90 â—¦). We then transition into skill 

k and replay the skill clip sequentially; during the skill, we set the command to go straight ( 0â—¦) while keeping the same speed level as the preceding locomotion segment. After reaching the annotated end frame ek, we return to locomotion and continue for an additional 2 seconds before stopping. Through-out synthesis, we record the per-timestep velocity commands alongside the generated kinematic reference poses {qt}, and use these paired trajectories for subsequent policy training. 

Transition Density. Motion matching naturally induces a high density of transitions by allowing a skill to be entered from multiple locomotion states that are nearby in the motion feature space. We exploit this to generate diverse skill en-trances spanning different approach distances and stride phases (e.g., adding a preparatory step before a jump, or initiating a vault from different phases of a running gait), densifying the distribution of pre-skill states. As illustrated in Fig. 3 (a), varying the initial approach distance (e.g., 3.9 m vs. 4.8 m) forces the motion matching engine to select different stride sequences, resulting in distinct entry poses such as left-leg versus right-leg leads. To prevent non-causal shortcuts (e.g., relying on elapsed time or step count), we randomize the pre-skill locomotion duration by sampling it uniformly from [0 .1, 3] s, with an average interval of 0.3 s. Such diverse motion-terrain pairs encourage context-based reaction, and are critical for learning a policy that can reliably trigger the correct skill under varying distances and timings. 

Terrain Randomization. To improve robustness beyond the training obstacles while keeping the reference feasible, we ran-domize obstacle geometry and pose around each synthesized trajectory. Specifically, obstacle width is sampled from the minimum required by the reference up to 1.5 m; the remaining dimensions are perturbed within Â±5 cm; and obstacle yaw is randomized within Â±45 â—¦, as illustrated in Fig. 3(c). This exposes the policy to variations in obstacle shape and pose without invalidating the underlying reference. 

Distractors. We place distractor boxes with random sizes and poses near the reference trajectory to improve robustness to irrelevant objects and reduce overfitting in the real world. 

C. Learning a Highly-Dynamic Visuomotor Policy 

Our goal is to train a single perceptive policy capable of var-ious long-horizon parkour skills. Commanded by a target ve-locity, the humanoid will autonomously perform various park-our skills based on the obstacles it perceives. Because the skills are highly dynamic, we train skill-specific experts to achieve high motion quality and then distill them into a single visuo-motor policy. To ensure scalability, we use a unified expert and distillation formulation without motion-specific tuning. 

1) Training Expert Policies with Motion Tracking: We follow BeyondMimic [20] and OmniRetarget [43] for motion tracking, and refer readers to these prior works for details. 

Observations include reference joint position/velocity, ref-erence pelvis pose error, pelvis linear/angular velocity, joint position/velocity, and the previous action. We additionally provide the expert with a 0.7 m Ã— 0.7 m height scan, allowing it to adapt to terrain randomizations. Unlike [43], we enable global tracking with privileged observations (pelvis global position and velocity) so the expert can learn recovery behaviors. This is important because the reference motion is tightly coupled with the terrain, meaning small drift or timing errors can quickly accumulate and must be corrected to stay on the intended trajectory. While these privileged states are not available on hardware, they can be inferred from visual inputs by the student policy. 

Adaptive Sampling is essential for learning difficult skills in expert training, which prioritizes sampling from regions that fail more frequently. For example, without it, the high-wall climbing expert fails to converge to a meaningful behavior. 

Rewards, Terminations, and Domain Randomization 

follow BeyondMimic [20]: DeepMimic-style tracking rewards with action rate, joint limits, and collision penalties, tracking-based early termination, and lightweight randomizations. 

Actions are joint PD targets normalized by a fixed action scale. Due to challenging RL exploration, we set the action scale to 1 for all experts, instead of the heuristics used in [20]. 

2) Distilling a Unified Student Policy with DAgger and RL: A common approach for learning a unified policy from multiple experts is to apply DAgger-style imitation learn-ing [32, 8, 51, 33, 20]. While effective for easier motions such as stepping, we find that DAgger alone is insufficient for highly dynamic skills such as climbing and vaulting. These skills depend on brief, high-magnitude torque bursts, but per-step imitation objectives like DAgger do not account for episode outcomes and therefore do not explicitly favor such high-torque actions. For example, actions that result in higher or lower root positions that are symmetric about the reference may receive identical DAgger loss, even though only the higher-root trajectory successfully clears the obstacle. To address this, we apply PPO alongside DAgger with a curriculum, 

L = Î»PPO LPPO + Î»D LD , Î»PPO + Î»D = 1 , (3) where Î»PPO and Î»D are their curriculum weights. Note that the primary role of PPO is to provide a success-driven signal that encourages exploiting expert behaviors, such as high-torque actions, rather than exploring beyond the expert skill distri-bution. This hybrid setup substantially improves the unified policyâ€™s performance on diverse, highly dynamic skills. 

Observations, Actions, and Domain Randomization. The policy observes proprioception signals including pelvis gravity vector and angular velocity, joint positions and velocities, and the previous action. For vision, we use depth images rendered with Nvidia WARP [25] for high-throughput training. The policy also receives velocity commands defined in Sec. III-B2. The action space and domain randomization for sim-to-real transfer are identical to those used in expert training setup. 

Camera modeling and depth artifacts. We calibrate the simulated camera by matching robot self-visibility across a set of poses in simulation and hardware using ROI overlap, and randomize camera extrinsics within 2.5 cm translation and 2.5Â° rotation around the calibrated value to improve robustness to viewpoint shifts and mounting variability. We inject realistic depth noise following prior work [33], but exclude Gaussian blur since it can obscure obstacles at high speed. Finally, we randomize observation delay between 60 ms and 80 ms to simulate hardware latency fluctuations. 

Curriculum. Since PPO gradients are noisy in the early stages and can otherwise undermine distillation, we apply a warmup curriculum that gradually shifts from DAgger to PPO. The curriculum includes three parts. First, we linearly tune down Î»D in Eq. (3) during the first half of training, capped at 0.1: Î»D (k) = max 



0.1, 1 âˆ’ kK/ 2



,where k is the current iteration and K is the total iterations. Second, left-right symmetry introduces multimodality: many skills admit two equally valid mirrored executions, for example, clearing a hurdle with either the left- or right-leg lead, while the reference trajectory represents only one t = 2.37 s t = 2.85 s t = 3.63 s          

> t = 0.62 st = 2.32 s t = 2.69 s t = 3.6 2s
> t = 0.62 s

Fig. 4: Side-by-side comparison of high-climb agility. The robot climbs onto a 1.25 m wall within 3.63 s. of them. As a result, the distilled policy may perform the mirrored mode, which still completes the skill but incurs a large tracking error and would be terminated incorrectly. This spurious failure signal can cause high reward variance for PPO. To mitigate this issue, we relax the termination threshold from 0.5 m for the expert to 1 m using the same linear schedule, so mirrored modes are not terminated prematurely. Finally, we enable adaptive learning rate and KL-based exploration control only when Î»PPO exceeds 0.1. 

Adaptive Sampling of rollout start points is disabled during student training. While it helps experts focus on failures to learn more difficult segments, it can undersample â€œborderlineâ€ clips that do not fail in simulation but exhibit jittery behavior, which often leads to large sim-to-real gap on hardware. To further avoid data imbalance across skills, we sample each skill evenly and sample uniformly within each skill. IV. E XPERIMENTS 

We evaluate the proposed framework through a series of simulation and real-world experiments on a Unitree G1 hu-manoid (1.3 m tall with 29 DoFs). For training, we use a 3-layer CNN and a 5-layer MLP with hidden sizes [2048, 1024, 512, 256, 128], trained with 16,384 parallel environments. Both expert and student policies are trained for 20K iterations. 

A. Real-World Results 

We evaluate our system on real-world parkour tasks requir-ing both highly-dynamic individual skills, long-horizon multi-skill composition and adaptation to environmental changes. All skill execution is autonomous, while only simple 2D velocity commands are provided for navigation. 

1) Human-Level Agility: We first demonstrate that the robot can execute highly dynamic parkour skills, including a direct comparison with a human parkourist on a challenging high-wall climb. 

High-Wall Climb with Human Comparison. We compare the robotâ€™s high-wall climb against a human performing the same maneuver [34]. While often considered a fundamental parkour technique, the high-wall climb demands substantial upper-body strength and precise whole-body coordination, and remains difficult for untrained individuals. Despite this, the robot successfully performs the climb at a pace comparable to the human. As shown in Fig. 4, the robot executes a fast and coherent sequence with closely matched timing across key events (toe-off â†’ pull-up â†’ swing â†’ stable stand). For a 1.25 m wall (96% of the robotâ€™s height), the robot climbs onto the platform in 3.63 s measured from toe-off. 

Additional Parkour Skills. We demonstrate additional highly dynamic parkour skills that require rapid contact tran-sitions and momentum preservation. As shown in Fig. 5(a), the robot clears a 0.4 m-high, 0.5 m-long obstacle within 0.8 sfrom toe-off to toe-on, while covering more than 2 m forward (154% of its height). The motion reaches a peak forward speed of 3.41 m/s with an average speed of 2.53 m/s, highlighting its effective momentum preservation across the contact. Fig. 5(b) shows a drop landing from a 1.25 m platform. Upon landing, the robot flexes its lower-body joints to absorb impact and stabilize its posture. 

2) Multi-Obstacle Course: A key strength of our frame-work is that the policy generalizes to complex, multi-obstacle courses despite the training data containing only single-obstacle traversal. This capability emerges from motion-matching-based composition, which synthesizes long-horizon reference trajectories that explicitly chain skills through shared locomotion segments and expose the policy to diverse ap-proach distances and timings. As a result, the policy learns to execute skills reliably across varying obstacle sequences without explicit multi-obstacle supervision. 

Various Skills and Adaptivity to Obstacle Changes. 

As shown in Fig. 5(c), the robot composes multiple skills, including stepping, low and high wall climb, into a continuous run over courses with several obstacles. The visuomotor policy generates transitions online, enabling smooth skill switching throughout the traversal. We further demonstrate closed-loop adaptivity by randomly displacing multiple obstacles by ap-proximately 0.5 m during execution. The policy adapts by adjusting its approach and maneuver timing, allowing the robot to continue and complete the traversal in response to these obstacle changes. These results demonstrate the adaptivity of our policy in long-horizon terrain traversal. 

B. Quantitative Results in Simulation 1) Experiment Setup: We evaluate all methods using suc-cess rate on parkour traversal tasks. Each task requires the robot to move forward at a fixed command speed (1.0 m/s or 2.0 m/s) and clear a single obstacle of a specified height and 20 â—¦ yaw randomization. To vary approach conditions, the humanoid is initialized at a random distance in front of the obstacle: for 1 m/s tasks, distances are sampled uniformly from 1.5 m to 3.0 m; for 2 m/s tasks, from 3.0 m to 4.5 m. For each task, we evaluate 100 obstacle instances with different initial distances. A trial is successful if the robot traverses the obstacle and travels an additional 1.5 m without falling within a fixed time horizon. We report average success rates of 5 trials per obstacle per task (500 trials total per task). We train all variants with the full skill set, with details in Appx.B. 

2) Baseline Comparison: We evaluate the contribution of three key components: human reference motions, motion-matching-based skill composition, and the two-stage teacher-student training framework, by comparing our method against t = 0.00 s t = 0.23 s t = 0.78 st = 0.52 s             

> t = 0.00 s t = 0.23 s t = 0.55 s t = 0.82 s
> (a)
> (b)
> (c) t = 4 s t = 9 s t = 11 s t = 14 s t = 27 s
> t = 34 s t = 40 s t = 43 s t = 46 s t = 48 s

Fig. 5: Hardware results demonstrating agile, long-horizon parkour behaviors, including (a) a cat vault, (b) a drop landing from a 1.25 m wall, and (c) a 48-second terrain traversal with online adaptation to real-time obstacle displacement. TABLE I: Baseline success rate on parkour tasks with different commanded speeds and obstacle heights.                                

> Method 1.0 m/s 2.0 m/s
> 36 cm 58 cm 76 cm 36 cm 58 cm 76 cm Velocity Tracking 1.00 0.00 0.00 1.00 0.00 0.00 Uncomposed Data 0.06 0.02 0.00 0.37 0.27 0.07 End-to-end Depth 0.95 0.07 0.08 0.78 0.19 0.14
> Ours 1.00 0.99 0.95 1.00 0.99 0.95

the following baselines, as shown in Table I.  

> â€¢

Velocity Tracking. We train a humanoid to traverse terrain using IsaacLabâ€™s [27] standard velocity-tracking RL pipeline. The policy is learned purely with reward shaping, without any human reference motion.  

> â€¢

Uncomposed Motion Data. This baseline removes mo-tion matching, training instead on uncomposed locomo-tion data and atomic parkour skill clips.  

> â€¢

End-to-end Depth Policy. This baseline removes distil-lation and trains a single depth-based visuomotor policy end-to-end on the motion-matching data, using the same observations as the student and the same motion-tracking reward as the experts. We found that the velocity-tracking baseline achieves sim-ilar performance to prior reward-shaping works [52, 22] and succeeds in traversing the 36 cm obstacle, but fails on higher obstacles. Specifically, it largely relies on foot-only stepping and does not discover whole-body climbing strategies that use the arms for support, highlighting the limitations of reward-shaping RL alone for highly dynamic parkour. The uncomposed motion data baseline performs poorly despite access to atomic skills, showing that isolated motions are insufficient. A common failure mode is that the robot walks up to an obstacle but fails to climb or jump over it. Without ex-plicit long-horizon composition, the policy neither experiences skill transitions during training nor observes obstacles during the walking phase to prepare the appropriate upcoming skill. In comparison, our motion-matching-based approach addresses this limitation by both generating coherent long-horizon skill composition with smooth transitions and exposing the policy to diverse visual contexts during skill execution. While end-to-end depth-based training can handle low obstacles, its performance degrades on more challenging tasks, suggesting difficulty in RL exploration when training from scratch. In contrast, our expert-distillation pipeline achieves substantially higher success rates across obstacle heights, particularly for highly dynamic skills. 

3) Ablation Study: We conduct ablation studies to study the effect of motion-matching reference data density, training scal-ability, and the role of RL during policy distillation (Table II). 

Motion Matching Density. We hypothesize that diversity in motion-matching data, especially approach distances, is critical for accurate timing and task success. To test this, we ablate approach-distance coverage in the reference dataset:  

> â€¢

Extreme Distances. Only minimum and maximum ap-TABLE II: Success rate on parkour tasks with different motion matching densities and RL strategies during distillation.                                                                    

> Method 1.0 m/s 2.0 m/s
> 58 cm 76 cm 94 cm 36 cm 58 cm 76 cm Extreme Distances 0.99 0.62 0.64 0.98 0.60 0.58 Half Density 0.95 0.32 0.57 0.99 0.85 0.81 DAgger Only 0.16 0.03 0.12 0.63 0.09 0.10 DAgger & Alive Reward 1.00 0.90 0.96 0.94 0.91 0.84 DAgger & Root Tracking 1.00 0.79 0.75 1.00 0.92 0.87 1/4 Training Envs 0.97 0.00 0.59 0.94 0.65 0.58 1/2 Training Envs 0.94 0.60 0.68 0.97 0.79 0.75 3-layer MLP 0.99 0.02 0.00 0.98 0.89 0.81 4-layer MLP 1.00 0.94 0.08 1.00 0.94 0.88
> Ours 0.99 0.95 1.00 1.00 0.98 0.90

proach distances.  

> â€¢

Half Density. Randomly selected half of the full motion-matching data. Using Extreme distances data leads to reduced success rates across all tasks, as the policy fails to generalize to intermediate distances where contact-timing is critical. Training on Half density data generally yields lower success rates on harder skills, especially when the remaining samples are skewed toward one end of the distance range. For example, in the 1.0 m/s climbing task on 76 cm and 94 cm obstacles, reduced local density leads to unreliable hand-placement timing. In contrast, the full dataset densely covers approach conditions, enabling robust skill execution across varying approach distances. 

Training Scalability. We ablate the number of parallel train-ing environments and model capacity to assess the scalability.  

> â€¢

1/4 Training Envs. Use 1/4 of the training environments.  

> â€¢

1/2 Training Envs. Use 1/2 of the training environments.  

> â€¢

3-Layer MLP. Use a 3-layer MLP with hidden sizes of [512, 256, 128].  

> â€¢

4-Layer MLP. Use a 4-layer MLP with hidden sizes of [1024, 512, 256, 128]. Unlike training from scratch, where additional rollouts often yield diminishing returns due to exploration limits, our distil-lation framework scales favorably with both model capacity and rollout throughput. Increasing the number of parallel environments or using a deeper network generally improves success, especially on more challenging parkour tasks. 

RL in Distillation. We ablate the RL objective and its reward design in the distillation stage to understand its role.  

> â€¢

DAgger Only. Remove the RL loss during distillation.  

> â€¢

DAgger + RL Alive Reward. Use only an alive/progress reward, without motion-tracking terms.  

> â€¢

DAgger + RL Root Tracking Reward. Use a root-tracking reward instead of full whole-body tracking. We find that RL is critical for effective distillation. The 

DAgger only student exhibits a clear performance drop, indicating that DAgger alone is insufficient to capture highly dynamic skills even with strong experts. For example, on the 76 cm obstacle, the DAgger student consistently stalls at the pull-up phase: although it learns the accurate hand placement, it fails to produce the brief, high-magnitude torque burst needed to lift the torso. As discussed in Sec. III-C2, this likely occurs because the decisive torque burst spans only a few timesteps, and per-step imitation loss barely penalizes slightly underestimated actions. In contrast, since RL accounts for episode success, it encourages torque bursts that are more likely to complete the pull-up, yielding both higher reward and lower DAgger loss. We further evaluate how sensitive the DAgger+RL stage is to reward design. Interestingly, using root tracking or even only an alive reward achieves success rates comparable to whole-body tracking on difficult skills. This suggests that, when co-trained with DAgger, RL is relatively robust to reward choice and mainly acts as a success-driven exploitation signal that compensates for DAggerâ€™s underestimation, rather than relying on detailed task-specific shaping. Accordingly, while we use whole-body tracking in this work, a simple alive reward may suffice when scaling to larger skill sets. In addition, our approach differs from prior work that first trains a strong DAgger policy and then applies a separate RL fine-tuning stage [33]. Here, we use RL during distillation to correct imitation-induced conservatism and improve skill learning. We also find that the DAgger term must remain active throughout training: if we drop the DAgger loss after the curriculum and continue with pure RL, the policy often develops jittery, unnatural behaviors, suggesting that in ahigh-dimensional action space the behavior cloning objective provides a critical regularization for RL. V. C ONCLUSION 

We have presented Perceptive Humanoid Parkour, a modular framework that enables humanoid robots to autonomously execute long-horizon, highly dynamic parkour behaviors using onboard perception. By combining motion-matching-based skill composition with a teacher-student RL pipeline, our ap-proach preserves the agility of human motions while enabling perception-driven adaptation to diverse obstacles. We find that dense motion matching is critical in providing coherent long-horizon references and exposes the policy to a wide range of approach conditions, while augmenting distillation with RL transfers the capability from single-skill, privileged-information experts to the multi-skill, depth-based student efficiently. Through extensive simulation studies and zero-shot deployment on a Unitree G1 robot, we demonstrate state-of-the-art agile, adaptive, whole-body parkour in the real world. While our pipeline enables long-horizon, highly dynamic humanoid parkour, it currently lacks semantic scene under-standing. Incorporating richer conditioning signals, such as language, could enable finer control over diversity and styles. In addition, our real-world capabilities are constrained by perception and hardware. With a short-range, narrow field-of-view camera at a high running speed, obstacle geometries may not be visible sufficiently early, forcing the robot to commit under perceptual ambiguity. Improved sensing and semantic scene understanding could reduce this ambiguity and support richer context reasoning. Finally, our hardware lacks sufficiently strong hands or grippers for interactions with edges and bars to be tested, preventing more extreme climbing beyond the robotâ€™s height or hanging maneuvers. REFERENCES 

[1] Ananye Agarwal, Ashish Kumar, Jitendra Malik, and Deepak Pathak. Legged locomotion in challenging ter-rains using egocentric vision. In Conference on robot learning , pages 403â€“415. PMLR, 2023. [2] Qingwei Ben, Botian Xu, Kailin Li, Feiyu Jia, Wentao Zhang, Jingping Wang, Jingbo Wang, Dahua Lin, and Jiangmiao Pang. Gallant: Voxel grid-based humanoid locomotion and local-navigation across 3d constrained terrains, 2025. URL https://arxiv.org/abs/2511.14625. [3] Kevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. Drecon: data-driven responsive control of physics-based characters. ACM Transactions On Graphics (TOG) , 38(6):1â€“11, 2019. [4] David Bollo. Inertialization: High-performance anima-tion transitions in Gears of War. Proc. of GDC, 2018. [5] Michael BÂ¨ uttner and Simon Clavet. Motion matching -the road to next gen animation. Proc. of Nucl.ai, 2015. [6] Ken Caluwaerts, Atil Iscen, J Chase Kew, Wenhao Yu, Tingnan Zhang, Daniel Freeman, Kuang-Huei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang, et al. Bark-our: Benchmarking animal-level agility with quadruped robots. arXiv preprint arXiv:2305.14654 , 2023. [7] Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, and Xiaolong Wang. Gmt: General motion tracking for humanoid whole-body control. arXiv preprint arXiv:2506.14770 , 2025. [8] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme parkour with legged robots. In 2024 IEEE International Conference on Robotics and Automa-tion (ICRA) , pages 11443â€“11450. IEEE, 2024. [9] Simon Clavet. Motion matching and the road to next-gen animation. Proc. of GDC, 2016. [10] Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wet-zstein, and Chelsea Finn. Humanplus: Humanoid shad-owing and imitation from humans. arXiv preprint arXiv:2406.10454 , 2024. [11] Ruiyu Gou, Michiel van de Panne, and Daniel Holden. Control operators for interactive character animation. 

ACM Transactions on Graphics (TOG) , 2025. [12] Junzhe He, Chong Zhang, Fabian Jenelten, Ruben Grandia, Moritz BÂ¨ acher, and Marco Hutter. Attention-based map encoding for learning generalized legged locomotion. Science Robotics , 10(105):eadv3604, 2025. [13] David Hoeller, Nikita Rudin, Dhionis Sako, and Marco Hutter. Anymal parkour: Learning agile navigation for quadrupedal robots. Science Robotics , 9(88):eadi7566, 2024. [14] Daniel Holden, Anas Kanoun, Michiel BË˜ uttner, Sofien Bouaziz, Sebastian Thrun, and Aaron Hertzmann. Learned motion matching. ACM Transactions on Graph-ics (TOG) , 2020. [15] Xiaoyu Huang, Takara Truong, Yunbo Zhang, Fangzhou Yu, Jean Pierre Sleiman, Jessica Hodgins, Koushil Sreenath, and Farbod Farshidian. Diffuse-cloc: Guided diffusion for physics-based character look-ahead control. 

arXiv preprint arXiv:2503.11801 , 2025. [16] Dvij Kalaria, Sudarshan S Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, Shankar Sastry, Sri-nath Sridhar, Sai Vemprala, Ashish Kapoor, and Jonathan Chung-Kuan Huang. Dreamcontrol: Human-inspired whole-body humanoid control for scene interaction via guided diffusion. arXiv preprint arXiv:2509.14353 , 2025. [17] Dongho Kang, Simon Zimmermann, and Stelian Coros. Animal gaits on quadrupedal robots using motion match-ing and model-based control. In 2021 IEEE/RSJ Inter-national Conference on Intelligent Robots and Systems (IROS) , pages 8500â€“8507. IEEE, 2021. [18] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. 

arXiv preprint arXiv:2107.04034 , 2021. [19] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics , 5 (47):eabc5986, 2020. [20] Qiayuan Liao, Takara E Truong, Xiaoyu Huang, Yu-man Gao, Guy Tevet, Koushil Sreenath, and C Karen Liu. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241 , 2025. [21] Junfeng Long, Zirui Wang, Quanyi Li, Jiawei Gao, Liu Cao, and Jiangmiao Pang. Hybrid internal model: Learning agile legged locomotion with simulated robot response. arXiv preprint arXiv:2312.11460 , 2023. [22] Junfeng Long, Junli Ren, Moji Shi, Zirui Wang, Tao Huang, Ping Luo, and Jiangmiao Pang. Learning hu-manoid locomotion with perceptive internal model. In 

2025 IEEE International Conference on Robotics and Automation (ICRA) , pages 9997â€“10003. IEEE, 2025. [23] Shixin Luo, Songbo Li, Ruiqi Yu, Zhicheng Wang, Jun Wu, and Qiuguo Zhu. Pie: Parkour with implicit-explicit learning framework for legged robots. IEEE Robotics and Automation Letters , 2024. [24] Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando CastaËœ neda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, et al. Sonic: Supersizing motion tracking for natural humanoid whole-body con-trol. arXiv preprint arXiv:2511.07820 , 2025. [25] Miles Macklin. Warp: A high-performance python frame-work for gpu simulation and graphics. https://github.com/ nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC). [26] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science robotics , 7(62):eabk2822, 2022. [27] Mayank Mittal, Pascal Roth, James Tigue, Antoine Richard, Octi Zhang, Peter Du, Antonio Serrano-MuËœ noz, Xinjie Yao, RenÂ´ e ZurbrÂ¨ ugg, Nikita Rudin, et al. Isaac lab: A gpu-accelerated simulation framework for multi-modal robot learning. arXiv preprint arXiv:2511.04831 ,2025. [28] I Nahrendra, Byeongho Yu, and Hyun Myung. Dreamwaq: Learning robust quadrupedal locomotion with implicit terrain imagination via deep reinforcement learning. arXiv preprint arXiv:2301.10602 , 2023. [29] Yixuan Pan, Ruoyi Qiao, Li Chen, Kashyap Chitta, Liang Pan, Haoguang Mai, Qingwen Bu, Hao Zhao, Cunyuan Zheng, Ping Luo, et al. Agility meets stability: Versa-tile humanoid control with heterogeneous data. arXiv preprint arXiv:2511.17373 , 2025. [30] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided deep re-inforcement learning of physics-based character skills. 

ACM Transactions On Graphics (TOG) , 37(4):1â€“14, 2018. [31] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transac-tions on Graphics (TOG) , 2021. [32] StÂ´ ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelli-gence and statistics , pages 627â€“635. JMLR Workshop and Conference Proceedings, 2011. [33] Nikita Rudin, Junzhe He, Joshua Aurand, and Marco Hutter. Parkour in the wild: Learning a general and exten-sible agile locomotion policy using multi-expert distilla-tion and rl fine-tuning. arXiv preprint arXiv:2505.11164 ,2025. [34] Salgadopk. Learn parkour - climb up tutorial. URL https://youtu.be/6U1sIgqgPFo?si=339TPTxlFB5lWGB1. [35] Jingkai Sun, Gang Han, Pihai Sun, Wen Zhao, Jiahang Cao, Jiaxu Wang, Yijie Guo, and Qiang Zhang. Dpl: Depth-only perceptive humanoid locomotion via realistic depth synthesis and cross-attention terrain reconstruction. 

arXiv preprint arXiv:2510.07152 , 2025. [36] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR , 2023. [37] Huayi Wang, Zirui Wang, Junli Ren, Qingwei Ben, Tao Huang, Weinan Zhang, and Jiangmiao Pang. Beamdojo: Learning agile humanoid locomotion on sparse footholds. In Robotics: Science and Systems (RSS) , 2025. [38] Huayi Wang, Wentao Zhang, Runyi Yu, Tao Huang, Junli Ren, Feiyu Jia, Zirui Wang, Xiaojie Niu, Xiao Chen, Jiahe Chen, et al. Physhsi: Towards a real-world gener-alizable and natural humanoid-scene interaction system. 

arXiv preprint arXiv:2510.11072 , 2025. [39] Jinze Wu, Guiyang Xin, Chenkun Qi, and Yufei Xue. Learning robust and agile legged locomotion using ad-versarial motion priors. IEEE Robotics and Automation Letters , 8(8):4975â€“4982, 2023. [40] Weiji Xie, Jinrui Han, Jiakun Zheng, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai, and Xue-long Li. Kungfubot: Physics-based humanoid whole-body control for learning highly-dynamic skills. arXiv preprint arXiv:2506.12851 , 2025. [41] Michael Xu, Yi Shi, KangKang Yin, and Xue Bin Peng. Parc: Physics-based augmentation with reinforcement learning for character controllers. In ACM SIGGRAPH ,2025. [42] Pei Xu, Zhen Wu, Ruocheng Wang, Vishnu Sarukkai, Kayvon Fatahalian, Ioannis Karamouzas, Victor Zordan, and C Karen Liu. Learning to ball: Composing policies for long-horizon basketball moves. ACM Transactions on Graphics (TOG) , 44(6):1â€“14, 2025. [43] Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C Karen Liu, Rocky Duan, and Guanya Shi. Omniretarget: Interaction-preserving data generation for humanoid whole-body loco-manipulation and scene interaction. arXiv preprint arXiv:2509.26633 , 2025. [44] Ruihan Yang, Ge Yang, and Xiaolong Wang. Neural volumetric memory for visual locomotion control. In 

Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 1430â€“1440, 2023. [45] Alan Yu, Ge Yang, Ran Choi, Yajvan Ravan, John Leonard, and Phillip Isola. Learning visual parkour from generated images. In 8th Annual Conference on Robot Learning , 2024. [46] Ruiqi Yu, Qianshi Wang, Yizhen Wang, Zhicheng Wang, Jun Wu, and Qiuguo Zhu. Walking with terrain recon-struction: Learning to traverse risky sparse footholds. 

arXiv preprint arXiv:2409.15692 , 2024. [47] Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, and C Karen Liu. Twist2: Scalable, portable, and holistic humanoid data collection system. arXiv preprint arXiv:2511.02832 , 2025. [48] Tong Zhang, Boyuan Zheng, Ruiqian Nai, Yingdong Hu, Yen-Jen Wang, Geng Chen, Fanqi Lin, Jiongye Li, Chuye Hong, Koushil Sreenath, et al. Hub: Learning extreme humanoid balance. CoRL , 2025. [49] Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Tay-lor, and Xue Bin Peng. Add: Physics-based motion imi-tation with adversarial differential discriminators. arXiv preprint arXiv:2505.04961 , 2025. [50] Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, and Hang Zhao. Hiking in the wild: A scalable perceptive parkour framework for humanoids. arXiv preprint arXiv:2601.07718 , 2026. [51] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christo-pher Atkeson, Soeren Schwertfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning. arXiv preprint arXiv:2309.05665 , 2023. [52] Ziwen Zhuang, Shenzhe Yao, and Hang Zhao. Humanoid parkour learning. arXiv preprint arXiv:2406.10759 ,2024. APPENDIX 

A. Motion Matching Implementation Details 

This section provides implementation details for the motion matching procedure used to synthesize long-horizon parkour reference trajectories. 

1) Motion Database and Feature Precomputation: All mo-tion clips are first retargeted to a 29-DOF Unitree G1 hu-manoid using OmniRetarget [43] and represented as frame sequences. At each frame i, we store the robot configuration 

qi = ( pi, ri, Î¸i), consisting of the root translation pi âˆˆ R3,root quaternion ri âˆˆ R4, and joint angles Î¸i âˆˆ R29 . For each frame, we also precompute a matching feature vector 

xi derived from qi. Following [14], xi âˆˆ R27 is expressed in the characterâ€™s local coordinate frame and consists of:  

> â€¢

Future root trajectory ti âˆˆ R12 : Planar root positions and facing directions at 0.33s, 0.67s, and 1s into the future.  

> â€¢

Local foot state fi âˆˆ R12 : Positions and linear velocities of the left and right feet expressed in the root frame.  

> â€¢

Root velocity hi âˆˆ R3: Root linear velocity. To improve data coverage, we augment the motion database by mirroring all motion clips. For parkour motion clips, we manually fit a box-shaped terrain aligned with each motion. 

2) Query Feature Construction: At runtime, a query feature 

Ë†xt is constructed from the current robot configuration qt and a 2D velocity command. We first extract the kinematic features from qt to form the pose-based part of the query, namely the local foot state Ë†ft and the root velocity Ë†ht. We then compute the short-horizon future root trajectory from the 2D velocity command to form the command-based part Ë†tt.Following [14], we convert the 2D velocity command into a future root trajectory using a critically damped spring model. We apply the spring to (i) the 2D root velocity and (ii) the root heading direction. The target 2D velocity is set to the commanded 2D velocity ucmd  

> t

âˆˆ R2. The target heading Ïˆcmd 

> t

is set to atan2( ucmd  

> t,y

, u cmd  

> t,x

).

Critically damped spring closed form. Let s denote the spring position and Ë™s its velocity, with goal sgoal and damping parameter y > 0. Define j0 = s0 âˆ’ sgoal and j1 = Ë™ s0 + y j0.Then the spring state at any future time Ï„ admits the closed form 

s(Ï„ ) = eâˆ’yÏ„  j0 + Ï„ j1

 + sgoal . (4) 

Planar position from target velocity. For planar root translation, we use the spring in velocity space: the spring â€œpositionâ€ corresponds to planar velocity (and its derivative to acceleration). We obtain future root positions by integrating the closed-form velocity: 

p(Ï„ ) = p0 âˆ’ j1

y2 eâˆ’yÏ„ + âˆ’j0 âˆ’ Ï„ j1

y eâˆ’yÏ„ + j1

y2 + j0

y + ucmd  

> t

Ï„, 

(5) applied component-wise in the plane. 

Heading direction from target heading. For rotation, we apply the spring directly to the heading angle Ïˆ toward Ïˆcmd 

> t

and evaluate the resulting Ïˆ(Ï„ ) at the same horizons via Eq. (4) (no integration is needed). We evaluate at Ï„ âˆˆ { 0.33 , 0.67 , 1.0} s, and transform the resulting future positions p(Ï„ ) and facing directions into the characterâ€™s local coordinate frame to form the future trajectory feature Ë†tt.

3) Transition Smoothing via Inertialization: To ensure smooth transitions when switching the playback index to a newly retrieved frame, we adopt inertialization [4]. The key idea is to compute an offset between the currently playing motion and the target motion at the transition instant, apply this offset after switching so the output remains continuous, and then gradually decay the offset to zero. We decay this offset using the same critically damped spring model as in Eq. (4), but with the goal set to zero. 

B. Skill List and Training Implementation Details 1) Skill List: Our motion library includes locomotion and a set of atomic parkour skills. Locomotion provides a shared transition manifold and includes standing, walking, and run-ning motions spanning commanded speeds from 0.8 to 3.5 m/s. Most parkour skills are instantiated at 1.0 m/s and 2.0 m/s. We additionally include a single 3.0 m/s cat-vault skill to cover extreme-speed vaulting behaviors. Table III summarizes the full skill list and the total duration of motion clips for each category. 

2) Motion Tracking Details: Specific reward formulations and domain randomization settings used for expert policy learning from [20] are summarized in Table IV and Table V for reference. 

3) Distillation Details: During student training, we relax the termination conditions relative to the expert to prevent premature termination of valid but mirrored executions. While this improves PPO stability, the student may visit states that are out-of-distribution for the expert policies, which were trained under the original termination thresholds and may not provide meaningful actions in these regimes. In particular, when the student violates the expertâ€™s original termination condition but remains within the relaxed one, querying the expert would yield unreliable supervision. To avoid introducing incorrect DAgger signals, we disable the DAgger loss at such timesteps and rely solely on the PPO objective. For depth sensing, beyond the aforementioned camera noise, we also add a random depth offset within Â±3 cm and inject i.i.d. Gaussian noise with a standard deviation of 3 cm into the depth observations during training. The onboard depth camera operates at 30 Hz. 

4) Training Hyperparameters: We include all hyperparam-eters for two-stage training in Table VI for reference. 

C. Details for Baselines 1) Velocity Tracking Baseline: To show the importance of human reference motion in our framework, we include a standard reward-shaping velocity-tracking baseline that learns locomotion purely from handcrafted rewards and a terrain Skill Duration (s) Locomotion 

Locomotion 495.5 

Parkour skills @ 1.0 m/s 

Step (36 cm) 2.2 Climb (58 cm) 12.1 Climb (76 cm) 8.8 Climb (94 cm) 10.3 

Parkour skills @ 2.0 m/s 

Step (36 cm) 1.6 Climb (58 cm) 6.1 Climb (76 cm) 4.4 Climb (94 cm) 5.2 Climb (125 cm) 5.9 Dash Vault 5.0 Speed Vault 3.1 

Parkour skills @ 3.0 m/s 

Cat Vault 1.5 

TABLE III: Motion clips used in our motion library. curriculum, without any motion imitation or human refer-ence trajectories. We follow IsaacLabâ€™s standard rough-terrain velocity-tracking recipe using its Unitree G1 rough-terrain configuration 1 , which is widely used for humanoid locomo-tion and terrain traversal and is largely consistent with state-of-the-art reward-shaping-based setups [13]. In alignment with prior works [52, 13], we also employ a terrain curriculum to ease learning. Specifically, we gradually increase terrain difficulty from 0.3 m to 1.0 m over 10 levels (with a 2 m run-up), providing a smooth progression of tasks that helps the policy bootstrap stable locomotion on easier terrain before tackling harder contact and balance challenges as the terrain becomes progressively harder. The curriculum advances the robot to a harder level once it achieves sufficient success on the current level, and moves it back to an easier level if its performance drops. Notably, different from our student policy which relies on an onboard depth camera for sensing, this baseline directly receives a local terrain height map (i.e., privileged height observations from simulation), which has been shown highly effective in prior parkour systems [13, 33]. 

2) AMP Baseline: Since AMP [31] is a popular algorithm for chaining skills with human reference data, we also im-plemented an AMP baseline by following the MimicKit 2

AMP implementation released by the original AMP authors. In our experiments, this baseline can walk stably and track the commanded velocity, but it does not perform well on obstacle traversal: it fails on most tasks, especially the harder ones, which is broadly consistent with prior reports that AMP can be difficult to extend to agile motions [49]. At the same time, we recognize that AMP performance can depend strongly on 

> 1

Code available at https://github.com/isaac-sim/IsaacLab/blob/main/source/ isaaclab tasks/isaaclab tasks/manager based/locomotion/velocity/config/g1/ rough env cfg.py. 

> 2

Code available at https://github.com/xbpeng/MimicKit. 

TABLE IV: Reward formulation using Gaussian-shaped track-ing scores.                                                        

> Reward Terms Equation Weight
> Task (Tracking)
> Body Position exp
> 
> âˆ’ 1
> |B target |
> P
> bâˆˆB target âˆ¥pdes
> bâˆ’pbâˆ¥2/0.32
> 
> 1.0
> Body Orientation exp
> 
> âˆ’ 1
> |B target |
> P
> bâˆˆB target âˆ¥log( Rdes
> bRâŠ¤
> b)âˆ¥2/0.42
> 
> 1.0
> Body Linear velocity exp
> 
> âˆ’ 1
> |B target |
> P
> bâˆˆB target âˆ¥vdes
> bâˆ’vbâˆ¥2/1.02
> 
> 1.0
> Body Angular velocity exp
> 
> âˆ’ 1
> |B target |
> P
> bâˆˆB target âˆ¥Ï‰des
> bâˆ’Ï‰bâˆ¥2/3.14 2
> 
> 1.0
> Anchor Position exp
> 
> âˆ’ âˆ¥ pdes
> anchor âˆ’panchor âˆ¥2/0.32
> 
> 1.0
> Anchor Orientation exp
> 
> âˆ’ âˆ¥ log( Rdes
> anchor RâŠ¤
> anchor )âˆ¥2/0.42
> 
> 1.0
> Regularization
> Action smoothness âˆ¥atâˆ’atâˆ’1âˆ¥2âˆ’0.1
> Joint position limit PNj=1
> max( ljâˆ’Î¸j,0) + max( Î¸jâˆ’uj,0) âˆ’10 .0
> Undesired self-contacts P
> b / âˆˆB ee 1âˆ¥fself
> bâˆ¥>1Nâˆ’0.5

TABLE V: Domain randomization parameters. ( U[Â·]: uniform distribution)                                    

> Domain Randomization Sampling Distribution
> Physical parameters
> Static friction coefficients Î¼static âˆ¼ U [0 .4,1.3]
> Dynamic friction coefficients Î¼dynamic âˆ¼ U [0 .4,1.1]
> Restitution coefficient erest âˆ¼ U [0 ,0.5]
> Default joint positions (except ankle) [rad] âˆ†Î¸0
> jâˆ¼ U [âˆ’0.01 ,0.01]
> Default ankle joint positions [rad] âˆ†Î¸0
> jâˆ¼ U [âˆ’0.03 ,0.03]
> Torso COM offset [m] âˆ†xâˆ¼ U [âˆ’0.025 ,0.025] ,âˆ†y, âˆ†zâˆ¼ U [âˆ’0.05 ,0.05]
> Root velocity perturbations
> Root linear vel [m/s] vx, v yâˆ¼ U [âˆ’0.1,0.1] , v zâˆ¼ U [âˆ’0.05 ,0.05]
> Push duration [s] âˆ†tâˆ¼ U [1 .0,3.0]
> Root angular vel [rad/s] Ï‰x, Ï‰ y, Ï‰ zâˆ¼ U [âˆ’0.1,0.1]

implementation details and careful tuning [38], and we did not have the bandwidth to fully explore this tuning space. For this reason, we do not include AMP in the formal comparison, and instead leave this result as a note for context. TABLE VI: Training hyperparameters. 

Hyperparameter Motion Tracking Distillation Architecture Actor / Student MLP hidden dims [512, 256, 128] [2048, 1024, 512, 256, 128] Critic MLP hidden dims [512, 256, 128] [512, 256, 128] Activation function ELU ELU Init noise std 1.0 0.01 Depth backbone â€“ 3-layer CNN + GAP Depth input resolution â€“ 58 Ã— 87 

Depth output dim â€“ 32 Training Steps per environment 24 24 Max iterations 20,000 20,000 Learning rate 1 Ã— 10 âˆ’3 3 Ã— 10 âˆ’4

Schedule adaptive adaptive after 1000 iterations Clip parameter 0.2 0.2 Entropy coefficient 0.005 0.001 Discount factor ( Î³) 0.99 0.99 GAE Î» 0.95 0.95 Desired KL 0.01 0.01 Learning epochs 5 2Mini-batches 4 96 Max grad norm 1.0 1.0 Distillation-Specific Curriculum end epoch â€“ 10,000 Distill loss type â€“ mse DAgger loss coefficient â€“ 10.0
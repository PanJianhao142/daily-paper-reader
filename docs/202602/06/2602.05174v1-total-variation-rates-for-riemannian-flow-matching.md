---
title: Total Variation Rates for Riemannian Flow Matching
title_zh: 黎曼流匹配的全变分收敛率
authors: "Yunrui Guan, Krishnakumar Balasubramanian, Shiqian Ma"
date: 2026-02-05
pdf: "https://arxiv.org/pdf/2602.05174v1"
tags: ["keyword:FM"]
score: 6.0
evidence: 黎曼流匹配理论
tldr: 本文研究了黎曼流匹配（RFM）生成模型的非渐近收敛性，针对流形上的欧拉离散化采样器提出了全变差（TV）误差界限。通过引入控制流形ODE流TV演化的微分不等式，研究成功分离了数值离散误差与向量场学习误差。该工作为超球面和正定矩阵流形上的采样提供了显式的多项式迭代复杂度保证，填补了流形流匹配理论分析的空白。
motivation: 旨在解决流形流匹配模型缺乏非渐近收敛理论的问题，量化离散化步长和学习误差对生成分布精度的影响。
method: 开发了一种基于微分不等式的分析框架，利用平行移动和曲率界限来控制向量场偏差及参考流的分数函数。
result: 在紧致流形和Hadamard流形上推导出了显式的TV收敛速率，并给出了超球面和SPD流形上的多项式级迭代复杂度。
conclusion: 该研究为流形上的流匹配采样提供了坚实的理论支撑，证明了在满足一定平滑性和学习精度条件下，RFM能够高效收敛。
---

## 摘要
黎曼流匹配 (RFM) 通过学习一个随时间变化的切向量场，将基于流的生成建模扩展到流形上的数据，该向量场的流常微分方程 (flow-ODE) 将简单的基分布传输到数据分布。我们为使用学习到的向量场以及流形上欧拉离散化的 RFM 采样器开发了一种非渐近全变分 (TV) 收敛分析。我们的关键技术要素是一个控制两个流形 ODE 流之间 TV 演化的微分不等式，它通过向量场失配的散度和参考流的分数 (score) 来表达 TV 的时间导数；控制这些项需要建立明确考虑平行移动和曲率的新界限。在总体流匹配场的平滑性假设以及学习场的一致（紧致流形）或均方（Hadamard 流形）近似保证下，我们获得了形式为 $\mathrm{TV}\le C_{\mathrm{Lip}}\,h + C_{\varepsilon}\,\varepsilon$ 的显式界限（在紧致流形上还有一个额外的高阶 $\varepsilon^2$ 项），清晰地分离了数值离散化误差和学习误差。其中，$h$ 是步长，$\varepsilon$ 是目标精度。具体实例在超球面 $S^d$ 以及在温和矩条件下在 SPD$(n)$ 流形上产生了显式的多项式迭代复杂度。

## Abstract
Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\mathrm{TV}\le C_{\mathrm{Lip}}\,h + C_{\varepsilon}\,\varepsilon$ (with an additional higher-order $\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\varepsilon$ is the target accuracy. Instantiations yield \emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.
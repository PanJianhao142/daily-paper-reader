Title: Total Variation Rates for Riemannian Flow Matching

URL Source: https://arxiv.org/pdf/2602.05174v1

Published Time: Fri, 06 Feb 2026 01:19:35 GMT

Number of Pages: 99

Markdown Content:
# Total Variation Rates for Riemannian Flow Matching 

Yunrui Guan 1, Krishnakumar Balasubramanian 2, and Shiqian Ma 11Department of Computational Applied Mathematics and Operations Research, Rice University. 

> 2

Department of Statistics, University of California, Davis. 

> 1

{yg83,sqma }@rice.edu 

> 2

{kbala }@ucdavis.edu 

Abstract 

Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form TV ≤ CLip h+Cε ε (with an additional higher-order ε2 term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, h is the step-size and ε is the target accuracy. Instantiations yield explicit 

polynomial iteration complexities on the hypersphere Sd, and on the SPD( n) manifolds under mild moment conditions. 

# Contents 

1 Introduction 22 Basics and Problem Formulation 4

2.1 Riemannian Geometry Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42.2 Riemanian Flow Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

3 Total Variation Rates: Compact Manifolds 74 Total Variation Rates: Hadamard Manifolds 95 Proof Sketch and Intermediate Results 10 

5.1 Continuous Time Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.2 Discretization analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2.1 Score Regularity and Vector Field Regularity . . . . . . . . . . . . . . . . . . 12 5.2.2 The Divergence Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1

> arXiv:2602.05174v1 [stat.ML] 5 Feb 2026

A Additional Background for Riemannian Geometry 17 

A.1 Distances used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 Comparison Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 Riemannian Submanifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 

B Proof of Main Theorem 18 

B.1 Proof for Lemma 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 Velocity Vector Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3 Divergence Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.4 Results on Riemannian manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.5 Main Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.6 Example: Hypersphere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 B.7 Example: SPD Manifold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 

C Auxiliary Results for Proof of Main Theorems 36 

C.1 Jacobi Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 C.2 Divergence Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

D Hypersphere Regularity Results 42 

D.1 Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 D.2 Regularity for Flow Matching Vector Field . . . . . . . . . . . . . . . . . . . . . . . . 53 D.3 Regularity for Divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 D.4 Regularity of v(t, x) and log pt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 D.5 Finiteness of Score Regularity: Proof of Proposition 5.3 . . . . . . . . . . . . . . . . 63 

E SPD Manifold Regularity Results 65 

E.1 Proof of Proposition 5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 E.2 Auxiliary Results for Regularity for Hadamard and SPD manifolds . . . . . . . . . . 70 E.2.1 Expectation Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 E.2.2 Regularity Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 E.3 Auxiliary Bounds on Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 E.4 Bounding Third Derivative of Exp Map . . . . . . . . . . . . . . . . . . . . . . . . . 87 E.5 Bounds involving Riemannian Log . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 E.6 Auxiliary results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 

# 1 Introduction 

Flow-based generative modeling offers a conceptually clean sampling paradigm by representing a complex target distribution as the endpoint of a continuous-time transport from a simple reference law. In flow matching , one fixes a family of interpolating measures ( πt)t∈[0 ,1] between an easy base distribution π0 and the data distribution π1, and learns a time-dependent vector field whose flow pushes π0 to π1. When the data lie on a Riemannian manifold ( M, g ), particles evolve according to an intrinsic ODE ˙Xt = vt(Xt), and the associated densities satisfy the manifold continuity equation 

∂tpt + div g(ptvt) = 0, yielding a sampler that respects the geometry by construction. Crucially, because Riemannian Flow Matching (RFM) is purely deterministic and relies only on integrating intrinsic ODEs, it avoids discretizing manifold-valued Brownian motion as required by Riemannian diffusion models (De Bortoli et al., 2022), where both theoretical analysis and practical simulation are considerably more delicate. 2Despite their ever-increasing practical usage (Yue et al., 2025; Sriram et al., 2024; Mathieu and Nickel, 2020; Miller et al., 2024; Collas et al., 2025; Luo et al., 2025), a central theoretical question is unexplored: how fast does a discretized RFM sampler with a learned velocity field converge to the target distribution, in distributional metrics? Our main contribution is to provide the first 

non-asymptotic Total Variation (TV) bounds for RFM samplers on both compact and Hadamard manifolds. We develop intrinsic stability estimates for solutions of the Riemannian continuity equation under perturbations of the driving vector field, yielding explicit control of TV ( ˆπT , π T ) for 

T = 1 − δ, ( δ > 0) in terms of (i) the approximation quality of the learned field ˆvt to an ideal field vt

in geometry-aware norms, (ii) regularity of the dynamics (e.g., bounds on covariant derivatives and divergence), and (iii) geometric characteristics of ( M, g ) that govern volume distortion. We then combine these stability estimates with a careful analysis of discretized flows to obtain end-to-end guarantees for practical samplers, isolating the statistical and computational contributions. The resulting theory clarifies which properties of the learned velocity field are truly required for strong sampling accuracy on manifolds, and how curvature and anisotropy enter the constants and rates, providing a principled foundation for understanding Riemannian flow matching algorithms. 

Related works. Generative modeling on Riemannian manifolds was introduced by De Bortoli et al. (2022), who generalized score-based generative modeling (Song et al., 2021) from Euclidean space to Riemannian manifolds, and proved an error bound of sampling error. The error bound in De Bortoli et al. (2022) fails to be polynomial, which is caused by technical difficulties in analyzing discretization error in manifold Brownian motion simulation. Other works including Huang et al. (2022) and Lou et al. (2023) also studied generative modeling on mainfolds without theoretical guarantees. This difficulty is related to sampling on Riemannian manifolds via diffusion processes. Early work analyzed KL divergence along Langevin diffusions, e.g., on hyperspheres (Li and Erdogdu, 2023) and Hessian manifolds (Gatmiry and Vempala, 2022). Later, Cheng et al. (2022) used a geometric approach with coupling to obtain W1 bounds; see also Kong and Tao (2024) for Lie groups and Guan et al. (2025) for high-accuracy proximal sampling. These results highlight that Riemannian Brownian motion—the basic component of Riemannian diffusion models—is challenging both theoretically and computationally: discretization analyses are largely geometric and typically give Wasserstein error bounds (e.g., Cheng et al. (2022)), while exact simulation is generally unavailable since its transition density lacks a closed form except in special cases. This in turn complicates denoising score matching and sample generation for Riemannian diffusion models. More recently, Xu et al. (2026, Lemma 19) established a polynomial discretization error bound in total variation distance. Building on this, Xu et al. (2026) further proved a polynomial iteration complexity for Riemannian score-based generative models. However, the established bounds are only qualitative and do not reveal the precise dependencies on the problem parameters. Another line of research is the Riemannian Flow Matching method, first proposed in Chen and Lipman (2024) and later explored by Cheng et al. (2025) and Wu et al. (2025). We remark that, due to its deterministic formulation, RFM does not require access to the heat kernel or simulation of manifold Brownian motion, thereby avoiding the main bottleneck of Riemannian diffusion models. From the case of Euclidean flow matching, works including Li et al. (2025) and Huang et al. (2025) established discretization error bounds in TV distance for probability flow ODEs with polynomial dependence on the Lipschitz constant. For flow matching with deterministic samplers via ODE discretization, Benton et al. (2024); Bansal et al. (2024); Zhou and Liu (2025); Guan et al. (2026) establish bounds in the Wasserstein distance, and Su et al. (2025) considered KL divergence. However, without a contraction/dissipativity-type structure for the score or flow-matching vector field, the bounds typically do not yield polynomial rates in the associated Lipschitz constants. To 3obtain a fully polynomial error bound, existing works (Li et al., 2024a, 2025) analyze flow matching samplers in the TV distance. Liu et al. (2025) provided a polynomial error bound for stochastic interpolant (which can be understood as a smoothed variant of flow matching) with deterministic sampling, by making use of the framework in Li et al. (2025). Very recently, Roy et al. (2026) examined adaptivity of Euclidean FM to low-dimensional structures. To the best of our knowledge, no prior discretization error analysis is available for flow matching on general Riemannian manifolds. 

# 2 Basics and Problem Formulation 

2.1 Riemannian Geometry Basics 

Let ( M, g ) be a d-dimensional Riemannian manifold, not necessarily realized as a submanifold of Euclidean space, where g denotes the Riemannian metric. Unless otherwise specified, ∥ · ∥ denotes the norm induced by g. We use TxM to denote tangent space at location x ∈ M , and remark that ∥ · ∥ is well-defined on each tangent space TxM . We let Exp to denote exponential map, and also define the inverse of the exponential map, i.e., the logarithm map: Given X0, X 1 ∈ M ,Log X0 (X1) ∈ TX0 M is such that Exp X0 (Log X0 (X1)) = X1. We use P yx to denote the parallel transport from x to y, along the minimizing geodesic, unless otherwise specified. Based on this, we can define geodesic interpolation as follows. 

Xt := Exp X0 (t Log X0 (X1)) (1) Note that X′ 

> t

= P Xt 

> X0

Log X0 (X1) = 11−t Log Xt (X1). We use grad to denote the Riemannian gradient, div the Riemannian divergence, and ∇ the Levi–Civita connection on ( M, g ). For a smooth function f , the gradient grad f is the unique vector field satisfying ⟨grad f, v ⟩ = df (v) for all v ∈ TxM . For a smooth vector field u, the divergence is defined by div u := tr (∇u); equivalently, for any local g-orthonormal frame {ei}di=1 ,div u(x) = Pdi=1 ∇ei u, e i x. For a vector field u and a tangent vector v ∈ TxM , the covariant derivative ∇vu(x) is defined as follows: choose any smooth local vector field V with V (x) = v,and set ∇vu(x) := ( ∇V u)( x); this is well-defined (i.e., independent of the extension V ) because 

∇ is C∞(M )-linear in its first argument. The map ∇u(x) may be viewed as a (1 , 1)-tensor (an endomorphism) ∇u(x) : TxM → TxM defined by ( ∇u(x))( v) := ∇vu(x). Accordingly, we define the operator norm induced by g as 

∥∇ u(x)∥op := sup  

> v̸=0

∥∇ vu(x)∥∥v∥ = sup 

> ∥v∥=1

∥∇ vu(x)∥.

We now provide some intuition on Levi–Civita connection and covariant derivatives. On a manifold, the tangent spaces TxM and TyM at different points are distinct vector spaces, so one cannot subtract vectors at x and y directly. The Levi–Civita connection ∇ provides a canonical way to differentiate vector fields by comparing nearby tangent vectors in a manner that is compatible with the metric ( ∇g = 0) and has no torsion. Along a curve γ, the induced covariant derivative 

Dt := ∇γ′(t) plays the role of a directional derivative: it measures the intrinsic rate of change of a vector field along γ after accounting for the variation of tangent spaces, and it is the notion of derivative that makes “constant velocity” along γ coincide with the geodesic equation. Let x ∈ M . The injectivity radius at x, denoted inj ( x), is defined as inj ( x) := sup 

n

a > 0 : Exp x Ba(0) : Ba(0) ⊂ TxM → M is a diffeomorphism onto its image 

o

,

4where Ba(0) is the open metric ball in ( TxM, g x). The injectivity radius of M is inj (M ) := inf x∈M inj ( x). Let R denote the (Riemann) curvature tensor associated with ∇, defined for smooth vector fields 

X, Y, Z by R(X, Y )Z := ∇X ∇Y Z − ∇ Y ∇X Z − ∇ [X,Y ]Z. For linearly independent u, v ∈ TxM ,the sectional curvature of the plane σ = span {u, v } is 

K(σ) := ⟨R(u, v )v, u ⟩x

∥u∥2∥v∥2 − ⟨ u, v ⟩2

> x

,

and in particular, if u ⊥ v and ∥u∥ = ∥v∥ = 1, then K(u, v ) = ⟨R(u, v )v, u ⟩x.Let γ : [ a, b ] → M be a geodesic, and write Dt := ∇γ′(t) for the covariant derivative along γ. A vector field J along γ is called a Jacobi field if it satisfies the Jacobi equation D2 

> t

J + R(J, γ ′) γ′ = 0. Let p = γ(a) and q = γ(b). We say that p and q are conjugate along γ if there exists a nonzero Jacobi field J along γ such that J(a) = J(b) = 0. For a unit-speed geodesic γ : [0 , ∞) → M with γ(0) = p, define its cut time tcut (γ) := sup {t > 

0 : γ|[0 ,t ] minimizes distance between its endpoints }. If tcut (γ) < ∞, the cut point of p along γ is 

γ(tcut (γ)). The cut locus of p, denoted Cut (p), is the set of all cut points of p over all unit-speed geodesics emanating from p. It is a classical fact that along any geodesic starting at p, the cut time occurs no later than the first conjugate time (if a conjugate point occurs at all); moreover, a cut point may occur strictly before the first conjugate point when there are multiple minimizing geodesics to the same endpoint. We now provide some intuition on the above concepts. The curvature tensor quantifies how covariant derivatives fail to commute; it encodes the intrinsic bending of the manifold. Sectional curvature reduces this information to a 2-dimensional direction σ: positive sectional curvature tends to make nearby geodesics in σ focus toward each other, while negative curvature tends to make them spread apart. Jacobi fields describe the infinitesimal separation between nearby geodesics (they arise from variations of geodesics), so zeros of a Jacobi field correspond to a loss of local uniqueness/minimality of geodesics. Conjugate points describe a differential property, corresponding to the degeneracy of d exp , hence naturally enter comparison theory used for our analysis later (see Section A.2). On the other hand, cut points describe a geometric property, describing when geodesics cease to be minimizing, as well as when Log x stops being single-valued. 

2.2 Riemanian Flow Matching 

Let π0 and π1 be two probability distributions supported on a Riemannian manifold M . The Riemannian Flow Matching (RFM) framework aims to learn a time-dependent vector field v(t, x )whose induced flow transports samples from π0 to π1. Concretely, RFM seeks v such that if X0 ∼ π0,then the solution {Xt}t∈[0 ,1] to the ODE 

dX t = v(t, X t) dt, X0 ∼ π0, (2) satisfies X1 ∼ π1. The ODE exhibits a maximal solution as long as v is continuous in time and locally-Lipschitz in space; see, for example (Lang, 2012, Chapter IV). Moreover, if the manifold is complete and v(t, x ) satisfies a linear growth bound, the solution exists globally in time. We also remark here that Wan et al. (2025) provided more refined conditions that hold more generally for the case of Euclidean spaces. A key geometric constraint is that for every ( t, x ), the velocity must lie in the tangent space at the current point :

v(t, x ) ∈ TxM, t ∈ [0 , 1] .

5That is, v(t, x ) represents a valid instantaneous direction of motion along the manifold at x. This is where the Riemannian setting differs fundamentally from the Euclidean one. When M = Rd, all tangent spaces are canonically identical, i.e., TxRd ∼= Rd for all x, so we can view the vector field simply as a global map v : [0 , 1] × Rd → Rd with a single, fixed vector space as codomain. On a general manifold M , however, the tangent spaces {TxM }x∈M form a family of different vector spaces attached to different points . Although each TxM has the same dimension, there is no canonical identification between TxM and TyM when x̸ = y. As a consequence, the “output space” of v(t, ·) depends on x: v is a section of the tangent bundle rather than a map into a single fixed 

Rd. This location-dependence forms a main source of technical differences between Euclidean flow matching and RFM, and it will matter in the analysis later. In flow matching, one typically parameterizes the time-dependent vector field with a neural network and learns it by minimizing the conditional flow matching objective. In the Riemannian setting, choosing the coupling between the endpoints to be the independent coupling , i.e., sampling 

X0 ∼ π0 and X1 ∼ π1 independently, yields the training loss min  

> u

E t, X 0∼π0, X 1∼π1

∥u(Xt, t ) − P Xt 

> X0

Log X0 (X1)∥2, (3) where Log x(·) is the Riemannian logarithm map, and P ba denotes parallel transport along the interpolation curve from a to b. In Euclidean space, the standard choice is the straight-line interpolation Xt := tX 1 + (1 − t)X0 between X0 and X1; on a Riemannian manifold, this naturally generalizes to the geodesic interpolation Xt in (1) . We denote the population minimizer by v(t, x )and the learned (neural) approximation by ˆ v(t, x ). It is useful to note that the population minimizer admits a closed-form expression as a conditional expectation (see, e.g., Guan et al. (2026)). Specifically, for any fixed t < 1 and x ∈ M ,

v(t, x ) = EP xX0 Log X0 (X1) Xt = x = 11 − t E[Log x(X1) | Xt = x]= 11 − t

Z

> M

Log x(x1) pt(x1 | x) dV g(x1),

(4) where pt(x1 | x) denotes the conditional density of X1 given Xt = x under the above independent coupling and the chosen interpolation, and dV g is the Riemannian volume measure. 

To generate samples, we numerically simulate the learned flow-matching ODE in Algorithm 1. While the ideal (popula-tion) flow driven by the true minimizer v

would transport π0 to π1 at time t = 1, in practice we only have access to a learned approximation ˆv and a time-discretized in-tegrator. A key difficulty is the behavior of the conditional flow-matching vector field near the terminal time. Note from (4) that the population minimizer contains an ex-plicit 1 /(1 − t) factor. Along the exact 

geodesic bridge used to define Xt, the term 

E[Log x(X1) | Xt = x] typically scales like (1 − t) so that v(t, x ) remains finite; however, this cancellation is fragile. Any mismatch between the learned field ˆv and the population field v, or any deviation of the simulated trajectory from the 6training interpolation, can destroy the cancellation and make the effective dynamics increasingly stiff as t → 1 (large velocities and/or large sensitivities with respect to x). This stiffness directly motivates early stopping . In Algorithm 1, we use {hi} to denote the step size schedule. In the constant step size case, we simply have hi ≡ h, ∀i. With a pre-designed step size schedule and early stopping time T , the algorithm generates {xtk } where tk := Pk−1 

> i=0

hi

and consequently by definition tN = T . The Euler discretization accumulates both numerical discretization error and statistical/approximation error from learning ˆv. Near t = 1, the factor 11−t

amplifies errors in estimating the small conditional mean E[ Log x(X1) | Xt = x]: a small absolute error in this conditional expectation can translate into a much larger error in the velocity, which then produces an O(1) perturbation over the last few integration steps and shifts the terminal law away from π1. For this reason, following existing probability-flow analyses in the Euclidean setting (e.g., Li et al. (2025, 2024a)) we establish convergence rates to an approximate distribution at a terminal time T < 1, corresponding to stopping the integration before the most ill-conditioned regime. 

# 3 Total Variation Rates: Compact Manifolds 

We now establish TV error bounds for compact manifolds, which covers application including SO (3) (Yue et al., 2025), Tori and hypersphere (Mathieu and Nickel, 2020; Sriram et al., 2024). We then specialize to the case of hypersphere to obtain explicit iteration complexity results. We start with the following assumption on curvature of manifold. 

Assumption 1 (Assumptions on Riemannian Manifold) . M is a complete manifold without boundary. There exists LR ≥ 0 s.t. ∥R(u, v )v∥ ≤ LR∥u∥∥ v∥2, ∀u, v, w ∈ TxM , and all sectional curvatures of 

M are bounded below by Kmin and bounded above by Kmax .

This assumption enforces standard global regularity of the geometry of M : completeness guarantees geodesics and the exponential map are well-defined for all times. The bound ∥R(u, v )v∥ ≤ 

LR∥u∥∥ v∥2 controls the norm of the curvature operator, ensuring that curvature-induced deviations along geodesics are uniformly bounded. Finally, the two-sided sectional curvature bound Kmin ≤

K ≤ Kmax prevents the manifold from being too positively curved (excessive focusing of geodesics) or too negatively curved (overly rapid divergence), which is crucial for stability and comparison estimates. We impose the following assumptions on flow matching vector field v.

Assumption 2. (Regularity of Flow Matching Vector Field) The true vector field v(t, x ) is at least 

C2 in t, x and satisfies the following for ∀x ∈ M, t ∈ [0 , 1) :(1) ∥∇ v(t, x )∥op ≤ Lv,x t .(2) ∥v(t, x )∥ ≤ Lv and ddt v(t, x ) ≤ Lv,t t .(3) ∥ grad x div v(t, x )∥ ≤ Ldiv ,x t and ddt div v(t, x ) ≤ Ldiv ,t t .

The above conditions require the population vector field v(t, x ) to be smooth and uniformly well-behaved so that the induced flow depends stably on time and initial conditions. The bounds on 

∥∇ v(t, x )∥op , ∥v(t, x )∥, and ddt v(t, x ) ensure the dynamics are Lipschitz in space and controlled in magnitude and time variation, preventing trajectories from separating too quickly or developing instabilities as t evolves. In Euclidean space M = Rd with the standard metric, ∇v(t, x ) reduces to the Jacobian Dxv(t, x ), so ∥∇ v(t, x )∥op is exactly the usual spectral/operator norm ∥Dxv(t, x )∥2→2

(i.e., the Lipschitz constant of v(t, ·) at x). The bounds on ∥ grad x div v(t, x )∥ and ddt div v(t, x )

7control how local volume change induced by the flow varies across space and time, and is important for analyzing how densities evolve under the continuity equation. 

Assumption 3. (Estimation Error) Tthere exists some small ε > 0 such that the the followings hold, ∀x ∈ M, t ∈ [0 , 1) : (1) ∥ˆv(t, x ) − v(t, x )∥ ≤ ε, and (2) ∥∇ ˆv(t, x ) − ∇ v(t, x )∥op ≤ ε.

We remark that such an assumption on estimation accuracy for derivative of v(t, x ) is standard in the literature for error analysis of ODE based generative models (Li et al., 2024a, 2025; Liu et al., 2025). The second item implies an error bound on uniform divergence estimation error, which together with the first item, helps bounding the discretization error. We also emphasize that uniform estimation error bounds obtainable on compact manifolds (Yarotsky, 2017; Mena et al., 2025). Under our assumptions, we can prove the following bound on TV distance (see Definition A.1). 

Theorem 1. Let Assumptions 1, 2 and 3 hold. Define 

CLip := 3 pLscore  

> t

Lv,x t Lv + pLscore  

> t

Lv,t t + 3 LvLdiv ,x t + Ldiv ,t t + LR(Lv)2d (5) 

Ceps := p2Lscore  

> t

+ 1 + 2 pLscore  

> t

Lv + pLscore  

> t

Lv,x t + Ldiv ,x t + 2 LRdL v (6) 

Picking the constant step size h to satisfy the requirements in Lemma 5.2, we have 

TV ( πT , ˆπT ) ≤ hCLip + εCeps + ε2(pLscore  

> t

+ LRd).

We imposed an upper bound on the step size (as in Lemma 5.2) to ensure that each Euler step remains within a controlled neighborhood—in particular, within the injectivity-radius regime where the exponential map does not “fold” and the map Ftk ,h (x) = Exp x(hˆv(tk, x )) stays invertible—so that the continuous-time interpolation and the associated vector field ˜v are well defined. For more details, see Section 5 on the choice of h. At a high level, the constant CLip aggregates the Lipschitz regularity moduli of the population flow (and the score) that control how local Euler truncation errors are amplified when transported through time; thus it governs the O(h) discretization contribution in the TV bound. In contrast, Ceps collects the same geometric and score-dependent stability factors but attached to the estimation perturbation ˆv − v (and its divergence), quantifying how an ε-accurate field estimate propagates through the continuity equation; this is why it multiplies the leading 

O(ε) term, with the remaining ε2(pLscore  

> t

+ LRd) capturing higher-order interactions between score regularity and field mismatch. 

Remark 1. The early stopping error can be controlled in the Wasserstein distance (see Defini-tion A.1) as: W1(π1, π T ) ≲ 1 − T . On a compact manifold, TV bound implies a W1 bound, as 

W1(πT , ˆπT ) ≤ diam (M ) TV (πT , ˆπT ) (Villani, 2008, Theorem 6.15). Thus we can convert our TV bound into a bound in W1 distance, and obtain an error bound for W1( ˆπT , π 1), consider both sampling error and early stopping error. 

We now provide explicit rates on the d-dimensional hypersphere. Following Li et al. (2024a, 2025); Liu et al. (2025), we consider the case when the vector field is well-estimated (i.e., ε ≈ 0) and report the number of sampling steps N required to get εtarget close to πT .

Proposition 3.1. For some finite d ≥ 4, let M = Sd, and T < 1 be the early stopping time. Assume 

π1 has a smooth density and satisfies 0 < m 1 ≤ p1(x) ≤ M1. Pick π0(x) to be uniform on the hypersphere. Then Assumption 2 is satisfied and the constants in Assumption 2 are of polynomial order (see Lemma B.5). Now if Assumption 3 is satisfied, to reach εtarget accuracy in TV distance, 

• For constant step size as in (14) , the complexity is N = O(d2/ε target (1 − T )2).

• For a carefully designed step size schedule as in (15) , it can be improved to N = O(d2/ε target (1 − T ) ).

84 Total Variation Rates: Hadamard Manifolds 

In this section, we establish error bounds in terms of total variation distance on Hadamard manifolds (which are non-compact), which covers applications including SPD manifolds (Li et al., 2024b; Collas et al., 2025). While retaining the same curvature condition in Assumption 1, we modify Assumption 2 and 3 as follows, to naturally handle the non-compactness of Hadamard manifolds. 

Assumption 4 (Regularity of the true Vector Field) . The true vector field v satisfies ∀t ∈ [0 , 1) :(1) Ext ∥∇ v(t, x t)∥2op ≤ (Lv,x t )2 and Ext 

> ddt

v(t, x t) 2 ≤ (Lv,t t )2.(2) Ext 

> ddt

div v(t, x t) ≤ Ldiv ,t t and Ext ∥ grad x div v(t, x t)∥2 ≤ (Ldiv ,x t )2.(3) Ext ∥v(t, x t)∥2 ≤ (Lvt )2.

Assumption 5 (Regularity of Learned Vector Field) . Assume that the learned vector field ˆv

satisfies: (1) ∥ˆv(t, x )∥ ≤ Lˆvt , (2) ∥∇ ˆv(t, x )∥ ≤ Lˆv,x t and (3) ∥ grad div ˆ v(t, x )∥ ≤ Ldiv ˆ v,x t ,where Lˆvt , L ˆv,x t , L div ˆ v,x t are of the same order as Lvt , L v,x t , L div ,x t .

Assumption 6 (Estimation Error) . There exists some small ε > 0 s.t. the following hold ∀t ∈{tk}N −1 

> k=0

: (1) Ext [∥ˆv(tk, X tk ) − v(tk, X tk )∥2] ≤ ε, (2) Ext [|div ˆ v(tk, X tk ) − div v(tk, X tk )|] ≤ ε.

The conditions in Assumption 4, 5 and 6 serve the same purpose as Assumption 2 and 3. We remark that on a non-compact manifold, similar to the Euclidean space, it is more natural to assume regularity holds in expectation instead of holding uniformly. Compared to the compact case, we need to additionally enforce Assumption 5, which is of essential importance to guarantee the well-behavedness of the sampling ODE. The condition that the Lipschitz constants of the learned vector field are of the same order of the true field is made purely for convenience to avoid a more complicated looking bound. Note that for the compact case, since we can assume point-wise regularity, the condition made in Assumption 5 is a direct consequence of Assumption 2 and 3. We also remark that for existing works in Euclidean space, Liu et al. (2025) and Huang et al. (2025) assumed uniformly bounded regularity for the learned vector field, and estimation error in expectation, which are similar in spirit to Assumptions 5 and 6. Below, we present our rates. 

Theorem 2 (Sampling Error for Hadamard Manifold) . Let Assumptions 1, 4, 5 and 6, hold. Picking the step size h to satisfy the requirements in Lemma 5.2, we have 

TV ( πT , ˆπT ) ≤ hCLip + εCeps ,1,

where CLip is as in (5) and Ceps ,1 defined in (13) represents the (vector field) estimation error. 

We now specialize our results to the case when M ≡ SPD (n) := {X ∈ Rn×n : X ≻ 0, X T = X},the manifold of symmetric positive definite matrices endowed with the affine invariant metric 

gX (U, V ) = tr (X−1U X −1V ). To do so, we compute the explicit order of regularity constants CLip 

and Ceps ,1 (see Proposition B.6), which in turn yields the result below. 

Proposition 4.1. Let M = SPD (n) for which the dimension d = n(n + 1) /2. We adopt early stopping, i.e., simulate the ODE on the interval [0 , T ] ⊊ [0 , 1] . Choose prior distribution as 

π0(x) ∝ exp (− n(n+1) dg (x,I )2 

> 2

) being a Riemannian Gaussian distribution (the center of Riemannian Gaussian is arbitrary, here we set as I ∈ SPD (n) for notation simplicity). We further assume the data distribution π1 satisfies 

max E[d(X1, I )2eλ1d(X1,I )], E[eλ1d(X1,I )] ≤ Mλ1 , where λ1 = 24 max {1, κ }, (7) 9for some constant Mλ1 . Then Assumption 4 is satisfied and the constants in Assumption 4 are of polynomial order (see Proposition B.6 for explicit bounds). Under Assumption 1, 5 and Assumption 6, (with constant step size) the iteration complexity to reach εtarget accuracy in TV distance is 

N = O(d24 L3

> R

M 32 

> λ1

/ε target (1 − T )3).

Here the moment condition (7) plays a role analogous to finite-moment conditions (e.g., 

E[∥X1∥4] < ∞ and stronger conditions like bounded support) appeared in Euclidean discretization analysis (Liu et al., 2025; Li et al., 2025; Zhou and Liu, 2025). The appearance of exponential moments is due to curvature distortion: the model function sKmin (r) grows linearly in r when 

Kmin = 0 but exponentially in r when Kmin < 0 (see Section 5.2.1). Although the order of d is polynomial, we expect it could be improved further by more refined computation and additional assumptions. 

# 5 Proof Sketch and Intermediate Results 

We now outline the main ideas behind the proof of our principal TV-rate bound. At a high level, we view both the population flow-matching dynamics and its numerical approximation as deterministic transports on the manifold driven by (possibly different) time-dependent vector fields. Randomness enters only through the initialization, so the objects of interest are the time-marginal laws ( pt)t∈[0 ,1] 

and ( qt)t∈[0 ,1] induced by these transports. Our argument has three conceptual steps: 1. (TV stability under transport ) derive a differential identity for ∂t TV (pt, q t) in terms of the mismatch between the driving vector fields (Lemma 5.1); 2. (continuous-time interpolation of Euler ) construct an interpolation of the Euler scheme that is itself an ODE on M , so that Lemma 5.1 applies; 3. (term-by-term control ) bound the resulting RHS by establishing (i) score regularity for pt,(ii) regularity of the relevant vector fields, and (iii) a curvature-dependent estimate for the divergence of the interpolated field. Integrating the differential inequality over time then yields a bound on the terminal sampling error TV ( X1, Y 1) (or, when needed, the limit t ↑ 1). We begin with the key TV-derivative lemma, which is the Riemannian analogue of (Li et al., 2025, Lemma 4.2). The proof involves the following steps: (i) write the evolution of p and q through the continuity equation, and (ii) differentiate R (p − q)+, and integrate by parts—but all differential operators must be interpreted intrinsically (gradient, divergence, and the Riemannian volume form). The second line of (10) is obtained by taking absolute values and applying Cauchy–Schwarz to the inner-product term. 

Lemma 5.1. Let Xt, Y t be stochastic processes on M , satisfying the following ODE: 

dX t = v(t, X t) dt, X0 ∼ p0 (8) 

dY t = ˜ v(t, Y t) dt, Y0 ∼ q0 (9) 

Let p(t, x ) be the law of Xt and q(t, x ) be the law of Yt. Define Ωt = {x ∈ M : p(t, x ) − q(t, x ) > 0}.Then we have 

∂ TV ( Xt, Y t)

∂t =

Z

> Ωt

p(t, x ) (div (˜ v(x, t ) − v(x, t )) + ⟨grad log p(t, x ), ˜v(x, t ) − v(x, t )⟩) dV g(x)

≤E[| div (˜ v(x, t ) − v(x, t )) |] + E[∥ grad log p(t, x )∥∥ ˜v(x, t ) − v(x, t )∥]. (10) 10 5.1 Continuous Time Interpolation 

We now explain how Lemma 5.1 is used to analyze the Euler discretization error. We take (8) to be the flow-matching ODE driven by the (population) vector field v(t, ·), and we want (9) to represent the numerical method. A direct interpolation of the Euler scheme would be 

yt = Exp yk

 (t − tk)ˆ v(tk, y k), t ∈ [tk, t k+1 ),

i.e., the velocity is frozen at the left endpoint. However, this curve is not immediately of the form 

dy t = ˜v(t, y t) dt with a vector field ˜v(t, ·) on M , because the frozen vector ˆv(tk, y k) lives in Tyk M ,whereas the ODE requires the instantaneous velocity to belong to Tyt M .The natural remedy is to transport the frozen velocity along the curve . Concretely, for t ∈ [tk, t k+1 )we set ˜v(t, y t) := P yt 

> yk

ˆv(tk, y k), so that dy t = ˜ v(t, y t) dt, 

where P yt 

> yk

denotes parallel transport along the (interpolated) trajectory. This ensures ˜v(t, y t) ∈ Tyt M

for all t, so the interpolation is an honest ODE on M . To apply Lemma 5.1, we further need ˜v

to be defined as a function of ( t, x ) (not implicitly through a particular path). This requires that, given ( t, x ), we can uniquely recover the footpoint yk that generated x under the Euler interpolation. Equivalently, we need the map yk 7 −→ Exp yk

 (t − tk) ˆv(tk, y k) to be invertible on the relevant step size range. For notational convenience, for t ∈ [0 , 1) and h ≥ 0 we define Ft,h (x) := Exp x(hˆv(t, x )). In particular, for t ∈ [tk, t k+1 ), the Euler interpolation can be written as Ftk ,t −tk (xk) = Exp xk (( t −

tk) ˆv(tk, x k)). Lemma 5.2 below shows that for sufficiently small h the map Ftk ,h is invertible. Assuming invertibility, we can express the interpolation vector field at an arbitrary point x by pulling back to the unique preimage xk = F −1  

> tk,t −tk

(x) and then parallel-transporting the frozen velocity to TxM :˜v(t, x ) = P xF −1  

> tk ,t −tk(x)

ˆv(tk, F −1  

> tk,t −tk

(x)) ,

where, for a trajectory ( xt) of (9), we have F −1  

> tk,t −tk

(xt) = xtk .

Lemma 5.2. Let M be simply connected Riemannian manifold that satisfies Assumption 1. Let 

b be any vector field on M , satisfying ∥b(x)∥ ≤ B, ∀x ∈ M . Assume ∥∇ vb(x)∥ ≤ L∇∥v∥. Let 

R = inj ( M ). To guarantee Ftk ,t −tk being invertible, we require 

h < min { RB , 14L∇

,

s 34∥b(x)∥2LR(2 + 2 L∇ max { 1√Kmin 

, 1}) }, if Kmin > 0,h < min { RB , 14L∇

,

vuut 34∥b(x)∥2LR(2 sinh( √−Kmin )

> √−Kmin

+ 4 cosh( √−Kmin )−1 

> −Kmin

L∇)

}, if Kmin < 0,h < min { RB , 14L∇

,

s

34∥b(x)∥2LR(2 + hL ∇) }, if Kmin = 0 .

The invertibility requirement is precisely the condition that the exponential map used in one Euler step stays in a regime where it does not “fold” the manifold (e.g., by crossing conjugate points or exiting the injectivity radius). If ( t − tk) ˆv(tk, Y tk ) is too large, the map x 7 → Exp x(( t − tk) ˆv(tk, x )) may fail to be injective, and then multiple preimages could map to the same Yt. Lemma 5.2 enforces a step-size restriction that prevents this pathology by combining (i) injectivity-radius control and 11 (ii) bounds on the differential of Ft,h via curvature comparison and covariant-derivative estimates. When inj (M ) = ∞ (e.g., M = Rd or M = SPD (n)), the injectivity-radius constraint becomes vacuous and the step-size condition simplifies accordingly. 

5.2 Discretization analysis 

Although (8) and (9) are deterministic ODEs, the random initialization makes ( Xt) and ( Yt)stochastic processes through their induced laws. To isolate discretization error, we couple them by taking the same initial distribution: X0 ∼ π0, and Y0 ∼ π0. Then TV (Xt, Y t) measures the discrepancy between the population flow-matching transport and the transport induced by the Euler interpolation. In particular, at terminal time (or in the limit t ↑ 1), TV (X1, Y 1) is exactly the sampling error attributable to time discretization (and, if present, to the use of ˆ v instead of v). Applying Lemma 5.1 with p(t, ·) the law of Xt and q(t, ·) the law of Yt, and integrating in time, yields TV ( X1, Y 1) ≤ TV ( X0, Y 0) + 

Z 10

(E[| div (˜ v(t, X t) − v(t, X t)) |]+ 

E[∥ grad log pt(Xt)∥∥ ˜v(t, X t) − v(t, X t)∥]) dt. 

Since TV (X0, Y 0) = 0 under the shared initialization, the task reduces to controlling the two error terms on the RHS. A convenient way to organize the analysis is to bound three quantities that repeatedly appear in the argument: E∥ grad log pt(Xt)∥2, E∥˜v(t, X t) − v(t, X t)∥2, and 

E| div ( ˜v(t, X t) − v(t, X t)) |. Specifically, the second expectation in (10) can be decoupled via Cauchy–Schwarz: 

E[∥ grad log pt(Xt)∥∥ ˜v(t, X t) − v(t, X t)∥] ≤ pE[∥ grad log pt(Xt)∥2]pE[∥˜v(t, X t) − v(t, X t)∥2].

Thus, once we establish suitable bounds for these three terms (uniformly for t away from 1, and with explicit dependence on t as t ↑ 1), we can integrate over time to obtain a quantitative TV-rate. 

5.2.1 Score Regularity and Vector Field Regularity 

We now summarize the regularity inputs needed to control the score term and the vector-field mismatch. The score bound is the main “density regularity” ingredient, while the vector-field bounds ensure that (i) the Euler interpolation is well defined and (ii) the mismatch ˜v − v can be quantified in geometry-aware norms. 

• Score regularity. We show that there exists a (possibly time-dependent) constant Lscore  

> t

such that, for all t ∈ [0 , 1), E∥ grad log pt(Xt)∥2 ≤ Lscore  

> t

. The bound is allowed to deteriorate as 

t ↑ 1; this is unavoidable in general and matches the behavior of population conditional flow fields near the terminal time. We establish this in Proposition 5.3 for compact manifolds and in Proposition 5.4 for Hadamard manifolds (both stated below). A crucial point is that the existence of such an Lscore  

> t

relies on sufficient smoothness/positivity of pt; without it, grad log pt may not be square-integrable and the bound can fail even for t < 1. 

• Regularity on compact manifolds. On compact manifolds, curvature is bounded and distances are uniformly controlled, but the presence of cut points can obstruct smoothness of conditional densities such as pt(x1 | xt = x), which enter the explicit formulae for the population flow field. We verify Assumption 2 on Sd in Appendix D by exploiting explicit computations on the sphere. For more general compact geometries, controlling smoothness across the cut locus is substantially more delicate and is the main reason we phrase the required assumptions abstractly. 12 • Regularity on Hadamard manifolds. On Hadamard manifolds, the absence of cut points guarantees global smoothness of the exponential map and of the relevant conditional densities, which greatly simplifies differentiability issues. The trade-off is that negative curvature and unbounded distance can amplify derivative bounds along geodesics. In particular, pointwise bounds for derivatives of v(t, x ) typically involve factors that are Poly (d), Poly (1 /(1 − t) ) and Poly (sKmin (d(x0, x 1))), where for Kmin < 0 the model function sKmin (r) = sinh( r√−Kmin )

> √−Kmin

grows exponentially in r. This motivates seeking regularity in expectation rather than uniform-in-x

bounds. We verify Assumption 4 for SPD (n) in Appendix E by combining curvature comparison with a moment condition (7) , which plays the role of a Euclidean second-moment bound and compensates for curvature-induced growth. 

Proposition 5.3. Let M be a compact manifold satisfying Assumption 1. Under Assumption 2, exists some finite number Lscore  

> t

that depends on t s.t. E∥ grad log pt(Xt)∥2 ≤ Lscore  

> t

, ∀t ∈ [0 , 1) .

Proposition 5.4. Let M be a Hadamard manifold with sectional curvature satisfying −κ2 = Kmin ≤

sec ≤ 0. For prior being chosen as X0 ∼ e−dd (x0,z )2

, we have 

E∥ grad log pt(Xt)∥2 ≲ M d2

(1 − t)2 L2

> R

d2λscore ,

where λscore = 6 max {1, κ } and M = EX1 [eλd (x1,z )] depends on the data distribution. 

5.2.2 The Divergence Term 

Finally, we comment on the divergence contribution E| div ( ˜v(t, X t) − v(t, X t)) |. This is the most geometry-specific part of the discretization analysis. In Euclidean space, the Euler interpolation simply freezes the velocity and no parallel transport is required; consequently, div ˜ v can be handled with elementary calculus. On a manifold, by contrast, our interpolated field ˜v is defined through (i) parallel transport and (ii) an implicit inverse mapping F −1  

> tk,t −tk

. Both operations contribute nontrivially to ∇˜v and hence to div ˜ v.Our approach is to explicitly differentiate the representation ˜v(t, x ) = P xF −1  

> tk ,t −tk(x)

ˆv tk, F −1  

> tk,t −tk

(x),

for t ∈ [tk, t k+1 ), along the interpolation geodesic under a parallel orthonormal frame, and decompose div ˜ v(t, x ) = Pdi=1 ∇Ei(t) ˜v(t, x ), E i(t) into two pieces: 1. Derivatives of ˆ v(tk, ·) evaluated at the preimage point; 2. Curvature distortion from differentiating parallel transport. Combining these estimates and using the curvature bounds in Assumption 1, we obtain a bound on div ˜ v and hence on div ( ˜v − v), stated precisely in Lemma C.4. This divergence control, together with the score/vector-field regularity results above, completes the term-by-term bounds needed to integrate (10) and prove the stated TV-rate. 

Acknowledgements 

KB is supported in part by National Science Foundation (NSF) grant DMS-2413426. 13 References 

F. Alimisis, A. Orvieto, G. B´ ecigneul, and A. Lucchi. A continuous-time perspective for modeling acceleration in Riemannian optimization. In International Conference on Artificial Intelligence and Statistics , pages 1297–1307. PMLR, 2020. (Cited on page 54.) 

V. Bansal, S. Roy, P. Sarkar, and A. Rinaldo. On the Wasserstein Convergence and Straightness of Rectified Flow. arXiv preprint arXiv:2410.14949 , 2024. (Cited on page 3.) 

J. Benton, G. Deligiannidis, and A. Doucet. Error bounds for flow matching methods. Transactions on Machine Learning Research , 2024. (Cited on page 3.) 

J. Cheeger, D. G. Ebin, and D. G. Ebin. Comparison theorems in Riemannian geometry , volume 9. North-Holland publishing company Amsterdam, 1975. (Cited on pages 17, 18, and 38.) 

R. T. Q. Chen and Y. Lipman. Flow matching on general geometries. In The Twelfth Interna-tional Conference on Learning Representations , 2024. URL https://openreview.net/forum? id=g7ohDlTITL . (Cited on page 3.) 

A. H. Cheng, A. Lo, K. L. K. Lee, S. Miret, and A. Aspuru-Guzik. Stiefel Flow Matching for Moment-Constrained Structure Elucidation. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=84WmbzikPP . (Cited on page 3.) 

X. Cheng, J. Zhang, and S. Sra. Efficient sampling on Riemannian manifolds via Langevin MCMC. 

Advances in Neural Information Processing Systems , 35:5995–6006, 2022. (Cited on page 3.) 

A. Collas, C. Ju, N. Salvy, and B. Thirion. Riemannian flow matching for brain connectivity matrices via pullback geometry. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. URL https://openreview.net/forum?id=NY3LzmUXl7 . (Cited on pages 3 and 9.) 

C. Criscitiello and N. Boumal. An accelerated first-order method for non-convex optimization on manifolds. Foundations of Computational Mathematics , 23(4):1433–1509, 2023. (Cited on page 36.) 

V. De Bortoli, E. Mathieu, M. Hutchinson, J. Thornton, Y. W. Teh, and A. Doucet. Riemannian score-based generative modelling. Advances in neural information processing systems , 35:2406– 2422, 2022. (Cited on pages 2 and 3.) 

K. Gatmiry and S. S. Vempala. Convergence of the Riemannian Langevin algorithm. arXiv preprint arXiv:2204.10818 , 2022. (Cited on page 3.) 

Y. Guan, K. Balasubramanian, and S. Ma. Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. URL https://openreview.net/forum?id=KxhCJc8BOg . (Cited on page 3.) 

Y. Guan, K. Balasubramanian, and S. Ma. Mirror flow matching with heavy-tailed priors for generative modeling on convex domains. In The Fourteenth International Conference on Learning Representations , 2026. URL https://openreview.net/forum?id=dZKl7uc0XQ . (Cited on pages 3, 6, and 42.) 

H. Hirai, H. Nieuwboer, and M. Walter. Interior-point methods on manifolds: theory and applications. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS) , pages 2021–2030. IEEE, 2023. (Cited on page 98.) 

14 C.-W. Huang, M. Aghajohari, J. Bose, P. Panangaden, and A. C. Courville. Riemannian diffusion models. Advances in Neural Information Processing Systems , 35:2750–2761, 2022. (Cited on page 3.) 

D. Z. Huang, J. Huang, and Z. Lin. Convergence analysis of probability flow ODE for score-based generative models. IEEE Transactions on Information Theory , 2025. (Cited on pages 3 and 9.) 

S. Kobayashi. Transformation groups in differential geometry . Springer, 1972. (Cited on page 98.) 

L. Kong and M. Tao. Convergence of kinetic Langevin Monte Carlo on Lie groups. In The Thirty Seventh Annual Conference on Learning Theory , pages 3011–3063. PMLR, 2024. (Cited on page 3.) 

S. Lang. Differential and Riemannian manifolds , volume 160. Springer Science & Business Media, 2012. (Cited on page 5.) 

J. M. Lee. Introduction to Smooth Manifolds . Springer, New York, 2012. ISBN 978-1-4419-9982-5. 

> (Cited on page 45.)

J. M. Lee. Introduction to Riemannian manifolds , volume 2. Springer, 2018. (Cited on pages 18, 27, 36, 51, 57, 87, and 98.) 

M. Lezcano-Casado. Curvature-dependant global convergence rates for optimization on manifolds of bounded geometry. arXiv preprint arXiv:2008.02517 , 2020. (Cited on pages 17, 87, 89, 91, 92, 93, 94, and 95.) 

G. Li, Y. Wei, Y. Chi, and Y. Chen. A sharp convergence theory for the probability flow odes of diffusion models. arXiv preprint arXiv:2408.02320 , 2024a. (Cited on pages 4, 7, and 8.) 

M. Li and M. A. Erdogdu. Riemannian Langevin algorithm for solving semidefinite programs. 

Bernoulli , 29(4):3093–3113, 2023. (Cited on page 3.) 

R. Li, Q. Di, and Q. Gu. Unified convergence analysis for score-based diffusion models with deterministic samplers. In The Thirteenth International Conference on Learning Representations ,2025. URL https://openreview.net/forum?id=HrdVqFSn1e . (Cited on pages 3, 4, 7, 8, and 10.) 

Y. Li, Z. Yu, G. He, Y. Shen, K. Li, X. Sun, and S. Lin. SPD-DDPM: Denoising diffusion probabilistic models in the symmetric positive definite space. In Proceedings of the AAAI conference on artificial intelligence , volume 38, pages 13709–13717, 2024b. (Cited on page 9.) 

Y. Liu, R. Hu, Y. Chen, and L. Huang. Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants. arXiv preprint arXiv:2508.07333 , 2025. (Cited on pages 4, 8, 9, and 10.) 

A. Lou, M. Xu, A. Farris, and S. Ermon. Scaling Riemannian diffusion models. Advances in Neural Information Processing Systems , 36:80291–80305, 2023. (Cited on page 3.) 

X. Luo, Z. Wang, Q. Wang, X. Shao, J. Lv, L. Wang, Y. Wang, and Y. Ma. Crystalflow: a flow-based generative model for crystalline materials. Nature Communications , 16(1):9267, 2025. (Cited on page 3.) 

J. E. Marsden, T. Ratiu, and R. Abraham. Manifolds, Tensor Analysis, and Applications . Applied Mathematical Sciences. Springer, 3rd edition, 2002. ISBN 0-201-10168-S. (Cited on pages 19 and 45.) 

E. Mathieu and M. Nickel. Riemannian continuous normalizing flows. Advances in neural information processing systems , 33:2503–2515, 2020. (Cited on pages 3 and 7.) 

15 G. Mena, A. K. Kuchibhotla, and L. Wasserman. Statistical properties of rectified flow. arXiv preprint arXiv:2511.03193 , 2025. (Cited on page 8.) 

B. K. Miller, R. T. Q. Chen, A. Sriram, and B. M. Wood. FlowMM: Generating materials with Riemannian flow matching. In Proceedings of the 41st International Conference on Machine Learning , volume 235, pages 35664–35686, 2024. URL https://proceedings.mlr.press/v235/ miller24a.html . (Cited on page 3.) 

S. Roy, A. Rinaldo, and P. Sarkar. Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization. arXiv preprint arXiv:2601.15500 , 2026. 

> (Cited on page 4.)

Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=PxTIG12RRHS . (Cited on page 3.) 

A. Sriram, B. K. Miller, R. T. Chen, and B. M. Wood. FlowLLM: Flow matching for material generation with large language models as base distributions. Advances in Neural Information Processing Systems , 37:46025–46046, 2024. (Cited on pages 3 and 7.) 

M. Su, J. Y.-C. Hu, S. Pi, and H. Liu. On flow matching kl divergence. arXiv preprint arXiv:2511.05480 , 2025. (Cited on page 3.) 

C. Villani. Optimal transport: old and new , volume 338. Springer, 2008. (Cited on page 8.) 

Z. Wan, Q. Wang, G. Mishne, and Y. Wang. Elucidating Flow Matching ODE Dynamics via Data Geometry and Denoisers. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=f5czhqYK3H . (Cited on page 5.) 

J. Wu, B. Chen, Y. Zhou, Q. Meng, R. Zhu, and Z.-M. Ma. Riemannian Neural Geodesic Interpolant. 

arXiv preprint arXiv:2504.15736 , 2025. (Cited on pages 3 and 19.) 

X. Xu, Z. Zhang, Y. Nakahira, G. Qu, and Y. Chi. Polynomial Convergence of Riemannian Diffusion Models. arXiv preprint arXiv:2601.02499 , 2026. (Cited on page 3.) 

D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural networks , 94: 103–114, 2017. (Cited on page 8.) 

A. Yue, Z. Wang, and H. Xu. ReQFlow: Rectified quaternion flow for efficient and high-quality protein backbone generation. In Forty-second International Conference on Machine Learning ,2025. URL https://openreview.net/forum?id=f375uEmYDf . (Cited on pages 3 and 7.) 

Z. Zhou and W. Liu. An Error Analysis of Flow Matching for Deep Generative Modeling. In 

Forty-second International Conference on Machine Learning , 2025. URL https://openreview. net/forum?id=vES22INUKm . (Cited on pages 3, 10, and 42.) 

16 Appendix 

# A Additional Background for Riemannian Geometry 

A.1 Distances used 

Definition A.1 (Total variation distance and W1 distance) . Let (M, g ) be a Riemannian manifold, let d denote the geodesic distance induced by g, and let ρ1, ρ 2 be probability measures on the Borel 

σ-algebra B(M ).

• The total variation distance between ρ1 and ρ2 is 

TV ( ρ1, ρ 2) := sup  

> A∈B (M)

ρ1(A) − ρ2(A) .

If ρ1 and ρ2 are absolutely continuous with respect to the Riemannian volume measure dV g,with densities p1 = dρ 1 

> dV g

and p2 = dρ 2 

> dV g

, then 

TV ( ρ1, ρ 2) = 12

Z

> M

|p1(x) − p2(x)| dV g(x) = 

Z

> {p1>p 2}

 p1(x) − p2(x) dV g(x).

• The 1-Wasserstein distance (induced by d) is defined for measures with finite first moment (e.g., R 

> M

d(x0, x ) dρ i(x) < ∞ for some x0 ∈ M and i ∈ { 1, 2}) by 

W1(ρ1, ρ 2) := inf 

> γ∈Π( ρ1,ρ 2)

Z 

> M×M

d(x, y ) dγ (x, y ),

where Π( ρ1, ρ 2) denotes the set of couplings of (ρ1, ρ 2), i.e., probability measures γ on M × M

whose first and second marginals are ρ1 and ρ2, respectively. 

A.2 Comparison Theorem 

Comparison theorems (Cheeger et al., 1975) will be used frequently throughout the Appendix. For example, we used Rauch theorem in Appendix C.1, which is a key step in proving Lemma 5.2; we also use comparison theorems frequently in Appendix E to verify Assumption 4. Here we provide a brief introduction to comparison theory, focusing on intuition. Let k ∈ R

be some constant, and assume we have functions f, g ≥ 0 defined on [0 , t ]. If both f, g share the same initial value f (0) = g(0), and we further assume f ′(0) ≤ g′(0). If we know, for second order ODEs, f ′′ + κf ≤ g′′ + κg , then we can conclude, at least locally, f ≤ g. For more details, see, for example Lezcano-Casado (2020, Lemma 4.8). Such an observation suggests a more general comparison principle, which is the key idea behind comparison theorems in Riemannian geometry. For example, we have the following theorem. 

Theorem 3 (Proposition 4.9 in Lezcano-Casado (2020)) . Let M be a Riemannian manifold with bounded sectional curvature satisfying Kmin ≤ Sec ≤ Kmax . Let γ : [0 , r ] → M be a geodesic, and let 

17 X, Y be vector fields along γ with X, Y ⊥ γ′ satisfying the following ODE ( X′is covariant derivative of X along γ): 

X′′ + R(X, γ ′)γ′ = Y, X(0) = 0 , X ′(0) = 0 .

Assume that there exists a continuous function η that upper bounds Y : ∥Y ∥ ≤ η on [0 , r ]. Then ρ

defined as the solution of ρ′′ + Kmin ρ = η, ρ (0) = 0 , ρ ′(0) = 0 upper bounds X: ∥X∥ ≤ ρ.

To summarize, using the comparison principle for ODE, we can make use of curvature information to establish bounds for certain vector fields defined on a Riemannian manifold, as long as the vector field satisfies some specific ODE. For a comprehensive discussion on comparison theorems, see for example Lee (2018) and Cheeger et al. (1975). 

A.3 Riemannian Submanifolds 

We briefly introduce some results of Riemannian submanifolds following Lee (2018), and these results will be used in proving Lemma E.15. Let N ⊆ M be a Riemannian submanifold of M , with induced metric. To avoid ambiguity, we denote ∇M to be the connection on M , and ∇N to be the connection on N . Let X, Y be vector fields on N . We can extend them to vector fields on M , and the covariant derivative ( ∇M )X Y can be decomposed as (∇M )X Y = (( ∇M )X Y )⊥ + (( ∇M )X Y )∥.

The normal component (( ∇M )X Y )⊥ defines the second fundamental form, which is a map from 

X(N ) × X(N ) to a section of the normal bundle of N , formally defined as II( X, Y ) := (( ∇M )X Y )⊥.

Gauss formula (Lee, 2018, Theorem 8.2) states that, for X, Y being vector fields on N and extended arbitrarily to M , then (∇M )X Y = ( ∇N )X Y + II( X, Y ).

A closely related terminology is totally geodesic manifold. N is said to be totally geodesic if every geodesic in N is also a geodesic in M . Furthermore, by Lee (2018, Proposition 8.12), we know the following are equivalent: 

• N is a totally geodesic submanifold of M .

• The second fundamental form of N vanishes identically. 

• Every geodesic in N is also a geodesic in M .

# B Proof of Main Theorem 

This Section is organized as follows. We first prove Lemma 5.1 in Appendix B.1, which provides a way to control the propagation of TV distance along ODE simulation: 

∂ TV ( Xt, Y t)

∂t =

Z

> Ωt

p(t, x ) (div (˜ v(x, t ) − v(x, t )) + ⟨grad log p(t, x ), ˜v(x, t ) − v(x, t )⟩) dV g(x).

Note that the time derivative of TV distance consists of two parts: 18 • We control the the “velocity vector” term p(t, x )⟨grad log p(t, x ), ˜v(x, t ) − v(x, t )⟩ in Appendix B.2. 

• We control the “divergence term” p(t, x ) div (˜ v(x, t ) − v(x, t )) in Appendix B.3. Moreover, recall that the well-definedness of ˜v depends on the invertibility of Ft,h (x) := Exp x(hˆv(t, x )), which we justify in Appendix B.4. The proof of the main results (sampling error bound in Theorems 1 and 2) is presented in Appendix B.5. Finally, we present proofs for iteration complexity on the hypersphere and SPD manifold in Appendix B.6 and B.7, respectively. Appendix C contains auxiliary results needed in the proofs of Theorems 1 and 2. Finally, Appendix D and E proves the required regularity results needed for proving Propositions 3.1 and 4.1. 

B.1 Proof for Lemma 5.1 

We start with the following result. 

Theorem 4 (Transport Theorem (Marsden et al., 2002, Theorem 8.1.12)) . Let (M, μ ) be a volume manifold and X be a vector field on M with flow Ft. For smooth function f defined on F(M × R),let ft(m) = f (m, t ). We have that for any open set U ⊆ M ,

ddt 

Z 

> Ft(U)

ftμ =

Z 

> Ft(U)

 ∂f ∂t + div μ(ftX)



μ. 

Proof. [Proof of Lemma 5.1] We can write TV ( Xt, Y t) = 

Z

> Ωt

p(t, x ) − q(t, x )dV g(x).

By transport Theorem, 

∂ TV ( Xt, Y t)

∂t = ddt 

Z

> Ωt

p(t, x ) − q(t, x )dV g(x)=

Z

> Ωt

 ∂(p(t, x ) − q(t, x )) 

∂t + div (( p(t, x ) − q(t, x )) X)



dV g(x)=

Z

> Ωt

∂(p(t, x ) − q(t, x )) 

∂t dV g(x) + 

Z

> ∂Ωt

(p(t, x ) − q(t, x )) ⟨X, n ⟩dV ˆg(x)=

Z

> Ωt

∂(p(t, x ) − q(t, x )) 

∂t dV g(x) + 

Z

> ∂Ωt

0 × ⟨ X, n ⟩dV ˆg(x)=

Z

> Ωt

∂(p(t, x ) − q(t, x )) 

∂t dV g(x).

The following continuity equation was proved in Wu et al. (2025, Theorem 2). We provide a proof for completeness. For any test function φ that is smooth (and of bounded support if the manifold is non-compact and does not have boundary), we have 

Z

> M

φ(x) ∂∂t p(t, x )dV g(x) = ddt 

Z

> M

φ(x)p(t, x )dV g(x) = ddt E[φ(xt)] = E[∇φ(xt) ◦ ddt xt]=

Z

> M

⟨grad φ(x), v (x, t )⟩p(t, x )dV g(x)19 = −

Z

> M

φ(x) div ( v(x, t )p(t, x )) dV g(x).

Hence we conclude that 

∂∂t p(t, x ) = − div ( v(x, t )p(t, x )) .

Therefore 

∂ TV ( Xt, Y t)

∂t =

Z

> Ωt

∂(p(t, x ) − q(t, x )) 

∂t dV g(x)=

Z

> Ωt

− div ( v(x, t )p(t, x )) + div (˜ v(x, t )q(t, x )) dV g(x)=

Z

> ∂Ωt

−p(t, x )⟨v(x, t ), n ⟩ + q(t, x )⟨˜v(x, t ), n ⟩dV ˆg(x)=

Z

> ∂Ωt

p(t, x )⟨˜v(x, t ) − v(x, t ), n ⟩dV ˆg(x)=

Z

> Ωt

div ((˜ v(x, t ) − v(x, t )) p(t, x )) dV g(x)=

Z

> Ωt

p(t, x ) div (˜ v(x, t ) − v(x, t )) + ⟨grad p(t, x ), ˜v(x, t ) − v(x, t )⟩dV g(x)=

Z

> Ωt

p(t, x ) (div (˜ v(x, t ) − v(x, t )) + ⟨grad log p(t, x ), ˜v(x, t ) − v(x, t )⟩) dV g(x),

where observe that p = q on ∂Ωt, and the last equality is due to the product rule of divergence. Note that grad p(t, x ) = p(t, x ) grad log p(t, x ), thereby concluding the proof. 

B.2 Velocity Vector Term 

In this section, we bound the velocity vector term. We remark that Lemma B.1 and Lemma B.2 are essentially the same, except that they are under different assumptions. 

Lemma B.1. Under Assumption 2 and 3, we can bound 

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]

≤εp2Lscore  

> t

+



Lscore  

> t

(2( t − tk)2(Lˆv,x t Lˆv + LvLˆv,x t + Lv,t t + LvLv,x t )2)

 12

.

Proof. Using triangle inequality, we can write 

∥˜v(t, X t) − v(t, X t)∥≤∥ P Xtk 

> Xt

˜v(t, X t) − ˆv(tk, X tk )∥ + ∥ˆv(tk, X tk ) − v(tk, X tk )∥ + ∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥.

Denote Xt→tk = F −1  

> tk,t −tk

(Xt). For the first term above, we have 

∥P Xtk 

> Xt

˜v(t, X t) − ˆv(tk, X tk )∥

=∥P Xtk 

> Xt

P Xt

> Xt→tk

ˆv(tk, X t→tk ) − ˆv(tk, X tk )∥ = ∥P Xt

> Xt→tk

ˆv(tk, X t→tk ) − P Xt

> Xtk

ˆv(tk, X tk )∥

20 ≤∥ P Xt

> Xt→tk

ˆv(tk, X t→tk ) − ˆv(tk, X t)∥ + ∥ˆv(tk, X t) − P Xt

> Xtk

ˆv(tk, X tk )∥≤Lˆv,x t d(Xt→tk , X t) + Lˆv,x t d(Xt, X tk ) = Lˆv,x t

 d(F −1  

> tk,t −tk

(Xt), X t) + d(Xt, X tk ) ,

where we used the fact that parallel transport preserve norm. Note that Xt is the trajectory of X at time t. Starting from Xt→tk , we have Exp Xt→tk (( t − tk)ˆ v(t, X t→tk )) = Xt.

Hence 

d(F −1  

> tk,t −tk

(Xt), X t) = ( t − tk)∥ˆv(t, X t→tk )∥ ≤ (t − tk)Lˆv.

Also, we know d(Xt, X tk ) is the distance of ODE trajectory, hence 

d(Xt, X tk ) ≤

Z ttk

∥v(s, X s)∥ds ≤ (t − tk)Lv.

Now, for the second term, we have by assumption that ∥ˆv(tk, X tk ) − v(tk, X tk )∥ ≤ ε. For the third term, 

∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥ =∥v(tk, X tk ) − v(t, X tk )∥ + ∥v(t, X tk ) − P Xtk 

> Xt

v(t, X t)∥≤(t − tk)Lv,t t + d(Xt, X tk )Lv,x t ≤ (t − tk)( Lv,t t + LvLv,x t ).

Putting the above estimates together, we obtain 

∥˜v(t, X t) − v(t, X t)∥2

≤



(t − tk)Lˆv,x t (Lˆv + Lv) + ε + ( t − tk)( Lv,t t + LvLv,x t )

2

≤2ε2 + 2( t − tk)2(Lˆv,x t Lˆv + LvLˆv,x t + Lv,t t + LvLv,x t )2.

Hence 

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]

≤E[∥ grad log p(t, x )∥2] 12 E[∥˜v(x, t ) − v(x, t )∥2] 12

≤



Lscore  

> t

(2 ε2 + 2( t − tk)2(Lˆv,x t Lˆv + LvLˆv,x t + Lv,t t + LvLv,x t )2)

 12

≤εp2Lscore  

> t

+



Lscore  

> t

(2( t − tk)2(Lˆv,x t Lˆv + LvLˆv,x t + Lv,t t + LvLv,x t )2)

 12

,

where note that pE[A + B] ≤ pE[A] + pE[B]. For Hadamard manifolds, when regularity conditions hold in expectation, we have the following result. Note that the proof strategy is essentially the same as the previous case. 

Lemma B.2. Under Assumption 4, 5 and 6, we can bound 

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]

≤εp3Lscore  

> t

+



Lscore  

> t

(6( t − tk)2 

(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2

)

 12

.

21 Proof. Using triangle inequality, we can write 

∥˜v(t, X t) − v(t, X t)∥≤∥ P Xtk 

> Xt

˜v(t, X t) − ˆv(tk, X tk )∥ + ∥ˆv(tk, X tk ) − v(tk, X tk )∥ + ∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥.

Hence using Cauchy-Schwarz, 

E[∥˜v(t, X t) − v(t, X t)∥2]

≤E[3 ∥P Xtk 

> Xt

˜v(t, X t) − ˆv(tk, X tk )∥2 + 3 ∥ˆv(tk, X tk ) − v(tk, X tk )∥2 + 3 ∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥2].

Denote Xt→tk = F −1  

> tk,t −tk

(Xt). For the first term, we have 

E∥P Xtk 

> Xt

˜v(t, X t) − ˆv(tk, X tk )∥2

=E∥P Xtk 

> Xt

P Xt

> Xt→tk

ˆv(tk, X t→tk ) − ˆv(tk, X tk )∥2 = E∥P Xt

> Xt→tk

ˆv(tk, X t→tk ) − P Xt

> Xtk

ˆv(tk, X tk )∥2

≤2E[∥P Xt

> Xt→tk

ˆv(tk, X t→tk ) − ˆv(tk, X t)∥2 + 2 ∥ˆv(tk, X t) − P Xt

> Xtk

ˆv(tk, X tk )∥2]

≤2( Lˆv,x t )2E[d(Xt→tk , X t)2] + 2( Lˆv,x t )2E[d(Xt, X tk )2]=2( Lˆv,x t )2E d(F −1  

> tk,t −tk

(Xt), X t)2 + d(Xt, X tk )2 ,

where we used the fact that parallel transport preserve norm. Note that Xt is the trajectory of X at time t. Starting from Xt→tk , we have Exp Xt→tk (( t − tk)ˆ v(t, X t→tk )) = Xt.

Hence 

E[d(F −1  

> tk,t −tk

(Xt), X t)2] = E[( t − tk)2∥ˆv(t, X t→tk )∥2] ≤ (t − tk)2(Lˆvt )2.

Also, we know d(Xt, X tk ) is the distance of ODE trajectory, hence 

E[d(Xt, X tk )2] ≤ E[( 

Z ttk

∥v(s, X s)∥ds )2] ≤ (t − tk)E[

Z ttk

∥v(s, X s)∥2ds ]= ( t − tk)

Z ttk

E[∥v(s, X s)∥2]ds ≤ (t − tk)2(Lvt )2.

Now, for the second term, we have 

E∥ˆv(tk, X tk ) − v(tk, X tk )∥2 ≤ ε2.

Finally, for the third term, by swapping the order of the derivative and parallel transport, we get 

∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥ ≤ 

Z ttk

Dds P Xtk 

> Xs

v(s, X s) ds =

Z ttk

∂sv(s, X s) + ∇ ˙Xs v(s, X s) ds. 

Therefore, 

E∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥2 ≤ (t − tk)

Z ttk

E ∂sv(s, X s) + ∇ ˙Xs v(s, X s) 2

ds. 

22 Notice that 

∂sv(s, X s) + ∇ ˙Xs v(s, X s) 2

≲ ∥∂sv(s, X s)∥2 + ∥∇ v(s, X s)∥2∥v(s, X s)∥2,

so we have 

E∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥2 ≤(t − tk)

Z ttk

E ∂sv(s, X s) + ∇ ˙Xs v(s, X s) 2

ds 

≲(t − tk)2E[∥∂sv(s, X s)∥2 + ∥∇ v(s, X s)∥2∥v(s, X s)∥2]

≲(t − tk)2(( Lv,t t )2 + ( Lv,x t Lvt )2).

Putting the above estimates together, we obtain 

E[∥˜v(t, X t) − v(t, X t)∥2]

≤E[3 ∥P Xtk 

> Xt

˜v(t, X t) − ˆv(tk, X tk )∥2 + 3 ∥ˆv(tk, X tk ) − v(tk, X tk )∥2 + 3 ∥v(tk, X tk ) − P Xtk 

> Xt

v(t, X t)∥2]

≤6( Lˆv,x t )2 

(t − tk)2(Lˆvt )2 + ( t − tk)2(Lvt )2

+ 3 ε2 + 6( t − tk)2 

(Lv,t t )2 + ( Lvt )2(Lv,x t )2

=6( t − tk)2 

(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2

+ 3 ε2.

Hence 

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]

≤E[∥ grad log p(t, x )∥2] 12 E[∥˜v(x, t ) − v(x, t )∥2] 12

≤



Lscore  

> t

(6( t − tk)2 

(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2

+ 3 ε2)

 12

≤εp3Lscore  

> t

+



Lscore  

> t

(6( t − tk)2 

(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2

)

 12

.

B.3 Divergence Term 

In this section, we bound the divergence term. We remark that Lemma B.3 and Lemma B.4 are essentially the same, except that they are under different assumptions. Denote z = F −1  

> tk,t −tk

(x). We know Exp z (( t − tk)ˆ v(tk, z )) = x. Recall we denote ˜v(t, x ) = P xF −1  

> tk ,t −tk(x)

ˆv(tk, F −1  

> tk,t −tk

(x)) ,

by simply, ˜v(x) = P Exp z (hˆv(z))  

> z

ˆv(z)for notational simplicity. 

Lemma B.3. Under Assumption 2 and 3, we have 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] ≤ ε + ( t − tk)



Ldiv ,x t (Lˆv + 2 Lv) + Ldiv ,t t + LR(Lˆv)2d



.

23 Proof. By Lemma C.4, we have 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] 

≤Ep(t,x )[|div ˆ v(tk, z ) − div v(t, X t)|] + LR(t − tk)dEp(t,x )[∥ˆv(tk, z )∥2]

≤Ep(t,x )[|div ˆ v(tk, z ) − div v(tk, z )|] + Ep(t,x )[|div v(tk, z ) − div v(tk, X tk )|]+ Ep(t,x )[|div v(tk, X tk ) − div v(t, X t)|] + LR(t − tk)( Lˆv)2d. 

Recall 

d(F −1  

> tk,t −tk

(Xt), X t) = ( t − tk)∥ˆv(t, X t→tk )∥ ≤ (t − tk)Lˆv,d(Xt, X tk ) ≤

Z ttk

∥v(s, X s)∥ds ≤ (t − tk)Lv.

For the first term, we simply have 

Ep(t,x )[|div ˆ v(tk, z ) − div v(tk, z )|] ≤ ε. 

For the second term, we have 

Ep(t,x )[|div v(tk, z ) − div v(tk, X tk )|] ≤ Ldiv ,x t Ep(t,x )[d(z, X tk )] 

≤Ldiv ,x t

 E[d(F −1  

> tk,t −tk

(Xt), X t) + d(Xt, X tk )] 

≤Ldiv ,x t (t − tk)( Lˆv + Lv).

Similarly, for the third term, 

Ep(t,x )[|div v(tk, X tk ) − div v(t, X t)|]

≤Ep(t,x )[|div v(tk, X tk ) − div v(t, X tk )|] + Ep(t,x )[|div v(t, X tk ) − div v(t, X t)|]

≤Ldiv ,t t (t − tk) + Ldiv ,x t d(Xt, X tk ) ≤ (t − tk)( Ldiv ,t t + Ldiv ,x t Lv).

Putting the above estimates together, we obtain 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] 

≤Ep(t,x )[|div ˆ v(tk, z ) − div v(tk, z )|] + Ep(t,x )[|div v(tk, z ) − div v(tk, X tk )|]+ Ep(t,x )[|div v(tk, X tk ) − div v(t, X t)|] + LR(t − tk)( Lˆv)2d

≤ε + Ldiv ,x t (t − tk)( Lˆv + Lv) + ( t − tk)( Ldiv ,t t + Ldiv ,x t Lv) + LR(t − tk)( Lˆv)2d

≤ε + ( t − tk)



Ldiv ,x t (Lˆv + 2 Lv) + Ldiv ,t t + LR(Lˆv)2d



.

Lemma B.4. Under Assumption 4, 5 and 6, we have 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] 

≤3ε + ( t − tk)



Ldiv ˆ v,x t (Lˆvt + Lvt ) + Ldiv ,x t Lvt + Ldiv ,t t + LR(Lˆvt )2d



.

24 Proof. By Lemma C.4, we have 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] 

≤Ep(t,x )[|div ˆ v(tk, z ) − div v(t, X t)|] + LR(t − tk)dEp(t,x )[∥ˆv(tk, z )∥2]

≤Ep(t,x )[|div ˆ v(tk, z ) − div v(tk, z )|] + Ep(t,x )[|div v(tk, z ) − div v(tk, X tk )|]+ Ep(t,x )[|div v(tk, X tk ) − div v(t, X t)|] + LR(t − tk)( Lˆv)2d. 

Recall 

E[d(F −1  

> tk,t −tk

(Xt), X t)2] ≤(t − tk)2(Lˆvt )2,

E[d(Xt, X tk )2] ≤(t − tk)2(Lvt )2.

For the first term, we simply have 

Ep(t,x )[|div ˆ v(tk, z ) − div v(tk, z )|] ≤ ε. 

For the second term, by triangle inequality, we have 

div v(tk, z ) − div v(tk, X tk ) ≤ div v(tk, z ) − div ˆ v(tk, z )

+ div ˆ v(tk, z ) − div ˆ v(tk, X tk )

+ div ˆ v(tk, X tk ) − div v(tk, X tk ) .

Taking expectation under p(t, x ) and using Assumption 6, 

Ep(t,x )

 div v(tk, z ) − div v(tk, X tk )  ≤ 2ε + Ep(t,x )

 div ˆ v(tk, z ) − div ˆ v(tk, X tk ) .

Moreover, by the pointwise spatial regularity of ˆv (equivalently, a pointwise bound on ∥ grad x div ˆ v(tk, ·)∥), the function div ˆ v(tk, ·) is Lipschitz: 

div ˆ v(tk, z ) − div ˆ v(tk, X tk ) ≤ Ldiv ˆ v,x t d(z, X tk ).

Finally, using d(z, X tk ) ≤ d(z, X t) + d(Xt, X tk ) and the bounds 

E[d(z, X t)] ≤ (t − tk)Lˆvt , E[d(Xt, X tk )] ≤ (t − tk)Lvt ,

we obtain 

Ep(t,x )

 div v(tk, z ) − div v(tk, X tk )  ≤ 2ε + ( t − tk)Ldiv ˆ v,x t (Lˆvt + Lvt ).

For the third term, by the fundamental theorem of calculus along the curve, we have div v(t, X t) − div v(tk, X tk ) = 

Z ttk



∂s div v(s, X s) + ⟨grad x div v(s, X s), ˙Xs⟩



ds. 

Equivalently, 

div v(t, X t) − div v(tk, X tk ) ≤

Z ttk

|∂s(div v)( s, X s) + ⟨grad x div v(s, X s), v (s, X s)⟩| ds. 

Hence we obtain 

Ep(t,x )

 div v(t, X t) − div v(tk, X tk ) 

25 ≤Ep(t,x )

h Z ttk

|∂s(div v)( s, X s) + ⟨grad x div v(s, X s), v (s, X s)⟩| ds 

i

≤

Z ttk

Ep(t,x )

h

|∂s(div v)( s, X s)| + |⟨ grad x div v(s, X s), v (s, X s)⟩| 

i

ds 

≲(t − tk)( Ldiv ,t t + Ldiv ,x t Lvt ).

Putting together, the above estimates, we obtain 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] 

≤Ep(t,x )[|div ˆ v(tk, z ) − div v(tk, z )|] + Ep(t,x )[|div v(tk, z ) − div v(tk, X tk )|]+ Ep(t,x )[|div v(tk, X tk ) − div v(t, X t)|] + LR(t − tk)( Lˆvt )2d

≤3ε + Ldiv ˆ v,x t (t − tk)( Lˆvt + Lvt ) + ( t − tk)( Ldiv ,t t + Ldiv ,x t Lvt ) + LR(t − tk)( Lˆvt )2d

≤3ε + ( t − tk)



Ldiv ˆ v,x t (Lˆvt + Lvt ) + Ldiv ,x t Lvt + Ldiv ,t t + LR(Lˆvt )2d



.

B.4 Results on Riemannian manifolds 

Note that in Lemma 5.1, we do not require vector fields v, ˜v to be the flow matching vector field. To apply the Lemma for analyzing the discretization scheme, we will set v to be the true vector field for flow matching, and ˜ v to be corresponds to the learned vector field. But however, there is a discrepency between continuous time ODE dY t = ˜v(t, Y t)dt and the Euler discretization scheme. Hence, to apply the Lemma to discretization (the actual method in Algorithm 1), we need to define a continuous time interpolation. Define F as Ft,h (x) := Exp x(hˆv(t, x )). Note that 

Ftk ,t −tk (xk) = Exp xk (( t − tk)ˆ v(tk, x k)) corresponds to the continuous time interpolation of Euler discretization. Then we are able to define a interpolation vector field: we want to define 

dY t = P Yt

> Ytk

ˆv(tk, Y tk )dt ?

:= ˜ v(t, Y t)dt. 

Here we use the question mark to emphasize that ˜v has not yet been proved to be well defined. We want to write the right hand side as a function of ( t, Y t) = ( t, F tk ,t −tk (Ytk )). 

Lemma (Restated Lemma 5.2) . Let M be simply connected Riemannian manifold that satisfies Assumption 1. Let b be any vector field on M , satisfying ∥b(x)∥ ≤ B, ∀x ∈ M . Assume ∥∇ vb(x)∥ ≤ 

L∇∥v∥. Let R = inj ( M ). To guarantee Ftk ,t −tk being invertible: 1. If Kmin > 0, we require 

h < min { RB , 14L∇

,

s 34∥b(x)∥2LR(2 + 2 L∇ max { 1√Kmin 

, 1}) }.

2. If Kmin < 0, we require 

h < min { RB , 14L∇

,

vuut 34∥b(x)∥2LR(2 sinh( √−Kmin )

> √−Kmin

+ 4 cosh( √−Kmin )−1 

> −Kmin

L∇)

}.

26 3. If Kmin = 0 , we require 

h < min { RB , 14L∇

,

s

34∥b(x)∥2LR(2 + hL ∇) }.

Proof. [Proof of Lemma 5.2] For ease of notation, we use h to denote the time step and b to denote the learned vector field, for the map F . That is, for h ∈ R and b ∈ X(M ), we write 

Fh : M → M, Fh(x) := Exp x

 hb (x).

We first compute the derivative of F . Let x ∈ M and v ∈ TxM . Note that dF h(x) can be viewed as an operator that maps v ∈ TxM to dF h(x)v ∈ TFh(x)M . We compute dF h(x)v. Let c(s) be a smooth curve s.t. c(0) = x and c′(0) = v.Define a variation through geodesics as Λ : ( −ϵ, ϵ ) × [0 , 1] → M, Λ( s, t ) := Exp c(s)

 thb (c(s)) .

For each fixed s, the t direction is the geodesic starting at c(s) with initial velocity hb (c(s)). In particular, for every s we have 

Fh(b(c(s))) = Exp c(s)(hb (c(s))) = Λ( s, 1) .

Now define a vector field Jv along the central geodesic γ(t) := Λ(0 , t ) = Exp x

 thb (x) by 

Jv(t) := ∂sΛ(0 , t ).

Then, by construction (view the left hand side as directional derivative along the curve induced by 

v), 

dF h(x)( v) = dds Fh(b(c(s))) s=0 = ∂sΛ(0 , 1) = Jv(1) .

This expresses the differential of Fh at x applied to v as the value at t = 1 of the variation field 

Jv along the geodesic γ. Note that Jv is a Jacobi field, hence satisfies the Jacobi equation, see Lee (2018, Theorem 10.1). We have initial conditions Jv(0) = v, and DtJv(0) = Dt∂sΛ(0 , 0) = 

Ds∂tΛ(0 , 0) = h∇vb(x) =: ω.Now we analyze conditions that guarantee the invertibility of F . Define 

Y (t) = P γ(0)  

> γ(t)

Jv(t).

To show that dF h(x) is invertible, it suffices to show inf v ∥dF h(x)v∥∥v∥ > 0, equivalently, 

∥dF h(x)v∥ = ∥P γ(0)  

> γ(1)

Jv(1) ∥ = ∥Y (1) ∥ ≥ C∥v∥ > 0, ∀v̸ = 0 for some constant C > 0. Applying Lemma C.5 with c = γ and Y = Jv, we obtain 

Y ′(t) = ddt 



P γ(0)  

> γ(t)

Jv(t)



= P γ(0)  

> γ(t)

DtJv(t).

27 Differentiating once more and using the same lemma with Y = DtJv, we get 

Y ′′ (t) = ddt 



P γ(0)  

> γ(t)

DtJv(t)



= P γ(0)  

> γ(t)

D2 

> t

Jv(t).

Apply the Jacobi equation D2 

> t

Jv(t) + R(Jv(t), γ ′(t)) γ′(t) = 0 we obtain 

Y ′′ (t) = P γ(0)  

> γ(t)

D2 

> t

Jv(t) = −P γ(0) 

> γ(t)

 R(Jv(t), γ ′(t)) γ′(t) .

Using Jv(t) = P γ(t) 

> γ(0)

Y (t), we rewrite the curvature term and obtain 

Y ′′ (t) + P γ(0) 

> γ(t)



R P γ(t) 

> γ(0)

Y (t), γ ′(t)γ′(t)



= 0 .

Now we apply Taylor’s theorem with integral remainder entry-wisely to Y (t). 

Y (t) = Y (0) + tY ′(0) + 

Z t

> 0

(t − s)Y ′′ (s)ds 

= v + th ∇vb(x) −

Z t

> 0

(t − s)P γ(0) 

> γ(s)



R P γ(s) 

> γ(0)

Y (s), γ ′(s)γ′(s)



ds. 

In particular, for t = 1, 

Y (1) − v − h∇vb(x) = −

Z 10

(1 − s)P γ(0) 

> γ(s)



R P γ(s) 

> γ(0)

Y (s), γ ′(s)γ′(s)



ds. 

Since we assumed ∥R(u, v )w∥ ≤ LR∥u∥∥ v∥∥ w∥,

∥R P γ(t) 

> γ(0)

Y (t), γ ′(t)γ′(t)∥ ≤ LR∥γ′(t)∥2∥Y (t)∥.

Taking norms, we get 

∥Y (1) − v − h∇vb(x)∥ ≤ 

Z 10

(1 − s)LR∥γ′(s)∥2∥Y (s)∥ds 

≤ h2∥b(x)∥2LR

Z 10

∥Y (s)∥ds. 

where note ∥γ′(s)∥ = h∥b(x)∥. We remark that it suffices to upper bound R 10 ∥Y (s)∥ds . Then we can simply apply triangle inequality as ∥Y (1) ∥ ≥ ∥ v + h∇vb(x)∥ − ∥ Y (1) − v − h∇vb(x)∥.Now we bound R 10 ∥Y (s)∥ds through comparison theory. Since the computation that involves comparison theory is complicated, we summarize them into a separate Lemma, see Lemma C.2. By Lemma C.2, when Kmin > 0, we have 

Z 10

∥Y (t)∥dt ≤∥ v∥∥ + ∥ω∥∥ + ∥ω⊥∥√Kmin 

+ ∥v⊥∥ ≤ 2∥v∥ + 2 h∥∇ vb(x)∥ max { 1

√Kmin 

, 1}≤∥ v∥(2 + 2 hL ∇ max { 1

√Kmin 

, 1}) =: C+,

where note that we did orthogonal decomposition on v, ω .

∥Y (1) − v − h ∇vb(x)∥ ≤ h2∥b(x)∥2LRC+.

28 By triangle inequality, 

∥Y (1) ∥ ≥∥ v + h∇vb(x)∥ − ∥ Y (1) − v − h∇vb(x)∥≥∥ v∥ − h∥∇ vb(x)∥ − h2∥b(x)∥2LRC+

≥∥ v∥



1 − hL ∇ − h2∥b(x)∥2LR(2 + 2 hL ∇ max { 1

√Kmin 

, 1})



.

Clearly h < 1 is required, so we only need to find h s.t. 1 − hL ∇ > h 2∥b(x)∥2LR(2 + 2 L∇ max { 1

√Kmin 

, 1})

> h 2∥b(x)∥2LR(2 + 2 hL ∇ max { 1

√Kmin 

, 1}).

If h ≤ 14L∇ , then 1 − hL ∇ ≥ 34 . The value h ≤

r 34∥b(x)∥2LR(2+2 L∇ max { 1√Kmin  

> ,1})

suffices. When Kmin < 0, we have 

Z 10

∥Y (t)∥dt ≤∥ v∥∥ + ∥ω∥∥ + cosh( √−Kmin ) − 1

−Kmin 

∥ω⊥∥ + sinh( √−Kmin )

√−Kmin 

∥v⊥∥≤∥ v∥



2 sinh( √−Kmin )

√−Kmin 

+ 4 cosh( √−Kmin ) − 1

−Kmin 

hL ∇



=: C−.

Then we have 

∥Y (1) − v − h ∇vb(x)∥ ≤ h2∥b(x)∥2LRC−.

By triangle inequality, 

∥Y (1) ∥ ≥ ∥ v + h∇vb(x)∥ − ∥ Y (1) − v − h∇vb(x)∥≥ ∥ v∥ − h∥∇ vb(x)∥ − h2∥b(x)∥2LRC−

≥ ∥ v∥



1 − hL ∇ − h2∥b(x)∥2LR(2 sinh( √−Kmin )

√−Kmin 

+ 4 cosh( √−Kmin ) − 1

−Kmin 

hL ∇)



.

Cleraly h < 1 is required, so we only need to find h s.t. 1 − hL ∇ > h 2∥b(x)∥2LR(2 sinh( √−Kmin )

√−Kmin 

+ 4 cosh( √−Kmin ) − 1

−Kmin 

L∇)

> h 2∥b(x)∥2LR(2 sinh( √−Kmin )

√−Kmin 

+ 4 cosh( √−Kmin ) − 1

−Kmin 

hL ∇).

If h ≤ 14L∇ , then 1 − hL ∇ ≥ 34 . The value h ≤ r 34∥b(x)∥2LR(2 sinh( √−Kmin )  

> √−Kmin
> +4 cosh( √−Kmin )−1
> −Kmin L∇)

suffices. When Kmin = 0, we have 

Z 10

∥Y (t)∥dt ≤∥ v∥∥ + 12 ∥ω∥∥ + 12 ∥ω⊥∥ + ∥v⊥∥≤∥ v∥ (2 + hL ∇) =: C=.

29 Then we have 

∥Y (1) − v − h ∇vb(x)∥ ≤ h2∥b(x)∥2LRC=.

By triangle inequality, 

∥Y (1) ∥ ≥ ∥ v + h∇vb(x)∥ − ∥ Y (1) − v − h∇vb(x)∥≥ ∥ v∥ − h∥∇ vb(x)∥ − h2∥b(x)∥2LRC=

≥ ∥ v∥  1 − hL ∇ − h2∥b(x)∥2LR(2 + hL ∇) .

Cleraly h < 1 is required, so we only need to find h s.t. 1 − hL ∇ > h 2∥b(x)∥2LR(2 + L∇) > h 2∥b(x)∥2LR(2 + hL ∇).

If h ≤ 14L∇ , then 1 − hL ∇ ≥ 34 . The value h ≤

q 34∥b(x)∥2LR(2+ hL ∇) suffices. 

B.5 Main Theorem 

Now we prove our main theorem. We remark that the proof of Theorem 1 and 2 are essentially the same. 

Proof. [Proof of Theorem 1] By Lemma 5.1, we have TV ( πT , ˆπT ) = TV ( πtN , ˆπtN )

≤ TV ( πt0 , ˆπt0 ) + 

Z tN

> t0

E[⟨grad log p(t, x ), ˜v(x, t ) − v(x, t )⟩]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt 

≤ TV ( πt0 , ˆπt0 ) + 

Z tN

> t0

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt, 

where πt, ˆπt denote the law for Xt, Y t respectively. Notice that 

Z tk+1 

> tk

(t − tk)dt = ( 12 t2 − tt k)|tk+1  

> tk

= 12 t2 

> k+1

− 12 t2 

> k

− tk(tk+1 − tk) = 12 (tk+1 − tk)2. (11) Hence using constant step size for discretization, we obtain TV ( πT , ˆπT ) = TV ( XtN , Y tN )

≤ TV ( Xt0 , Y t0 ) + 

Z tN

> t0

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt 

≤ TV ( Xt0 , Y t0 ) + 12 N h 2p 2Lscore  

> t

(Lˆv,x t Lˆv + LvLˆv,x t + Lv,t t + LvLv,x t )+ Ldiv ,x t (Lˆv + 2 Lv) + Ldiv ,t t + LR(Lˆv)2d



+ N h (εp2Lscore  

> t

+ ε)

≤ TV ( Xt0 , Y t0 ) + h

p Lscore  

> t

(Lˆv,x t Lˆv + LvLˆv,x t + Lv,t t + LvLv,x t )+ Ldiv ,x t (Lˆv + 2 Lv) + Ldiv ,t t + LR(Lˆv)2d



+ ( εp2Lscore  

> t

+ ε).

= TV ( Xt0 , Y t0 ) + hCLip + εCeps ,

30 where note that N h < 1 and we compute 

pLscore  

> t

(( Lv,x t + ε)( Lv + ε) + Lv(Lv,x t + ε) + Lv,t t + LvLv,x t )+ Ldiv ,x t (3 Lv + ε) + Ldiv ,t t + LR(Lv + ε)2d

=ε2(pLscore  

> t

+ LRd) + ε(2 pLscore  

> t

Lv + pLscore  

> t

Lv,x t + Ldiv ,x t + 2 LRdL v)+ 3 pLscore  

> t

Lv,x t Lv + pLscore  

> t

Lv,t t + 3 LvLdiv ,x t + Ldiv ,t t + LR(Lv)2d, 

and denote 

CLip := 3 pLscore  

> t

Lv,x t Lv + pLscore  

> t

Lv,t t + 3 LvLdiv ,x t + Ldiv ,t t + LR(Lv)2d, 

Ceps := p2Lscore  

> t

+ 1 + 2 pLscore  

> t

Lv + pLscore  

> t

Lv,x t + Ldiv ,x t + 2 LRdL v + ε(pLscore  

> t

+ LRd).

Proof. [Proof of Theorem 2] We have that 

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]

≤εp3Lscore  

> t

+ ( t − tk)



6Lscore 

> t



(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2 12

,

and 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] ≤ 3ε + ( t − tk)



Ldiv ˆ v,x t (Lˆvt + Lvt ) + Ldiv ,x t Lvt + Ldiv ,t t + LR(Lˆvt )2d



.

Also recall TV ( πT , ˆπT ) = TV ( πtN , ˆπtN )

≤ TV ( πt0 , ˆπt0 ) + 

Z tN

> t0

E[⟨grad log p(t, x ), ˜v(x, t ) − v(x, t )⟩]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt 

≤ TV ( πt0 , ˆπt0 ) + 

Z tN

> t0

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt. 

Notice that 

Z tk+1 

> tk

(t − tk)dt = ( 12 t2 − tt k)|tk+1  

> tk

= 12 t2 

> k+1

− 12 t2 

> k

− tk(tk+1 − tk) = 12 (tk+1 − tk)2. (12) Hence using constant step size for discretization, we obtain TV ( πT , ˆπT )

≤ TV ( Xt0 , Y t0 ) + 

Z tN

> t0

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt 

≤ TV ( Xt0 , Y t0 ) + 12 N h 2p 6Lscore 

> t



(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2

+



Ldiv ˆ v,x t (Lˆvt + Lvt ) + Ldiv ,x t Lvt + Ldiv ,t t + LR(Lˆvt )2d

 

+ N h (εp2Lscore  

> t

+ 3 ε) (13) 

≤ TV ( Xt0 , Y t0 ) + hCLip + εCeps, 1 ,

where note that N h < 1. Take t0 = 0, tN = T < 1, we obtain TV ( πT , ˆπT ) ≤ hCLip + εCeps, 1 .

31 B.6 Example: Hypersphere 

For compact manifolds, under uniform estimation error (Assumption 3, we can establish the regularity for ˆv. In particular, under Assumption 3 and Assumption 2, since ∥∇ ˆv(x)∥op = ∥∇ ˆv(x) −∇v(x) + ∇v(x)∥op , we have 1. ∥ˆv(t, x )∥ ≤ Lˆv = Lv + ε.2. ˆ v(t, x ) is Lipschitz in x variable with Lˆv,x t = Lv,x t + ε.

Lemma B.5. On Sd, Assumption 2 holds with the following constants. 1. Lv,x t = 12 πM 1(d−1)  

> m1(1 −t)

, Lˆv,x t = ε + 12 πM 1(d−1)  

> m1(1 −t)

.2. Lv,t t := 8π2d 

> 1−tM1
> m1

.3. Ldiv ,x t := 128 π(d−1) 2 

> (1 −t)3
> M1
> m1

.4. Ldiv ,t t := 128 π2(d−1) 2 

> (1 −t)3
> M1
> m1

.5. Lscore  

> t

:= 8( d−1) 2 

> (1 −t)2
> M1
> m1

.6. Lv = π.7. LR = 1 .

As the proof is technical, we defer it to Appendix D. Now we prove Proposition 3.1. Proof. [Proof of Proposition 3.1] We first recall the extra condition on h imposed by Lemma 5.2: 

h < min { R

∥b(x)∥ , 14L∇

,

s 34∥b(x)∥2LR(2 + 2 L∇ max { 1√Kmin 

, 1}) }.

Since ∥ˆv(t, x )∥ ≤ Lˆv = Lv + ε, we know ∥b(x)∥ is of constant order, consequently R 

> ∥b(x)∥

is of constant order. Thus such an condition is dominated by the term 1 

> L∇

, which in our case is exactly 1

> Lˆv,x t

. Plug in constants in Lemma B.5, we obtain 

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]

≤ε

s

2 8( d − 1) 2

(1 − t)2

M1

m1

+ ( t − tk)

 8( d − 1) 2

(1 − t)2

M1

m1

(2(( ε + 12 πM 1(d − 1) 

m1(1 − t) )( π + ε)+ π(ε + 12 πM 1(d − 1) 

m1(1 − t) ) + 8π2d

1 − tM1

m1

+ π 12 πM 1(d − 1) 

m1(1 − t) )2)

 12

≤4ε d − 11 − t

r M1

m1

+ ( t − tk)4 d − 11 − t

r M1

m1

 (ε + 12 πM 1(d − 1) 

m1(1 − t) )( π + ε)+ π(ε + 12 πM 1(d − 1) 

m1(1 − t) ) + 8π2d

1 − tM1

m1

+ π 12 πM 1(d − 1) 

m1(1 − t)



≲ε d − 11 − t

 M1

m1

 12

+ ( t − tk) (d − 1) 2

(1 − t)2

 M1

m1

 32

.

32 And similarly, 

Ep(t,x )[div (˜ v(x, t ) − v(t, X t))] ≤ ε + ( t − tk)



Ldiv ,x t (Lˆv + 2 Lv) + Ldiv ,t t + LR(Lˆv)2d



≤ε + ( t − tk)

 128 π(d − 1) 2

(1 − t)3

M1

m1

(3 π + ε) + 128 π2(d − 1) 2

(1 − t)3

M1

m1

+ ( π + ε)2d



≲ε + ( t − tk)

 (d − 1) 2

(1 − t)3

M1

m1

+ d



.

With early stopping, we terminate the sampling algorithm (Algorithm 1) at time T < 1 (i.e., 

t0 = 0 , t N = T )TV ( πT , ˆπT ) = TV ( XtN , Y tN )

≤ TV ( Xt0 , Y t0 ) + 

Z tN

> t0

E[∥ grad log p(t, x )∥ · ∥ ˜v(x, t ) − v(x, t )∥]dt +

Z tN

> t0

E[| div (˜ v(x, t ) − v(x, t )) |]dt 

≲ 

> N−1

X

> i=0

Z ti+1 

> ti

ε d − 11 − t

 M1

m1

 12

+ ( t − ti) (d − 1) 2

(1 − t)2

 M1

m1

 32

dt + 

> N−1

X

> i=0

Z ti+1 

> ti

ε + ( t − ti)

 (d − 1) 2

(1 − t)3

M1

m1

+ d



dt 

≲

 M1

m1

 12

(d − 1) ε 

> N−1

X

> i=0

Z ti+1 

> ti

11 − t dt +

 M1

m1

 32

(d − 1) 2 

> N−1

X

> i=0

Z ti+1 

> ti

t − ti

(1 − t)3 dt. 

We first discuss the case of constant step size. Using  

> N−1

X

> i=0

Z ti+1 

> ti

11 − t dt = − log(1 − tN )and (using s := 1 − t so that ds = −dt ) 

> N−1

X

> i=0

Z ti+1 

> ti

t − ti

(1 − t)3 dt ≤ h 

> N−1

X

> i=0

Z ti+1 

> ti

1(1 − t)3 dt = h 

> N−1

X

> i=0

Z 1−ti

> 1−ti+1

1

s3

=h 

> N−1

X

> i=0

− 1(1 − ti)2 + 1(1 − ti+1 )2 ≲ h 1(1 − T )2 ,

we can finally bound the error as 

ε

 M1

m1

 12

(d − 1) log( 11 − T ) + h

 M1

m1

 32

(d − 1) 2 1(1 − T )2 .

Now we have TV ( XT , Y T ) ≲ ε

 M1

m1

 12

(d − 1) log( 11 − T ) + h

 M1

m1

 32

(d − 1) 2 1(1 − T )2 .

To obtain a sample up to εtarget accuracy, we assume ε is sufficiently small. Then we need 

h

 M1

m1

 32

(d − 1) 2 1(1 − T )2 = O(εtarget ),

which means 

h = εtarget (1 − T )2

(d − 1) 2

 m1

M1

 32

. (14) 33 Next, we discuss a specific step size schedule that can improve the dependency on 11−T . Denote 

hk = tk+1 − tk. Then 

Z ti+1 

> ti

1 − ti

(1 − t)3 dt = 12 (1 − ti)( 1(1 − ti+1 )2 − 1(1 − ti)2 ) = 12 (1 − ti)( t2 

> i

− 2ti − t2 

> i+1

+ 2 ti+1 

(1 − ti+1 )2(1 − ti)2 )= 12 (1 − ti) (ti+1 − ti)(2 − ti − ti+1 )(1 − ti+1 )2(1 − ti)2 = 12 (1 − ti)hi

2 − ti − ti+1 

(1 − ti+1 )2(1 − ti)2 ,

Z ti+1 

> ti

t − 1(1 − t)3 dt = −

Z ti+1 

> ti

1(1 − t)2 dt = 11 − ti

− 11 − ti+1 

= − hi

(1 − ti+1 )(1 − ti) .

Together, 

Z ti+1 

> ti

t − ti

(1 − t)3 dt = 12 (1 − ti)hi

2 − ti − ti+1 

(1 − ti+1 )2(1 − ti)2 − hi

(1 − ti+1 )(1 − ti)= hi

(1 − ti+1 )(1 − ti) (−1 + 122 − ti − ti+1 

(1 − ti+1 ) )= h2

> i

2(1 − ti+1 )2(1 − ti) ≲ h2

> i

(1 − ti+1 )3 .

Now we want to control  

> N−1

X

> i=0

Z ti+1 

> ti

t − ti

(1 − t)3 dt ≲ 

> N−1

X

> i=0

h2

> i

(1 − ti+1 )3 .

With ti = 1 − 1(1+ ηi )2 . Then hi = 1(1+ ηi )2 − 1(1+ η(i+1)) 2 = η(2+ η(2 i+1)) (1+ ηi )2(1+ η(i+1)) 2 . 

> N−1

X

> i=0

h2

> i

(1 − ti+1 )3 = 

> N−1

X

> i=0

η2(2 + η(2 i + 1)) 2

(1 + ηi )4(1 + η(i + 1)) 4 (1 + η(i + 1)) 6

= η2 

> N−1

X

> i=0

(2 + η(2 i + 1)) 2

(1 + ηi )4 (1 + η(i + 1)) 2

≲ 4N η 2.

Note that by construction of early stopping, it must hold that 1 − 1(1+ ηN )2 = T , which implies   

> 1√1−T−1
> η

= N . Hence  

> N−1

X

> i=0

h2

> i

(1 − ti)3 ≲ 4N η 2 ≲ η 1

√1 − T .

We want to reach 

 M1

m1

 32

(d − 1) 2η 1

√1 − T = O(εtarget ).

So we need 

η ≲

 εtarget 

√1 − T

(d − 1) 2



.

34 and consequently 

N = 

> 1√1−T

− 1

η = O( 

> 1√1−T

− 1

> εtarget
> √1−T
> (d−1) 2

) = O( d2

(1 − T )εtarget 

).

We remark that the step size schedule can be described by the discretized time points as follows: 

ti = 1 − 1(1 + ηi )2 , where η = O

 εtarget 

√1 − T

(d − 1) 2



. (15) 

B.7 Example: SPD Manifold 

We first state the following result proved later in Appendix E. 

Proposition B.6. Let M be SPD (n). Assume Asumption 1. We impose the following moment condition: there exists Mλ1 be such that 

max E[d(X1, z )2eλ1d(X1,z )], E[eλ1d(X1,z )] ≤ Mλ1 , where λ1 = 24 max {1, κ }.

We choose the prior distribution to be a Riemannian Gaussian distribution centered at some z ∈ M :

p0(x) ∝ exp   − d d (x, z )2. We then have the following regularity results. 

E[∥v(t, x )∥2] ≲d, 

E[∥∇ v(t, x )∥] ≲ d2+6 λ

1 − t LRM 12 

> λ1

, E[∥∇ v(t, x )∥2] ≲ d3+12 λ

(1 − t)2 L2

> R

M 12 

> λ1

,

E[| ddt v(t, x )|] ≲ d2+6 λ

1 − t LRM 12 

> λ1

, E[| ddt v(t, x )|2] ≲ d3+12 λ

(1 − t)2 L2

> R

M 12 

> λ1

,

E[∥ grad x div v(t, x )∥] ≲ d3+12 λ

(1 − t)2 L3

> R

, E[∥ grad x div v(t, x )∥2] ≲ d5+24 λ

(1 − t)4 L6

> R

Mλ1 ,

E[| ddt div v(t, x )|] ≲ d3+12 λ

(1 − t)2 L3

> R

M 12 

> λ1

,

where λ = max {1, κ }.

Proof. [Proof of Proposition 4.1] Using Proposition B.6, and following Theorem 2, we have TV ( πT , ˆπT ) = TV ( XtN , Y tN )

≤ TV ( Xt0 , Y t0 ) + 12 N h 2p 6Lscore 

> t



(Lˆv,x t )2(Lˆvt )2 + ( Lˆv,x t )2(Lvt )2 + ( Lv,t t )2 + ( Lvt )2(Lv,x t )2

+



Ldiv ˆ v,x t (Lˆvt + Lvt ) + Ldiv ,x t Lvt + Ldiv ,t t + LR(Lˆvt )2d

 

+ N h (εp2Lscore  

> t

+ ε)

≲N h 2 d5+12 λ

(1 − t)2 L2

> R

Mλ1

s

M d2

(1 − t)2 L2

> R

d12 λ



+ N h (ε

s

M d2

(1 − t)2 L2

> R

d12 λ + ε)

≲h

 d6+18 λ

(1 − t)3 L3

> R

M 32

> λ1



+ ( d6λ+1 

1 − t LR)M 12 

> λ1

ε. 

35 Thus to reach εtarget accuracy, we need 

h = O

 εtarget (1 − T )3

d6+18 λL3

> R

M 32

> λ1

 .

Consequently, the iteration complexity would be 

N = O

 d6+18 λL3

> R

M 32

> λ1

εtarget (1 − T )3

 .

We remark that since Kmin = − 12 (Criscitiello and Boumal, 2023), so λ = max {1, κ } = 1. In the meanwhile, the upper bound on h required by Lemma 5.2 can be reduced to be of order min { 1

L∇

,

s

1

∥b(x)∥2LRL∇

} = 1 − Td8LRM 12

> λ1

,

which is lower than our required order of h.Therefore, we conclude the iteration complexity as 

N = O

 d6+18 λL3

> R

M 32

> λ1

εtarget (1 − T )3

 = O

 d24 L3

> R

M 32

> λ1

εtarget (1 − T )3

 .

# C Auxiliary Results for Proof of Main Theorems 

C.1 Jacobi Equation 

Lemma C.1 (Solution for Jacobi equation) . Let M be of constant sectional curvature c. Let J be a normal Jacobi field along γ, with initial condition J(0) = v⊥, J ′(0) = 0 . Then we have 

J(t) = s(2)  

> c

(t)∥v⊥∥E(t) := 



∥v⊥∥E(t), if c = 0 ,

∥v⊥∥ cos( √ct )E(t), if c > 0,

∥v⊥∥ cosh( √−ct )E(t), if c < 0.

where E is a parallel normal unit vector field with E(0) = v⊥ 

> ∥v⊥∥

.

Proof. [Proof of Lemma C.1] Similar to the proof of Lee (2018, Proposition 10.12), the solution is of the form J(t) = f (t)E(t) where E(t) is a parallel unit normal vector field along γ. Since the curvature is constant and J is a normal Jacobi field, the Jacobi equation reduces to D2 

> t

J + cJ = 0, thus we only need to solve for f ′′ (t) + cf (t) = 0, where f (t) ∈ R, ∀t.

• When c = 0, we obtain f ′′ (t) = 0, hence (to satisfy initial condition) f (t) = ∥v⊥∥.

• When c > 0, we obtain f (t) = ∥v⊥∥ cos( √ct ). 

• When c < 0, we obtain f (t) = ∥v⊥∥ e√−ct +e−√−ct  

> 2

= ∥v⊥∥ cosh( √ct ). 36 Note that E(0) = v⊥ 

> ∥v⊥∥

. Therefore, 

J(t) = 



∥v⊥∥E(t), if c = 0 ,

∥v⊥∥ cos( √ct )E(t), if c > 0,

∥v⊥∥ cosh( √−ct )E(t), if c < 0.

Lemma C.2. Let Jv(t) be a Jacobi field along γ(t) = Exp x(thb (x)) , with Jv(0) = v, J′

> v

(0) = ω.Define Y (t) = P γ(0)  

> γ(t)

Jv(t). Up to the first conjugate point of x, we have 

∥Y (t)∥ ≤ ∥ v∥∥ + ∥ω∥∥t + sKmin (t)∥ω⊥∥ + sKmin (t)

sKmin (t0) ∥v⊥∥.

Take h < R 

> ∥b(x)∥

, we have 

Z 10

∥Y (t)∥dt ≤



∥v∥∥ + ∥ω∥∥ + ∥ω⊥∥√Kmin 

+ ∥v⊥∥, if Kmin > 0,

∥v∥∥ + ∥ω∥∥ + cosh( √−Kmin )−1 

> −Kmin

∥ω⊥∥ + sinh( √−Kmin )

> √−Kmin

∥v⊥∥, if Kmin < 0,

∥v∥∥ + 12 ∥ω∥∥ + 12 ∥ω⊥∥ + ∥v⊥∥, if Kmin = 0 .

Proof. [Proof of Lemma C.2] Using isometric property of parallel transport, ∥Y (t)∥ = ∥Jv(t)∥.Decompose v = v∥ + v⊥, where v∥ is the component in γ′(0) direction, and ⟨v⊥, γ ′(0) ⟩ = 0. Similarly, we decompose ω = ω∥ + ω⊥.Define Jv =: J(0) + J(1) + J(2) , where 

J(0) (0) = v∥, DtJ(0) (0) = ω∥;

J(1) (0) = 0 , DtJ(1) (0) = ω⊥;

J(2) (0) = v⊥, DtJ(2) (0) = 0 .

Note that both J(1) (0), DtJ(1) (0) are orthogonla to γ′(0), so J(1) (t) is a normal Jacobi field. We know J(0) is a tangential Jacobi field, so it has the form 

J(0) (t) = ( a + bt )γ′(t).

Plug in the initial value, we get (note that Dtγ′(0) = 0) 

aγ ′(0) = v∥, bγ ′(0) = ω∥.

Hence |a| = ∥v∥∥∥γ′(0) ∥ , and |b| = ∥ω∥∥∥γ′(0) ∥ .By definition, ∥γ′(0) ∥ = ∥hb (x)∥ = ∥γ′(t)∥. So we get 

∥J(0) (t)∥ ≤ ∥ v∥∥ + ∥ω∥∥t. 

If all sectional curvatures of M are bounded below by a constant Kmin , Jacobi field comparison theorem yield 

∥J(1) (t)∥ ≤ sK (t)∥DtJ(1) (0) ∥ = sKmin (t)∥ω⊥∥.

37 Now consider the term J2(0). By Cheeger et al. (1975, Theorem 1.34) applied with Jacobi field formula given in Lemma C.1, we obtain ∥J(2) (t)∥ ≤ ∥ ˜J(t)∥. We remark that the focal point free condition is saying J(2) (t)̸ = 0. As long as Cut locus is not reached, the geodesic is minimizing. Hence it suffices to guarantee that the geodesic γ satisfies ∥γ′(0) ∥ < inj ( M ). Finally, recall ∥Y (t)∥ = ∥Jv(t)∥.

∥Y (t)∥ =∥Jv(t)∥ = ∥J(0) (t) + J(1) (t) + J(2) (t)∥≤∥ v∥∥ + ∥ω∥∥t + |sKmin (t)|∥ ω⊥∥ + |s(2)  

> Kmin

(t)|∥ v⊥∥.

We split into cases. The first case is Kmin > 0. In this case we denote R = inj ( M ) and 

Z 10

|sKmin (t)|dt = 1

√Kmin 

Z 10

| sin( tpKmin )|dt ≤ 1

√Kmin 

.

Also, 

Z 10

|s(2)  

> Kmin

(t)|dt =

Z 10

| cos( pKmin t)|dt ≤ 1.

To summarize, when h < R 

> ∥b(x)∥

,

Z 10

∥Y (t)∥dt ≤∥ v∥∥ + ∥ω∥∥ + ∥ω⊥∥√Kmin 

+ ∥v⊥∥.

The second case is Kmin < 0. Then sKmin (t) = 1√−Kmin 

sinh( √−Kmin t). Consider 0 < t ≤ h < 1. 

Z 10

sKmin (t)dt =

Z 10

1

√−Kmin 

sinh tp−Kmin dt = 1

−Kmin 

(cosh( p−Kmin ) − 1) .

For the second integral, 

Z 10

cosh( p−Kmin t)dt = sinh( √−Kmin )

√−Kmin 

.

To summarize, when h < R 

> ∥b(x)∥

,

Z 10

∥Y (t)∥dt ≤∥ v∥∥ + ∥ω∥∥ + cosh( √−Kmin ) − 1

−Kmin 

∥ω⊥∥ + ∥v⊥∥ sinh( √−Kmin )

√−Kmin 

.

The third case is Kmin = 0. In this case sKmin (t) = t.

∥Y (t)∥ =∥Jv(t)∥ = ∥J(0) (t) + J(1) (t) + J(2) (t)∥≤∥ v∥∥ + ∥ω∥∥t + ∥ω⊥∥t + ∥v⊥∥.

Then we have 

Z 10

∥Y (t)∥dt ≤∥ v∥∥ + 12 ∥ω∥∥ + 12 ∥ω⊥∥ + ∥v⊥∥.

38 C.2 Divergence Term 

Lemma C.3. Let c denote the geodesic with c(0) = z, c(h) = x. Define a variation of geodesics 

Λ( s, t ) as Λ( s, t ) = Exp c(t)(sP c(t) 

> c(0)

Ei(t)) where Ei(t) is basis vector field along c. Let V be a vector field. For every z, we can obtain a new vector field by parallel transport, denote as P c(t) 

> c(0)

V (z). Then 

DsP c(h) 

> c(0)

V (z) = P c(h) 

> c(0)

DsV (z) − P c(h)

> c(0)

Z h

> 0

P c(tk )  

> c(τ)

R(∂sΛ(0 , τ ), ∂ tΛ(0 , τ )) P c(τ ) 

> c(tk)

V (z)dτ, 

where Ds denote the covariant derivative along Ei(t).

Note that the s direction is actually arbitrary. The goal is to compute the divergence at a point, so we can enumerate over all possible s direction, in all basis vectors. Each s direction, roughly speaking, defines a Ds.

Proof. [Proof of Lemma C.3] By construction, it holds that Λ(0 , 0) = z, Λ(0 , h ) = x. 

and the parallel transport P xz = P Λ(0 ,h )Λ(0 ,0) is along c. Define W (s, t ) = P Λ( s,t )Λ( s, 0) V (Λ( s, 0)), where we perform parallel transport along curve t 7 → Λ( s, t ) which might not be a geodesic. By definition of parallel transport, DtW (s, t ) = 0 , ∀s.We know 

−DtDsW (s, t ) = DsDtW (s, t ) − DtDsW (s, t ) = R(∂sΛ( s, t ), ∂ tΛ( s, t )) W (s, t ).

Hence evaluating at s = 0, we obtain 

DtDsW (0 , t ) = −R(∂sΛ(0 , t ), ∂ tΛ(0 , t )) W (0 , t ).

We perform parallel transport P c(0)  

> c(t)

on both sides of the equation, and by Lemma C.5 we have 

ddt P c(0)  

> c(t)

DsW (0 , t ) = P c(0)  

> c(t)

DtDsW (0 , t ) = −P c(0)  

> c(t)

R(∂sΛ(0 , t ), ∂ tΛ(0 , t )) W (0 , t ).

Observe that both side of the equation is a time dependent vector field in Tc(0) M = Tz M . Hence we can perform integration 

Z h

> 0

ddτ P c(tk )  

> c(τ)

DsW (0 , τ )dτ = −

Z h

> 0

P c(tk )  

> c(τ)

R(∂sΛ(0 , τ ), ∂ tΛ(0 , τ )) W (0 , τ )dτ. 

Hence 

P c(0)  

> c(h)

DsW (0 , h ) = DsW (0 , 0) −

Z h

> 0

P c(tk )  

> c(τ)

R(∂sΛ(0 , τ ), ∂ tΛ(0 , τ )) W (0 , τ )dτ. 

Note that DsW (0 , 0) = DsV (Λ(0 , 0)). Perform parallel transport P c(h) 

> c(0)

on both sides of the equation, we have 

DsW (0 , h ) = P c(h) 

> c(0)

DsV (Λ(0 , 0)) − P c(h)

> c(0)

Z h

> 0

P c(tk )  

> c(τ)

R(∂sΛ(0 , τ ), ∂ tΛ(0 , τ )) W (0 , τ )dτ. 

39 Recall W (s, t ) = P Λ( s,t )Λ( s, 0) V (Λ( s, 0)). 

DsP c(h) 

> c(0)

V (Λ(0 , 0)) = P c(h) 

> c(0)

DsV (Λ(0 , 0)) − P c(h)

> c(0)

Z h

> 0

P c(tk )  

> c(τ)

R(∂sΛ(0 , τ ), ∂ tΛ(0 , τ )) P c(τ ) 

> c(tk)

V (z)dτ. 

Notice that ˜v is defined through ˆv and the inverse of F . We need to control div ˜ v(x, t ) in our analysis, by writing it as some expression involving div ˆ v.

Lemma C.4. Under Assumption 1, we have 

| div (˜ v(x, t ) − v(x, t )) | ≤ | div ˆ v(tk, z ) − div v(x, t )| + LR(t − tk)d∥ˆv(tk, z )∥2.

where Exp z (( t − tk)ˆ v(tk, z )) = x.

Proof. [Proof of Lemma C.4] We follow the setting in Lemma C.3 with V replaced by ˆv. We use 

{Ei(t)}di=1 to denote an orthonormal basis vector field along geodesic c. Note that a slight difference is the time shift, where in Lemma C.3 we have the curve is from time 0 to h, but here we are from tk

to t (so we have h = t − tk). We denote our time variable as τ . In most cases we mean the variable 

τ is in [ tk, t ]. By definition, Exp z (( t − tk) ˆv(tk, z )) = x. So we know Λ(0 , τ ) = Exp z (( τ − tk) ˆv(tk, z )). By construction, 

∂tΛ(0 , τ ) = P c(τ )  

> c(tk)

ˆv(tk, z ), ∂sΛ(0 , τ ) = Ei(τ ).

By Lemma C.3 with c(τ ) = Exp z (( τ − tk)ˆ v(tk, z )), 

DsP c(t)  

> c(tk)

ˆv(tk, z ) = P c(t) 

> c(tk)

Ds ˆv(tk, z ) − P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ. 

By definition of Riemannian divergence, div ˜ v(t, x ) = 

> d

X

> i=1

⟨∇ Ei(t) ˜v(t, x ), E i(t)⟩ =

> d

X

> i=1

⟨∇ Ei(t)P xF −1  

> tk ,t −tk(x)

ˆv(tk, F −1  

> tk,t −tk

(x)) , E i(t)⟩

=

> d

X

> i=1

⟨D(i) 

> s

P c(t)  

> c(tk)

ˆv(tk, z ), E i(t)⟩

=

> d

X

> i=1

⟨P c(t) 

> c(tk)

D(i) 

> s

ˆv(tk, z ) − P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ, E i(t)⟩

=

> d

X

> i=1

⟨P c(t) 

> c(tk)

∇Ei(tk ) ˆv(tk, z ) − P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ, E i(t)⟩

=

> d

X

> i=1

⟨P c(t) 

> c(tk)

∇Ei(tk ) ˆv(tk, z ), E i(t)⟩−

> d

X

> i=1

⟨P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ, E i(t)⟩

40 = div ˆ v(tk, z ) −

> d

X

> i=1

⟨P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ, E i(t)⟩,

where D(i) 

> s

represent the covariant derivative corresponds to Ei(τ ). We have 

> d

X

> i=1

⟨P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ, E i(t)⟩

=

> d

X

> i=1

Z ttk

⟨P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z ), E i(tk)⟩dτ 

=

> d

X

> i=1

Z ttk

⟨R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z ), E i(τ )⟩dτ 

≤

> d

X

> i=1

Z ttk

LR∥ˆv(tk, z )∥2dτ ≤ (t − tk)dL R∥ˆv(tk, z )∥2.

It follows that 

| div (˜ v(x, t ) − v(x, t )) |

=| div ˆ v(tk, z ) −

> d

X

> i=1

⟨P c(t) 

> c(tk)

Z ttk

P c(tk )  

> c(τ)

R(Ei(τ ), P c(τ )  

> c(tk)

ˆv(tk, z )) P c(τ )  

> c(tk)

ˆv(tk, z )dτ, E i(t)⟩ − div v(x, t )|≤ | div ˆ v(tk, z ) − div v(x, t )| + LR(t − tk)d∥ˆv(tk, z )∥2.

Lemma C.5. We have that 

ddt P c(0)  

> c(t)

Y (t) = P c(0)  

> c(t)

DtY (t).

Proof. [Proof of Lemma C.5] Denote the geodesic along t direction as c(t). Let v ∈ Tc(0) M be arbitrary, and define Z(t) = P c(t) 

> c(0)

v.

ddt ⟨P c(0)  

> c(t)

Y (t), v ⟩ = ddt ⟨Y (t), P c(t) 

> c(0)

v⟩ = ⟨DtY (t), P c(t) 

> c(0)

v⟩ + 0 = ⟨DtY (t), P c(t) 

> c(0)

v⟩ = ⟨P c(0)  

> c(t)

DtY (t), v ⟩.

On the other hand, 

ddt ⟨P c(0)  

> c(t)

Y (t), v ⟩ = ⟨ ddt P c(0)  

> c(t)

Y (t), v ⟩.

Since the above holds for any v, we have 

ddt P c(0)  

> c(t)

Y (t) = P c(0)  

> c(t)

DtY (t).

41 D Hypersphere Regularity Results 

Recall that 

v(t, x ) = 11 − t

Z

> M

Log x(x1) pt(x1|x) dV g(x1).

To establish regularity of v, it is natural to study the formula for conditional density, pt(x1|x). In a Euclidean space, with Gaussian distribution as prior p0, we have that pt(x1 | xt = x) ∝

p1(x1) exp (− ∥tx 1−x∥2 

> 2(1 −t)2

). This is a standard result for flow matching, see for example Guan et al. (2026) and Zhou and Liu (2025). But on a Riemannian manifold, the curvature would introduce an extra term due to the change of variable formula, and the existence of cut points would introduce an extra indicator function. We provide the formula for conditional density on the hypersphere Sd

with uniform distribution as prior, and geodesic interpolation. See Lemma D.1 below. 

Lemma D.1 (Conditional density on Sd for geodesic interpolation) . Let Sd be the unit sphere with round metric, d ≥ 2. Let X1 ∼ p1 be the data distribution with smooth densities p1 > 0 w.r.t. dV g,and X0 ∼ p0 being uniform distribution independent of X1. Consider geodesic interpolation with minimizing geodesic. Fix t ∈ [0 , 1) and x, x 1 ∈ Sd. Write r = d(x, x 1). Denote 

Jt(x | x1) = 11 − t

 sin  r/ (1 − t)

sin r

d−1

1{d(x,x 1)<(1 −t)π}

=



> 11−t

 sin 

 r/ (1 −t)

 

> sin r

d−1

, if r < (1 − t)π, 

0, if r ≥ (1 − t)π. 

Then the conditional density of X1 given Xt = x is 

pt(x1 | x) = p1(x1) Jt(x | x1)

Z

> Sd

p1(z) Jt(x | z)dV g(z)

.

Proof. The proof strategy is as follows. We first write out the joint distribution of X0, X 1, and then use the change of variable formula to obtain joint distribution of Xt, X 1.We apply the change of variable formula. We need a diffeomorphism between ( X0, X 1) and (Xt, X 1). Fix t ∈ (0 , 1) and x1 ∈ Sd. Use polar coordinates at x1: every y ∈ Sd \ {− x1} can be written uniquely as y = Exp x1 (rω ) for some r ∈ (0 , π ), ω ∈ Sd−1. And we know the Riemannian volume element is 

dV g(y) = (sin r)d−1dr dω. 

In particular, for any x0, we set r0 = d(x0, x 1) ∈ (0 , π ) and ω = Log x1 (x0)    

> ∥Log x1(x0)∥

. Then we can write 

x0 = Exp x1 (r0ω), x1 = Exp x1 (0 · ω).

and 

Xt = Exp x0

 t Log x0 (x1) = Exp x1

 (1 − t)r0 ω.

Thus we can define the desired diffeomorphism as Ft : ( r0, ω ) 7 → (r, ω ) satisfying 

Ft,x 1 (r0, ω ) = ((1 − t)r0, ω ), F −1 

> t,x 1

(r, ω ) = ( r

1 − t , ω ).

42 But however, note that we have to restrict r < (1 − t)π, otherwise r 

> 1−t

/∈ (0 , π ), consequently 

F −1 

> t,x 1

(r, ω ) is no longer under the polar coordinate. Now recall the change of variable formula. We should have 

Z

> Sd

p0(F −1 

> t,x 1

(x)) | det dF −1 

> t,x 1

|dV g(x) = 

Z

> Sd

p0(x0)dV g(x0).

Written in polar coordinates: the volume element at x0 and x are 

dV g(x0) = (sin r0)d−1dr 0 dω, dV g(x) = (sin r)d−1dr dω. 

Define a function Jt(x | x1) to satisfy 

dV g(x0) = Jt(x | x1) dV g(x).

Using r0 = r/ (1 − t) (hence dr 0 = dr/ (1 − t)), we get (sin r0)d−1dr 0 dω = Jt(x | x1)(sin r)d−1dr dω 

(sin( r/ (1 − t))) d−1 dr 

1 − t dω = Jt(x | x1)(sin r)d−1dr dω, 

Now we derive the density. Note that by our construction of geodesic interpolation, given any 

x0 and t, the resulting x must satisfy d(x1, x ) = (1 − t)d(x1, x 0) ≤ (1 − t)π. Hence 

Jt(x | x1) = 11 − t

 sin  r/ (1 − t)

sin r

d−1

, r = d(x, x 1),

for all r < (1 − t)π, and Jt(x | x1) = 0 otherwise. Thus, we can equivalently write 

Jt(x | x1) = 11 − t

 sin  r/ (1 − t)

sin r

d−1

1{d(x,x 1)<(1 −t)π}.

The joint density of ( X0, X 1) is 

p(x0, x 1) = p0(x0)p1(x1)with respect to dV g(x0) dV g(x1). By the change of variable formula, 

pt(x, x 1) = p0(F −1 

> t,x 1

(x)) p1(x1)Jt(x | x1).

Finally, since pt(x, x 1) = pt(x1 | x)pt(x) = pt(x1 | x) R 

> Sd

pt(x, z ) dV g(z), the conditional density of X1 given Xt = x is 

pt(x1 | x) = pt(x, x 1)

R 

> Sd

pt(x, z ) dV g(z) ,

which gives the formula stated in the lemma. Following Lemma D.1, we can derive the following formula for interpolated density. 

Lemma D.2. We can write the interpolated density as 

pt(x) = 

Z

> Sd

pt(x, x 1) dV g(x1) = 1Vol ( Sd)

Z

> Sd

p1(x1)Jt(x | x1)dV g(x1).

43 Proof. Following Lemma D.1, we have 

pt(x, x 1) = p0(F −1 

> t,x 1

(x)) p1(x1)Jt(x | x1).

Consequently, we have (note that p0 is uniform distribution) 

pt(x) = 

Z

> Sd

pt(x, x 1) dV g(x1) = 1Vol ( Sd)

Z

> Sd

p1(x1)Jt(x | x1)dV g(x1).

Before justifying the regularity of v, we first need to show that its smoothness is not destroyed by the indicator function, which corresponds to the cut point. In the following Lemma, we show that on a Hypersphere, the sin function (that appeared in the conditional density function) would smooth out the indicator function, resulting in a smooth vector field. 

Lemma D.3. Let Sd be the unit hypersphere with round metric, d ≥ 2. Let x1 ∈ S d and define, for 

t ∈ [0 , 1) and x ∈ S d. Denote r(x) = d(x, x 1) as the radial distance function. Then for integer m

with 0 ≤ m ≤ d − 2, the function J(t, x | x1) viewed as a function on [0 , 1) × S d is Cm.Consequently, v(t, x ) as a conditional expectation is C2 in (t, x ) for t ∈ [0 , 1) . As a result, the solution for flow matching ODE exists and unique. 

Proof. Denote U := {(t, x ) ∈ [0 , 1) × S d : r(x) < (1 − t)π}. We first show the smoothness on 

U. Fix ( t0, x 0) ∈ U where x0̸ = x1, and write r0 := r(x0) > 0. Since r0 < (1 − t0)π < π , we have 

x0̸ = −x1. Moreover, by continuity of r(·) there exists a neighborhood V of ( t0, x 0) such that for all (t, x ) ∈ V , 1{r(x)<(1 −t)π} ≡ 1, and consequently 

J(t, x | x1) = 11 − t

 sin  r(x)/(1 − t)

sin r(x)

d−1

.

Since r = d(x, x 1) is smooth when x / ∈ { x1, −x1}, J is smooth as a composition of smooth functions. It remains to check the smoothness of J(t, x | x1) at point x = x1 (r = 0). Introduce normal coordinates at x1: for x near x1 write 

v := Log x1 (x) ∈ Tx1 Sd, r(x) = d(x, x 1) = ∥v∥.

It suffices to show that under normal coordinates, J viewed as a function of v, is smooth. Notice that ∥v∥ is not differentiable at v = 0, hence functions such as v 7 → sin ∥v∥ are not C1 at 0. But notice that in J, the “non-smooth” part on ∥v∥ cancels. By checking smoothness of sin( ∥v∥/(1 − t)) sin ∥v∥ ,

we conclude the smoothness of J on U .We extend the smoothness result by checking the boundary r = (1 − t)π. To prove Cm, fix (t0, x 0) with r(x0) = (1 − t0)π. Since r(x0) ∈ (0 , π ), we have x 7 → r(x) is smooth near x0. Set s(t, x )as follows, describing how far will x reach the boundary: 

s(t, x ) := (1 − t)π − r(x).

On the side s > 0 we have u := r/ (1 − t) ∈ (0 , π ) and sin 

 r

1 − t



= sin 



π − s

1 − t



= sin 

 s

1 − t



.

44 Therefore, for s > 0, 

J(t, x | x1) = 11 − t

 sin  s(t, x )/(1 − t)

sin r(x)

d−1

= s(t, x )d−1(1 − t)−d



> sin

 s(t,x )/(1 −t)



> s(t,x )/(1 −t)

sin r(x)



> d−1

.

On the whole manifold Sd, we have 

J(t, x | x1) =  s(t, x ) d−1+ (1 − t)−d



> sin

 s(t,x )/(1 −t)



> s(t,x )/(1 −t)

sin r(x)



> d−1

, (s)+ := max {s, 0}.

Since s 7 → (s) d−1+ is Cm as a one-dimensional function for every m ≤ d − 2, and since s(t, x ) is smooth in ( t, x ), the composition  s(t, x ) d−1+ is Cm in ( t, x ) for all m ≤ d − 2. It follows that J is 

Cm, as a product of a smooth function and  s(t, x ) d−1+ .

Remark 2. The fact that v being C2 in (t, x ) for t ∈ [0 , 1) follows from smoothness of J, and the following properties of Log : (1) Log being uniformly bounded, and (2) singularity of derivatives of 

Log x(x1) as x1 → − x is well controlled under polar coordinates. The existence and uniqueness of flow matching ODE follows from classical theory on time-dependent flow, see for example (Marsden et al., 2002, Section 4.1) and (Lee, 2012, Theorem 9.48). 

D.1 Auxiliary Lemmas 

The following result studies the derivative of a function that only depends on the radial distance. In other words, if a function ϕ(x) on M only depends on x through r(x) = d(x, x 1) for some fixed x1,then we can control its derivative through derivative of r.

Lemma D.4. Fix x1 ∈ S d and define r(x) := d(x, x 1). Assume x / ∈ Cut (x1), so that r is smooth in a neighborhood of x and ∥ grad x r(x)∥ = 1 . Let F : (0 , π ) → R be C2 and set ϕ(x) := F (r(x)) . Then 

grad x ϕ(x) = F ′(r(x)) grad x r(x), ∥ grad x ϕ(x)∥ = |F ′(r(x)) |.

Moreover, 

∥∇ grad ϕ(x)∥op ≤ | F ′(r(x)) | ∥∇ grad r(x)∥op + |F ′′ (r(x)) |.

Proof. We apply the chain rule for the Riemannian gradient. For any u ∈ TxSd, the differential satisfies 

dϕ (x)[ u] = d(F ◦ r)( x)[ u] = F ′(r(x)) dr (x)[ u].

By the definition of the Riemannian gradient, 

⟨grad x ϕ, u ⟩ = F ′(r(x)) ⟨grad x r, u ⟩, ∀u ∈ TxSd,

hence grad x ϕ(x) = F ′(r(x)) grad x r(x).

45 Taking norms gives 

∥ grad x ϕ(x)∥ = |F ′(r(x)) | ∥ grad x r(x)∥.

Since x / ∈ Cut ( x1), the distance function satisfies ∥ grad x r(x)∥ = 1, and therefore 

∥ grad x ϕ(x)∥ = |F ′(r(x)) |.

Next, for any u ∈ TxSd,

∇u grad ϕ(x) = ∇u

 F ′(r(x)) grad r(x)

= F ′(r(x)) ∇u grad r(x) + u  F ′(r(x))  grad r(x)= F ′(r(x)) ∇u grad r(x) + F ′′ (r(x)) u(r(x)) grad r(x)= F ′(r(x)) ∇u grad r(x) + F ′′ (r(x)) ⟨grad r(x), u ⟩ grad r(x).

Hence, for ∥u∥ = 1, 

∥∇ u grad ϕ(x)∥ ≤ | F ′(r(x)) | ∥∇ u grad r(x)∥ + |F ′′ (r(x)) | |⟨ grad r(x), u ⟩| ∥ grad r(x)∥≤ | F ′(r(x)) | ∥∇ u grad r(x)∥ + |F ′′ (r(x)) |,

using ∥ grad r(x)∥ = 1 and |⟨ grad r(x), u ⟩| ≤ ∥ u∥ = 1. Taking the supremum over ∥u∥ = 1 yields 

∥∇ grad ϕ(x)∥op ≤ | F ′(r(x)) | ∥∇ grad r(x)∥op + |F ′′ (r(x)) |.

The following result controls the conditional expectation of some certain “functions with singularity”, which will be used to establish regularity. 

Lemma D.5 (Expectation of 1sin a u .) . Consider d ≥ 3 and a = 1 , 2. Let r = d(x, x 1) and set 

u = r/ (1 − t) ∈ (0 , π ). We have 

E[ 1sin a u | Xt = x] ≤ 2 M1

m1

,

where 0 < m 1 ≤ p1 ≤ M1.

Proof. We have 

E 1sin a u | Xt = x =

Z

> Sd

1sin a u pt(x1 | x) dV g(x1)

≤ M1

m1 Vol ( Sd)

Z

> Sd

1sin a u Jt(x | x1) dV g(x1).

Use r = (1 − t)u so that 11−t dr = du ,

Z

> Sd

1sin a u Jt(x | x1) dV g(x1)=

Z

> Sd−1

Z π

> 0

1sin a u Jt(x | x1) (sin r)d−1drdω 

= Vol ( Sd−1)

Z π

> 0

1sin a u sin( u)d−1 du 

46 = Vol ( Sd−1)

Z π

> 0

sin( u)d−1−a du. 

Therefore 

E 1sin a u | Xt = x =

Z

> Sd

1sin a u pt(x1 | x) dV g(x1)

≤ M1 Vol ( Sd−1)

m1 Vol ( Sd)

Z π

> 0

sin( u)d−1−a du. 

= M1 Vol ( Sd−1)

m1 Vol ( Sd) Vol ( Sd−1−a) Vol ( Sd−1−a)

Z π

> 0

sin( u)d−1−a du 

= M1 Vol ( Sd−1)

m1 Vol ( Sd) Vol ( Sd−1−a) Vol ( Sd−a).

Recall Vol ( Sn) = 2π(n+1) /2

Γ(( n + 1) /2) .

Hence Vol ( Sd−1)Vol ( Sd) =

> 2πd/ 2
> Γ( d/ 2) 2π(d+1) /2
> Γ(( d+1) /2)

= π−1/2 Γ(( d + 1) /2) Γ( d/ 2) ,

Vol ( Sd−a)Vol ( Sd−1−a) =

> 2π(d−a+1) /2
> Γ(( d−a+1) /2) 2π(d−a)/2
> Γ(( d−a)/2)

= π1/2 Γ(( d − a)/2) Γ(( d − a + 1) /2) .

By Kershaw’s inequality, for d ≥ 3, Γ(( d + 1) /2) Γ( d/ 2) ≤ (d/ 2 − 12 +

r 34 ) 12 ,

and Γ( d/ 2) Γ(( d − 1) /2) ≥ ( d

2 − 1 + 12 ) 12 ,

Γ(( d − a + 1) /2) Γ(( d − a)/2) ≥ ( d − a − 12 + 12 ) 12 .

Together, Vol ( Sd−1) Vol ( Sd−a)Vol ( Sd) Vol ( Sd−1−a) = Γ(( d + 1) /2)Γ(( d − a)/2) Γ( d/ 2)Γ(( d − a + 1) /2) ≤ ( d/ 2 − 12 +

q 34 

> d−a−12

+ 12

) 12

= ( d − a + a − 1 + 2 

q 34

d − a ) 12 ≤ (1 + a − 1 + 2 

q 34

d − a ) 12 .

For a = 1 , 2, we have (1 + a−1+2   

> q34
> d−a

) 12 ≤ 2. The following results (Lemmas D.6, D.7, D.8, and D.9) provide bounds on the building blocks that appear in the derivative formulas for v(t, x ). For all the results below, we assume 0 < m 1 ≤

p1(x) ≤ M1.47 Lemma D.6 (Moment bounds for grad x log pt(X1 | x).) . We have 

∥ grad x log Jt(x | x1)∥ ≤ 2( d − 1) (1 − t) sin u .

Furthermore, 

E[∥ grad x log pt(X1 | x)∥ | Xt = x] ≤ 8( d − 1) (1 − t)

M1

m1

,

E[∥ grad x log Jt(x | X1)∥2 | Xt = x] ≤ 8( d − 1) 2

(1 − t)2

M1

m1

,

E[∥ grad x log pt(X1 | x)∥2 | Xt = x] ≤ 32( d − 1) 2

(1 − t)2

M1

m1

.

Proof. We first bound ∥ grad x log Jt(x | x1)∥. Let r = d(x, x 1) and set u = r/ (1 − t) ∈ (0 , π ). On the set r < (1 − t)π,

∂∂r log Jt(r) = ( d − 1) 

 11 − t cot r

1 − t − cot r



= ( d − 1) 

 11 − t cot u − cot((1 − t)u)



.

By the Lemma D.4, 

∥ grad x log Jt(x | x1)∥ = ∂∂r log Jt(r) = ( d − 1) 11 − t cot u − cot((1 − t)u) .

For u ∈ (0 , π ), we have | cot u| ≤ 1/ sin u. Since sin((1 − t)u) ≥ (1 − t) sin u, we have 

| cot((1 − t)u)| ≤ 1sin((1 − t)u) ≤ 1(1 − t) sin u .

Therefore for all u ∈ (0 , π ), we can bound 

11 − t cot u − cot((1 − t)u) ≤ 11 − t | cot u| + | cot((1 − t)u)| ≤ 1(1 − t) sin u + 1(1 − t) sin u

= 2(1 − t) sin u .

So 

∥ grad x log Jt(x | x1)∥ ≤ 2( d − 1) (1 − t) sin u .

Now we bound E∥ grad x log pt∥ by E∥ grad x log Jt∥. Let 

Zt(x) := 

Z

> Sd

p1(z)Jt(x | z) dV g(z),

so log pt(x1 | x) = log p1(x1) + log Jt(x | x1) − log Zt(x).

Since p1 does not depend on x,grad x log pt(x1 | x) = grad x log Jt(x | x1) − grad x log Zt(x).

48 Also, grad x Zt(x) = 

Z

> Sd

p1(z) grad x Jt(x | z) dV g(z) = 

Z

> Sd

p1(z)Jt(x | z) grad x log Jt(x | z) dV g(z),

hence grad x log Zt(x) = 

Z

> Sd

grad x log Jt(x | z) pt(z | x) dV g(z) = Egrad x log Jt(x | X1) | Xt = x.

Therefore grad x log pt(x1 | x) = grad x log Jt(x | x1) − Egrad x log Jt(x | X1) | Xt = x,

and by triangle inequality, 

E∥ grad x log pt(X1 | x)∥ | Xt = x ≤ 2 E∥ grad x log Jt(x | X1)∥ | Xt = x ≤ 8( d − 1) (1 − t)

M1

m1

,

where we used Lemma D.5. For second moment, we have 

E[∥ grad x log pt(X1 | x)∥2 | Xt = x] ≤ 4 E[∥ grad x log Jt(x | X1)∥2 | Xt = x].

Recall ∥ grad x log Jt(x | x1)∥ ≤ 2( d−1) (1 −t) sin u , using Lemma D.5, 

E[∥ grad x log Jt(x | X1)∥2 | Xt = x] ≤ 4( d − 1) 2

(1 − t)2 E

h 1sin 2 u | Xt = x

i

≤ 8( d − 1) 2

(1 − t)2

M1

m1

.

Hence 

E[∥ grad x log pt(X1 | x)∥2 | Xt = x] ≤ 32( d − 1) 2

(1 − t)2

M1

m1

.

Lemma D.7 (Moment bounds for ∂t log pt(X1 | x).) . We have 

E

h ∂∂t log pt(X1 | x) | Xt = x

i

≤ 21 − t + 4 π d − 11 − tM1

m1

,

E

h ∂∂t log pt(X1 | x) 2

| Xt = x

i

≤ 8(1 − t)2 + 16 π2(d − 1) 2

(1 − t)2

M1

m1

.

Proof. Let 

Zt(x) = 

Z

> Sd

p1(z)Jt(x | z) dV g(z),

so we have log pt(x1 | x) = log p1(x1) + log Jt(x | x1) − log Zt(x). Since p1 does not depend on t,

∂∂t log pt(x1 | x) = ∂∂t log Jt(x | x1) − ∂∂t log Zt(x).

49 Moreover, for d ≥ 2 the function ∂tJt(x | z) is integrable against p1(z)dV g(z), and the map t 7 → Zt(x)is C1 with 

∂∂t Zt(x) = 

Z

> Sd

p1(z) ∂∂t Jt(x | z) dV g(z) = 

Z

> Sd

p1(z)Jt(x | z) ∂∂t log Jt(x | z) dV g(z).

Dividing by Zt(x) yields 

∂∂t log Zt(x) = 

Z

> Sd

∂∂t log Jt(x | z) pt(z | x) dV g(z) = E

h ∂∂t log Jt(x | X1) | Xt = x

i

.

Therefore 

∂∂t log pt(x1 | x) = ∂∂t log Jt(x | x1) − E

h ∂∂t log Jt(x | X1) | Xt = x

i

,

and by triangle inequality, 

E

h ∂∂t log pt(X1 | x) | Xt = x

i

≤ 2 E

h ∂∂t log Jt(x | X1) | Xt = x

i

,

E

h ∂∂t log pt(X1 | x) 2

| Xt = x

i

≤ 4 E

h ∂∂t log Jt(x | X1) 2

| Xt = x

i

.

We compute ∂t log Jt explicitly and bound its conditional expectation. For r = d(x, x 1) < (1 −t)π,log Jt(x | x1) = − log(1 − t) + ( d − 1) 



log sin( r/ (1 − t)) − log sin r



.

Holding r fixed and differentiating in t gives 

∂∂t log Jt(x | x1) = 11 − t + ( d − 1) r

(1 − t)2 cot( r/ (1 − t)) .

Write u = r/ (1 − t) ∈ (0 , π ). Then r = (1 − t)u and 

∂∂t log Jt(x | x1) ≤ 11 − t + d − 11 − t |u cot u|,

∂∂t log Jt(x | x1) 2

≤ 2 1(1 − t)2 + 2 (d − 1) 2

(1 − t)2 |u cot u|2.

Hence 

E

h ∂∂t log Jt(x | X1) | Xt = x

i

≤ 11 − t + π d − 11 − t

2M1

m1

,

E

h ∂∂t log Jt(x | X1) 2

| Xt = x

i

≤ 2(1 − t)2 + 2π2(d − 1) 2

(1 − t)2

2M1

m1

.

where we used |u cot u| ≤ π  

> sin u

. Consequently, 

E

h ∂∂t log pt(X1 | x) | Xt = x

i

≤ 21 − t + 4 π d − 11 − tM1

m1

,

E

h ∂∂t log pt(X1 | x) 2

| Xt = x

i

≤ 8(1 − t)2 + 16 π2(d − 1) 2

(1 − t)2

M1

m1

.

50 Lemma D.8. We have 

E∥∇ 2 

> x

log pt(X1 | x)∥op | Xt = x ≤ 64( d − 1) 2

(1 − t)2

M1

m1

,

Proof. Recall that grad x log pt(x1 | x) = grad x log Jt(x | x1) − grad x log Zt(x),

Take derivative again, we obtain 

∇ grad x log pt(x1 | x) = ∇ grad x log Jt(x | x1) − ∇ grad x log Zt(x),

Recall grad x log Zt(x) = Egrad x log Jt(x | X1) | Xt = x.

Hence 

∇u grad x log Zt(x) = ∇uEgrad x log Jt(x | X1) | Xt = x

=E∇u grad x log Jt(x | X1) | Xt = x + Egrad x log Jt(x | X1)∇u log pt(x1 | x) | Xt = x,

and we can bound 

∥∇ grad x log Zt(x)∥op 

≤ E∥∇ 2 log Jt(x | X1)∥op | Xt = x

+

q

E∥ grad x log Jt(x | X1)∥2 | Xt = xE∥∇ log pt(x1 | x)∥2 | Xt = x

≤ E∥∇ 2 log Jt(x | X1)∥op | Xt = x + 16( d − 1) 2M1

(1 − t)2m1

.

To control ∥∇ 2 log Jt(x | X1)∥op , we consider 

∂2

∂r 2 log Jt(x | X1) = ∂∂r (d − 1)( 11 − t cot r

1 − t − cot r) = ( d − 1)( − 1(1 − t)2

1sin 2 r

> 1−t

+ 1sin 2 r )= ( d − 1)( 1sin 2(1 − t)u − 1(1 − t)2

1sin 2 u ).

Applying Lemma D.4 with ϕ = log Jt(r), we have 

∥∇ grad log Jt(r)∥op ≤ | log J′

> t

(r(x)) |∥∇ grad r(x)∥op + | log J′′  

> t

(r(x)) |≤ | log J′

> t

(r(x)) | cot r + | log J′′  

> t

(r(x)) |,

where ∥∇ grad r(x)∥op ≤ | cot r| by Lee (2018, Proposition 11.3) Now using 

∂∂r log Jt(x | X1) = ( d − 1)( 11 − t cot r

1 − t − cot r),∂2

∂r 2 log Jt(x | X1) = ( d − 1)( 1sin 2(1 − t)u − 1(1 − t)2

1sin 2 u ),

51 we obtain 

∥∇ 2 log Jt(x | X1)∥op ≤ 4( d − 1) (1 − t)2 sin 2 u ,

hence 

E[∥∇ 2 log Jt(x | X1)∥ | Xt = x] ≤ 16( d − 1) (1 − t)2

M1

m1

.

Now we bound E[∥∇ 2 

> x

log pt(X1 | x)∥op | Xt = x]. Recall 

∇ grad x log pt(x1 | x) = ∇ grad x log Jt(x | x1) − ∇ grad x log Zt(x),

∥∇ grad x log Zt(x)∥op ≤ E∥∇ 2 log Jt(x | X1)∥op | Xt = x + 16( d − 1) 2M1

(1 − t)2m1

.

By triangle inequality, we obtain 

E∥∇ 2 

> x

log pt(X1 | x)∥op | Xt = x ≤ 2 E∥∇ 2 

> x

log Jt(x | X1)∥op | Xt = x + 16( d − 1) 2

(1 − t)2

M1

m1

.

Finally, using the bound 

E∥∇ 2 

> x

log Jt(x | X1)∥op | Xt = x ≤ 16( d − 1) (1 − t)2

M1

m1

,

we conclude 

E∥∇ 2 

> x

log pt(X1 | x)∥op | Xt = x ≤ 32( d − 1) (1 − t)2

M1

m1

+ 16( d − 1) 2

(1 − t)2

M1

m1

< 64( d − 1) 2

(1 − t)2

M1

m1

.

Lemma D.9. We have 

E[∥∂t grad x log pt(X1 | x)∥ | Xt = x] ≤ 16 π2(d − 1) 2

(1 − t)2

M1

m1

.

Proof. We have grad x log pt(x1 | x) = grad x log Jt(x | x1) − grad x log Zt(x)= grad x log Jt(x | x1) − Egrad x log Jt(x | X1) | Xt = x,

so we have 

∥∂t grad x log pt(x1 | x)∥ ≤ ∥ ∂t grad x log Jt(x | x1)∥ + E∥∂t grad x log Jt(x | X1)∥ | Xt = x

+ E[∥ grad x log Jt(x | X1)∥ | ∂t log pt(X1 | x)| | Xt = x].

A direct differentiation at fixed r gives 

∥∂t grad x log Jt(x | x1)∥ = ∥∂t

∂∂r log Jt(r) grad r(x)∥ = ( d − 1) ∂t

11 − t cot u − cot((1 − t)u)

= ( d − 1) 1(1 − t)2 cot u − u

sin 2(1 − t)u

52 ≤ d − 1(1 − t)2

2π

sin 2 u .

Taking expectation and using E[1 / sin a u] ≤ 2M1/m 1 for a = 1 , 2, 

E[∥∂t grad x log Jt(x | X1)∥ | Xt = x] ≤ 4π(d − 1) (1 − t)2

M1

m1

,

Recall that 

E

h

∂t log pt(X1 | x) 2

| Xt = x

i

≤ 4E

h

∂t log Jt

> 2

| Xt = x

i

≤ 4E

h (d − 1) 2π2

(1 − t)2

1sin 2 u | Xt = x

i

≤ 8π2(d − 1) 2

(1 − t)2

M1

m1

.

Together with ∥ grad x log Jt(x | x1)∥ ≤ 2( d−1) (1 −t) sin u , by Cauchy-Schwarz, 

E[∥ grad x log Jt(x | X1)∥ | ∂t log pt(X1 | x)| | Xt = x]

≤



E[∥ grad x log Jt(x | X1)∥2 | Xt = x]

1/2

E[|∂t log pt(X1 | x)|2 | Xt = x]

1/2

≤

s

8π(d − 1) 2

(1 − t)2

M1

m1

8π2(d − 1) 2

(1 − t)2

M1

m1

≤ 8π2(d − 1) 2

(1 − t)2

M1

m1

.

Thus 

E[∥∂t grad x log pt(X1 | x)∥ | Xt = x] ≤ 16 π2(d − 1) 2

(1 − t)2

M1

m1

.

D.2 Regularity for Flow Matching Vector Field 

In this section, we show that both the spatial derivative (Lemma D.10) and time derivative (Lemma D.11) of v(t, x ) are uniformly bounded. 

Lemma D.10. Assume d ≥ 3 and t ∈ (0 , 1) . Let p1 be a smooth density on Sd such that 

0 < m 1 ≤ p1(z) ≤ M1 < ∞.

For all unit tangent vector w, we have 

∥∇ wv(t, x )∥ ≤ 12 πM 1(d − 1) 

m1(1 − t) .

Hence v(t, x ) is L-Lipschitz with L = 12 πM 1(d−1)  

> m1(1 −t)

.

Proof. Fix t < 1 and x ∈ M , and let w ∈ TxM be a unit tangent vector. We consider the covariant derivative of v in the direction w:

∇wv(t, x ) = 11 − t ∇w

Z

> M

Log x(x1) pt(x1|x) dV g(x1)53 = 11 − t

Z

> M

∇w

  Log x(x1) pt(x1|x) dV g(x1)+ 11 − t

Z

> M

Log x(x1) ⟨grad x pt(x1|x), w ⟩ dV g(x1).

Define 

P (t, x ) := 11 − t

Z

> Sd

Log x(x1)⟨grad x pt(x1 | x), w ⟩ dV g(x1),G(t, x ) := 11 − t

Z

> Sd

∥∇ w Log x(x1)∥pt(x1 | x) dV g(x1).

We show that ∥P (t, x )∥ ≤ 8πM 1(d−1)  

> m1(1 −t)

, ∀∥ w∥ = 1. We first write ∥P (t, x )∥ as a conditional moment of ∥ grad x log pt∥. Using grad x pt = pt grad x log pt and ∥ Log x(x1)∥ = d(x, x 1), 

∥P (t, x )∥ ≤ 11 − t

Z

> Sd

∥ Log x(x1)∥ ∥ grad x pt(x1 | x)∥ dV g(x1)= 11 − t

Z

> Sd

∥ Log x(x1)∥ pt(x1 | x) ∥ grad x log pt(x1 | x)∥ dV g(x1)= 11 − t E∥ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥ | Xt = x.

On the support of pt(· | x) we have d(x, X 1) < (1 − t)π, hence 

∥ Log x(X1)∥ ≤ (1 − t)π, 

so 

∥P (t, x )∥ ≤ π E∥ grad x log pt(X1 | x)∥ | Xt = x.

Using Lemma D.6, 

∥P (t, x )∥ ≤ 8πM 1(d − 1) 

m1(1 − t) .

Next, we show that G(t, x ) ≤ 4π M1 

> m1(1 −t)

.Notice that ∇w Log x(x1) can be related to a Hessian. Denote r = d(x, x 1) ∈ (0 , π ), it is easy to show (see, for example, Alimisis et al. (2020, Appendix B, Proof of Lemma 2)) 

∥∇ w Log x(x1)∥ ≤ 1 + |r cot( r)|.

Now we bound the cot term. On the support of pt(· | x) we have r < (1 − t)π. Let u = r/ (1 − t) ∈

(0 , π ), so r = (1 − t)u. Using | cot r| ≤ 1/ sin r and the concavity bound sin ((1 − t)u) ≥ (1 − t) sin u,

|r cot r| ≤ r

sin r = (1 − t)u

sin((1 − t)u) ≤ (1 − t)u

(1 − t) sin u = u

sin u ≤ π

sin u .

Therefore for r < (1 − t)π,

∥∇ w Log x(x1)∥ ≤ 1 + π

sin u < 2π

sin u .

54 Using the same technique as before, 

Z

> Sd

∥∇ w Log x(x1)∥ pt(x1 | x) dV g(x1) ≤ 4π M1

m1

.

We conclude that Multiplying by (1 − t)−1 yields 

G(t, x ) ≤ 4π M1

m1(1 − t) .

Lemma D.11. Fix d ≥ 3 and t ∈ (0 , 1) . Let p1 be a smooth density on Sd satisfying 

0 < m 1 ≤ p1(z) ≤ M1 < ∞.

For 

v(t, x ) = 11 − t

Z

> Sd

Log x(x1) pt(x1 | x) dV g(x1),

we have for every x ∈ Sd,

ddt v(t, x ) ≤ 3π

1 − t + 4π2d

1 − tM1

m1

≤ 8π2d

1 − tM1

m1

.

Proof. Observe that on the support of pt(· | x) we have d(x, x 1) < (1 − t)π, hence 

∥ Log x(x1)∥ = d(x, x 1) ≤ (1 − t)π. 

Therefore 

∥v(t, x )∥ ≤ 11 − t

Z

> Sd

∥ Log x(x1)∥ pt(x1 | x) dV g(x1) ≤ 11 − t (1 − t)π = π. 

We differentiate v(t, x ) in t. Since Log x(x1) does not depend on t,

ddt v(t, x ) = ddt 

 11 − t

 Z

> Sd

Log x(x1) pt(x1 | x) dV g(x1) + 11 − t

Z

> Sd

Log x(x1) ∂∂t pt(x1 | x) dV g(x1)= 1(1 − t)2

Z

> Sd

Log x(x1) pt(x1 | x) dV g(x1) + 11 − t

Z

> Sd

Log x(x1) ∂∂t pt(x1 | x) dV g(x1).

Using 

Z

> Sd

Log x(x1) pt(x1 | x) dV g(x1) = (1 − t) v(t, x ),

we have 

ddt v(t, x ) = 11 − t v(t, x ) + 11 − t

Z

> Sd

Log x(x1) ∂∂t pt(x1 | x) dV g(x1).

Since R 

> Sd

pt(x1 | x) dV g(x1) = 1, we have 

Z

> Sd

∂∂t pt(x1 | x) dV g(x1) = 0 .

55 Also, wherever pt(x1 | x) > 0, 

∂∂t pt(x1 | x) = pt(x1 | x) ∂∂t log pt(x1 | x).

Hence 

Z

> Sd

Log x(x1) ∂∂t pt(x1 | x) dV g(x1) ≤

Z

> Sd

∥ Log x(x1)∥ pt(x1 | x) ∂∂t log pt(x1 | x) dV g(x1)

≤ (1 − t)π E

h ∂∂t log pt(X1 | x) | Xt = x

i

.

Therefore 

ddt v(t, x ) ≤ π

1 − t + π E

h ∂∂t log pt(X1 | x) | Xt = x

i

.

Using Lemma D.7, 

ddt v(t, x ) ≤ π

1 − t + π E

h ∂∂t log pt(X1 | x) | Xt = x

i

≤ π

1 − t + 2 π( 11 − t + d − 11 − t

2πM 1

m1

) ≤ 11 − t (3 π + 4 π2 dM 1

m1

).

D.3 Regularity for Divergence 

In this section, we show that both the gradient (Lemma D.12) and the time derivative (Lemma D.13) of div v(t, x ) are uniformly bounded. 

Lemma D.12. Assume d ≥ 3 and t ∈ (0 , 1) . Let p1 be a smooth density on Sd such that 

0 < m 1 ≤ p1(z) ≤ M1 < ∞.

Then for every x ∈ Sd we have 

∥ grad x div v(t, x )∥ ≤ 128 π(d − 1) 2

(1 − t)3

M1

m1

.

Proof. Write 

v(t, x ) = 11 − t

Z

> Sd

Log x(x1) pt(x1 | x) dV g(x1).

Using div ( f W ) = ⟨grad f, W ⟩ + f div W with f = pt(· | x) and W = Log x(·), div v(t, x ) = 11 − t

Z

> Sd

div x Log x(x1) pt(x1 | x) dV g(x1)+ 11 − t

Z

> Sd

⟨grad x pt(x1 | x), Log x(x1)⟩ dV g(x1)= 11 − t E[div x Log x(X1) | Xt = x]56 + 11 − t E[⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x].

Differentiate this identity in x along a unit vector ξ ∈ TxSd and take norms. We take gradient, and obtain (for a smooth integrand F (x, x 1)), grad x E[F (x, X 1) | Xt = x] = E[grad x F (x, X 1) | Xt = x] + E[F (x, X 1) grad x log pt(X1 | x) | Xt = x].

Applying this formula and using triangle inequality yields 

∥ grad x div v(t, x )∥ ≤ 11 − t



T1 + T2 + T3 + T4



,

where 

T1 = E[∥ grad x div x Log x(X1)∥ | Xt = x],T2 = E[| div x Log x(X1)| ∥ grad x log pt(X1 | x)∥ | Xt = x],T3 = E[∥ grad x⟨grad x log pt(X1 | x), Log x(X1)⟩∥ | Xt = x],T4 = E[|⟨ grad x log pt(X1 | x), Log x(X1)⟩| ∥ grad x log pt(X1 | x)∥ | Xt = x].

Now we compute div x Log x(x1) and its x-gradient. Let r = d(x, x 1) ∈ (0 , π ). Notice that Log x(x1) = − grad x

12 r2,

hence div x Log x(x1) = − div x(r grad r) = −r∆r − ⟨ grad r, grad r⟩ = −(d − 1) r cot r − 1,

where we used ∆ r = ( d − 1) cot r (see for example (Lee, 2018, Theorem 11.11)) and ∥ grad r∥2 = 1. Differentiate in x using that r is a radial function and ∥ grad r∥ = 1: grad x div x Log x(x1) = −(d − 1)  cot r − r csc 2 r grad x r, 

which implies 

∥ grad x div x Log x(x1)∥ = ( d − 1) cot r − r csc 2 r = ( d − 1) cot r − r 1sin 2 r .

We bound T1 = E[∥ grad x div x Log x(X1)∥ | Xt = x]. Using r = (1 − t)u and sin ((1 − t)u) ≥

(1 − t) sin u (concavity of sin on [0 , π ]), 

| cot r| ≤ 1sin r ≤ 1(1 − t) sin u , r 1sin 2 r ≤ (1 − t)π

sin 2 r ≤ π

(1 − t) sin 2 u .

Hence 

∥ grad x div x Log x(x1)∥ ≤ (d − 1) 

 1(1 − t) sin u + π

(1 − t) sin 2 u



.

Therefore 

T1 ≤ (d − 1) 11 − tM1

m1

(2 π + 2) .

57 We bound T2 = E[| div x Log x(X1)| ∥ grad x log pt(X1 | x)∥ | Xt = x]. Recall | div x Log x(x1)| ≤ 

1 + ( d − 1) r| cot r|, so we have 

| div x Log x(x1)| ≤ 1 + ( d − 1) π 1sin r ≤ 1 + ( d − 1) π 1(1 − t) sin u .

Thus by Cauchy-Schwarz, 

T2 ≤



E[| div x Log x(X1)|2 | Xt = x]

1/2

E[∥ grad x log pt(X1 | x)∥2 | Xt = x]

1/2

.

Using ( a + b)2 ≤ 2a2 + 2 b2,

E[| div x Log x(X1)|2 | Xt = x] ≤ 2 + 2( d − 1) 2π2 1(1 − t)2 E

h 1sin 2 u | Xt = x

i

≤ 2 + 2( d − 1) 2π2 1(1 − t)2

2M1

m1

.

Also recall 

E[∥ grad x log pt(X1 | x)∥2 | Xt = x] ≤ 32( d − 1) 2

(1 − t)2

M1

m1

.

Hence we obtain 

T2 ≤ (2 + 2( d − 1) 2π2 1(1 − t)2

2M1

m1

) 12 ( 32( d − 1) 2

(1 − t)2

M1

m1

) 12

≤ 16 πM 1

m1

(d − 1) 2

(1 − t)2 .

We bound T3 = E[∥ grad x⟨grad x log pt(X1 | x), Log x(X1)⟩∥ | Xt = x]. We use the product rule: denote g(x) = ⟨g1(x), g 2(x)⟩. For g1, g 2, we have (viewing as directional derivative along u and use compatiblity) 

⟨grad x⟨g1(x), g 2(x)⟩, u ⟩ = ∇u⟨g1(x), g 2(x)⟩ = ⟨∇ ug1(x), g 2(x)⟩ + ⟨g1(x), ∇ug2(x)⟩.

Hence we can write 

∥ grad x⟨grad x log pt(x1 | x), Log x(x1)⟩∥ 

=∥∇ grad x log pt(x1 | x)∥op ∥ Log x(x1)∥ + ∥∇ Log x(x1)∥op ∥ grad x log pt(x1 | x)∥,

so 

T3 ≤ E[∥∇ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥ | Xt = x]+ E[∥∇ 2 log pt(X1 | x)∥ ∥ Log x(X1)∥ | Xt = x].

On the support of pt(· | x), ∥ Log x(X1)∥ ≤ (1 − t)π, hence 

E[∥∇ 2 log pt(X1 | x)∥ ∥ Log x(X1)∥ | Xt = x] ≤ (1 − t)π E[∥∇ 2 log pt(X1 | x)∥ | Xt = x].

By Lemma D.8, 

E[∥∇ 2 log pt(X1 | x)∥ ∥ Log x(X1)∥ | Xt = x] ≤ 64 π(d − 1) 2

1 − tM1

m1

.

58 Also, on Sd,

∥∇ Log x(x1)∥ ≤ 1 + r| cot r| < π 2(1 − t) sin u ,

so by Cauchy-Schwarz inequality, 

E[∥∇ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥ | Xt = x] ≤

 32( d − 1) 2

(1 − t)2

M1

m1

8π2

(1 − t)2

M1

m1

 12

≤ 16 πM 1

m1

(d − 1) (1 − t)2 .

Hence we get 

T3 ≤ 16 πM 1

m1

(d − 1) (1 − t)2 + 64 π(d − 1) 2

1 − tM1

m1

≤ 64 πM 1

m1

(d − 1) 2

(1 − t)2 .

We bound T4 = E[|⟨ grad x log pt(X1 | x), Log x(X1)⟩| ∥ grad x log pt(X1 | x)∥ | Xt = x]. We have 

T4 ≤ E[∥ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥2 | Xt = x]

≤ (1 − t)π E[∥ grad x log pt(X1 | x)∥2 | Xt = x]

≤ (1 − t)π · 32( d − 1) 2

(1 − t)2

M1

m1

= 32 π(d − 1) 2

1 − tM1

m1

.

Together, 

∥ grad x div v(t, x )∥≤ 11 − t



T1 + T2 + T3 + T4



≤ 1(1 − t)

M1

m1



(d − 1) 11 − t (2 π + 2) + 16 π (d − 1) 2

(1 − t)2 + 64 π (d − 1) 2

(1 − t)2 + 32 π(d − 1) 2

1 − t



≤ 128 π(d − 1) 2

(1 − t)3

M1

m1

.

Lemma D.13. Assume d ≥ 3 and t ∈ (0 , 1) . Let p1 be a smooth density on Sd such that 

0 < m 1 ≤ p1(z) ≤ M1 < ∞.

Then for every x ∈ Sd,

ddt div v(t, x ) ≤ 128 π2(d − 1) 2

(1 − t)3

M1

m1

.

Proof. We start by computing the time derivative of divergence. Recall div v(t, x ) = 11 − t E[div x Log x(X1) | Xt = x] + 11 − t E[⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x].

Let F (t, x 1) be smooth in t and integrable under pt(· | x). Then 

ddt E[F (t, X 1) | Xt = x] = E[∂tF (t, X 1) | Xt = x] + E[F (t, X 1) ∂t log pt(X1 | x) | Xt = x].

59 Applying this with F (t, x 1) = div x Log x(x1) and F (t, x 1) = ⟨grad x log pt(x1 | x), Log x(x1)⟩

respectively, 

ddt E[div x Log x(X1) | Xt = x] = E[div x Log x(X1) ∂t log pt(X1 | x) | Xt = x],ddt E[⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x] = E[∂t⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x]+ E[⟨grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x) | Xt = x].

Therefore, for fixed x,

ddt div v(t, x ) = 1(1 − t)2 Ediv x Log x(X1) + ⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x

+ 11 − t E[div x Log x(X1) ∂t log pt(X1 | x) | Xt = x]+ 11 − t E[∂t⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x]+ 11 − t E[⟨grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x) | Xt = x].

Define 

T1 = 1(1 − t)2 E| div x Log x(X1) + ⟨grad x log pt(X1 | x), Log x(X1)⟩| | Xt = x,T2 = 11 − t E[| div x Log x(X1) ∂t log pt(X1 | x)| | Xt = x],T3 = 11 − t E[|∂t⟨grad x log pt(X1 | x), Log x(X1)⟩| | Xt = x],T4 = 11 − t E[|⟨ grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x)| | Xt = x].

For T1, recall 1(1 − t)2 Ediv x Log x(X1) | Xt = x ≤ 1(1 − t)2 E1 + ( d − 1) π 1(1 − t) sin u | Xt = x

≤ π (d − 1) (1 − t)3

2M1

m1

,

and 1(1 − t)2 E⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x

≤ 1(1 − t)2 (1 − t)πE∥ grad x log pt(X1 | x)∥ | Xt = x ≤ 8π(d − 1) (1 − t)2

M1

m1

.

Hence 

T1 ≤ π (d − 1) (1 − t)3

2M1

m1

+ 8π(d − 1) (1 − t)2

M1

m1

≤ 16 π(d − 1) (1 − t)3

M1

m1

.

For T2, recall 

E[| div x Log x(X1)|2 | Xt = x] ≤ 2 + 2( d − 1) 2π2 1(1 − t)2

2M1

m1

≤ (d − 1) 2

(1 − t)2

4π2M1

m1

.

60 Hence we have 

T2 = 11 − t E[| div x Log x(X1) ∂t log pt(X1 | x)| | Xt = x]

≤ 11 − t



E[| div x Log x(X1)|2 | Xt = x]E[|∂t log pt(X1 | x)|2 | Xt = x]

 12

≤ 11 − t

 (d − 1) 2

(1 − t)2

4π2M1

m1

32 π2(d − 1) 2

(1 − t)2

M1

m1

 12

≤ 16 π2(d − 1) 2

(1 − t)3

M1

m1

,

where by Lemma D.7, 

E

h ∂∂t log pt(X1 | x) 2

| Xt = x

i

≤ 8(1 − t)2 + 16 π2(d − 1) 2

(1 − t)2

M1

m1

≤ 32 π2(d − 1) 2

(1 − t)2

M1

m1

.

For T3, since Log x(x1) does not depend on t,

|∂t⟨grad x log pt(x1 | x), Log x(x1)⟩| ≤∥ ∂t grad x log pt(x1 | x)∥ ∥ Log x(x1)∥ ≤ (1 − t)π ∥∂t grad x log pt(x1 | x)∥.

Using Lemma D.9, we have 

E[∥∂t grad x log pt(X1 | x)∥ | Xt = x] ≤ 16 π2(d − 1) 2

(1 − t)2

M1

m1

.

Hence 

E[|∂t⟨grad x log pt(X1 | x), Log x(X1)⟩| | Xt = x] ≤ (1 − t)π · 16 π2(d − 1) 2

(1 − t)2

M1

m1

= 16 π3(d − 1) 2

(1 − t)

M1

m1

.

Last, we bound T4. Similarly, by Cauchy-Schwarz and the bound we did for T3,

E[⟨grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x) | Xt = x]

≤



E[|⟨ grad x log pt(X1 | x), Log x(X1)⟩| 2 | Xt = x]

1/2

E[|∂t log pt(X1 | x)|2 | Xt = x]

1/2

.

We bound the first factor by 

E[|⟨ grad x log pt(X1 | x), Log x(X1)⟩| 2 | Xt = x]

≤(1 − t)2π2E[∥ grad x log pt(X1 | x)∥2 | Xt = x] ≤ 32 π2(d − 1) 2 M1

m1

.

Therefore 

T4 = 11 − t E[⟨grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x) | Xt = x]

≤32 π2 (d − 1) 2

(1 − t)2

M1

m1

.

Together, 

| ddt div v(t, x )| ≤ T1 + T2 + T3 + T4

61 ≤ 16 π(d − 1) (1 − t)3

M1

m1

+ 16 π2(d − 1) 2

(1 − t)3

M1

m1

+ 16 π3(d − 1) 2

(1 − t)2

M1

m1

+ 32 π2 (d − 1) 2

(1 − t)2

M1

m1

≤ 128 π2(d − 1) 2

(1 − t)3

M1

m1

.

D.4 Regularity of v(t, x ) and log pt

Finally, we bound ∥v(t, x )∥ and E[∥ grad log pt(Xt)∥2] in Lemma D.14 and D.15. 

Lemma D.14. Assume d ≥ 2 and t ∈ (0 , 1) . Let p1 be a smooth density on Sd such that 

0 < m 1 ≤ p1(z) ≤ M1 < ∞.

For every x ∈ Sd,

∥v(t, x )∥ ≤ π. 

Proof. Fix x ∈ Sd. By Jensen and the definition of v,

∥v(t, x )∥ = 11 − t

Z

> Sd

Log x(x1) pt(x1 | x) dV g(x1) ≤ 11 − t

Z

> Sd

∥ Log x(x1)∥ pt(x1 | x) dV g(x1).

On the support of pt(· | x) we have d(x, x 1) < (1 − t)π, hence 

∥ Log x(x1)∥ = d(x, x 1) ≤ (1 − t)π. 

Therefore 

∥v(t, x )∥ ≤ 11 − t

Z

> Sd

(1 − t)π p t(x1 | x) dV g(x1) = π

Z

> Sd

pt(x1 | x) dV g(x1) = π, 

since pt(· | x) is a probability density. 

Lemma D.15. Assume d ≥ 3 and t ∈ (0 , 1) . Let p0 ≡ Vol (Sd)−1 be the uniform prior on Sd, and let p1 be a smooth density on Sd such that 

0 < m 1 ≤ p1(z) ≤ M1 < ∞.

Let X0 ∼ p0 and X1 ∼ p1 be independent, and define the geodesic interpolation 

Xt = Exp X0 (t Log X0 (X1)) .

Let pt denote the marginal density of Xt with respect to dV g. Then 

E∥ grad log pt(Xt)∥2 ≤ 8( d − 1) 2

(1 − t)2

M1

m1

.

62 Proof. Using Lemma D.2, notice that log pt(x) = log Zt(x) − log Vol ( Sd), we have grad log pt(x) = grad log Zt(x).

Take gradient, we obtain grad Zt(x) = 

Z

> Sd

p1(x1) grad x Jt(x | x1) dV g(x1).

Using grad x Jt = Jt grad x log Jt and the definition of the conditional density 

pt(x1 | x) = p1(x1)Jt(x | x1)

Zt(x) ,

we obtain grad log Zt(x) = grad Zt(x)

Zt(x) =

Z

> Sd

grad x log Jt(x | x1) pt(x1 | x) dV g(x1)= Egrad x log Jt(x | X1) | Xt = x.

Now we have grad log pt(Xt) = E[grad x log Jt(Xt | X1) | Xt],

which implies 

∥ grad log pt(Xt)∥2 ≤ E[∥ grad x log Jt(Xt | X1)∥2 | Xt].

Taking expectation again gives 

E[∥ grad log pt(Xt)∥2] ≤ E

h

E[∥ grad x log Jt(Xt | X1)∥2 | Xt]

i

≤ 8( d − 1) 2

(1 − t)2

M1

m1

.

where the last inequality follows from Lemma D.6. 

D.5 Finiteness of Score Regularity: Proof of Proposition 5.3 

Proof. [Proof of Proposition 5.3] Notice that 

E∥ grad log pt(Xt)∥2 =

Z

> M

∥ grad log pt(x)∥2 pt(x) dV g(x) =: I(pt),

i.e., it is exactly the (Riemannian) Fisher information of pt. Consider the continuity equation 

∂tpt + div ( pt v(t, ·)) = 0. Let st := grad log pt denote the score. We first compute time derivative of I. Differentiate and use the ordinary product rule: 

ddt I(pt) = 

Z

> M

∂t

 ⟨st, s t⟩ pt dV g +

Z

> M

⟨st, s t⟩ ∂tpt dV g

= 2 

Z

> M

∂tst, s t pt dV g +

Z

> M

∥st∥2 ∂tpt dV g.

63 Observe that 

∂t log pt = − 1

pt

div ( ptv) = − div v − 1

pt

⟨grad pt, v ⟩

= − div v − ⟨ grad log pt, v ⟩ = − div v − ⟨ st, v ⟩.

Taking the gradient yields 

∂tst = grad ( ∂t log pt) = − grad (div v) − grad  ⟨st, v ⟩.

We compute the terms in the time derivative of I(pt) separately. For the first term, 2

Z

> M

∂tst, s t pt dV g = 2 

Z

> M

− grad (div v) − grad  ⟨st, v ⟩, s t pt dV g

= −2

Z

> M

grad (div v), s t pt dV g − 2

Z

> M

grad  ⟨st, v ⟩, s t pt dV g

= −2

Z

> M

grad (div v), s t pt dV g − 2

Z

> M

⟨∇ st st, v ⟩pt + ⟨∇ st v, s t⟩pt dV g.

For the second term, 

Z

> M

∥st∥2 ∂tpt dV g = −

Z

> M

∥st∥2 div ( ptv) dV g =

Z

> M

grad ∥st∥2, v pt dV g

= 2 

Z

> M

∇vst, s t pt dV g.

Since Hess is symmetric, we have 

∇vst, s t = ⟨∇ v grad log pt, s t⟩ =  Hess log pt

(v, s t) = ⟨∇ st grad log pt, v ⟩ = ∇st st, v .

Together, ddt I(pt) = −2

Z

> M

⟨st, ∇st v⟩ pt dV g − 2

Z

> M

⟨grad (div v), s t⟩ pt dV g.

Now we derive an ODE that helps to bound I. Using Cauchy–Schwarz and the pointwise bounds 

∥∇ v(t, x )∥op ≤ Lv,x t and ∥ grad div v(t, x )∥ ≤ Ldiv ,x t (from Assumption 2), we obtain 

ddt I(pt) ≤ 2Lv,x t I(pt) + 2 Ldiv ,x t

Z

> M

∥st(x)∥ pt(x) dV g(x)

≤ 2Lv,x t I(pt) + 2 Ldiv ,x t

pI(pt). (16) Let y(t) := pI(pt). Since I′ = 2 yy ′, (16) implies 

y′(t) ≤ Lv,x t y(t) + Ldiv ,x t .

It remains to solve the ODE and obtain a finite upper bound. Define 

A(t) := 

Z t

> 0

Lv,x s ds, μ(t) := e−A(t).

Then μ is absolutely continuous and 

μ′(t) = −Lv,x t e−A(t) = −Lv,x t μ(t) for a.e. t. 

64 Multiply by μ(t): 

μ(t) y′(t) ≤ μ(t) Lv,x t y(t) + μ(t) Ldiv ,x t .

Using the product rule and the identity for μ′(t), 

ddt 

 μ(t) y(t) = μ′(t) y(t) + μ(t) y′(t)= −Lv,x t μ(t) y(t) + μ(t) y′(t)

≤ − Lv,x t μ(t) y(t) + μ(t) Lv,x t y(t) + μ(t) Ldiv ,x t

= μ(t) Ldiv ,x t .

Hence, for a.e. t,

ddt 

 μ(t) y(t) ≤ μ(t) Ldiv ,x t .

Integrating over [0 , t ] yields 

μ(t) y(t) − μ(0) y(0) ≤

Z t

> 0

μ(s) Ldiv ,x s ds. 

Since μ(0) = e−A(0) = 1, we obtain 

μ(t) y(t) ≤ y(0) + 

Z t

> 0

e−A(s) Ldiv ,x s ds. 

Multiply both sides by eA(t):

y(t) ≤ eA(t)



y(0) + 

Z t

> 0

e−A(s) Ldiv ,x s ds 



= exp 

Z t

> 0

Lv,x s ds 

 

y(0) + 

Z t

> 0

Ldiv ,x s exp 



−

Z s

> 0

Lv,x r dr 



ds 



.

Therefore, for all t < 1, 

pI(pt) ≤ exp 

Z t

> 0

Lv,x s ds 

 pI(p0) + 

Z t

> 0

Ldiv ,x s exp 



−

Z s

> 0

Lv,x r dr 



ds 



,

and hence 

E∥ grad log pt(Xt)∥2 = I(pt) ≤ Lscore 

> t

for some finite number Lscore  

> t

.

# E SPD Manifold Regularity Results 

In this section, we work on the SPD manifold with affine invariant metric, denoted as SPD ( n). We briefly discuss the similarity and difference between the regularity analysis in this section and that of the hypersphere. 65 • On a hypersphere, there exist cut points, resulting indicator function in the conditional density. We provide the formula for the conditional density function, on a general Hadamard manifold, see Lemma E.1. We see that with non-positive curvature, there would be no cut points. Consequently, there would be no indicator function in the expression of the conditional density, guaranteeing better smoothness. 

• The hypersphere is a compact manifold, but SPD (n) is non-compact. On a compact manifold, the vector field v(t, x ) itself is uniformly bounded. But on a non-compact manifold, ∥v(t, x )∥

could possibly blow up. Therefore, on SPD (n), it’s unlikely that regularity will hold pointwise, but we can still expect its norm to be bounded, in expectation. 

• Moreover, from a high-level idea, the procedure for providing upper bounds on derivatives of 

v(t, x ) remains the same. For example, in Lemma D.12, we expanded the gradient of div v(t, x )as terms involving E[∥ grad x div x Log x(X1)∥ | Xt = x], E[∥ grad x log pt(X1 | x)∥2 | Xt = x], just to name a few. To establish regularity on SPD (n), we still have roughly the same expansion, involving the same collection of terms. The difference is that, instead of obtain uniform upper bounds (as in Lemma D.6, D.7, D.8, and D.9), our bounds for SPD (n) will depend on radial distance function r(x) = d(x, x 0) for some x0.Throughout this section, we use the following notation: κ := √−Kmin > 0 and the model function is defined as 

sKmin (r) := 1

κ sinh( κr ), r ≥ 0.

Define the radial contraction map and its inverse by Φt,x 1 (x) := Exp x1

 (1 − t) Log x1 (x),

Ψt,x 1 (x) := Exp x1

 11 − t Log x1 (x)



.

and note that under our geodesic interpolation, xt = Φ t,x 1 (x0), and x0 = Ψ t,x 1 (x). We first provide the density formula. 

Lemma E.1 (Density of pt(x1 | x) on a Hadamard manifold) . Let (M, g ) be a complete, simply-connected, d-dimensional Riemannian manifold with non-positive curvature. Let p0, p 1 be the prior and target distributions, assuming independence. Then the conditional density of X1 given Xt = x is 

pt(x1 | x) = p1(x1) p0

 Ψt,x 1 (x) Jt(x | x1)

Z

> M

p1(z) p0

 Ψt,z (x) Jt(x | z) dV g(z)

,

where Jt(x | x1) = (1 − t)−d det( d Exp x1 ) 11−t Log x1 (x)

det( d Exp x1 )Log x1 (x)

.

Furthermore, we have 

pt(x) = 

Z

> M

p1(z) p0

 Ψt,z (x) Jt(x | z) dV g(z).

Proof. 

Notice that Φ t,x 1 is a global diffeomorphism, and Ψ t,x 1 is its inverse. For fixed x1 and x, the change-of-variables formula gives 

dV g(Ψ t,x 1 (x)) = det( dΨt,x 1 )x dV g(x),

66 which implies 

p(Ψ t,x 1 (x), x 1) dV g(Ψ t,x 1 (x)) dV g(x1) = p0

 Ψt,x 1 (x) p1(x1) det( dΨt,x 1 )x dV g(x) dV g(x1).

We define 

Jt(x | x1) := det( dΨt,x 1 )x .

Equivalently, the joint measure of ( Xt, X 1) can be written as 

pt(x, x 1) dV g(x) dV g(x1) = p0

 Ψt,x 1 (x) p1(x1) Jt(x | x1) dV g(x) dV g(x1),

Integrating the joint density over x1 yields the marginal 

pt(x) = 

Z

> M

p1(z) p0

 Ψt,z (x) Jt(x | z) dV g(z).

Therefore, by Bayes’ rule, 

pt(x1 | x) = pt(x, x 1)

pt(x) = p1(x1) p0

 Ψt,x 1 (x) Jt(x | x1)

Z

> M

p1(z) p0

 Ψt,z (x) Jt(x | z) dV g(z)

.

It remains to compute Jt(x | x1). Recall that Ψ t,x 1 (x) = Exp x1

 11−t Log x1 (x)



. Using chain rule, (dΨt,x 1 )x = ( d Exp x1 )v ◦  (1 − t)−1Id  ◦ (d Log x1 )x.

Using ( d Log x1 )x =  (d Exp x1 )Log x1 (x)

−1, we have (dΨt,x 1 )x = ( d Exp x1 ) 11−t Log x1 (x) ◦  (1 − t)−1Id  ◦  (d Exp x1 )Log x1 (x)

−1.

Taking determinants, 

Jt(x | x1) = ( dΨt,x 1 )x = (1 − t)−d det( d Exp x1 ) 11−t Log x1 (x)

det( d Exp x1 )Log x1 (x)

.

Now we prove Proposition B.6, which verifies Assumption 4. 

Proof. [Proof of Proposition B.6] The desired result is directly implied by Lemma E.4 and Lemma E.6: 

E[∥v(t, x )∥2] ≲E[d(x0, x 1)2] ≲ d, 

E[∥∇ v(t, x )∥] ≲ d

1 − t LRE[d(x0, x 1)2] 12 E[sKmin (d(x0, x 1)) 6] 12

≲ d2+6 λ

1 − t LRM 12 

> λ1

,

E[| ddt v(t, x )|] ≲ d

1 − t LRE[d(x0, x 1)2sKmin (d(x0, x 1)) 6] 12 E[d(x0, x 1)2] 12

67 ≲ d2+6 λ

1 − t LRM 12 

> λ1

,

E[∥ grad x div v(t, x )∥] ≲E[d(x0, x 1)sKmin (d(x0, x 1)) 6] d2

(1 − t)2 L3 

> R

≲ d3+12 λ

(1 − t)2 L3

> R

,

E[| ddt div v(t, x )|] ≲E[d(x0, x 1)2] 12

d2

(1 − t)2 L3

> R

E[d(x1, x 0)2sKmin (d(x0, x 1)) 12 ] 12

≲ d3+12 λ

(1 − t)2 L3

> R

M 12 

> λ1

,

and 

E[∥∇ v(t, x )∥2] ≲ d2

(1 − t)2 L2

> R

E[d(x0, x 1)4] 12 E[sKmin (d(x0, x 1)) 12 ] 12 ≲ d3+12 λ

(1 − t)2 L2

> R

M 12 

> λ1

,

E[| ddt v(t, x )|2] ≲ d2

(1 − t)2 L2

> R

E[d(x0, x 1)4sKmin (d(x0, x 1)) 12 ] 12 E[d(x0, x 1)4] 12 ≲ d3+12 λ

(1 − t)2 L2

> R

M 12 

> λ1

,

E[∥ grad x div v(t, x )∥2] ≲E[d(x0, x 1)2sKmin (d(x0, x 1)) 12 ] d4

(1 − t)4 L6 

> R

≲ d5+24 λ

(1 − t)4 L6

> R

Mλ1 .

E.1 Proof of Proposition 5.4 

We start to prove the score regularity result. In the following Lemma, we decompose E∥ grad log pt(Xt)∥2

as the sum of two terms. 

Lemma E.2. For Xt ∼ pt,

E∥ grad log pt(Xt)∥2 ≤ 2 E

h

grad x log p0(Ψ t,X 1 (Xt)) 2i

+ 2 E

h

grad x log Jt(Xt | X1) 2i

.

Proof. By Lemma E.1, 

pt(x) = 

Z

> M

p1(z) p0

 Ψt,z (x) Jt(x | z) dV g(z).

Taking the Riemannian gradient with respect to x and differentiating under the integral sign, grad pt(x) = 

Z

> M

p1(z) grad x



p0(Ψ t,z (x)) Jt(x | z)



dV g(z). (17) Using the identity grad ( ϕ) = ϕ grad log ϕ, we have grad x



p0(Ψ t,z (x)) Jt(x | z)



= p0(Ψ t,z (x)) Jt(x | z) grad x log 



p0(Ψ t,z (x)) Jt(x | z)



.

Plugging into (17) and dividing by pt(x) yields grad log pt(x) = grad pt(x)

pt(x) =

Z

> M

grad x log 



p0(Ψ t,z (x)) Jt(x | z)

 p1(z) p0(Ψ t,z (x)) Jt(x | z)

pt(x) dV g(z).

Observe that 

p1(z) p0(Ψ t,z (x)) Jt(x | z)

pt(x) = p1(z) p0(Ψ t,z (x)) Jt(x | z)

R 

> M

p1(z) p0

 Ψt,z (x) Jt(x | z) dV g(z) = pt(z | x).

68 Hence we have grad log pt(x) = grad log pt(x) = 

Z

> M

grad x log 



p0(Ψ t,z (x)) Jt(x | z)



pt(z | x) dV g(z)= E

h

grad x log  p0(Ψ t,X 1 (x)) Jt(x | X1) | Xt = x

i

.

Next, apply Jensen’s inequality, 

E[∥ grad log pt(x)∥2] = E

h

∥E[grad x log  p0(Ψ t,X 1 (x)) Jt(x | X1) | Xt = x]∥2i

≤ E

h

E[∥ grad x log  p0(Ψ t,X 1 (x)) Jt(x | X1)∥2 | Xt = x]

i

= E

h

∥ grad x log  p0(Ψ t,X 1 (x)) Jt(x | X1)∥2i

.

The result follows from grad x log  p0(Ψ) Jt

 = grad x log p0(Ψ) + grad x log Jt, ∥a + b∥2 ≤ 2∥a∥2 + 2 ∥b∥2.

Now we prove Proposition 5.4. 

Proof. [Proof of Proposition 5.4] From Lemma E.2, we have 

E∥ grad log pt(Xt)∥2 ≤ 2 E

h

grad x log p0(Ψ t,X 1 (Xt)) 2i

+ 2 E

h

grad x log Jt(Xt | X1) 2i

.

We show the following in Appendix E.3 (Lemma E.7 and Lemma E.8): 

∥ grad x log Jt(x | x1)∥ ≤ d ∥(d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

grad x log p0(Ψ t,X 1 (Xt)) ≤2dd (x0, z ) (d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op .

We also prove the following in Lemma E.10 and Lemma E.12: 

∥(d Log x1 )x∥ ≤ 1,

(d Exp x1 ) 11−t Log x1 (x) op ≤ sKmin (∥ 11−t Log x1 (x)∥)/∥ 11−t Log x1 (x)∥ = sKmin (d(x0, x 1)) /d (x0, x 1),

∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ ≤ 16 3 sKmin (d(x0, x 1)/2) 2LRsKmin (d(x0, x 1)) .

So together, 

∥ grad x log Jt(x | x1)∥ ≲ d

1 − t LRsKmin (d(x0, x 1)) 3,

grad x log p0(Ψ t,X 1 (Xt)) ≲ d

1 − t sKmin (d(x0, x 1)) d(x0, z ).

Using Lemma E.4, we have 

E∥ grad log pt(Xt)∥2 ≲ d2

(1 − t)2 L2

> R

E[sKmin (d(x0, x 1)) 6] ≲ d2

(1 − t)2 L2

> R

E[sKmin (d(x0, x 1)) 6]

≲ d2+12 λ

(1 − t)2 L2

> R

M, 

where λ = max {1, κ }.69 E.2 Auxiliary Results for Regularity for Hadamard and SPD manifolds 

E.2.1 Expectation Control 

Due to the non-compact nature of a Hadamard manifold as well as curvature distortion, bounding expectations of the form 

E[d(X0, X 1)2 sKmin 

 d(X0, X 1)a], a ∈ N,

is a key step to establish regularity. We show that by choosing X0 following a Riemannian Gaussian distribution, the above expectation can be controlled for a class of data distribution X1 satisfying certain moment condition (7). 

Lemma E.3. Let (M, g ) be a complete, simply connected d-dimensional Riemannian manifold (Hadamard) and assume 

Kmin ≤ sec ≤ 0, where Kmin < 0 is independent of d. 

Fix a basepoint z ∈ M and set r(x) := d(x, z ). Define 

p0(x) = 1

Z e(−β r (x)m), Z := 

Z

> M

e−β r (x)m

dV g(x), where β = d, m = 2 .

Let λ ≥ 0 be independent of d. Then for b ∈ { 0, 1, 2, 3, 4},

Ep0

r(x)beλr (x) = O(d2λ(log d)b).

Proof. We first write the expectation as an integral over tangent space. Since M is Hadamard, Exp z : Tz M → M is a global C∞ diffeomorphism. We equip the vector space Tz M with the inner product g, and denote by dv the corresponding Lebesgue measure. For any measurable 

F : M → [0 , ∞], the change-of-variables formula gives 

Z

> M

F (x) dV g(x) = 

Z 

> TzM

F (Exp z (v)) det  d(Exp z )v

 dv. 

We now apply this formula with F1(x) = e−d r (x)2

, F2(x) = r(x)beλr (x)e−d r (x)2

. Since r(Exp z (v)) = 

∥v∥, we obtain 

Ep0 [rbeλr ] = 

R  

> TzM

∥v∥beλ∥v∥e−d∥v∥2

det( d(Exp z )v) dv 

R  

> TzM

e−d∥v∥2 det( d(Exp z )v) dv . (18) Recall that using comparison theorem (note that the d − 1 comes from det) we have 1 ≤ det( d(Exp z )rθ ) ≤

 sKmin (r)

r

d−1

, ∀r > 0. (19) Since sKmin (r) 

> r

= sinh( κr ) 

> κr

≤ eκr , we have 

det( d(Exp z )rθ ) ≤ eκ(d−1) r. (20) We now integrate in Tz M using Euclidean polar coordinates: v = rθ with r ∈ [0 , ∞), θ ∈ Sd−1,and 

dv = rd−1 dr dθ. 

70 We split at R = 2 log d and bound the expectation. Consider the decomposition below 

Ep0 [rbeλr ] = Ep0 [rbeλr 1{r≤R}] + Ep0 [rbeλr 1{r>R }].

We first consider the central part. On {r ≤ R}, rbeλr ≤ RbeλR , hence 

Ep0 [rbeλr 1{r≤R}] ≤ RbeλR = (2 log d)b d2λ. (21) We next consider the tail part. Write the tail contribution as Ntail /Z . Using change of variable formula and the upper bound (20), we obtain 

Ntail := 

Z 

> {x:r(x)>R }

r(x)beλr (x)e−dr (x)2

dV g(x)=

Z 

> {v:∥v∥>R }

∥v∥beλ∥v∥e−d∥v∥2

det( d(Exp z )v) dv 

≤ | Sd−1|

Z ∞

> R

rd−1+ be(−dr 2+( λ+κ(d−1)) r) dr. (22) Define Ψd(r) := −dr 2 + ( λ + κ(d − 1)) r + ( d − 1 + b) log r. 

We justify that this Ψ d(r) is dominated by the −dr 2 term. For r ≥ R = 2 log d, we have (d − 1 + b) log r ≤ d 

> 4

r2, ∀d ≥ 4. Also, for r ≥ R and d large, κ(d − 1) r ≤ d 

> 4

r2 because this is equivalent to r ≥ 4κ(1 − 1/d ), which holds once R ≥ 4κ (i.e. d ≥ e2κ). Finally, λr ≤ d 

> 8

r2 holds for all r ≥ R

once d ≥ 8λ. Summing these inequalities yields, there exists some constant d1 s.t. for all d ≥ d1

and all r ≥ R,(λ + κ(d − 1)) r + ( d − 1 + b) log r ≤ d

2 r2, hence Ψd(r) ≤ − d

2 r2.

Therefore, for d ≥ d1,

Z ∞

> R

eΨd(r) dr ≤

Z ∞

> R

e−(d/ 2) r2

dr =

Z ∞

> R

1

rd (− ddr e−(d/ 2) r2

) dr ≤ 1

Rd 

Z ∞

> R

(− ddr e−(d/ 2) r2

) dr 

= 1

dR exp 



− d

2 R2

= 1

dR exp   − 2d(log d)2.

Plugging into (22) gives 

Ntail ≤ | Sd−1| · 1

dR exp   − 2d(log d)2. (23) For the denominator Z, recall that det( d(Exp z )v) ≥ 1, so we get 

Z 

> TzM

e−d∥v∥2

det( d(Exp z )v) dv ≥

Z 

> TzM

e−d∥v∥2

dv =

 πd

d/ 2

.

Hence, using (23), 

Ep0 [rbeλr 1{r>R }] = Ntail 

Z ≤ |Sd−1|

dR exp   − 2d(log d)2 ·

 dπ

d/ 2

= |Sd−1|

2d log d d−2d log d ·

 dπ

d/ 2

≲ 1.

Together, we conclude that for all d ≥ d1, combining (21) with the tail bound gives 

Ep0 [rbeλr ] ≤ (2 log d)bd2λ + 1 = O(d2λ(log d)b).

Now we present intermediate steps. 71 Lemma E.4 (Auxiliary bounds for expectation of sKmin ). Let M be a Hadamard manifold. Fix 

z ∈ M . Choose prior as X0 ∼ p0 where 

p0(x) ∝ exp   − d d (x, z )2.

Assume Assumption 1 holds. Let κ = √−Kmin and λ0 = a max {1, κ }, where 0 ≤ a ≤ a0 is some constant. Assume 

max E[d(X1, z )4eλ1d(X1,z )], E[eλ1d(X1,z )] ≤ M, where λ1 = a1 max {1, κ }.

Then we have, for b ∈ { 0, 1, 2, 3, 4},

E

h

d(X0, X 1)b sKmin 

 d(X0, X 1)ai

≲ M EX0∼p0

h

d(X0, z )beλ0d(X0,z )i

≲ d2λ0 (log d)bM, 

E

h

d(X0, z )b sKmin 

 d(X0, X 1)ai

≲ M EX0∼p0

h

d(X0, z )beλ0d(X0,z )i

≲ d2λ0 (log d)bM. 

Proof. Observe that 

sKmin (d(x0, x 1)) a = sinh a(κd (x0, x 1)) 

κa ≤ (emax {1,κ }d(x0,x 1))a.

Hence 

d(x0, x 1)b sKmin 

 d(x0, x 1)a ≤ d(x0, x 1)b(emax {1,κ }d(x0,x 1))a. (24) Taking expectation, it suffices to bound Ed(x0, x 1)beλ0d(x0,x 1) with λ0 = a max {1, κ }.The triangle inequality gives 

d(x0, x 1) ≤ d(x0, z ) + d(x1, z ).

Therefore, 

d(x0, x 1)beλ0d(x0,x 1) ≤ d(x0, z ) + d(x1, z )b exp  λ0(d(x0, z ) + d(x1, z )) 

≲(d(x0, z )b + d(x1, z )b) exp( λ0d(x0, z )) exp( λ0d(x1, z )) .

Take expectation and use independence of X0 and X1:

E

h

d(X0, X 1)beλ0d(X0,X 1)i

≤ 2 E

h

eλ0d(X1,z )i

EX0∼p0

h

d(X0, z )beλ0d(X0,z )i

+ 2 E

h

d(X1, z )beλ0d(X1,z )i

EX0∼p0

h

eλ0d(X0,z )i

. (25) By Assumption, both E

h

eλ0d(X1,z )

i

and E

h

d(X1, z )beλ0d(X1,z )

i

are controlled by M . It hence follows that 

E

h

d(X0, X 1)beλ0d(X0,X 1)i

≲ M EX0∼p0

h

d(X0, z )beλ0d(X0,z )i

≲ d2λ0 (log d)bM, 

where we applied Lemma E.3. 72 E.2.2 Regularity Control 

The following lemma summarizes the building blocks needed to establish regularity, serving as the same purpose as Lemma D.6, D.7, D.8, and D.9. 

Lemma E.5. On SPD (n), with prior distribution being p0 ∝ exp (−dd (x0, z )2), assume Assumption 1, we have the following bounds. 

∥ grad x log Jt(x | x1)∥ ≲d 11 − t LRsKmin (d(x0, x 1)) 3,

grad x log p0(Ψ t,x 1 (x)) ≲d 11 − t sKmin (d(x0, x 1)) d(x0, z ),

|∂t log Jt(x | x1)| ≲ d

1 − t d(x0, x 1)sKmin (d(x0, x 1)) 3LR,

|∂t log p0(Ψ t,x 1 (x)) | ≲ d

1 − t d(x0, z )sKmin (d(x0, x 1)) ,

∥∇ 2 

> x

log p0(Ψ t,x 1 (x)) ∥op ≲ d

(1 − t)2 sKmin (d(x0, x 1)) 4LRd(x0, z ),

∥∇ 2 

> x

log Jt(x | x1)∥op ≲ d

(1 − t)2 L3

> R

sKmin (d(x0, x 1)) 6,

∂t grad x log p0(Ψ t,x 1 (x)) ≲d(x0, z ) d

(1 − t)2 d(x0, x 1)LRsKmin (d(x0, x 1)) 3,

∥∂t grad x log Jt(x | x1)∥ ≲ d

(1 − t)2 L3

> R

d(x1, x 0)sKmin (d(x0, x 1)) 6,

∥∇ x Log x(x1)∥2op ≲d(x0, x 1)2,

| div x Log x(x1)|2 ≲d2d(x0, x 1)2,

∥ grad div x Log x(x1)∥ ≲dd (x0, x 1) 32 .

Here we emphasize that Log x(x1) is viewed as a vector field, for fixed x1, so ∇ Log x(x1) is covariant derivative of the vector field Log x(x1). Consequently, 

E[∥ grad x log pt(x1 | x)∥2 | Xt = x] ≲ d2

(1 − t)2 L2

> R

E[sKmin (d(x0, x 1)) 6],

E[∥ grad x log pt(x1 | x)∥4 | Xt = x] ≲ d4

(1 − t)4 L4

> R

E[sKmin (d(x0, x 1)) 12 ],

E[|∂t log pt(x1 | x)|2 | Xt = x] ≲ d2

(1 − t)2 L2

> R

E[d(x0, x 1)2sKmin (d(x0, x 1)) 6],

E[|∂t log pt(x1 | x)|4 | Xt = x] ≲ d4

(1 − t)4 L4

> R

E[d(x0, x 1)4sKmin (d(x0, x 1)) 12 ],

E[∥∇ 2 

> x

log pt(x1 | x)∥2op | Xt = x] ≲ d2

(1 − t)4 L6

> R

E[sKmin (d(x0, x 1)) 12 ]+ d4

(1 − t)4 L4

> R

E[sKmin (d(x0, x 1)) 6]2,

E[∥∇ 2 

> x

log pt(x1 | x)∥4op | Xt = x] ≲ d4

(1 − t)8 L12  

> R

E[sKmin (d(x0, x 1)) 24 ]+ d8

(1 − t)8 L8

> R

E[sKmin (d(x0, x 1)) 12 ]2,

73 E[∥∂t grad x log pt(x1 | x)∥2 | Xt = x] ≲ d2

(1 − t)4 L6

> R

E[d(x1, x 0)2sKmin (d(x0, x 1)) 12 ]+ d4

(1 − t)4 L4

> R

E[sKmin (d(x0, x 1)) 6]E[d(x0, x 1)2sKmin (d(x0, x 1)) 6].

Proof. Throughout the proof, we use r(x) to denote the radial distance function d(x, z ), where z

is the center of p0 ∝ exp (−βd (x0, z )2). We show the following in Appendix E.3 (Lemma E.7 and Lemma E.8): 

∥ grad x log Jt(x | x1)∥ ≤ d ∥(d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

grad x log p0(Ψ t,X 1 (Xt)) ≤2dd g(x0, z ) (d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op .

We also prove the following in (Lemma E.10 and Lemma E.12: 

∥(d Log x1 )x∥ ≤ 1,

 ∇(d Log x) 

> y

≤  ∇(d Exp x)  

> Log x(y)

,

(d Exp x1 ) 11−t Log x1 (x) op ≤ sKmin (∥ 11−t Log x1 (x)∥)/∥ 11−t Log x1 (x)∥ ≲ sKmin (d(x0, x 1)) ,

∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ ≤ 16 3 sKmin (d(x0, x 1)/2) 2LRsKmin (d(x0, x 1)) ≲ LRsKmin (d(x0, x 1)) 3,

∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥ ≲ L3

> R

sKmin (d(x0, x 1)) 5.

We estimate the terms as follows. For gradient and time derivative, we have 

∥ grad x log Jt(x | x1)∥ ≲d 11 − t LRsKmin (d(x0, x 1)) 3,

grad x log p0(Ψ t,X 1 (Xt)) ≲d 11 − t sKmin (d(x0, x 1)) d(x0, z ),

and 

|∂t log Jt(x | x1)| ≤ d

1 − t + d

(1 − t)2 ∥ Log x1 (x)∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

≲ d

1 − t d(x0, x 1)sKmin (d(x0, x 1)) 3LR,

|∂t log p0(Ψ t,x 1 (x)) | ≤ 2βr (d Exp x1 ) 11−t Log x1 (x) op 

1(1 − t)2 Log x1 (x)

≲ d

1 − t d(x0, z )sKmin (d(x0, x 1)) .

For second covariant derivative, 

∥∇ 2 

> x

log p0(Ψ t,x 1 (x)) ∥op 

≤



(d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op 

2

2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x)



+ ∇(d Exp x1 ) 11−t Log x1 (x)

 11 − t (d Log x1 )x op 

2

2βr 

74 + (d Exp x1 ) 11−t Log x1 (x) op 

11 − t ∇(d Log x1 )x 2βr 

≲ 1(1 − t)2 sKmin (d(x0, x 1)) 2(dd (x0, z )κ coth( κd (x0, z ))) + d

(1 − t)2 sKmin (d(x0, x 1)) 3LRd(x0, z ) + d

1 − t d(x0, z )sKmin (d(x0, x 1)) 4LR

≲ d

(1 − t)2 sKmin (d(x0, x 1)) 4LRd(x0, z ),

where notice that x coth( x) is of order x. And we have 

∥∇ 2 

> x

log Jt(x | x1)∥op 

≤d ∥(d Log x1 )x∥2

"

1(1 − t)2



∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2 + ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥



+



∥∇ (d Exp x1 )Log x1 (x)∥2 + ∥∇ 2(d Exp x1 )Log x1 (x)∥

#

+ d ∥∇ (d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

≲ d

(1 − t)2 (L2

> R

sKmin (d(x0, x 1)) 6 + L3

> R

sKmin (d(x0, x 1)) 5) + d

1 − t L2

> R

sKmin (d(x0, x 1)) 6

≲ d

(1 − t)2 L3

> R

sKmin (d(x0, x 1)) 6.

For time derivative of gradient, 

∂t grad x log p0(Ψ t,x 1 (x)) 

≤



(d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op 



×



2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x)



× (d Exp x1 ) 11−t Log x1 (x) op 

1(1 − t)2 Log x1 (x)

+

"

1(1 − t)2 (d Exp x1 ) 11−t Log x1 (x) op (d Log x1 )x op 

+ 1(1 − t)3 Log x1 (x) ∇(d Exp x1 ) 11−t Log x1 (x) (d Log x1 )x op 

#

2βr 

≲



d + dd (x0, z )



sKmin (d(x0, x 1)) 2 1(1 − t)2

+ dd (x0, z )

 1(1 − t)2 sKmin (d(x0, x 1)) + 1(1 − t)2 d(x0, x 1)LRsKmin (d(x0, x 1)) 3

≲d(x0, z ) d

(1 − t)2 d(x0, x 1)LRsKmin (d(x0, x 1)) 3.

And we have 

∥∂t grad x log Jt(x | x1)∥

75 ≤∥ (d Log x1 )x∥

"

d

(1 − t)2 ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

+ d

(1 − t)3 ∥ Log x1 (x)∥



∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2 + ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥

#

≲ d

(1 − t)2 LRsKmin (d(x0, x 1)) 3 + d

(1 − t)2 d(x1, x 0)



L2

> R

sKmin (d(x0, x 1)) 6 + L3

> R

sKmin (d(x0, x 1)) 5

≲ d

(1 − t)2 L3

> R

d(x1, x 0)sKmin (d(x0, x 1)) 6.

The three inequalities on Log follow from Appendix E.5: 

∥∇ x Log x(y)∥2op ≤ 2 + 2 d(x, y )2 s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) 

2

,

| div x Log x(y)|2 ≤ 2 + 2( d − 1) 2 d(x, y )2 s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) 

2

,

∥ grad div x Log x(y)∥ ≤ √2d

2 (2  1 + d(x, y ) s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) 

) 32 .

Now we prove the inequalities on derivatives of log pt. We remark that due to Lemma E.4, comparing point-wise upper bounds for derivative of log Jt and that of log p0, since the d(x0, z )term doesn’t increase the order of expectation compared with d(x0, x 1) term, we can treat d(x0, z )as d(x0, x 1) and see that derivatives of log Jt dominate over the same derivative of log p0. Thus in computing derivatives of log pt = log p0(Ψ t,x 1 (x)) + log Jt(x|x1) + const , it suffices to consider derivatives of log Jt(x|x1) terms. The first four inequalities are straightforward by applying the Cauchy-Schwarz inequality, and note that they are dominated by the log Jt terms. Following Lemma D.8 

∥∇ 2 

> x

log pt(x1 | x)∥2op ≲∥∇ 2 log Jt(x | X1)∥2op + E∥∇ 2 log Jt(x | X1)∥2op | Xt = x

+ E∥ grad x log Jt(x | X1)∥2 | Xt = xE∥ grad log pt(x1 | x)∥2 | Xt = x,

∥∇ 2 

> x

log pt(x1 | x)∥4op ≲∥∇ 2 log Jt(x | X1)∥4op 

+ E∥ grad x log Jt(x | X1)∥4 | Xt = xE∥ grad log pt(x1 | x)∥4 | Xt = x,

and plug in the corresponding expression (using Cauch-Schwarz), we obtain the bounds for the Hessian of log pt, and following Lemma D.9, 

∥∂t grad x log pt(x1 | x)∥ ≤ ∥ ∂t grad x log Jt(x | x1)∥ + E∥∂t grad x log Jt(x | X1)∥ | Xt = x

+ E[∥ grad x log Jt(x | X1)∥ | ∂t log pt(X1 | x)| | Xt = x],

we plug in the corresponding expressions (using Cauch-Schwarz), we obtain the last inequality. We summarize the required bound in the following Lemma, which is similar to Lemma D.10, D.11, D.12 and D.13. 

Lemma E.6. On SPD (n), with prior distribution being p0 ∝ exp (−dd (x0, z )2), under the conditions in Assumption 1, we have 

E[∥v(t, x )∥2] ≲E[d(x0, x 1)2],

76 E[∥∇ v(t, x )∥] ≲ d

1 − t LRE[d(x0, x 1)2] 12 E[sKmin (d(x0, x 1)) 6] 12 ,

E[| ddt v(t, x )|] ≲ d

1 − t LRE[d(x0, x 1)2sKmin (d(x0, x 1)) 6] 12 E[d(x0, x 1)2] 12 ,

E[∥ grad x div v(t, x )∥] ≲E[d(x0, x 1)sKmin (d(x0, x 1)) 6] d2

(1 − t)2 L3

> R

,

E[| ddt div v(t, x )|] ≲E[d(x0, x 1)2] 12

d2

(1 − t)2 L3

> R

E[d(x1, x 0)2sKmin (d(x0, x 1)) 12 ] 12 .

Furthermore, 

E[∥∇ v(t, x )∥2] ≲ d2

(1 − t)2 L2

> R

E[d(x0, x 1)4] 12 E[sKmin (d(x0, x 1)) 12 ] 12 ,

E[| ddt v(t, x )|2] ≲ d2

(1 − t)2 L2

> R

E[d(x0, x 1)4sKmin (d(x0, x 1)) 12 ] 12 E[d(x0, x 1)4] 12 ,

E[∥ grad x div v(t, x )∥2] ≲E[d(x0, x 1)2sKmin (d(x0, x 1)) 12 ] d4

(1 − t)4 L6

> R

.

Proof. Throughout the proof, we will use the bounds in Lemma E.5.We first control the vector field regularity. This is similar to Lemma D.10 and Lemma D.11. We have 

E[∥∇ v(t, x )∥] ≤ 11 − t E∥ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥ | Xt = x

+ 11 − t E[∥∇ Log x(x1)∥ | Xt = x]

≲ d

1 − t LRE[d(x0, x 1)2] 12 E[sKmin (d(x0, x 1)) 6] 12 ,

and 

E[| ddt v(t, x )|] = 1(1 − t)2 E[Log x(x1) | Xt = x] + 11 − t E[Log x(x1) ∂t log pt(x1 | x) | Xt = x]

≲ d

1 − t LRE[d(x0, x 1)2sKmin (d(x0, x 1)) 6] 12 E[d(x0, x 1)2] 12 .

Now we control the divergence regularity. Similar to Lemma D.12 and , we have 

∥ grad x div v(t, x )∥ ≤ 11 − t



T1 + T2 + T3 + T4



,

where 

E[T1] = E[∥ grad x div x Log x(X1)∥]

≲dE[d(x0, x 1) 32 ],

E[T2] = E[| div x Log x(X1)| ∥ grad x log pt(X1 | x)∥]

≲E[| div x Log x(X1)|2] 12 E[∥ grad x log pt(X1 | x)∥2] 12

≲ d2

1 − t LRE[d(x0, x 1)2] 12 E[sKmin (d(x0, x 1)) 6] 12 ,

E[T3] = E[∥ grad x⟨grad x log pt(X1 | x), Log x(X1)⟩∥ ]77 ≤E[∥∇ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥]+ E[∥∇ 2 log pt(X1 | x)∥ ∥ Log x(X1)∥]

≲



E[d(x, x 1)2] d2

(1 − t)2 L2

> R

E[sKmin (d(x0, x 1)) 6]

 12

+

 d2

(1 − t)4 L6

> R

E[sKmin (d(x0, x 1)) 12 ]E[d(x, x 1)2]

 12

≲ d

1 − t L3

> R

E[sKmin (d(x0, x 1)) 12 ] 12 E[d(x0, x 1)2] 12 ,

E[T4] = E[|⟨ grad x log pt(X1 | x), Log x(X1)⟩| ∥ grad x log pt(X1 | x)∥]

≤E[∥ Log x(X1)∥ ∥ grad x log pt(X1 | x)∥2]

≲E[d(x, x 1)sKmin (d(x0, x 1)) 6] d2

(1 − t)2 L2

> R

≲E[d(x0, x 1)sKmin (d(x0, x 1)) 6] d2

1 − t L2

> R

.

Similar to Lemma D.13, 

ddt div v(t, x ) = 1(1 − t)2 Ediv x Log x(X1) + ⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x

+ 11 − t E[div x Log x(X1) ∂t log pt(X1 | x) | Xt = x]+ 11 − t E[∂t⟨grad x log pt(X1 | x), Log x(X1)⟩ | Xt = x]+ 11 − t E[⟨grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x) | Xt = x].

We bound 

E[T1] = 1(1 − t)2 Ediv x Log x(X1) + ⟨grad x log pt(X1 | x), Log x(X1)⟩

≲ d

(1 − t)2 E[d(x, x 1)] + 1(1 − t)2 E[d(x, x 1)2] 12 E[ d2

(1 − t)2 L2

> R

sKmin (d(x0, x 1)) 6] 12

≲ d

1 − t E[d(x0, x 1)] + d

(1 − t)2 LRE[d(x0, x 1)2] 12 E[sKmin (d(x0, x 1)) 6]] 12 ,

E[T2] = 11 − t E[div x Log x(X1) ∂t log pt(X1 | x)] 

≲ 11 − t dE[d(x, x 1)2] 12

d

1 − t LRE[d(x0, x 1)2sKmin (d(x0, x 1)) 6] 12

≲ d2

1 − t LRE[d(x0, x 1)2] 12 E[d(x0, x 1)2sKmin (d(x0, x 1)) 6] 12 ,

E[T3] = 11 − t E[∂t⟨grad x log pt(X1 | x), Log x(X1)⟩]

≲E[d(x0, x 1)2] 12

d

(1 − t)2 L3

> R

E[d(x1, x 0)2sKmin (d(x0, x 1)) 12 ] 12 ,

E[T4] = 11 − t E[⟨grad x log pt(X1 | x), Log x(X1)⟩ ∂t log pt(X1 | x)] 

≲ 11 − t ( d3

(1 − t)3 L3

> R

E[sKmin (d(x0, x 1)) 9]) 13 E[d(x, x 1)3] 13 ( d3

(1 − t)3 L3

> R

E[d(x0, x 1)3sKmin (d(x0, x 1)) 9]) 13

78 ≲ d2

(1 − t)2 L2

> R

E[sKmin (d(x0, x 1)) 9] 13 E[d(x0, x 1)3] 13 E[d(x0, x 1)3sKmin (d(x0, x 1)) 9] 13 .

Higher order bounds follow exactly the same procedure. 

E.3 Auxiliary Bounds on Derivatives 

Recall that we are using the following notation Ψt,x 1 (x) = Exp x1

 11−t Log x1 (x)



.

Lemma E.7 (Controlling derivatives of log p0). Let M be a Hadamard manifold. Fix β > 0, m = 2 ,and z ∈ M , and define 

p0(x) ∝ exp   − β d (x, z )m.

Fix t ∈ [0 , 1) , x1 ∈ M , and x ∈ M . Define r := d(Ψ t,x 1 (x), z ), so that r represent the radial distance between x0 and z. Then the following bounds hold: 

grad x log p0Ψt,x 1 (x) ≤2βr (d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op ,

|∂t log p0(Ψ t,x 1 (x)) | ≤ 2βr (d Exp x1 ) 11−t Log x1 (x) op 

1(1 − t)2 Log x1 (x) ,

∥∇ 2 

> x

log p0(Ψ t,x 1 (x)) ∥op ≤



(d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op 

2



2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x)



+ ∇(d Exp x1 ) 11−t Log x1 (x)

 11 − t (d Log x1 )x op 

2

2βr 

+ (d Exp x1 ) 11−t Log x1 (x) op 

11 − t ∇(d Log x1 )x 2βr, 

∂t grad x log p0(Ψ t,x 1 (x)) ≤



(d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op 



×



2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x)



× (d Exp x1 ) 11−t Log x1 (x) op 

1(1 − t)2 Log x1 (x)

+

"

1(1 − t)2 (d Exp x1 ) 11−t Log x1 (x) op (d Log x1 )x op 

+ 1(1 − t)3 Log x1 (x) ∇(d Exp x1 ) 11−t Log x1 (x) (d Log x1 )x op 

#

2βr. 

where 

∥∇ 2r∥op Ψt,x 1 (x) ≤ s′ 

> Kmin

(r)

sKmin (r) = p−Kmin coth( rp−Kmin ).

and note that r 7 → r cosh( r) has no singularity at r = 0 .

79 Proof. We start with deriving bounds for ∥ grad log p0(Ψ t,x 1 (x)) ∥ and ∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op .Since log p0(y) = −β d (y, z )m + const, and ∥ grad d( · , z )∥ = 1, we have 

∥ grad log p0(Ψ t,x 1 (x)) ∥ = 2 βr. 

Moreover, notice that 

∇2(d(y, z )m) = 2 d(y, z )∇2d(y, z ) + 2 grad d(y, z ) ⊗ grad d(y, z ).

The above identity, together with ∥ grad d(y, z ) ⊗ grad d(y, z )∥op = 1, yields 

∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op ≤ 2β + 2 β r ∥∇ 2r∥op Ψt,x 1 (x).

We next derive the first (spatial) derivative of Ψ t,x 1 . Using the chain rule, (dΨt,x 1 )x[u] = ( d Exp x1 ) 11−t Log x1 (x)

h 11−t (d Log x1 )x[u]

i

, ∀ u ∈ TxM. 

Hence 

∥(dΨt,x 1 )x∥op ≤ (d Exp x1 ) 11−t Log x1 (x) op 11−t (d Log x1 )x op . (26) For any u ∈ TxM , the chain rule gives the identity 

grad x log p0(Ψ t,x 1 (x)) , u = grad log p0(Ψ t,x 1 (x)) , (dΨt,x 1 )x[u] .

Therefore, by Cauchy-Schwarz with supremum over ∥u∥ = 1, yields 

⟨grad x log p0(Ψ t,x 1 (x)) , u ⟩ ≤ ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∥ (dΨt,x 1 )x[u]∥≤ ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∥ (dΨt,x 1 )x∥op .

Substituting ∥ grad log p0(Ψ t,x 1 (x)) ∥ = 2 βr and using (26), we obtain 

grad x log p0



Exp x1

  11−t Log x1 (x)

≤ 2βr (d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op .

Now we work on the first time derivative. Recall Ψ t,x 1 (x) = Exp x1

 11−t Log x1 (x)



. By chain rule, 

∂t log p0(Ψ t,x 1 (x)) = grad log p0(Ψ t,x 1 (x)) , ∂ tΨt,x 1 (x) ,

where ∂tΨt,x 1 (x) = ( d Exp x1 ) 11−t Log x1 (x)

h 1(1 −t)2 Log x1 (x)

i

. By Cauchy-Schwarz, 

|∂t log p0(Ψ t,x 1 (x)) | ≤ ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∥ ∂tΨt,x 1 (x)∥≤ ∥ grad log p0(Ψ t,x 1 (x)) ∥ (d Exp x1 ) 11−t Log x1 (x) op 1(1 −t)2 ∥ Log x1 (x)∥≤ 2βr (d Exp x1 ) 11−t Log x1 (x) op 

1(1 − t)2 Log x1 (x) ,

80 where recall ∥ grad log p0(Ψ t,x 1 (x)) ∥ = 2 βr .Now we compute the Hessian. For any u, w ∈ TxM , using chain rule, we obtain 

∇2 

> x

log p0(Ψ t,x 1 (x))[ u, w ] = ∇2 log p0(Ψ t,x 1 (x)) (Ψ t,x 1 )x[u], (Ψ t,x 1 )x[w]

+ grad log p0(Ψ t,x 1 (x)) , ∇u

 (Ψ t,x 1 )x[w] .

By Cauchy-Schwarz and taking the supremum over ∥u∥ = ∥w∥ = 1, we obtain 

∥∇ 2 

> x

log p0(Ψ t,x 1 (x)) ∥op = sup 

> ∥u∥=∥w∥=1

∇2 

> x

log p0(Ψ t,x 1 (x))[ u, w ]

≤ sup 

> ∥u∥=∥w∥=1

∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op ∥(Ψ t,x 1 )x[u]∥ ∥ (Ψ t,x 1 )x[w]∥

+ ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∇u

 (Ψ t,x 1 )x[w]

≤∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op ∥(Ψ t,x 1 )x∥2op + ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∥∇ (Ψ t,x 1 )x∥.

It remains to bound ∥∇ (Ψ t,x 1 )x∥. Recall that (Ψ t,x 1 )x[w] = ( d Exp x1 ) 11−t Log x1 (x)

h 11−t (d Log x1 )x[w]

i

.

Recall the Leibniz rule with v = 11−t Log x1 (x) being a x-dependent tangent vector at Tx1 M .

∇u

 (Ψ t,x 1 )x[w] = ∇u



(d Exp x1 )v

h 11−t (d Log x1 )x[w]

i 

=



∇∇uv(d Exp x1 )



> v

h 11−t (d Log x1 )x[w]

i

+ ( d Exp x1 )v

h 11−t ∇u

 (d Log x1 )x[w]i

.

where note that v depends on x, so using chain rule, 

∇u(( d Exp x1 )v) = ∇u(( d Exp x1 ) 11−t Log x1 (x))=  ∇(d Exp x1 ) 11−t Log x1 (x)

(∇u 11−t Log x1 (x)) =



∇∇u 11−t Log x1 (x)(d Exp x1 )

   

> 11−tLog x1(x)

.

Notice that for v = 11−t Log x1 (x), its first derivative is a directional derivative. We have 

∇uv = 11−t (d Log x1 )x[u], and ∇u

 (d Log x1 )x[w] = ∇(d Log x1 )x[u, w ].

Taking norms and applying the definitions of the operator norms gives 

∇u

 (Ψ t,x 1 )x[w] ≤ ∇(d Exp x1 )v ∇uv 11−t (d Log x1 )x[w]

+ (d Exp x1 )v op 11−t ∇(d Log x1 )x[u, w ]

= ∇(d Exp x1 )v 11−t (d Log x1 )x[u] 11−t (d Log x1 )x[w]

+ (d Exp x1 )v op 11−t ∇(d Log x1 )x[u, w ] .

Taking supremum over ∥u∥ = ∥w∥ = 1, we obtain 

∇ (dΨt,x 1 )x

 ≤ ∇(d Exp x1 )v

 11−t (d Log x1 )x op 

2

+ (d Exp x1 )v op 11−t ∇(d Log x1 )x .

81 Finally using the fact that 

∥ grad log p0(Ψ t,x 1 (x)) ∥ = 2 βr, 

∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op ≤ 2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x),

and using (26), we have 

∥∇ 2 

> x

log p0(Ψ t,x 1 (x)) ∥op 

≤∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op ∥(Ψ t,x 1 )x∥2op + ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∥∇ (Ψ t,x 1 )x∥≤



(d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op 

2

2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x)



+ ∇(d Exp x1 ) 11−t Log x1 (x)

 11 − t (d Log x1 )x op 

2

2βr 

+ (d Exp x1 ) 11−t Log x1 (x) op 

11 − t ∇(d Log x1 )x 2βr. 

It remains to consider the mixed derivative term. For any u ∈ TxM , recall we have 

grad x log p0(Ψ t,x 1 (x)) , u = grad log p0(Ψ t,x 1 (x)) , (Ψ t,x 1 )x[u] .

Differentiate both sides in t (with x1, x, u fixed): 

∂t grad x log p0(Ψ t,x 1 (x)) , u =

D

∇2 log p0(Ψ t,x 1 (x)) ∂tΨt,x 1 (x), (Ψ t,x 1 )x[u]

E

+ grad log p0(Ψ t,x 1 (x)) , ∂ t

 (Ψ t,x 1 )x[u] .

By Cauchy-Schwarz, 

⟨∂t grad x log p0(Ψ t,x 1 (x)) , u ⟩ ≤ ∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op ∥∂tΨt,x 1 (x)∥ ∥ (Ψ t,x 1 )x∥op ∥u∥

+ ∥ grad log p0(Ψ t,x 1 (x)) ∥ ∂t

 (Ψ t,x 1 )x[u] .

We already have ∥∂tΨt,x 1 (x)∥ and ∥(Ψ t,x 1 )x∥op from previous steps. It remains to bound ∥∂t((Ψ t,x 1 )x[u]) ∥. Recall for any u ∈ TxM ,(Ψ t,x 1 )x[u] = ( d Exp x1 )v

h 11−t (d Log x1 )x[u]

i

, v = 11−t Log x1 (x).

Fix x1, x, u and differentiate with respect to t. Since x is fixed, the tensor ( d Log x1 )x[u] does not depend on t; only the scalar factor 11−t and the point v depend on t. By the Leibniz rule (product rule for a t-dependent linear map applied to a t-dependent vector), 

∂t

 (Ψ t,x 1 )x[u] = ∂t



(d Exp x1 )v

h 11−t (d Log x1 )x[u]

i

+ ( d Exp x1 )v

h

∂t

 11−t (d Log x1 )x[u]

i 

.

For the first term, ( d Exp x1 )v depends on t only through v(t), hence the chain rule gives 

∂t



(d Exp x1 )v



=



∇(d Exp x1 )v



(∂tv) = 



∇∂tv(d Exp x1 )



> v

.

For the second term, since ( d Log x1 )x[u] is t-independent, 

∂t

 11−t (d Log x1 )x[u]



= 1(1 −t)2 (d Log x1 )x[u].

82 Combining these identities yields the formula 

∂t

 (Ψ t,x 1 )x[u] =



∇∂tv(d Exp x1 )



> v

h 11−t (d Log x1 )x[u]

i

+ ( d Exp x1 )v

h 1(1 −t)2 (d Log x1 )x[u]

i

.

Noting ∂tv = 1(1 −t)2 Log x1 (x), we take norms and obtain (for ∥u∥ = 1) 

∂t

 (Ψ t,x 1 )x[u]

≤ ∇(d Exp x1 )v ∥∂tv∥ 11−t (d Log x1 )x[u]

+ (d Exp x1 )v op 1(1 −t)2 (d Log x1 )x op ∥u∥

= ∇(d Exp x1 )v 1(1 −t)2 ∥ Log x1 (x)∥ 11−t (d Log x1 )x op 

+ (d Exp x1 )v op 1(1 −t)2 (d Log x1 )x op 

= 1(1 −t)3 ∥ Log x1 (x)∥ ∇(d Exp x1 )v (d Log x1 )x op 

+ 1(1 −t)2 (d Exp x1 )v op (d Log x1 )x op .

Substituting the expressions for ∥ grad log p0(Ψ t,x 1 (x)) ∥ and ∥∇ 2 log p0(Ψ t,x 1 (x)) ∥op , we get 

∂t grad x log p0



Exp x1

  11−t Log x1 (x)

≤



(d Exp x1 ) 11−t Log x1 (x) op 

11 − t (d Log x1 )x op 



×



2β + 2 βr ∥∇ 2r∥op Ψt,x 1 (x)



× (d Exp x1 ) 11−t Log x1 (x) op 

1(1 − t)2 Log x1 (x)

+

"

1(1 − t)2 (d Exp x1 ) 11−t Log x1 (x) op (d Log x1 )x op 

+ 1(1 − t)3 Log x1 (x) ∇(d Exp x1 ) 11−t Log x1 (x) (d Log x1 )x op 

#

2βr. 

Finally, we remark that when Kmin < 0, the Hessian comparison theorem gives 

∥∇ 2r∥op Ψt,x 1 (x) ≤ s′ 

> Kmin

(r)

sKmin (r) = p−Kmin coth( rp−Kmin ).

Lemma E.8 (Controlling derivatives of Jt-terms) . Assume M is a Hadamard manifold. Fix x1 ∈ M ,

x ∈ M , and t ∈ (0 , 1) . Recall 

Jt(x | x1) = (1 − t)−d det( d Exp x1 ) 11−t Log x1 (x)

det( d Exp x1 )Log x1 (x)

,

log Jt(x | x1) = −d log(1 − t) + log det( d Exp x1 ) 11−t Log x1 (x) − log det( d Exp x1 )Log x1 (x).

83 Then the following pointwise bounds hold. 

∥ grad x log Jt(x | x1)∥ ≤ d ∥(d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

∥∇ 2 

> x

log Jt(x | x1)∥op ≤d ∥(d Log x1 )x∥2

"

1(1 − t)2



∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2

+ ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥



+



∥∇ (d Exp x1 )Log x1 (x)∥2 + ∥∇ 2(d Exp x1 )Log x1 (x)∥

#

+ d ∥∇ (d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

|∂t log Jt(x | x1)| ≤ d

1 − t + d

(1 − t)2 ∥ Log x1 (x)∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥,

∥∂t grad x log Jt(x | x1)∥ ≤∥ (d Log x1 )x∥

"

d

(1 − t)2 ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

+ d

(1 − t)3 ∥ Log x1 (x)∥



∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2 + ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥

#

.

Proof. We repeatedly use Lemma E.16 with A(·) chosen to be ξ 7 → (d Exp x1 )ξ, and the chain rule through Log x1 .We first estimate the gradient. From log Jt(x | x1) = −d log(1 − t) + log det( d Exp x1 ) 11−t Log x1 (x) − log det( d Exp x1 )Log x1 (x),

the constant −d log(1 − t) has zero x-gradient, hence grad x log Jt(x | x1) = grad x log det( d Exp x1 ) 11−t Log x1 (x) − grad x log det( d Exp x1 )Log x1 (x).

Consider the first term. Differentiating the map x 7 → 11−t Log x1 (x) yields a factor 11−t (d Log x1 )x.Thus, 

grad x log det( d Exp x1 ) 11−t Log x1 (x)

≤ 11 − t ∥(d Log x1 )x∥ grad ξ log det( d Exp x1 )ξ    

> ξ=11−tLog x1(x)

.

Now apply Lemma E.16 (specifically, the gradient bound in (33)) at A(ξ) = ( d Exp x1 )ξ to get 

grad ξ log det( d Exp x1 )ξ ≤ d ∥(d Exp x1 )−1 

> ξ

∥ ∥∇ (d Exp x1 )ξ∥.

Combining the above, we obtain 

grad x log det( d Exp x1 ) 11−t Log x1 (x)

84 ≤ d

1 − t ∥(d Log x1 )x∥ ∥ (d Exp x1 )−1 

> 11−t

Log x1 (x)∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥.

The same argument (without the factor 11−t ) yields 

grad x log det( d Exp x1 )Log x1 (x) ≤ d ∥(d Log x1 )x∥ ∥ (d Exp x1 )−1Log x1 (x)∥ ∥∇ (d Exp x1 )Log x1 (x)∥.

Finally, apply the triangle inequality, we get 

∥ grad x log Jt(x | x1)∥ ≤ d ∥(d Log x1 )x∥ 11 − t ∥(d Exp x1 )−1 

> 11−t

Log x1 (x)∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

+ ∥(d Exp x1 )−1Log x1 (x)∥ ∥∇ (d Exp x1 )Log x1 (x)∥

!

.

Now we estimate the Hessian. Write f (x) := log det (d Exp x1 ) 11−t Log x1 (x). Taking second derivative (with product rule) we have 

∥∇ 2

xf (x)∥op ≤ ∇2

ξ log det( d Exp x1 )ξ op ξ= 11−t Log x1 (x)

· 11 − t (d Log x1 )x

2

+ grad ξ log det( d Exp x1 )ξ

ξ= 11−t Log x1 (x)

· ∇

 11 − t (d Log x1 )x



.

Thus we have (here ∇2

ξ represent taking derivative w.r.t. ξ)

∥∇ 2

x log Jt(x | x1)∥op 

≤∥ (d Log x1 )x∥2 1(1 − t)2 ∇2

ξ log det( d Exp x1 )ξ op ξ= 11−t Log x1 (x)

+ ∇2

ξ log det( d Exp x1 )ξ op ξ=Log x1 (x)

!

+ ∥∇ (d Log x1 )x∥ 11 − t grad ξ log det( d Exp x1 )ξ

ξ= 11−t Log x1 (x)

+ grad ξ log det( d Exp x1 )ξ

ξ=Log x1 (x)

!

≤d ∥(d Log x1 )x∥2

"

1(1 − t)2



∥(d Exp x1 )−1 

> 11−t

Log x1 (x)∥2 ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2

+ ∥(d Exp x1 )−1 

> 11−t

Log x1 (x)∥ ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥



+



∥(d Exp x1 )−1Log x1 (x)∥2 ∥∇ (d Exp x1 )Log x1 (x)∥2 + ∥(d Exp x1 )−1Log x1 (x)∥ ∥∇ 2(d Exp x1 )Log x1 (x)∥

#

+ d ∥∇ (d Log x1 )x∥ 11 − t ∥(d Exp x1 )−1 

> 11−t

Log x1 (x)∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

+ ∥(d Exp x1 )−1Log x1 (x)∥ ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

where in the first inequality we used triangle inequality, and in the second inequality we applied Lemma E.16. 85 For time derivative, since x is fixed and only 11−t depends on t,

∂t log Jt(x | x1) = d

1 − t + ∂t



log det( d Exp x1 ) 11−t Log x1 (x)



.

By the chain rule, 

∂t



log det( d Exp x1 ) 11−t Log x1 (x)



= D log det( d Exp x1 )ξ ξ= 11−t Log x1 (x)

h

∂t

 11 − t Log x1 (x)

i 

.

But ∂t

  11−t Log x1 (x) = 1(1 −t)2 Log x1 (x), hence 

∂t



log det( d Exp x1 ) 11−t Log x1 (x)



≤ 1(1 − t)2 ∥ Log x1 (x)∥ grad ξ log det( d Exp x1 )ξ    

> ξ=11−tLog x1(x)

.

Apply Lemma E.16 (gradient bound (33)) at ξ = 11−t Log x1 (x), we obtain 

|∂t log Jt(x | x1)| ≤ d

1 − t + d

(1 − t)2 ∥ Log x1 (x)∥ ∥ (d Exp x1 )−1   

> 11−tLog x1(x)

∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥.

For mixed derivative, recall grad x log Jt(x | x1) = grad x log det( d Exp x1 ) 11−t Log x1 (x) − grad x log det( d Exp x1 )Log x1 (x).

Only the first term depends on t. Differentiate the first term: there are two contributions, one from differentiating the prefactor 11−t and one from differentiating grad ξ log det (d Exp x1 )ξ at ξ = 

> 11−t

Log x1 (x). This yields 

∥∂t grad x log det( d Exp x1 ) 11−t Log x1 (x)∥≤∥ (d Log x1 )x∥

"

1(1 − t)2 grad ξ log det( d Exp x1 )ξ    

> ξ=11−tLog x1(x)

+ 11 − t ∂t



grad ξ log det( d Exp x1 )ξ

    

> ξ=11−tLog x1(x)

#

.

Next, note that 

∂t(grad ξ log det( d Exp x1 )ξ) = ∇2 

> ξ

log det( d Exp x1 )ξ [∂tξ]with ∂tξ = 1(1 −t)2 Log x1 (x). Hence 

∂t



grad ξ log det( d Exp x1 )ξ



≤ 1(1 − t)2 ∥ Log x1 (x)∥ ∇2 

> ξ

log det( d Exp x1 )ξ op .

By Lemma E.16, we obtain 

∥∂t grad x log Jt(x | x1)∥≤∥ (d Log x1 )x∥

"

1(1 − t)2 d ∥(d Exp x1 )−1   

> 11−tLog x1(x)

∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

86 + 1(1 − t)3 ∥ Log x1 (x)∥ d



∥(d Exp x1 )−1   

> 11−tLog x1(x)

∥2 ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2

+ ∥(d Exp x1 )−1   

> 11−tLog x1(x)

∥ ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥

#

.

Finally, by Lezcano-Casado (2020, Theorem 3.12) with r = ∥ Log x1 (x)∥ 

> 1−t

, and noting that on SPD ( n) sectional curvature is upper bounded by zero, we have 

∥(d Exp x1 )−1   

> 11−tLog x1(x)

∥ ≤ 1.

Hence we get 

∥ grad x log Jt(x | x1)∥ ≤ d ∥(d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

∥∇ 2 

> x

log Jt(x | x1)∥op ≤d ∥(d Log x1 )x∥2

"

1(1 − t)2



∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2 + ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥



+



∥∇ (d Exp x1 )Log x1 (x)∥2 + ∥∇ 2(d Exp x1 )Log x1 (x)∥

#

+ d ∥∇ (d Log x1 )x∥ 11 − t ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥ + ∥∇ (d Exp x1 )Log x1 (x)∥

!

,

|∂t log Jt(x | x1)| ≤ d

1 − t + d

(1 − t)2 ∥ Log x1 (x)∥ ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥,

∥∂t grad x log Jt(x | x1)∥ ≤∥ (d Log x1 )x∥

"

d

(1 − t)2 ∥∇ (d Exp x1 ) 11−t Log x1 (x)∥

+ d

(1 − t)3 ∥ Log x1 (x)∥



∥∇ (d Exp x1 ) 11−t Log x1 (x)∥2 + ∥∇ 2(d Exp x1 ) 11−t Log x1 (x)∥

#

,

completing the proof. 

E.4 Bounding  ∇2d Exp p

 

> rv op

In this section, we bound the third derivative of Exp . We first recall some notation in Lezcano-Casado (2020), which allows us to write the third derivative as some ODE. Then we apply some comparison theorem to derive the upper bound. We remark that on a symmetric space, the (covariant) derivative of the curvature tensor is identically zero, see for example (Lee, 2018, Theorem 10.19). The techniques in this section works for non-symmetric spaces, but as we are working on SPD ( n), we assume M is a symmetric space, which would simplify our computation. Recall that ∇ denotes the (Levi–Civita) connection on T M over M . Let ∇flat be the flat connection on the vector space TpM . Set Exp p : U ⊂ TpM → M on a normal neighborhood of p.87 We first introduce pullback connection. Instead of differentiating along a vector field on M , we can also differentiate along a vector field X on TpM . Define the corresponding connection as 

 ∇pullback  

> X

Y (u) = ∇ (d Exp p)u(X(u)) Y, 

In other words, given u ∈ TpM , and we have q := Exp p(u). Then differentiate along X in the tangent space via the pullback connection, evaluated at u ∈ TpM , is equivalent to differentiate along 

d Exp p(X) (which can be viewed as a vector field on M ) evaluated at q.Recall the pullback bundle Exp ∗

> p

(T M ) is a vector bundle over the base TpM . Concretely, Exp ∗

> p

(T M ) := {(u, v ) : u ∈ TpM, v ∈ T M with base point Exp p(u)}.

For example, given an element of the tensor product space ( α, (u, v )) ∈ T ∗ 

> p

M ⊗ Exp ∗

> p

(T M ), we have ( α, (u, v ))( w) = α(w)v, where α ∈ T ∗ 

> p

M is a covector, so that α(w) ∈ R.Let {ei} be an orthonormal basis for TpM , and {ei} be the corresponding basis in the cotangent space T ∗ 

> p

M . By definition, 

  X 

> i

ei ⊗ (u, (d Exp p)u[ei]) (w) := X

> i

ei(w)( d Exp p)u[ei],

and by linearity 

X

> i

ei(w)( d Exp p)u[ei] = ( d Exp p)u[X

> i

ei(w)ei] = ( d Exp p)u(w).

Thus we see that d Exp p can be viewed as an element of the tensor product space: (d Exp p)u

X

> i

ei ⊗ (u, (d Exp p)u[ei]) ∈ T ∗ 

> p

M ⊗ Exp ∗

> p

(T M ).

We equip T ∗ 

> p

M with the connection induced by ∇flat and Exp ∗

> p

(T M ) with ∇pullback . These induce a connection on the tensor product bundle, denoted by ∇induced , characterized by the Leibniz rule. Now consider W 1, W 2 being vector fields in the tangent space TpM , and d Exp p(W 2) can be viewed as a section, i.e., d Exp p(W 2) : u 7 → (d Exp p)u(W 2(u)). We differentiate 

∇pullback  

> W1

(d Exp p(W 2)) = ∇pullback  

> W1

(X

> i

ei(W2)( d Exp p)[ ei]) = X

> i

W 1ei(W2)( d Exp p)[ ei] + X

> i

ei(W2)∇pullback  

> W1

(d Exp p)[ ei].

where we emphasize that ( d Exp p)[ ei] is a function of u, similar as before: (d Exp p)[ ei] : u 7 →

(d Exp p)u[ei]. By linearity of ( d Exp p), and definition of flat connection, we have 

X

> i

W 1ei(W2)( d Exp p)[ ei] = ( d Exp p)[ X

> i

W 1ei(W2)ei] = ( d Exp p)[ ∇flat   

> W1

W 2].

On the other hand, since ei are constants, we know ∇flat  

> W1

ei = 0, so that 

ei ⊗ ∇ pullback  

> W1

(d Exp p)[ ei] = ei ⊗ ∇ pullback  

> W1

(d Exp p)[ ei] + ∇flat   

> W1

ei ⊗ (d Exp p)[ ei]88 =∇induced   

> W1

(ei ⊗ (d Exp p)[ ei]) ,

hence 

X

> i

ei(W 2) ⊗ ∇ pullback  

> W1

(( d Exp p)[ ei]) = 

 X 

> i

ei ⊗ ∇ pullback  

> W1

(( d Exp p)[ ei]) 



(W 2)=

 X 

> i

∇induced   

> W1

(ei ⊗ (d Exp p)[ ei]) 



(W 2) = X

> i



∇induced   

> W1

(ei ⊗ (d Exp p)[ ei]) 



(W 2)=( ∇induced   

> W1

(X

> i

ei ⊗ (d Exp p)[ ei]))( W 2) = ( ∇induced   

> W1

d Exp p)( W 2).

Substitute into the previous expression, we obtain 

∇pullback  

> W1

(d Exp p(W 2)) = X

> i

W 1ei(W2)( d Exp p)[ ei] + X

> i

ei(W2)∇pullback  

> W1

(d Exp p)[ ei]=( d Exp p)[ ∇flat   

> W1

W 2] + ∇induced   

> W1

(d Exp p)( W 2)Thus, for vector fields W 1, W 2 on U ⊂ TpM we have 

∇pullback  

> W1

 d Exp p(W 2) =  ∇induced   

> W1

d Exp p

(W 2) + d Exp p

 ∇flat   

> W1

W 2

. (27) Define c(t, s 1, s 2, s 3) = Exp p(t(v + s1w1 + s2w2 + s3w3)), and γ(t) = Exp p(tv ). We briefly recall what Lezcano-Casado (2020) did, to control the second derivative of Exp map. Define 

J1(t) = ( d Exp p)tv (tw 1) to be the Jacobi field along γ with initial condition w1. We can define the extension of J1 in the w2-direction by eJ1(t, s ) := ( d Exp p)t(v+sw 2)(tw 1), where note that 

eJ1(t, 0) = J1(t). For fixed t, the tangent space vector field u 7 → tw 1 is constant on TpM , hence 

∇flat  

> tw 2

(tw 1) = 0 . (28) Applying (27) with W 1 ≡ tw 2 and W 2 ≡ tw 1, and evaluated at t(v + sw 2), we have 

 ∇pullback 

> tw 2

eJ1(t, s )(t(v + sw 2)) =  ∇induced  

> tw 2

d Exp p



> t(v+sw 2)

(tw 1) + ( d Exp p)t(v+sw 2)

 ∇flat  

> tw 2

(tw 1).

By (28) the last term vanishes. Using the fact  ∇pullback  

> X

Y (u) = ∇ (d Exp p)u(X(u)) Y with X = tw 2,

Y = eJ1, u = tv , we have (restricted to s = 0) 

 ∇pullback 

> tw 2

eJ1

(tv ) = ∇ (d Exp p)tv (tw 2) eJ1.

Together, with the previously defined notation J2 = ( d Exp p)tv (tw 2), we have 

∇J2 J1 = ∇J2 eJ1 s=0 = ∇pullback 

> tw 2

eJ1 s=0 =  ∇induced d Exp p



> tv

(tw 1, tw 2).

We therefore define the second-order variation field along γ by 

K12 (t) :=  ∇induced d Exp p



> tv

(tw 1, tw 2) = ∇J2 J1.

Then (Lezcano-Casado, 2020, Proposition 4.1) shows that K12 satisfies ¨K12 + R(K12 , ˙γ) ˙ γ + Y12 = 0 , K12 (0) = 0 , ˙K12 (0) = 0 , (29) 89 with Y12 given explicitly therein. Using the same technique, we can define 

K13 (t) :=  ∇induced d Exp p



> tv

(tw 1, tw 3) = ∇J3 J1, K 23 (t) :=  ∇induced d Exp p



> tv

(tw 2, tw 3) = ∇J3 J2,

satisfying the corresponding ODE as (29). Now we study the third derivative L123 =  (∇induced )2d Exp p

(tw 1, tw 2, tw 3). Define Ji(t) to be the Jacobi field along γ with initial condition wi for i = 2 , 3, in the same way as we did for J1.Extend K12 (t) =  ∇induced d Exp p



> tv

(tw 1, tw 2) = ∇J2 J1 off γ by 

eK12 (t, s 3) :=  ∇induced d Exp p



> t(v+s3w3)

(tw 1, tw 2),

so that eK12 (t, 0) = K12 (t). We then define the third-order variation field along γ by 

L123 (t) := ∇J3 K12 (t) := ∇J3 eK12 (t, s 3) s3=0 . (30) Apply ∇pullback  

> W3

to the first-order Leibniz rule (27) . For vector fields W 1, W 2, W 3 on TpM we obtain, at any u ∈ TpM ,

 ∇pullback  

> W3

∇pullback  

> W1

(d Exp p(W 2)) (u) =  (∇induced   

> W3

∇induced   

> W1

d Exp p)u

(W 2(u)) +  ∇induced   

> W1

d Exp p



> u

 (∇flat   

> W3

W 2)( u)

+  ∇induced   

> W3

d Exp p



> u

 (∇flat   

> W1

W 2)( u)

+ ( d Exp p)u

 (∇flat   

> W3

∇flat   

> W1

W 2)( u). (31) Now specialize (31) to the constant vector fields W 1 ≡ tw 2, W 2 ≡ tw 1, W 3 ≡ tw 3. Since 

∇flat  

> tw i

(tw j ) = 0 for all i, j , the last three terms in (31) vanish. Evaluating at u = tv yields 

L123 (t) =  ∇pullback 

> tw 3

eK12 

(tv ) =  (∇induced  

> tw 3

∇induced  

> tw 2

d Exp p)tv 

(tw 1),

where note that ( d Exp p)tv (tw 3) = J3(t). By definition of the second covariant derivative tensor ( ∇induced )2d Exp p, this is precisely 

L123 (t) =  (∇induced )2d Exp p



> tv

(tw 1, tw 2, tw 3). (32) Throughout this section, we consider covariant derivative along Ji, for example ∇J3 J2, as abbreviation of ∇J3 ˜J2. Here, the ˜· represent the corresponding extension along suitable direction, as constructed above. 

Lemma E.9. For L123 defined above, we have 

¨L123 + R(L123 , ˙γ) ˙ γ + Y123 = 0 , L123 (0) = 0 , ˙L123 (0) = 0 .

When M is a symmetric space, 

Y123 = R( ˙J3, ˙γ)K12 + 2 R(J3, ˙γ) ˙K12 + R(K12 , ˙J3) ˙ γ + R(K12 , ˙γ) ˙J3

+ 2 R(∇J3 J1, ˙γ) ˙J2 + 2 R(J1, ˙J3) ˙J2 + 2 R(J1, ˙γ)∇J3 ˙J2

+ 2 R(∇J3 J2, ˙γ) ˙J1 + 2 R(J2, ˙J3) ˙J1 + 2 R(J2, ˙γ)∇J3 ˙J1.

90 Proof. Now recall the following ODE is staisfied by K (Lezcano-Casado, 2020, Proposition 4.1.). ¨K12 + R(K12 , ˙γ) ˙ γ + Y12 = 0 .

By differentiating the ODE (note that the vector field to which we take covariant derivative, is viewed as the corresponding extension, i.e., ˜K12 for K12 , grad r for ˙γ and ∇grad r∇grad r ˜K12 for ¨K12 ), we obtain 

∇J3 ¨K12 + ∇J3 R(K12 , ˙γ) ˙ γ + ∇J3 Y12 = 0 ,

We first compute ∇J3 ¨K12 . Recall ˙X := ∇ ˙γ X and ¨X := ∇ ˙γ ∇ ˙γ X. Let L123 := ∇J3 K12 . We compute ∇J3 ¨K12 step by step. Recall by definition of curvature tensor 

∇J3 ∇ ˙γ X = ∇ ˙γ ∇J3 X + ∇[J3, ˙γ]X + R(J3, ˙γ)X, 

and the fact that [ J3, ˙γ] = 0 (for the same reason as in Lezcano-Casado (2020, Proposition 4.1)), we have 

∇J3 ¨K12 = ∇J3 ∇ ˙γ ˙K12 = ∇ ˙γ ∇J3 ˙K12 + R(J3, ˙γ) ˙K12 

= ∇ ˙γ ∇J3 ∇ ˙γ K12 + R(J3, ˙γ) ˙K12 

= ∇ ˙γ (∇ ˙γ ∇J3 K12 + R(J3, ˙γ)K12 ) + R(J3, ˙γ) ˙K12 = ∇ ˙γ ∇ ˙γ L123 + ∇ ˙γ (R(J3, ˙γ)K12 ) + R(J3, ˙γ) ˙K12 .

Notice that (by Leibniz rule) 

∇ ˙γ (R(J3, ˙γ)K12 ) = ( ∇ ˙γ R)( J3, ˙γ)K12 + R(∇ ˙γ J3, ˙γ)K12 + R(J3, ∇ ˙γ ˙γ)K12 + R(J3, ˙γ)∇ ˙γ K12 

= ( ∇ ˙γ R)( J3, ˙γ)K12 + R( ˙J3, ˙γ)K12 + R(J3, ˙γ) ˙K12 .

Hence we have 

∇J3 ¨K12 = ¨L123 + ( ∇ ˙γ R)( J3, ˙γ)K12 + R( ˙J3, ˙γ)K12 + 2 R(J3, ˙γ) ˙K12 .

Now we compute ∇J3

 R(K12 , ˙γ) ˙ γ. Note that ∇J3 ˙γ = ∇ ˙γ J3 = ˙J3, by Leibniz rule, 

∇J3

 R(K12 , ˙γ) ˙ γ = ( ∇J3 R)( K12 , ˙γ) ˙ γ + R(∇J3 K12 , ˙γ) ˙ γ + R(K12 , ∇J3 ˙γ) ˙ γ + R(K12 , ˙γ)∇J3 ˙γ

= ( ∇J3 R)( K12 , ˙γ) ˙ γ + R(L123 , ˙γ) ˙ γ + R(K12 , ˙J3) ˙ γ + R(K12 , ˙γ) ˙J3.

Recall 

Y := 2 R(J1, ˙γ) ˙J2 + 2 R(J2, ˙γ) ˙J1 + ( ∇ ˙γ R)( J2, ˙γ)J1 + ( ∇J2 R)( J1, ˙γ) ˙ γ. 

Also recall [ J3, ˙γ] = 0 and ∇J3 ˙γ = ∇ ˙γ J3 = ˙J3. Then, by the Leibniz rule for tensor fields, we have 

∇J3 Y = 2 ∇J3

 R(J1, ˙γ) ˙J2

 + 2 ∇J3

 R(J2, ˙γ) ˙J1



+ ∇J3



(∇ ˙γ R)( J2, ˙γ)J1



+ ∇J3



(∇J2 R)( J1, ˙γ) ˙ γ



,

where each term expands as follows. For the first term, 

∇J3

 R(J1, ˙γ) ˙J2

 = ( ∇J3 R)( J1, ˙γ) ˙J2 + R(∇J3 J1, ˙γ) ˙J2 + R(J1, ∇J3 ˙γ) ˙J2 + R(J1, ˙γ)∇J3 ˙J2

= ( ∇J3 R)( J1, ˙γ) ˙J2 + R(∇J3 J1, ˙γ) ˙J2 + R(J1, ˙J3) ˙J2 + R(J1, ˙γ)∇J3 ˙J2.

91 For the second term, 

∇J3

 R(J2, ˙γ) ˙J1

 = ( ∇J3 R)( J2, ˙γ) ˙J1 + R(∇J3 J2, ˙γ) ˙J1 + R(J2, ∇J3 ˙γ) ˙J1 + R(J2, ˙γ)∇J3 ˙J1

= ( ∇J3 R)( J2, ˙γ) ˙J1 + R(∇J3 J2, ˙γ) ˙J1 + R(J2, ˙J3) ˙J1 + R(J2, ˙γ)∇J3 ˙J1.

For the third term, 

∇J3



(∇ ˙γ R)( J2, ˙γ)J1



= ( ∇J3 ∇ ˙γ R)( J2, ˙γ)J1 + ( ∇ ˙γ R)( ∇J3 J2, ˙γ)J1

+ ( ∇ ˙γ R)( J2, ∇J3 ˙γ)J1 + ( ∇ ˙γ R)( J2, ˙γ)∇J3 J1

= ( ∇J3 ∇ ˙γ R)( J2, ˙γ)J1 + ( ∇ ˙γ R)( ∇J3 J2, ˙γ)J1

+ ( ∇ ˙γ R)( J2, ˙J3)J1 + ( ∇ ˙γ R)( J2, ˙γ)∇J3 J1.

For the last term, 

∇J3



(∇J2 R)( J1, ˙γ) ˙ γ



= ( ∇J3 ∇J2 R)( J1, ˙γ) ˙ γ + ( ∇J2 R)( ∇J3 J1, ˙γ) ˙ γ

+ ( ∇J2 R)( J1, ∇J3 ˙γ) ˙ γ + ( ∇J2 R)( J1, ˙γ)∇J3 ˙γ

= ( ∇J3 ∇J2 R)( J1, ˙γ) ˙ γ + ( ∇J2 R)( ∇J3 J1, ˙γ) ˙ γ

+ ( ∇J2 R)( J1, ˙J3) ˙ γ + ( ∇J2 R)( J1, ˙γ) ˙J3.

Plug in the expressions, we have ¨L123 + R(L123 , ˙γ) ˙ γ + Y123 = 0 ,

where 

Y123 = ( ∇ ˙γ R)( J3, ˙γ)K12 + ( ∇J3 R)( K12 , ˙γ) ˙ γ + R( ˙J3, ˙γ)K12 + 2 R(J3, ˙γ) ˙K12 

+ R(K12 , ˙J3) ˙ γ + R(K12 , ˙γ) ˙J3

+ 2( ∇J3 R)( J1, ˙γ) ˙J2 + 2 R(∇J3 J1, ˙γ) ˙J2 + 2 R(J1, ˙J3) ˙J2 + 2 R(J1, ˙γ)∇J3 ˙J2

+ 2( ∇J3 R)( J2, ˙γ) ˙J1 + 2 R(∇J3 J2, ˙γ) ˙J1 + 2 R(J2, ˙J3) ˙J1 + 2 R(J2, ˙γ)∇J3 ˙J1

+ ( ∇J3 ∇ ˙γ R)( J2, ˙γ)J1 + ( ∇ ˙γ R)( ∇J3 J2, ˙γ)J1 + ( ∇ ˙γ R)( J2, ˙J3)J1 + ( ∇ ˙γ R)( J2, ˙γ)∇J3 J1

+ ( ∇J3 ∇J2 R)( J1, ˙γ) ˙ γ + ( ∇J2 R)( ∇J3 J1, ˙γ) ˙ γ + ( ∇J2 R)( J1, ˙J3) ˙ γ + ( ∇J2 R)( J1, ˙γ) ˙J3.

On a symmetric space, 

Y123 = R( ˙J3, ˙γ)K12 + 2 R(J3, ˙γ) ˙K12 + R(K12 , ˙J3) ˙ γ + R(K12 , ˙γ) ˙J3

+ 2 R(∇J3 J1, ˙γ) ˙J2 + 2 R(J1, ˙J3) ˙J2 + 2 R(J1, ˙γ)∇J3 ˙J2

+ 2 R(∇J3 J2, ˙γ) ˙J1 + 2 R(J2, ˙J3) ˙J1 + 2 R(J2, ˙γ)∇J3 ˙J1.

Now we can apply comparison theory (Lezcano-Casado, 2020, Proposition 4.9) to control ∥L∥.

Lemma E.10. Let (M, g ) be a Riemannian symmetric space , i.e. ∇R ≡ 0. Fix a unit-speed geodesic γ : [0 , r ] → M with ∥ ˙γ∥ ≡ 1. Under Assumption 1, we have 

∥L(t)∥ ≤ 7L2

> R

3 sKmin (s)5 + 16 L3

> R

t2sKmin (t)5 + 20 L2

> R

ts Kmin (t)4.

92 Consequently, 

 (∇)2d Exp p

  

> rv op

= sup 

> ∥w1∥,∥w2∥,∥w3∥≤ 1

 (∇induced )2d Exp p



> rv

(w1, w 2, w 3)

= sup 

> ∥w1∥,∥w2∥,∥w3∥≤ 1

1

r3 ∥L123 (r)∥ ≤ 7L2

> R

3r3 sKmin (r)5 + 16 L3

> R

1

r sKmin (r)5 + 20 L2

> R

1

r2 sKmin (r)4.

Proof. Recall 

Y123 = R( ˙J3, ˙γ)K12 + 2 R(J3, ˙γ) ˙K12 + R(K12 , ˙J3) ˙ γ + R(K12 , ˙γ) ˙J3

+ 2 R(∇J3 J1, ˙γ) ˙J2 + 2 R(J1, ˙J3) ˙J2 + 2 R(J1, ˙γ)∇J3 ˙J2

+ 2 R(∇J3 J2, ˙γ) ˙J1 + 2 R(J2, ˙J3) ˙J1 + 2 R(J2, ˙γ)∇J3 ˙J1.

By definition of K13 , K 23 , we have ∇J3 J1 = K13 and ∇J3 J2 = K23 . Next we express ∇J3 ˙Ji in terms of ˙Ki3 and curvature. By definition of R and [ J3, ˙γ] = 0, 

∇J3 ˙Ji = ∇J3 ∇ ˙γ Ji = ∇ ˙γ ∇J3 Ji + R(J3, ˙γ)Ji

= ˙Ki3 + R(J3, ˙γ)Ji.

Therefore 2R(J1, ˙γ)∇J3 ˙J2 = 2 R(J1, ˙γ) ˙K23 + 2 R(J1, ˙γ) R(J3, ˙γ)J2

,

2R(J2, ˙γ)∇J3 ˙J1 = 2 R(J2, ˙γ) ˙K13 + 2 R(J2, ˙γ) R(J3, ˙γ)J1

.

Substituting these and using R(x, y )z ≤ LR∥x∥∥ y∥∥ z∥, we have 

∥Y123 ∥ ≤ 3LR∥K12 ∥∥ ˙J3∥ + 2 LR∥J3∥∥ ˙K12 ∥ + 2 LR∥K13 ∥∥ ˙J2∥ + 2 LR∥K23 ∥∥ ˙J1∥

+ 2 LR∥J1∥∥ ˙K23 ∥ + 2 LR∥J2∥∥ ˙K13 ∥ + 2 LR∥J1∥∥ ˙J2∥∥ ˙J3∥ + 2 LR∥J2∥∥ ˙J1∥∥ ˙J3∥

+ 4 L2

> R

∥J1∥∥ J2∥∥ J3∥≤ 7LR∥K∥∥ ˙J∥ + 6 LR∥J∥∥ ˙K∥ + 4 LR∥J∥∥ ˙J∥2 + 4 L2

> R

∥J∥3.

Using Lemma E.11, (Lezcano-Casado, 2020, Theorem 3.12, Theorem 4.11) we have 

∥K(t)∥ ≤ 16 3 sKmin (t/ 2) 2LRsKmin (t),

∥ ˙K(t)∥ ≤ L2

> R

16 3 ts Kmin (t)3 + 2 LRsKmin (t)2,

∥J(t)∥ ≤ sKmin (t),

∥ ˙J(t)∥ ≤ s′ 

> Kmin

(t),

hence we can bound 

∥Y123 ∥ ≤ 7LR

16 3 sKmin (t/ 2) 2LRsKmin (t)s′ 

> Kmin

(t) + 6 LRsKmin (t)( L2

> R

16 3 ts Kmin (t)3 + 2 LRsKmin (t)2)+ 4 LRsKmin (t)( s′ 

> Kmin

(t)) 2 + 4 L2

> R

(sKmin (t)) 3

≤ 112 L2

> R

3 sKmin (t/ 2) 2sKmin (t)s′ 

> Kmin

(t) + 32 L3

> R

ts Kmin (t)4 + 20 L2

> R

sKmin (t)3

≤ 28 L2

> R

3 sKmin (t)3s′ 

> Kmin

(t) + 32 L3

> R

ts Kmin (t)4 + 20 L2

> R

sKmin (t)3.

93 We apply (Lezcano-Casado, 2020, Proposition 4.9). Consider ODE 

ρ′′ (t) + Kmin ρ(t) = η(t).

Define y(t) = sKmin (t). Note that y solves y′′ (t) + Kmin y(t) = 0. Then the function ρ(t) = R t 

> 0

y(t − s)η(s)ds satisfies ρ′′ (t) + Kmin ρ(t) = η(t). We apply this to each part of the bound on Y123 :

Z t

> 0

sKmin (t − s) 28 L2

> R

3 sKmin (s)3s′ 

> Kmin

(s)ds ≤ 7L2

> R

3 sKmin (t)

Z t

> 0

4sKmin (s)3s′ 

> Kmin

(s)ds = 7L2

> R

3 sKmin (s)5,

Z t

> 0

sKmin (t − s)32 L3

> R

sKmin (s)4sds ≤ 32 L3

> R

sKmin (t)

Z t

> 0

sKmin (s)4sds 

≤ 32 L3

> R

sKmin (t)5

Z t

> 0

sds = 16 L3

> R

t2sKmin (t)5,

Z t

> 0

sKmin (t − s)20 L2

> R

sKmin (s)3ds ≤ 20 L2

> R

ts Kmin (t)4.

Hence we can use the following ˜ ρ as upper bound for ρ:

ρ(t) ≤ ˜ρ(t) := 7L2

> R

3 sKmin (s)5 + 16 L3

> R

t2sKmin (t)5 + 20 L2

> R

ts Kmin (t)4.

In other words, 

∥L(t)∥ ≤ 7L2

> R

3 sKmin (s)5 + 16 L3

> R

t2sKmin (t)5 + 20 L2

> R

ts Kmin (t)4.

Lemma E.11. When M is a symmetric space, we have 

∥ ˙K(t)∥ ≤ L2

> R

16 3 ts Kmin (t)3 + 2 LRsKmin (t)2.

Proof. 

Since K solves ¨K + R(K, ˙γ) ˙ γ + Y = 0, we have ¨K = −R(K, ˙γ) ˙ γ − Y. 

Integrate over [0 , t ], we obtain ˙K(t) − ˙K(0) = 

Z t

> 0

¨K(s)ds =

Z t

> 0

−R(K, ˙γ) ˙ γ(s) − Y (s)ds. 

Using ∥ ˙γ∥ = 1 and ˙K(0) = 0, we obtain 

∥ ˙K(t)∥ = ∥

Z t

> 0

¨K(s)ds ∥ ≤ 

Z t

> 0

∥R(K, ˙γ) ˙ γ(s)∥ + ∥Y (s)∥ds ≤

Z t

> 0

LR∥K(s)∥ + ∥Y (s)∥ds. 

It suffices to obtain bound for ∥Y ∥ and ∥K∥. Recall 

Y := 2 R(J1, ˙γ) ˙J2 + 2 R(J2, ˙γ) ˙J1 + ( ∇ ˙γ R)( J2, ˙γ)J1 + ( ∇J2 R)( J1, ˙γ) ˙ γ, 

94 and that for a symmetric space, we have Recall 

Y = 2 R(J1, ˙γ) ˙J2 + 2 R(J2, ˙γ) ˙J1.

Hence 

∥Y (s)∥ ≤ 2LR(∥J1∥∥ ˙J2∥ + ∥ ˙J1∥∥ J2∥) ≤ 4LRs′ 

> Kmin

(s)sKmin (s) = 2 LRsKmin (2 s),

where by Lezcano-Casado (2020, Theorem 3.12) we have for unit tangent vector v,

∥d(Exp p)rv (rw )∥ ≤ max {1, sKmin (r)

r }r∥w∥,

so that ∥J(s)∥ ≤ sKmin (s). Also, Lezcano-Casado (2020, Theorem 3.11) gives 

∥ Hess r∥ ≤ s′ 

> Kmin

(r)

sKmin (r) ,

so that ∥ ˙J(s)∥ ≤ ∥ J∥∥ Hess r∥ ≤ s′   

> Kmin (s)
> sKmin (s)

sKmin (s) ≤ s′ 

> Kmin

(s). Furthermore, Lezcano-Casado (2020, Theorem 4.11) implies (with w replaced by rw )

K(s) ≤ 16 3 sKmin (s/ 2) 2LRsKmin (s).

Hence, we have 

∥ ˙K(t)∥ ≤ 

Z t

> 0

LR( 16 3 sKmin (s/ 2) 2LRsKmin (s)) + 2( LRsKmin (2 s)) ds. 

Note that in our case, sKmin (t) = sinh( √|Kmin |t)

√|Kmin | , so we have sKmin (s/ 2) ≤ sKmin (s). For the first term, we have 

Z t

> 0

LR( 16 3 sKmin (s/ 2) 2LRsKmin (s)) ds ≤ L2

> R

16 3

Z t

> 0

sKmin (s)3ds ≤ L2

> R

16 3

Z t

> 0

sKmin (t)3ds 

= L2

> R

16 3 ts Kmin (t)3.

For the second term, 

Z t

> 0

2( LRsKmin (2 s)) ds ≤ 2LRsKmin (t)2.

Put together, we obtain 

∥ ˙K(t)∥ ≤ L2

> R

16 3 ts Kmin (t)3 + 2 LRsKmin (t)2.

95 E.5 Bounds involving Riemannian Log 

Here, we obtain bounds on ∥(d Log x)y∥,  ∇(d Log x) 

> y

, ∥∇ x Log x(y)∥op , ∥∇ x Log x(y)∥2op , | div x Log x(y)|,and | div x Log x(y)|2. We also obtain a bound ∥ grad div x Log x(y)∥ when the manifold is SPD (n). In this section, for different differential operators d and ∇, note that ∇ Log x(x1) is covariant derivative (viewing Log x(x1) as a vector field, with x1 being fixed. On the other hand, d Log x1 is considering fixed base point. Before presenting our results, we also recall some facts. Let M be a Hadamard manifold of dimension d. Fix x ∈ M and let y ∈ M , y̸ = x. Then: (i) ∥ Log x(y)∥ = d(x, y ). (ii) grad x

 12 d(x, y )2



= − Log x(y). (iii) div x Log x(y) = −∆x

 12 d(x, y )2



.

Lemma E.12 (Bounds for d Log x and ∇(d Log x)) . Let M be a Hadamard manifold of dimension 

d. Fix x ∈ M . Then, we have 

∥(d Log x)y∥ ≤ 1,

 ∇(d Log x) 

> y

≤  ∇(d Exp x)  

> Log x(y)

.

Proof. On a Hadamard manifold, Exp x : TxM → M is a global diffeomorphism, hence Log x =Exp −1 

> x

is smooth on M . Let u = Log x(y). Then Exp x(u) = y and the inverse function theorem gives (d Log x)y =  (d Exp x)u

−1 =  (d Exp x)Log x(y)

−1.

Take norm on both sides and recall  (d Exp x)Log x(y)

−1 ≤ 1, we get the first bound. Now we differentiate the identity (d Exp x)Log x(·) ◦ (d Log x)· = Id T·M

covariantly at y in the direction w1 ∈ TyM , and then apply the resulting operator to w2 ∈ TyM .Using Leibniz rule and that ∇(Id ) = 0, we get 

 ∇(d Exp x) 

> Log x(y)

h

(d Log x)yw1, (d Log x)yw2

i

+ ( d Exp x)Log x(y)

 ∇(d Log x)

> y

[w1, w 2]



= 0 .

Re-arrange the terms, we get 

 ∇(d Log x)

> y

[w1, w 2]= −  (d Exp x)Log x(y)

−1 ∇(d Exp x) 

> Log x(y)

h (d Exp x)Log x(y)

−1w1,  (d Exp x)Log x(y)

−1w2

i 

.

Take norms, and recalling that ∥ (d Exp x)Log x(y)

−1∥ ≤ 1, we obtain 

 ∇(d Log x) 

> y

≤  ∇(d Exp x)  

> Log x(y)

.

96 Lemma E.13 (Bounds for ∥∇ x Log x(y)∥). Let M be a Hadamard manifold of dimension d with sectional curvature bounded below by Kmin ≤ 0. Fix x ∈ M and y ∈ M , y̸ = x. Then 

∥∇ x Log x(y)∥op ≤ 1 + d(x, y ) s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) ,

∥∇ x Log x(y)∥2op ≤ 2 + 2 d(x, y )2 s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) 

2

.

Proof. Let r(·) := d(·, y ), so r(x) = d(x, y ). Since r grad x r = − Log x(y) (since grad x( 12 r2) = 

r grad x r), we have Log x(y) = − r(x) grad x r(x).

Differentiate covariantly in x, we obtain 

∇x Log x(y) = −∇ x

 r(x) grad x r(x) = − ∇xr(x) ⊗ grad x r(x) − r(x) ∇x grad x r(x).

Taking operator norms and using ∥ grad x r(x)∥ = 1 gives 

∥∇ x Log x(y)∥op ≤ ∥ grad x r(x)∥2 + r(x) ∥∇ grad x r(x)∥op = 1 + d(x, y ) ∥∇ grad x r(x)∥op .

By Hessian comparison under Sec ≥ Kmin , we have 

∥∇ grad x r(x)∥op ≤ s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) .

Combining the above estimates yields the first inequality. The squared bound follows from ( a + b)2 ≤

2a2 + 2 b2 with a = 1 and b = d(x, y ) s′   

> Kmin (d(x,y ))
> sKmin (d(x,y ))

.

Lemma E.14 (Bounds for div x Log x(y) and | div x Log x(y)|2). Let M be a Hadamard manifold of dimension d with sectional curvature bounded below by Kmin ≤ 0. Fix x ∈ M and y ∈ M , y̸ = x.Then 

| div x Log x(y)| ≤ 1 + ( d − 1) d(x, y ) s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) ,

| div x Log x(y)|2 ≤ 2 + 2( d − 1) 2 d(x, y )2 s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) 

2

.

Proof. Let r(·) := d(·, y ). Then we have div x Log x(y) = −∆x

 12 r(x)2



. Using the product rule for Laplacian, we have ∆( 12 r2) = ⟨grad r, grad r⟩ + r∆r = 1 + r∆r. For the upper bound, Laplacian comparison theorem under Sec ≥ Kmin gives ∆r ≤ (d − 1) s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) .

Substitute into | div x Log x(y)| = ∆ x( 12 r2) we obtain 

| div x Log x(y)| ≤ 1 + ( d − 1) d(x, y ) s′ 

> Kmin

(d(x, y )) 

sKmin (d(x, y )) .

Finally, using ( a + b)2 ≤ 2a2 + 2 b2 with a = 1 and b = ( d − 1) d(x, y ) s′   

> Kmin (d(x,y ))
> sKmin (d(x,y ))

gives the desired results. Moreover, on SPD ( n), we have the following results. 97 Lemma E.15 (∥ grad div Log ∥ bound on SPD (n)) . Let SPD (n) be the manifold of real symmetric positive-definite matrices endowed with the affine-invariant Riemannian metric 

⟨U, V ⟩P := Tr  P −1U P −1V , U, V ∈ TP SPD ( n) = Sym (n).

Fix y ∈ SPD ( n) and define 

r(x) := d(x, y ), f (x) := d(x, y )2 = r(x)2, d := dim(SPD ( n)) = n(n + 1) 2 .

Then we have 

∥ grad div Log ∥ ≤ √2d

2



2



1 + r s′ 

> Kmin

(r)

sKmin (r)

 32

.

Proof. We first show that SPD (n) is a totally geodesic submanifold of P D (n, C) under the affine-invariant metric. Let PD (n, C) denote the manifold of complex Hermitian positive-definite matrices endowed with the same affine-invariant metric ⟨U, V ⟩P = Tr (P −1U P −1V ). Define a smooth isometry by 

σ : PD ( n, C) → PD ( n, C), σ(P ) := P . 

Notice that σ is an isometry because for any P ∈ PD ( n, C) and U, V ∈ Herm (n), 

⟨dσ P [U ], dσ P [V ]⟩σ(P ) = Tr  P −1 U P −1 V  = Tr ( P −1U P −1V ) = Tr ( P −1U P −1V ) = ⟨U, V ⟩P ,

where we used that Tr (P −1U P −1V ) ∈ R for Hermitian P, U, V . Thus we see that the fixed-point set of σ is exactly SPD (n). According to Kobayashi (1972, Theorem 5.1) we have that the fixed-point set of an isometry is a (embedded) totally geodesic submanifold. Hence SPD (n) is totally geodesic in PD ( n, C). Let F (P ) := dPD ( n, C)(P, y )2 be the squared distance to y ∈ SPD (n) ⊂ P D (n, C), considered as a function on P D (n, C). By Hirai et al. (2023, Theorem 1.4), F is 2-self-concordant on PD (n, C): we have that for all P ∈ PD ( n, C) and u, v, w ∈ TP PD ( n, C), 

|(∇3F )P (u, v, w )| ≤ √2 p(∇2F )P (u, u ) p(∇2F )P (v, v ) p(∇2F )P (w, w ).

Now restrict F to the totally geodesic submanifold SPD (n). By definition, for a totally geodesic submanifold, the second fundamental form vanishes, and Gauss formula (Lee, 2018, Theorem 8.2) implies that the connection ∇ on SPD (n) is induced by that of PD (n, C). Moreover, the geodesic distance function on SPD (n) is induced by that on PD (n, C), so we know self-concordance of distance squared also holds on SPD ( n). Now we make use of the self-concordance property to bound ∥ grad div Log ∥. Let f (x) = 

dSPD (x, y )2 and note that on the Hadamard manifold SPD ( n), 

∥ grad x div x Log x(y)∥ = 12 ∥ grad x ∆xf (x)∥.

Let {ei}di=1 be an orthonormal basis of Tx SPD (n). Such an orthonormal basis can be extended to a neighborhood of x, and we obtain an orthonormal frame: ∇ei|x = 0 , ∀i (Lee, 2018, Exercise 5-21). Thus we can differentiate ∆ f as follows. For any unit w ∈ Tx SPD ( n), 

⟨grad ∆ f, w ⟩ =

> d

X

> i=1

(∇3f )( w, e i, e i) + 2 ∇2f (∇wei, e i) = 

> d

X

> i=1

(∇3f )( w, e i, e i).

98 By self-concordance, we have |(∇3f )( w, e i, e i)| ≤ √2 p(∇2f )( w, w ) ( ∇2f )( ei, e i). Therefore 

∥ grad div Log ∥ ≤ √2d

2

q

∥∇ 2f ∥3op .

Writing f = r2, we have 

∇2f = 2 dr ⊗ dr + 2 r ∇2r, ∆f = 2 + 2 r ∆r, 

using ∥ grad r∥ = 1. Under Sec ≥ Kmin , Hessian comparison gives ∥∇ 2r∥op ≤ s′   

> Kmin (r)
> sKmin (r)

, hence 

∥∇ 2f ∥op ≤ 2 + 2 r ∥∇ 2r∥op ≤ 2



1 + r s′ 

> Kmin

(r)

sKmin (r)



.

Substituting this in the previous estimate yields 

∥ grad div Log ∥ ≤ √2d

2



2



1 + r s′ 

> Kmin

(r)

sKmin (r)

1.5

.

E.6 Auxiliary results 

Given a matrix A(x), the directional derivative in v can be computed through 

D log det( A(x))[ v] = tr ( A−1DA (x)[ v]) .

Hence 

∥∇ log det( A(x)) ∥op = sup 

> ∥v∥=1

| tr ( A−1DA (x)[ v]) | ≤ d∥A−1∥ × ∥ DA (x)∥op .

Furthermore, using the product rule, and noting that DA −1(x) = −A−1(x)DA (x)A−1(x), we obtain 

D2 log det( A(x))[ v, v ]= tr ( DA −1(x)[ v]DA (x)[ v]) + tr ( A−1(x)D2A(x)[ v, v ]) = − tr ( A−1(x)DA (x)[ v]A−1(x)DA (x)[ v]) + tr ( A−1(x)D2A(x)[ v, v ]) .

Hence, we have 

∥∇ 2 log det( A(x)) ∥op ≤ d∥A−1(x)∥2 × ∥∇ A(x)∥2 + d∥A−1(x)∥ × ∥∇ 2A(x)∥.

This is summarized as the following result, and is used in Lemma E.8. 

Lemma E.16 (Derivatives of log det ). Let A(x) be a smooth family of invertible linear maps on a 

d-dimensional inner product space. Then for any unit vector v,

D log det( A(x))[ v] = tr  A(x)−1DA (x)[ v].

Consequently, 

∥∇ log det( A(x)) ∥ = sup 

> ∥v∥=1

tr  A(x)−1DA (x)[ v] ≤ d ∥A(x)−1∥ ∥∇ A(x)∥, (33) 

∥∇ 2 log det( A(x)) ∥op ≤ d ∥A(x)−1∥2 ∥∇ A(x)∥2 + d ∥A(x)−1∥ ∥∇ 2A(x)∥. (34) 99
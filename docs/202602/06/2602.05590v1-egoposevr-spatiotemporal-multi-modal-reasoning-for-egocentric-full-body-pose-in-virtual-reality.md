---
title: "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality"
title_zh: EgoPoseVR：虚拟现实中第一人称全身姿态的时空多模态推理
authors: "Haojie Cheng, Shaun Jing Heng Ong, Shaoyu Cai, Aiden Tat Yang Koh, Fuxi Ouyang, Eng Tat Khoo"
date: 2026-02-05
pdf: "https://arxiv.org/pdf/2602.05590v1"
tags: ["query:课题"]
score: 6.0
evidence: 人体姿态的时空推理
tldr: EgoPoseVR是一个针对VR头显设计的端到端全身姿态估计框架。它通过融合头显运动信号与第一视角RGB-D图像，利用时空编码器和交叉注意力机制提取多模态特征，并结合运动学优化模块提升稳定性和准确性。该研究还推出了包含180万帧的大规模合成数据集。实验和用户研究证明，该方法在准确性、稳定性和沉浸感方面均优于现有技术，为VR化身提供了无需额外传感器的实用方案。
motivation: 现有的VR第一视角姿态估计面临时间不稳定性、下半身估计不准以及实时性不足等挑战。
method: 提出一种双模态融合管线，结合时空编码器、交叉注意力机制和运动学优化模块，整合头显运动与RGB-D观测。
result: 在多项实验和真实场景用户研究中，该方法在准确性、稳定性和用户主观体验上均显著优于现有SOTA模型。
conclusion: EgoPoseVR为VR环境下的鲁棒全身姿态追踪提供了一种无需额外传感器的实用且高效的解决方案。
---

## 摘要
沉浸式虚拟现实（VR）应用需要准确且具有时间相干性的全身姿态追踪。近期基于头戴式摄像头的方案在第一人称姿态估计方面展现出潜力，但在应用于 VR 头戴式显示器（HMD）时面临挑战，包括时间不稳定性、下半身估计不准确以及缺乏实时性能。为了解决这些局限性，我们提出了 EgoPoseVR，这是一个用于 VR 中准确第一人称全身姿态估计的端到端框架，它通过双模态融合流水线将头显运动线索与第一人称 RGB-D 观测相结合。时空编码器提取帧级和关节级表示，并通过交叉注意力机制进行融合，以充分利用跨模态的互补运动线索。随后，运动学优化模块施加来自 HMD 信号的约束，增强了姿态估计的准确性和稳定性。为了便于训练和评估，我们引入了一个大规模合成数据集，包含跨多种 VR 场景的超过 180 万帧时间对齐的 HMD 和 RGB-D 图像。实验结果表明，EgoPoseVR 优于目前最先进的第一人称姿态估计模型。在真实场景中的用户研究进一步表明，与基准方法相比，EgoPoseVR 在准确性、稳定性、具身感和未来使用意愿方面的评估得分显著更高。这些结果表明，EgoPoseVR 能够实现鲁棒的全身姿态追踪，为准确的 VR 具身化提供了一种实用的解决方案，且无需额外的身体佩戴式传感器或房间级追踪系统。

## Abstract
Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.
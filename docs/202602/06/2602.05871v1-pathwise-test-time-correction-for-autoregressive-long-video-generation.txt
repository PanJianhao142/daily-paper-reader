Title: Pathwise Test-Time Correction for Autoregressive Long Video Generation

URL Source: https://arxiv.org/pdf/2602.05871v1

Published Time: Fri, 06 Feb 2026 02:25:39 GMT

Number of Pages: 18

Markdown Content:
# Pathwise Test-Time Correction for Autoregressive Long Video Generation 

Xunzhi Xiang * 1 2 Zixuan Duan * 1 Guiyu Zhang 3 Haiyu Zhang 2 Zhe Gao 1 Junta Wu 2 Shaofeng Zhang 4

Tengfei Wang â€  2 Qi Fan â€  1 Chunchao Guo 2Causvid Causvid + Ours 

A zoom -in shot focusing on the face of a young woman sitting on a bench in the middle of an empty school gym. 

A dynamic shot from behind a white vintage SUV with a black roof rack as it speeds up a steep dirt roa d.

A vibrant anime illustration in a thick painting style featuring a young man in his 20s sitting on a fluffy white cloud in the sky 

A close -up shot of a Victoria crowned pigeon in a naturalistic wildlife photography style, showcasing its striking blue plumage and red chest .

A close -up portrait of a woman , her face illuminated by the side lighting, capturing her delicate features and expressive eyes. 

A traditional Chinese painting -style portrait of a middle -aged woman sipping a steaming cup of tea. 

Self Forcing Self Forcing + Ours 

Figure 1. 30-second video generation examples. Our method reduces error accumulation in CausVid and Self-Forcing, enabling longer and more stable videos with improved visual consistency. All samples are generated with the same random seed for fair comparison. 

## Abstract 

Distilled autoregressive diffusion models facili-tate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimiza-tion (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward 

> *

Equal contribution 1Nanjing University 2Tencent Hunyuan 

> 3

Chinese University of Hong Kong, Shenzhen 4University of Sci-ence and Technology of China. Correspondence to: Tengfei Wang 

<tengfeiwang12@gmail.com >, Qi Fan <fanqi@nju.edu.cn >.

Preprint. February 6, 2026. 

landscapes and the hypersensitivity of distilled pa-rameters. To overcome these limitations, we intro-duce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with vari-ous distilled models, extending generation lengths with negligible overhead while matching the qual-ity of resource-intensive training-based methods on 30-second benchmarks. 1

> arXiv:2602.05871v1 [cs.CV] 5 Feb 2026 Pathwise Test-Time Correction for Autoregressive Long Video Generation

## 1. Introduction 

Video generation (Lu et al., 2025; Yesiltepe et al., 2025; Hong et al., 2023; Jia et al., 2025b) has advanced rapidly with the development of diffusion-based generative mod-els (Kong et al., 2024; Wan et al., 2025; Hong et al., 2023; Ma et al., 2025a; Peebles & Xie, 2023; Rombach et al., 2022), which now enable the high-quality synthesis of com-plex motion (Zhu et al., 2024; Hu, 2024) and visual ap-pearance (Guo et al., 2024; Zhang et al., 2025a). However, scaling these diffusion priors to extended video sequences remains a formidable challenge. Beyond the escalating com-putational costs associated with longer contexts, maintain-ing temporal coherence over extended horizons is difficult without incurring excessive latency, thereby limiting their deployment in real-time applications. To overcome these limitations, recent studies (Yin et al., 2025b; Huang et al., 2025d) have shifted from bidirectional modeling to step-distilled autoregressive generation, en-abling true real-time video synthesis. However, these meth-ods remain constrained by cascading error accumulation: since each frame is conditioned on prior outputs, initial in-accuracies compound over time, resulting in temporal drift and long-horizon degradation. While recent extensions (Yi et al., 2025; Cui et al., 2026) like Rolling Forcing (Liu et al., 2025b), LongLive (Yang et al., 2025), Self-Forcing ++ (Cui et al., 2025) , and WorldPlay (Sun et al., 2025) have achieved minute-level consistency through sink mechanisms and win-dowed DMD retraining, they necessitate substantial com-putational overhead for model fine-tuning. Consequently, a pivotal question arises: Can we improve the stability of autoregressive video generation purely at inference time, bypassing the need for retraining the base model? 

Test-Time Optimization (TTO) (Wang et al., 2025; Sun et al., 2020) has emerged as a compelling alternative for enhancing video quality without the need for retraining. However, while effective for short-video synthesis (Yu et al., 2025b; Eyring et al., 2025), our toy experiments reveal that scaling TTO to long-horizon autoregressive generation faces a dual bottleneck consisting of the inherent difficulty in defining reward functions for long-range consistency and the extreme optimization sensitivity of distilled models. We observe that in these distilled models, even infinitesimal test-time gradients often trigger reward collapse and fail to mitigate cumulative error. Therefore, we propose Test-Time Correction (TTC), which is a training-free framework that shifts the paradigm from parameter-space optimization to sampling-space stochastic intervention. TTC is grounded in the insight that few-step distilled samplers are inherently stochastic as they perturb intermediate states with injected noise. This property implies that intermediate predictions are not fixed outcomes but rather malleable latent states that can be rectified by subsequent diffusion steps to align with V!"#$ 

V%&"' 

> Original
> Path

V(()  

> Shift Distribution Target Distribution

X!"#$ 

> Sink Point
> (Dynamic Collapse)

X%&"'   

> Corrected
> Path Re -noise
> ð“ (ðŸŽ ,ð‘° )

X(( *          

> Sink -based
> Path
> Noise
> Distribution
> Figure 2. Comparison of sampling strategies. The Original Path
> suffers from error accumulation, while the Sink-based Path col-lapses into a Sink Point (dynamic collapse). In contrast, our TTC strategy avoids these failures by employing reference-conditioned denoising and explicit Re-noising , effectively steering the trajec-tory away from the sink to preserve target distribution.

the global initial context while preserving the underlying sampling distribution. Specifically, as shown in Figure 2, TTC applies a small num-ber of correction steps along the stochastic sampling path, 

only after the global structure has stabilized . This delay prevents the generation from falling into sink-collapse (Cui et al., 2026), a phenomenon where newly generated frames repeatedly regress toward the sink frames instead of evolv-ing naturally. At these chosen steps in the sampling path, TTC performs reference-conditioned denoising by utilizing the initial frame context to anchor a corrected clean pre-diction. Then, this corrected state is re-noised back to the variance level corresponding to the current timestep, which ensures that the intervention remains compatible with the ex-pected noise distribution. By integrating correction into the stochastic sampling path of the autoregressive diffusion pro-cess rather than directly replacing the denoised prediction, this mechanism suppresses long-term error accumulation and temporal drift without retraining, while preserving high-fidelity temporal coherence over extended durations. In this work, we show that long-horizon stability in autore-gressive video generation can be achieved through test-time intervention alone. Our method suppresses error accumu-lation with negligible computational overhead, without re-quiring any retraining. As a result, it extends the stable generation length of distilled autoregressive models from a few seconds to over 30 seconds, while achieving visual qual-ity comparable to state-of-the-art training-based methods. Consistent improvements across multiple model architec-2Pathwise Test-Time Correction for Autoregressive Long Video Generation 

tures demonstrate that TTC is a robust and general solution for stabilizing distilled autoregressive diffusion models. 

## 2. Related Work 

Bidirectional Models for Video Generation. Diffusion models have been widely adopted for video generation, with recent works typically formulating video synthesis as a sequence-level joint denoising problem. Under this bidi-rectional diffusion paradigm, all frames are denoised simul-taneously via spatiotemporal attention, enabling the model to leverage global temporal context and produce tempo-rally coherent, high-fidelity videos (Blattmann et al., 2023; Yin et al., 2023; Jia et al., 2025a; Ma et al., 2025b; Zhang et al., 2025c; Huang et al., 2025b). Large-scale systems such as Hunyuan (Kong et al., 2024) and Wan (Wan et al., 2025) further demonstrate the effectiveness of this joint de-noising formulation at scale. However, because the entire sequence must be processed as a whole during inference, this paradigm inherently precludes streaming or incremen-tal generation, limiting its applicability in real-time and interactive scenarios. 

Autoregressive Models for Video Generation. Autore-gressive Models for Video Generation. Autoregressive video diffusion models generate videos sequentially in a strict causal manner, conditioning each new frame or seg-ment on the historical context of previously generated con-tent (Chen et al., 2024; Yin et al., 2025b; Huang et al., 2025d; Liu et al., 2025b; Yang et al., 2025; Chen et al., 2025a; Ji et al., 2025; Teng et al., 2025; Deng et al., 2025; Guo et al., 2025). While this formulation naturally supports streaming generation with low initial latency, it is inherently susceptible to error accumulation, where minor deviations propagate and amplify across steps, leading to severe tempo-ral drift and degraded coherence in long videos. To mitigate this issue, recent works introduce planning methods (Zhang et al., 2025b; Xiang et al., 2025) or explicit memory mecha-nisms (Sun et al., 2025; Chen et al., 2025b; Yu et al., 2025a; Cai et al., 2025; Huang et al., 2025a; HunyuanWorld, 2025), strategies that typically necessitate complex architectural modifications and extensive re-training. 

Test-time Image/Video Generation. Test-time genera-tion methods (Wang et al., 2025; Sun et al., 2020; Liang et al., 2025) aim to enhance the performance of pre-trained models directly during the inference phase. Test-time scal-ing improves quality by iteratively searching over multiple candidates, as seen in Video-T1 (Liu et al., 2025a) and EvoSearch (He et al., 2025), yet this comes at the price of prohibitive computational cost. Similarly, Test-time opti-mization refines generation via auxiliary parameter updates that necessitate instance-specific training, such as Hyper-Noise (Eyring et al., 2025), AutoRefiner (Yu et al., 2025b), and SLOWFAST-VGEN (Hong et al., 2025). In contrast, our approach distinguishes itself from both paradigms by being fully training-free, avoiding both the overhead of candidate search and the complexity of parameter optimization. 

## 3. Test-time Optimization for Distilled Models 

3.1. Background: Few-step Distilled Sampling 

In this section, we formulate autoregressive video genera-tion as next-chunk prediction under a context-conditional generative model. Given a video sequence {x1, . . . , x N },the joint distribution factorizes as 

p(x1: N ) = 

> N

Y

> t=1

pÎ¸ (xt | St), St = {x1, . . . , x tâˆ’1}, (1) where St denotes the context at step t, consisting of all previously generated frames or chunks. As illustrated in Figure 3, existing autoregressive video generation methods typically fall into three categories. Discrete autoregressive models generate each chunk through a single deterministic prediction conditioned on past outputs, while multi-step au-toregressive diffusion models approximate the same condi-tional distribution via a deterministic ODE-based sampling trajectory. In contrast, few-step distilled diffusion models re-place deterministic ODE solvers with a stochastic sampling process that explicitly injects noise at intermediate steps. Under this formulation, each conditional distribution pÎ¸ (xt |

St) is no longer realized as a single deterministic map-ping, but through a stochastic diffusion sampling trajec-tory defined over a sparse set of diffusion steps {T0 =0, T 1, . . . , T K = Tmax }. Specially, distilled video gener-ation begins from Gaussian noise xTmax  

> t

âˆ¼ N (0 , I ) and evolves progressively along this trajectory via a sequence of denoiseâ€“renoise transitions. Specifically, at each denoising step Tj , the generation process starts from a noisy latent state and applies the denoising network to produce an esti-mate of the underlying clean latent representation, 

x Tj 

> t, 0

= GÎ¸



Î¨( x Tj+1  

> t, 0

, Ïµ Tj 

> t

, T j ); St, T j



, (2) where GÎ¸ (Â·) denotes the parameterized denoising network, and St represents the autoregressive context at step t.After each denoising step, distilled diffusion models proceed by re-injecting noise according to the predefined schedule, mapping the clean estimate back onto the diffusion trajec-tory. Concretely, the estimated clean latent is re-noised to obtain the latent state at the next diffusion step, 

x Tjâˆ’1 

> t

= Î¨ 



x Tj 

> t, 0

, Ïµ Tjâˆ’1 

> t

, T jâˆ’1



, Ïµ Tjâˆ’1 

> t

âˆ¼ N (0 , I ), (3) thereby yielding a stochastic transition that advances the generation process to the next noise level. The forward diffusion process Î¨( Â·) is defined as 

Î¨( x, Ïµ T , T ) = Î±T x + ÏƒT ÏµT , (4) 3Pathwise Test-Time Correction for Autoregressive Long Video Generation generator 

ð‘† !

ð‘¥ ! 

> "

ð‘¥ !

> "#$

ð‘† !%$

ð‘¥ !

> &

ð‘† !

generator 

ð‘† !

ð‘¥ ! 

> "

ð‘¥ !

> &

ð‘† !%$

ð‘¥ !

> &

ð‘† !

ð‘¥ !

> "#'

ODE Diffusion Autoregressive Few -step Diffusion Autoregressive 

generator 

ð‘† !

ð‘ž ! ð‘¥ !

ð‘† !%$

Discrete Autoregressive 

ð‘¥ !

> "#(

# â€¦ â€¦ð‘¥ !   

> &
> Figure 3. Variants of autoregressive video generation. Discrete AR uses single-step deterministic prediction, multi-step diffusion follows a deterministic ODE trajectory, while few-step distilled diffusion performs stochastic sampling with intermediate noise injection. TTO with denoising
> loss
> TTO with Semantic
> loss
> Figure 4. Comparison of two toy test-time optimization variants based on LoRA fine-tuning.

where Î±T and ÏƒT are predefined diffusion coefficients cor-responding to step T . Repeating this denoiseâ€“re-noise pro-cedure across diffusion steps forms the complete stochastic sampling trajectory of distilled diffusion models. 

3.2. Toy Experiment: Apply Test-time Optimization to Long Video Generation 

Existing test-time optimization (TTO) methods improve generation quality by aligning the model distribution with a predefined reward function. Given a pre-trained generative model with output distribution pbase , TTO typically defines a reward-weighted target distribution 

pâˆ—(x) âˆ pbase (x) exp( r(x)) , (5) where r(x) encodes preferences of samples x. The opti-mization objective can be formulated as minimizing the KL divergence between a parameterized distribution pÏ• and the target distribution pâˆ—,

min  

> Ï•

DKL (pÏ•âˆ¥pâˆ—) = min  

> Ï•

DKL (pÏ•âˆ¥pbase ) âˆ’ Exâˆ¼pÏ• [r(x)] ,

(6) which trades off reward maximization against deviation from the original model distribution. However, for long video generation, it remains challenging to design an ex-plicit reward that effectively suppresses error accumulation. Temporal drift arises from coupled inconsistencies in se-mantics, appearance, and motion, which are difficult to characterize with a single hand-crafted objective. A naive alternative is to constrain each subsequent chunkâ€™s predic-tive distribution to remain close to that of the initial frames, effectively anchoring generation to early content. To assess this idea, we conduct two toy experiments using direct LoRA fine-tuning (Hu et al., 2022) at test time, follow-ing HyperNoise (Eyring et al., 2025) and AutoRefiner (Yu et al., 2025b). Both variants use the same backbone and iden-tical LoRA adapters, and differ only in their optimization objectives. The first variant fine-tunes LoRA with a stan-dard denoising reconstruction loss on early frames across noise levels. The second variant replaces pixel-level recon-struction with a semantic consistency objective, enforcing similarity to early frames in pretrained feature spaces (Rad-ford et al., 2021; Oquab et al., 2024). Figure 4 shows that these two objectives lead to distinct failure modes. The reconstruction-based variant quickly collapses to a trivial solution, where later frames become near-duplicates of the initial frame, resulting in severe mo-tion loss. In contrast, the semantic objective fails to ef-fectively reduce long-horizon error accumulation, and the generated videos still exhibit temporal drift similar to the baseline. These results indicate that naive TTO, whether based on low-level reconstruction or high-level semantics, is insufficient for stable long-horizon generation. 

## 4. From Test-Time Optimization to Test-Time Correction 

Based on the above analysis, we identify two key limitations of TTO for long video generation. 

Reward design for error accumulation. Temporal drift stems from coupled errors in semantics, appearance, and motion, which are hard to capture with a single reward: low-level reconstruction suppresses motion, while high-level semantic objectives lack frame-wise correction signals. 

Optimization Challenges and Collapse. Performing test-time optimization on distilled models presents significant training difficulties. The models tend to overfit rapidly to the auxiliary reward, causing the optimization trajectory to collapse into specific, degenerate solutions that violate the pre-trained generative prior. Together, these limitations motivate a shift from parameter-updating test-time optimization to test-time correction ,which avoids model updates and instead performs trajectory-aware interventions during sampling. 4Pathwise Test-Time Correction for Autoregressive Long Video Generation â€¦

Chunk 1 Chunk 2 Chunk ð’• âˆ’ ðŸ Chunk ð’•     

> Stable reference ð‘º ðŸŽ Evolving Context ð‘º ð’• Stochastic Sampling Path

ð‘‡ !"# ð‘‡ $             

> Denoiser
> ð‘® ðœ½
> ð’™ ð’•
> ð‘» ð’‹ ð’™ ð’• ,ðŸŽ
> ð‘» ð’‹
> Current Noise
> Latent at Step ð’‹
> Prediction
> at Step ð’‹
> ð’™ ð’•
> ð‘» ð’‹ "ðŸ ð’™ ð’• ,ðŸŽ
> ð‘» ð’‹ "ðŸ
> Standard Path
> Denoiser
> ð‘® ðœ½
> ð ð’™ ð’• ,ðŸŽ
> ð‘» ð’‹ ,ð ð’•
> ð‘» ð’‹ "ðŸ ,ð‘» ð’‹ &ðŸ
> Test -Time
> Correction
> Add
> noise
> ð’™ ð’•
> ð‘» ð’‹ "ðŸ ,ð’„ ð’™ ð’• ,ðŸŽ
> ð‘» ð’‹ "ðŸ ,ð’„
> Self -Corrected
> Prediction
> ð‘» ð’‹ ð‘» ð’‹ &ðŸ
> ð‘º ð’• ð‘º ðŸŽ ð‘º ð’•

Chunk 0

> ð’™ ð’• ,ðŸŽ
> ð‘» ðŸŽ

# â€¦

> ð’™ ð’•
> ð‘» ð’‹ "ðŸ
> Standard Path

# â€¦   

> Prediction at
> Step ð’‹ âˆ’ðŸ
> Final
> Prediction
> Add
> noise

Figure 5. Overall pipeline of our method. A sparse set of correction steps is inserted into the stochastic sampling path until the global structure stabilizes. At selected steps, TTC performs reference-conditioned denoising using the initial frame to obtain a corrected prediction, which is then re-noised to the current timestep to remain consistent with the expected noise distribution. This on-path, training-free correction suppresses long-term error accumulation and stabilizes long-horizon generation. De -noise Re -noise   

> Step:2 Step:1 Step:3 Step:4

Figure 6. Intermediate predictions along the stochastic sam-pling path. High-noise steps determine global structure, while low-noise steps refine appearance details under a fixed layout. 

4.1. Correctability along the Stochastic Sampling Path 

Distilled few-step diffusion models maintain a stochas-tic sampling trajectory through iterative noise re-injection, which prevents premature convergence and preserves flex-ibility in intermediate states. As shown in Figure 6, this trajectory exhibits a clear functional phase transition. At high noise levels, the denoising process primarily deter-mines global structure, such as scene layout and spatial rela-tionships. As the noise level decreases, the generation pro-gressively shifts to an appearance refinement stage, where local textures and fine visual details are synthesized while the global structure remains largely fixed. This phase-wise behavior naturally suggests a principled test-time correc-tion strategy. Rather than intervening throughout the sam-pling process, we apply alternative conditioning only during the appearance refinement stage, after the global structure has stabilized. At this stage, the model is less sensitive to structural changes, allowing visual attributes to be adjusted without affecting layout or geometry. As a result, targeted test-time intervention can modulate appearance while pre-serving structural consistency. Motivated by planning-based video generation mod-els (Zhang et al., 2025b; Xiang et al., 2025), which relax strictly unidirectional prediction via cross-frame context, we consider applying test-time intervention at a single sam-pling step after structural stabilization. Specifically, at a designated step jâ‹†, we restrict the visible context state St

to include only the earliest frame, forcing the model to rely exclusively on the initial frame for subsequent appearance refinement and texture generation. This single-point cor-rection process can be formalized as: 

x Tjâˆ’1 

> t, 0

= GÎ¸



Î¨( x Tj 

> t, 0

, Ïµ Tjâˆ’1 

> t

, T jâˆ’1); S(jâ†’jâ‹†) 

> t

, T jâˆ’1



,

(7) where S(jâ†’jâ‹†) 

> t

denotes the modified context state. Specifi-cally, at the designated sampling step jâ‹†, the original autore-gressive context St is replaced by the earliest-frame context 

S0, while all other sampling steps remain unchanged. Here, 

jâ‹† corresponds to the stage at which the global layout and object structure have stabilized. 

4.2. Path-wise Test-time Correction 

Despite its conceptual simplicity, single-point latent cor-rection frequently leads to visible artifacts in distilled au-toregressive diffusion models, such as flickering, abrupt appearance changes, and temporal inconsistency. To over-come this, we propose a path-wise self-correction strategy as shown in Figure 5, which leverages the modelâ€™s stochastic nature to ensure smooth state transitions. Instead of performing a hard correction at a single denoising step, our method first applies the intervention to the current prediction and then re-noises it back to the current noise level. By restarting the sampling process from this re-noised state, the correction is naturally integrated into the stochastic 5Pathwise Test-Time Correction for Autoregressive Long Video Generation 

path. This avoids the abrupt state transitions typical of direct prediction replacement, allowing the model to smoothly assimilate the update while maintaining generation stability. Formally, consider generating chunk t under a denoising schedule Tmax > Â· Â· Â· > T j > T jâˆ’1 > Â· Â· Â· > T 0. At step j,given the current noisy latent xTj 

> t

and the evolving context 

St, the denoiser produces a clean prediction as: 

xTj 

> t, 0

= GÎ¸



xTj 

> t

; St, j 



. (8) Instead of directly executing the next denoising update fol-lowing the standard path in Figure 5, we first apply forward diffusion noise injection to the current prediction and explic-itly map it to the next noise level Tjâˆ’1. Then, we replace the evolving context St with a stable reference context S0

for denoising. This produces a reference-aligned corrected clean prediction as: 

xTjâˆ’1,c t, 0 = GÎ¸



Î¨



xTj

> t, 0

, Ïµ jâˆ’1 

> t

, T jâˆ’1



; S0, j âˆ’ 1



, (9) The corrected prediction is then mapped back to the same noise level Tjâˆ’1 through noise injection. Denoising is sub-sequently resumed under the true evolving context St

xTjâˆ’1 

> t, 0

= GÎ¸



Î¨



xTjâˆ’1,c t, 0 , ËœÏµjâˆ’1 

> t

, T jâˆ’1



; St, j âˆ’ 1



,

(10) This sequence of operations integrates test-time correction directly into the stochastic sampling process. All modified intermediate states are produced through valid diffusion tran-sitions, as summarized in Algorithm 1. This effectively sup-presses chunk-boundary flickering, mitigates long-horizon error accumulation, and preserves temporal coherence in autoregressive video generation. 

## 5. Experiments 

Baseline. We evaluate our test-time correction method on two baseline models, CausVid (Yin et al., 2025b) and Self-Forcing (Huang et al., 2025d). Both baselines are built on the Wan2.1-T2V-1.3B model (Wan et al., 2025) and generate 5-second video clips at 16 FPS a resolution of 832 Ã— 480 .

Standard Evaluation. We benchmark our method against representative autoregressive video diffusion baselines, in-cluding CausVid (Yin et al., 2025b), Self-Forcing (Huang et al., 2025d), Rolling Forcing (Liu et al., 2025b), and LongLive (Yang et al., 2025). Following standard pro-tocols, evaluations are conducted using VBench (Huang et al., 2024) on 128 prompts randomly sampled from MovieGen (Polyak et al., 2024). Unless otherwise spec-ified, we conduct all experiments in the 30-second video generation setting, using Self-Forcing as the default baseline. More experimental settings and implementation details for fair comparison are included in the supplementary material. 

Algorithm 1 Path-wise Test-time Correction 

1: Input: Noise schedule {TJ > Â· Â· Â· > T 0 = 0 }; Genera-tor GÎ¸ ; Evolving context St; Ref context S0; Correction indices J â‹†; Diffusion forward process Î¨

2: Output: Final prediction x0

> t, 0

3: Sample xTJ 

> t

âˆ¼ N (0 , I ) # Initial Noise 

4: for j = J down to 2 do 

5: xTj 

> t, 0

â† GÎ¸



xTj 

> t

; St, j 



# Initial prediction with St

6: Sample ÏµTjâˆ’1 

> t

âˆ¼ N (0 , I )

7: if j âˆ’ 1 âˆˆ J â‹† then 

8: â€” Phase A: Reference-guided Correction â€” 

9: xTjâˆ’1,c t â† Î¨



xTj

> t, 0

, Ïµ Tjâˆ’1 

> t

, T jâˆ’1



10: xTjâˆ’1,c t, 0 â† GÎ¸



xTjâˆ’1,c t ; S0, j âˆ’ 1



# Correct trajectory using S0

11: â€” Phase B: Re-noising & Re-denoising â€” 

12: Sample ËœÏµTjâˆ’1 

> t

âˆ¼ N (0 , I )

13: xTjâˆ’1 

> t

â† Î¨



xTjâˆ’1,c t, 0 , ËœÏµTjâˆ’1 

> t

, T jâˆ’1



# Inject new noise 

14: xTjâˆ’1 

> t, 0

â† GÎ¸



xTjâˆ’1 

> t

; St, j âˆ’ 1



# Finalize step with St

15: else 

16: xTjâˆ’1 

> t

â† Î¨



xTj

> t, 0

, Ïµ Tjâˆ’1 

> t

, T jâˆ’1



17: xTjâˆ’1 

> t, 0

â† GÎ¸



xTjâˆ’1 

> t

; St, j âˆ’ 1



18: end if 

19: end for 

20: return xT1

> t, 0

Additional Evaluation. To rigorously evaluate temporal co-herence, we complement standard VBench quality metrics with temporal color histograms and JEPA scores (Balestriero et al., 2025) to assess long-term temporal drift. Since tem-poral consistency scores can be artificially improved by suppressing motion, allowing models to effectively cheat the evaluation, we conduct comparisons under matched dy-namic degrees to ensure a fair and meaningful assessment. In addition, we use t-LPIPS (Zhang et al., 2018) to explicitly measure visual discontinuities, serving as a direct proxy for flickering artifacts at autoregressive chunk boundaries in our ablation studies. Comprehensive experimental settings and implementation details for fair comparison are included in the supplementary material. 

Qualitative Results. As shown in Figure 7, integrating our method with Self-Forcing and CausVid substantially reduces error accumulation in long-horizon video genera-tion. While the original baselines exhibit temporal drift and visual degradation over time, our integration maintains sta-ble temporal coherence and visual fidelity over 30-second 6Pathwise Test-Time Correction for Autoregressive Long Video Generation 

Table 1. Comprehensive comparison with SOTA methods on prompt-conditioned 30-second video generation. We report Throughput (fps), VBench metrics, Color-shift metrics (L1, Correlation), and JEPA consistency (Standard Deviation, Difference).                                                                                     

> Method Training Free Speed VBench Metrics Color-shift JEPA Consistency
> Total fps Subject Consistency Background Consistency Dynamic Degree Motion Smoothness Imaging Quality Aesthetic Quality L1 â†“Correlation â†‘Standard Deviation â†“Difference â†“
> Rolling Forcing âœ—15.38 95.8 95.1 35.9 98.9 72.5 63.6 0.436 0.858 0.0162 0.201 LongLive âœ—-95.5 95.4 44.5 98.8 71.7 65.0 0.701 0.724 0.0151 0.101 CausVid âœ—15.79 91.2 91.4 50.8 98.1 70.2 63.5 1.047 0.451 0.0199 0.313
> CV + Ours âœ“10.53 93.2 93.3 69.5 97.6 70.1 63.5 0.607 0.778 0.0157 0.164 Self-Forcing âœ—15.79 92.5 93.2 62.5 98.0 72.5 63.4 1.028 0.479 0.0145 0.191
> SF + Ours âœ“10.53 94.0 94.2 60.2 98.3 72.7 63.8 0.644 0.710 0.0108 0.170

Table 2. Comprehensive comparison with test-time scaling methods on prompt-conditioned 30-second video generation. 

We report Throughput (fps) and VBench metrics.                                   

> Method Train. Free Speed VBench Metrics
> Total fps Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. Self-Forcing âœ—15.79 92.5 93.2 62.5 98.0 72.5 63.4 SF + BoN âœ“3.16 92.4 93.2 62.5 98.4 72.7 63.3 SF + SoP âœ“3.16 92.7 93.4 60.2 98.6 72.7 63.1
> SF + Ours âœ“10.53 94.0 94.2 60.2 98.3 72.7 63.8

Table 3. Ablation study on noise-correction steps. We evaluate quality using VBench metrics alongside the Boundary metric.                                                                           

> Total NFE
> Timesteps VBench Metrics Boundary
> 750 500 250 Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. t-LPIPS 4âœ—âœ—âœ—92.5 93.2 62.5 98.0 72.5 63.4 0.178 5âœ“âœ—âœ—93.6 94.3 60.2 98.6 72.6 63.2 0.161 5âœ—âœ“âœ—93.2 93.9 60.9 98.5 72.8 63.1 0.182 5âœ—âœ—âœ“93.6 94.1 57.0 98.5 72.9 63.4 0.183 6âœ—âœ“âœ“94.0 94.2 60.2 98.3 72.7 63.8 0.176 6âœ“âœ“âœ—93.1 93.9 61.7 98.4 73.0 63.1 0.170 7âœ“âœ“âœ“93.4 94.2 62.5 98.5 72.4 63.8 0.169

sequences, especially in videos with complex motion and ap-pearance changes. Under the 30-second setting, our training-free approach achieves visual quality comparable to, and in some cases better than, Rolling Forcing and LongLive, which rely on additional training or specialized mechanisms. These results demonstrate that our method provides an effec-tive and general test-time solution for improving long-term temporal consistency in autoregressive video generation. 

Quantitative Results. All quantitative results are summa-rized in Table 1. Under the 30-second generation setting on VBench, integrating our method into standard autore-gressive baselines, Self-Forcing and CausVid, consistently improves long-horizon video generation quality across di-verse prompts and scenes. In particular, our method sub-stantially reduces error accumulation and temporal drift, leading to improved subject and background consistency while notably enhancing dynamic degree without sacrificing motion smoothness or imaging quality. Moreover, the pro-posed path-wise correction effectively stabilizes appearance evolution over time, as evidenced by lower color-shift L1 distances and higher histogram correlations between the first and last frames. At the semantic level, our method also improves JEPA consistency by reducing both the standard deviation and firstâ€“last score difference across the entire se-quence, indicating more coherent long-term representations. Compared with training-based methods such as Rolling Forcing and LongLive, our approach achieves comparable long-horizon consistency and visual quality while preserv-ing stronger motion dynamics and requiring no additional training or parameter updates at test time. 

Comparison with Test-Time Scaling. We benchmark our approach against test-time scaling strategies, includ-ing Best-of-N (BoN) and Search-over-Path (SoP), as shown in Table 2. While these methods attempt to mitigate er-rors through redundant candidate generation or iterative search, they incur prohibitive computational overhead and inference latency. In contrast, our method embeds correc-tion directly into a single stochastic sampling trajectory. This design drastically reduces inference costs compared to multi-sample scaling, and by actively rectifying structural deviations rather than passively selecting from drifting can-didates, it achieves superior suppression of long-term error accumulation with minimal overhead. More experimental settings and implementation details for are included in the supplementary material. 

Ablation Study on Path-wise Correction. We compare 

single-point and path-wise correction to evaluate the role of the stochastic sampling trajectory in practice. Single-point correction directly replaces the latent at a fixed denoising step, whereas path-wise correction re-noises the corrected prediction to the same noise level and resumes denoising along the original trajectory. As shown in Figure 9 and Ta-ble 4, single-point correction frequently introduces flicker-ing and temporal instability, leading to degraded consistency metrics and higher t-LPIPS scores. In contrast, path-wise correction achieves consistently higher temporal consistency and substantially lower t-LPIPS, resulting in more stable videos with improved temporal coherence. These results 7Pathwise Test-Time Correction for Autoregressive Long Video Generation LongLive 

Rolling Foricng 

Our s

Self Forcing 

Rolling Foricng 

LongLive 

Our s

Self Forcing    

> A detailed oil painting in a romantic style, showcasing a young woman standing amidst a vibrant garden filled with blooming flowers .
> A close -up view of a glass sphere containing a tranquil Zen garden.

Figure 7. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. 

demonstrate that effective test-time intervention requires integrating corrections along the sampling path rather than directly replacing latents. 

Ablation Study on Noise-correction Steps. After establish-ing the necessity of path-wise correction, we evaluate how different numbers and placements of correction steps along the sampling path affect long-horizon generation quality. Enabling correction at noise levels 750, 500, and 250, either individually or in combination, consistently outperforms the baseline without correction, demonstrating the robustness and effectiveness of our method across different configura-tions. Considering the trade-off between performance and inference cost, we adopt correction at noise levels 500 and 250 in our experiments. 

Comparison with the Sink-based Method. We com-pare our proposed path-wise correction with the Sink-based method. The Sink-based approach keeps the Sink frame as visible context throughout the entire denoising process, ef-LongLive  

> Rolling Foricng
> Our s

Figure 8. Comparison between the sink-based method and path-wise correction. The sink-based method overly constrains intermediate states, leading to degraded motion dynamics and reduced temporal variation. 

fectively imposing persistent conditioning. In contrast, path-wise correction is applied only at later stages after structural stabilization, where corrected predictions are re-noised and integrated along the stochastic sampling trajectory. Because 8Pathwise Test-Time Correction for Autoregressive Long Video Generation 

Table 4. Comparison of correction strategies. We evaluate quality using VBench metrics alongside the Boundary metric.                  

> Method VBench Metrics Boundary
> Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. t-LPIPS Single-point 93.4 94.0 57.0 98.3 71.6 62.8 0.205
> Path-wise 94.0 94.2 60.2 98.3 72.7 63.8 0.176

Table 5. Comparison on prompt-conditioned 5-second video generation. We evaluate quality using standard VBench metrics.                          

> Method VBench Metrics
> Sub. Cons. Bg. Cons. Dyn. Deg. Mot. Sm. Img. Qual. Aes. Qual. CausVid 96.2 94.9 54.7 98.2 70.5 63.8
> CausVid + Ours 96.6 95.2 68.0 97.8 70.5 64.2
> Self-Forcing 97.0 96.2 62.5 98.7 72.9 64.5
> Self-Forcing + Ours 97.0 96.3 62.5 98.7 73.0 64.6

the Sink frame continuously participates in all denoising steps, the model becomes overly conditioned on it, caus-ing generated content to remain visually and structurally close to the Sink frame. This static conditioning restricts motion and scene variation, suppressing temporal dynamics, as shown in Table 1 and Figure 8. By contrast, path-wise correction preserves structural flexibility in early stages and introduces correction only during appearance refinement, maintaining temporal coherence while retaining meaningful video dynamics. 

Comparison on Short Video Generation. As shown in Table 5, we evaluate our method on short video generation. Although error accumulation is less pronounced under short temporal horizons, our method still consistently outperforms the baseline across most metrics. This indicates that the pro-posed correction strategy is not specialized to long-horizon generation, but also remains effective in short video settings. Together with the significant improvements observed for long video generation, these results demonstrate the robust-ness and general applicability of our approach. 

## 6. Conclusion 

In this paper, we propose Test-Time Correction, a training-free test-time method for stabilizing distilled autoregressive diffusion models in long-horizon video generation. The pro-posed approach addresses error accumulation by introducing training-free, reference-based correction along the stochas-tic sampling process, allowing corrected predictions to be smoothly inherited by subsequent denoising steps. With-out modifying model parameters or requiring additional training, our method effectively suppresses temporal drift while preserving the original generation behavior. Exten-sive experiments demonstrate that Test-Time Correction Path -wise Correction    

> Single -point Correction
> Single -point Correction
> Path -wise Correction

Figure 9. Comparison of single-point and path-wise correction. 

Single-point correction causes temporal discontinuities, while on-path re-noising improves temporal stability and reduces flickering. 

consistently improves long-horizon stability across multiple distilled video generation models, extending the achievable generation length to 30 seconds with negligible computa-tional overhead and competitive visual quality. 

## Impact Statement 

This paper presents a training-free test-time method for improving the stability of autoregressive video generation models. The primary goal of this work is to advance the field of machine learning by enabling more reliable long-horizon video synthesis without additional training or model modification. While the proposed method may contribute to downstream applications that rely on long video generation, such as content creation and simulation, it does not introduce new capabilities beyond existing video generative models. The potential societal impacts of this work are therefore aligned with those already associated with video generation tech-nologies, and no specific additional ethical concerns are introduced by the method itself. 

## References 

Balestriero, R., Ballas, N., Rabbat, M., and LeCun, Y. Gaus-sian embeddings: How jepas secretly learn your data density. CoRR , 2025. Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR , 2023. Cai, S., Yang, C., Zhang, L., Guo, Y., Xiao, J., Yang, Z., Xu, Y., Yang, Z., Yuille, A., Guibas, L., et al. Mixture of contexts for long video generation. CoRR , 2025. Chen, B., Mart Â´Ä± Mons Â´o, D., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Next-9Pathwise Test-Time Correction for Autoregressive Long Video Generation 

token prediction meets full-sequence diffusion. NeurIPS ,2024. Chen, G., Lin, D., Yang, J., Lin, C., Zhu, J., Fan, M., Zhang, H., Chen, S., Chen, Z., Ma, C., et al. Skyreels-v2: Infinite-length film generative model. CoRR , 2025a. Chen, Y., Liang, Y., Wang, J., Chen, T., Cheng, J., Gu, Z., Huang, Y., Jiang, Z., Li, W., Li, T., et al. Teleworld: Towards dynamic multimodal synthesis with a 4d world model. CoRR , 2025b. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, A., Ban, Y., and Hsieh, C.-J. Self-forcing++: Towards minute-scale high-quality video generation. CoRR , 2025. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, A., Ban, Y., and Hsieh, C.-J. Lol: Longer than longer, scaling video generation to hour. CoRR , 2026. Deng, H., Pan, T., Diao, H., Luo, Z., Cui, Y., Lu, H., Shan, S., Qi, Y., and Wang, X. Autoregressive video generation without vector quantization. In ICLR , 2025. Eyring, L., Karthik, S., Dosovitskiy, A., Ruiz, N., and Akata, Z. Noise hypernetworks: Amortizing test-time compute in diffusion models. CoRR , 2025. Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR , 2024. Guo, Y., Yang, C., He, H., Zhao, Y., Wei, M., Yang, Z., Huang, W., and Lin, D. End-to-end training for autore-gressive video diffusion via self-resampling. CoRR , 2025. He, H., Liang, J., Wang, X., Wan, P., Zhang, D., Gai, K., and Pan, L. Scaling image and video generation via test-time evolutionary search. CoRR , 2025. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video gen-eration via transformers. In ICLR , 2023. Hong, Y., Liu, B., Wu, M., Zhai, Y., Chang, K.-W., Li, L., Lin, K., Lin, C.-C., Wang, J., Yang, Z., Wu, Y. N., and Wang, L. Slowfast-VGen: Slow-fast learning for action-driven long video generation. In ICLR , 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In ICLR , 2022. Hu, L. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In CVPR ,2024. Huang, J., Hu, X., Han, B., Shi, S., Tian, Z., He, T., and Jiang, L. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. CoRR , 2025a. Huang, T., Zheng, W., Wang, T., Liu, Y., Wang, Z., Wu, J., Jiang, J., Li, H., Lau, R. W. H., Zuo, W., and Guo, C. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. SIGGRAPH ,2025b. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. CoRR , 2025c. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In NeurIPS , 2025d. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Com-prehensive benchmark suite for video generative models. In CVPR , 2024. HunyuanWorld, T. Hy-world 1.5: A systematic framework for interactive world modeling with real-time latency and geometric consistency. arXiv preprint , 2025. Ji, S., Chen, X., Yang, S., Tao, X., Wan, P., and Zhao, H. Memflow: Flowing adaptive memory for consistent and efficient long video narratives. CoRR , 2025. Jia, W., Lu, Y., Huang, M., Wang, H., Huang, B., Chen, N., Liu, M., Jiang, J., and Mao, Z. Moga: Mixture-of-groups attention for end-to-end long video generation. CoRR ,2025a. Jia, W., Lu, Y., Huang, M., Wang, H., Huang, B., Chen, N., Liu, M., Jiang, J., and Mao, Z. Moga: Mixture-of-groups attention for end-to-end long video generation. CoRR ,2025b. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuan-video: A systematic framework for large video generative models. CoRR , 2024. Liang, J., He, R., and Tan, T. A comprehensive survey on test-time adaptation under distribution shifts. IJCV , 2025. Liu, F., Wang, H., Cai, Y., Zhang, K., Zhan, X., and Duan, Y. Video-t1: Test-time scaling for video generation. CoRR ,2025a. Liu, K., Hu, W., Xu, J., Shan, Y., and Lu, S. Rolling forcing: Autoregressive long video diffusion in real time. CoRR ,2025b. 10 Pathwise Test-Time Correction for Autoregressive Long Video Generation 

Lu, Y., Zeng, Y., Li, H., Ouyang, H., Wang, Q., Cheng, K. L., Zhu, J., Cao, H., Zhang, Z., Zhu, X., et al. Re-ward forcing: Efficient streaming video generation with rewarded distribution matching distillation. CoRR , 2025. Ma, X., Wang, Y., Chen, X., Jia, G., Liu, Z., Li, Y., Chen, C., and Qiao, Y. Latte: Latent diffusion transformer for video generation. TMLR , 2025a. Ma, Y., Liu, C., Wang, J., Liu, J., Huang, H., Wu, Z., Zhang, C., and Li, X. Tempomaster: Efficient long video genera-tion via next-frame-rate prediction. CoRR , 2025b. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P., Li, S., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., J Â´egou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learn-ing robust visual features without supervision. TMLR ,2024, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV , 2023. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: A cast of media foundation models. CoRR ,2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In 

ICML , 2021. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR , 2022. Sun, W., Zhang, H., Wang, H., Wu, J., Wang, Z., Wang, Z., Wang, Y., Zhang, J., Wang, T., and Guo, C. Worldplay: Towards long-term geometric consistency for real-time interactive world modeling. CoRR , 2025. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for general-ization under distribution shifts. In ICML , 2020. Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W., Luo, W., et al. Magi-1: Autore-gressive video generation at scale. CoRR , 2025. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. CoRR , 2025. Wang, R., Sun, Y., Tandon, A., Gandelsman, Y., Chen, X., Efros, A. A., and Wang, X. Test-time training on video streams. JMLR , 2025. Xiang, X., Chen, Y., Zhang, G., Wang, Z., Gao, Z., Xiang, Q., Shang, G., Liu, J., Huang, H., Gao, Y., et al. Macro-from-micro planning for high-quality and parallelized autoregressive long video generation. CoRR , 2025. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Real-time interactive long video generation. CoRR , 2025. Yesiltepe, H., Meral, T. H. S., Akan, A. K., Oktay, K., and Yanardag, P. Infinity-rope: Action-controllable infinite video generation emerges from autoregressive self-rollout. 

CoRR , 2025. Yi, J., Jang, W., Cho, P. H., Nam, J., Yoon, H., and Kim, S. Deep forcing: Training-free long video generation with deep sink and participative compression. CoRR , 2025. Yin, S., Wu, C., Yang, H., Wang, J., Wang, X., Ni, M., Yang, Z., Li, L., Liu, S., Yang, F., et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. In 

ACL , 2023. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast causal video generators. In CVPR , 2025a. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast autoregressive video diffusion models. In CVPR ,2025b. Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., and Liu, X. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In SIGGRAPH Asia , 2025a. Yu, Z., Hayakawa, A., Ishii, M., Yu, Q., Shibuya, T., Zhang, J., and Mitsufuji, Y. Autorefiner: Improving autoregres-sive video diffusion models via reflective refinement over the stochastic sampling path. CoRR , 2025b. Zhang, G., Shi, C., Jiang, Z., Xiang, X., Qian, J., Shi, S., and Jiang, L. Proteus-id: Id-consistent and motion-coherent video customization. CoRR , 2025a. 11 Pathwise Test-Time Correction for Autoregressive Long Video Generation 

Zhang, L., Cai, S., Li, M., Wetzstein, G., and Agrawala, M. Frame context packing and drift prevention in next-frame-prediction video diffusion models. In NeurIPS ,2025b. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. In ICML , 2025c. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR , 2018. Zhu, S., Chen, J. L., Dai, Z., Dong, Z., Xu, Y., Cao, X., Yao, Y., Zhu, H., and Zhu, S. Champ: Controllable and consistent human image animation with 3d parametric guidance. In ECCV , 2024. 12 Pathwise Test-Time Correction for Autoregressive Long Video Generation 

## A. Details on Samplers. 

Few-step stochastic sampling. The autoregressive video diffusion backbones used in our frameworkâ€” Self-Forcing and 

CausVid â€”are obtained by step distillation from a multi-step bidirectional video diffusion model trained with the Rectified Flow (RF) objective. In RF, the forward noising process is defined by a linear interpolation between a clean latent video x0

and an isotropic Gaussian terminal state xTmax âˆ¼ N (0 , I ):

xt = t x 0 + (1 âˆ’ t) xTmax , t âˆˆ [0 , 1] . (11) Differentiating (11) with respect to t gives the corresponding velocity along the path, 

vt â‰œ dxt

dt = x0 âˆ’ xTmax , (12) which is constant in t under this parameterization. A time-conditioned flow network vÎ¸0 (xt, t ) is trained to regress this velocity via mean squared error, 

Lflow = Ex0, x Tmax , t 

h

vÎ¸0 (xt, t ) âˆ’ vt

> 22

i

. (13) For long-horizon video generation, the bidirectional RF predictor vÎ¸0 is distilled into a causal autoregressive model vÎ¸ by replacing bidirectional attention with causal attention, so that the prediction for the i-th frame conditions only on previously generated frames x<i (with KV caching used to reuse past attention states during sequential generation). At inference, sampling is carried out on a small set of discrete timesteps {TJ > T Jâˆ’1 > Â· Â· Â· > T 0} with J much smaller than standard multi-step samplers. Given a noisy latent at step Tj , the model outputs a denoised estimate using the RF update form 

Ë†x i 

> 0|tj

= GÎ¸



x itj ; x<i , t j



= x itj + (1 âˆ’ tj ) vÎ¸



x itj ; x<i , t j



, (14) and then constructs the next noisy state x itjâˆ’1 by re-applying the RF forward interpolation with newly sampled Gaussian noise, i.e., using (11) with t = tjâˆ’1. This yields a stochastic few-step sampler in which independent Gaussian noise is injected at each transition between adjacent timesteps. 

ODE-based sampling. Rectified Flow also supports a deterministic sampler by treating the learned velocity predictor as an ordinary differential equation (ODE). Given the causal autoregressive velocity field vÎ¸ (Â·; x<i , t ), one can define the sampling dynamics as 

dxt

dt = vÎ¸ (xt; x<i , t ), xt=1 âˆ¼ N (0 , I ), (15) which maps an initial Gaussian state at t = 1 to a terminal sample at t = 0 through deterministic integration. In practice, (15) is approximated on a discrete time grid {TJ > T Jâˆ’1 > Â· Â· Â· > T 0}. Compared with the stochastic transition that re-samples Gaussian noise at each step, 

fÎ¸,t j



x itj



= Î¨ 



Ë†x i 

> 0|tj

, Ïµ ijâˆ’1, t jâˆ’1



, Ïµ ijâˆ’1 âˆ¼ N (0 , I ), (16) an ODE sampler removes the noise variable Ïµ ijâˆ’1 and replaces the transition with a deterministic one-step integrator. Using the explicit Euler method, the update from tj to tjâˆ’1 is 

ËœfÎ¸,t j



x itj



= x itj + ( tjâˆ’1 âˆ’ tj ) vÎ¸



x itj ; x<i , t j



. (17) Equation (17) is the standard explicit Euler discretization of the ODE (15) on the chosen timestep schedule. Finally, note that the two samplers differ only in whether the transition between adjacent timesteps introduces an additional Gaussian perturbation (stochastic) or performs a purely deterministic numerical integration step (ODE). Under the ODE formulation, once the initial state xt=1 and the discretization scheme are fixed, the generated trajectory is fully determined by repeated application of (17). 13 Pathwise Test-Time Correction for Autoregressive Long Video Generation 

## B. Details on Evaluations 

Boundary Continuity (t-LPIPS). To measure perceptual discontinuities at segment junctions in autoregressive generation, we compute LPIPS (Zhang et al., 2018) only on boundary-adjacent frame pairs. Our autoregressive model generates a video as K consecutive chunks. Let tk denote the last frame index of the k-th chunk; then the k-th boundary is the adjacent pair 

(ftk , f tk +1 ) for k âˆˆ { 1, . . . , K âˆ’ 1}. We define the boundary score as the mean LPIPS over these K âˆ’ 1 pairs: LPIPS boundary = 1

K âˆ’ 1

> Kâˆ’1

X

> k=1

LPIPS (ftk , f tk +1 ) . (18) This metric isolates changes that occur specifically when switching from one generated chunk to the next, rather than averaging over all within-chunk frame pairs. 

Color Shift (HSV Histogram). To quantify color distribution changes across the generated sequence, we compare the color histograms of the first and last frames. Let fstart and fend be the initial and final frames of a generated video. We convert both frames to HSV space and compute an L1-normalized histogram of the Hue channel using 180 bins, denoted by hstart , h end âˆˆ R180 with âˆ¥hstart âˆ¥1 = âˆ¥hend âˆ¥1 = 1 . We report two statistics between hstart and hend : (i) the L1 distance, 

âˆ¥hstart âˆ’ hend âˆ¥1, and (ii) the Pearson correlation coefficient, Ï(hstart , h end ).                                                                             

> import torch
> import torch.nn.functional as F
> from torch.autograd.functional import jacobian
> #phi(.) :frozen JEPA-style encoder (e.g., V-JEPA /DINOv2) #(1) JEPA-score: Jacobian-based local complexity /density proxy
> J=jacobian( lambda x: phi(x). sum (0), X) #X: (B,C,H,W)
> with torch.inference_mode(): J=J.flatten(2).permute(1, 0, 2) #(B, d, HW)
> sv =torch.linalg.svdvals(J) #singular values
> JEPA_score =sv.clamp( min =eps).log(). sum (1) #(B,) #(2) JEPA consistency: first-frame anchored temporal drift
> Z=phi(I_1_T) #(T, d)
> Z=F.normalize(Z, dim=-1) z_ref =Z[0] d_t =1.0 -(Z @z_ref) #(T,)
> JEPA_Std =d_t.std() JEPA_Diff =(d_t[-1] -d_t[0]). abs ()

Figure 10. JEPA-score and JEPA consistency for long-video evaluation. 

JEPA Consistency To rigorously quantify both the intrinsic distribution fidelity and the long-horizon semantic stability of autoregressive video generation, we adopt a dual-metric eval-uation framework based on a frozen V-JEPA encoder Ï•(Â·), grounded in recent theoretical findings that JEPA representations implicitly encode data density through Gaussian embed-dings and local volume changes of the encoder mapping. Specifically, for each generated frame (or short temporal clip) xt, we compute the encoder Jacobian JÏ•(xt) = âˆ‚Ï• (xt)/âˆ‚x t and define an Intrinsic Density Score as Sdens  

> t

âˆ 

> 12

log det  JÏ•(xt)âŠ¤JÏ•(xt), which estimates the local log-volume expansion induced by Ï•

and thus serves as a proxy for the sampleâ€™s like-lihood under the learned data manifold; a mono-tonic decay of Sdens  

> t

along time indicates pro-gressive manifold departure and hallucination as the generation drifts into low-density regions of the data distribution. In parallel, to measure global semantic consistency, we compute the normalized embedding trajectory zt = norm( Ï•(xt)) and define the Temporal Drift Distance relative to the initial semantic anchor as dt = 1 âˆ’ zâŠ¤ 

> t

z1, which captures distributional deviation in the JEPA-induced representation space. Aggregating these frame-wise measurements at a fixed temporal granularity (e.g., per second), we report two summary statistics: JEPA -Std = Std( {dt}Tt=1 ) to characterize the volatility of representation drift, and JEPA -Diff = |dT âˆ’ d1| to quantify the accumulated long-range semantic deviation, thereby providing a holistic assessment of a modelâ€™s robustness to both distributional collapse and semantic drift in long-horizon video generation. 

Test-time Scaling Configuration. We compare against two inference-time scaling protocols under a fixed sampling budget of N = 5 . Best-of-N (BoN) performs selection at the trajectory level. For each video segment, we run N independent sampling trajectories by drawing N independent initial noise latents. Each trajectory is rolled out to a complete segment, and we compute a scalar reward for the resulting segment. Among the N completed candidates, we keep the one with the highest reward score as the output of that segment. Search-over-Path (SoP) performs selection at the step level on the same timestep schedule. At each denoising timestep, we generate N candidate next-step latents by injecting N independent Gaussian noise realizations for that transition (equivalently, N candidate stochastic updates from the current latent). We then evaluate the reward for each candidate at that timestep and select the candidate with the highest reward as the current latent for the next timestep. This greedy selection is repeated until the segment is completed. 14 Pathwise Test-Time Correction for Autoregressive Long Video Generation 

## C. Details on Methods. 

Details on Test-time Optimization (TTO). Following the test-time adaptation protocols established in HyperNoise (Eyring et al., 2025) and AutoRefiner (Yu et al., 2025b), we perform gradient-based optimization at each sampling step. We employ an AdamW optimizer with a learning rate of 1 Ã— 10 âˆ’4. Specifically, at each denoising step for each latent chunk, the latent prediction is first decoded into pixel space via a pre-trained VAE decoder (Wan et al., 2025). We then compute the Mean Squared Error (MSE) loss and the CLIP score (Radford et al., 2021) on the decoded image with the initial image, which serve as proxies for pixel-level and semantic-level rewards, respectively, to guide the optimization process. 

## D. Further Quantitative Results. 

Full VBench Scores. We conduct a comprehensive evaluation on the full VBench benchmark, using all 946 prompts and covering all 16 metrics reported in Table 6. For detailed metric definitions, we refer readers to the VBench paper. All values are computed with the official standardized evaluation scripts. Our method achieves substantial improvements in overall quality, particularly in frame-wise fidelity, and also outperforms distilled baselines on semantic scores. 

Table 6. Full evaluation on VBench metrics. We evaluate the performance across all quality and semantic dimensions. 

Method Quality Metrics Quality Score Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality CausVid (Yin et al., 2025a) 89.1 90.7 99.3 98.0 62.5 61.7 65.3 80.8 

CausVid + Ours 91.4 92.2 99.2 97.4 71.9 61.1 66.4 81.9 Self-Forcing (Huang et al., 2025c) 89.8 90.7 98.1 98.5 69.4 60.0 68.7 81.4 

Self-Forcing + Ours 91.1 91.7 98.2 98.8 68.1 60.7 68.6 82.1                                              

> Method Semantic Metrics Semantic Score Object Class Multiple Objects Human Action Color Spatial Relationship Scene Temporal Style Appearance Style Overall Consistency CausVid (Yin et al., 2025a) 77.4 58.8 77.0 84.2 61.8 32.2 22.4 19.9 23.0 65.9
> CausVid + Ours 76.0 62.0 80.0 80.6 63.9 34.9 22.1 19.7 22.9 66.3 Self-Forcing (Huang et al., 2025c) 81.9 61.9 81.0 88.0 79.2 32.2 23.6 19.6 23.8 70.0
> Self-Forcing + Ours 81.6 66.5 82.0 92.2 80.2 30.8 23.1 19.4 23.6 70.7

Table 7. Dynamic Degree Analysis.               

> Method Dynamic Degree
> LPIPS â†‘SSIM â†“PSNR â†“
> Rolling Forcing (Liu et al., 2025b) 0.2956 0.5738 16.2365 Longlive (Yang et al., 2025) 0.3056 0.5969 16.8669 Self-Forcing (Huang et al., 2025c) 0.3548 0.5377 15.6360
> Ours 0.3489 0.5440 15.5279

Dynamic Preservation. Adhering to the evaluation protocols estab-lished in AutoRefiner (Yu et al., 2025b), we conduct a comparative analysis of the dynamic degree against baseline methods, including Self-Forcing, Rolling Forcing, and Longlive. To rigorously assess the dynamic degree, we quantify the perceptual variation between temporally strided frames utilizing metrics such as LPIPS, SSIM, and PSNR with a fixed sampling interval k (e.g., k = 12 ). We model the magnitude of motion and structural evolution over time by computing the average distance D = Et[M(ft, f t+k)] , where M represents the specific metric function (e.g., LPIPS) and ft denotes the frame at time step t. As evidenced in Tab. 7, unlike baseline approaches that often compromise motion magnitude to ensure stability, our method sustains a superior dynamic degree while preserving temporal coherence, thereby effectively maintaining the vividness of the generated content. 

## E. Further Qualitative Results. 

We provide additional visual results to further demonstrate the effectiveness of our method. Figure 11, Figure 12, and Figure 13 present more generated examples under diverse scenarios. These results consistently exhibit high visual quality and temporal coherence, reinforcing the robustness of our approach across different prompts and settings. 15 Pathwise Test-Time Correction for Autoregressive Long Video Generation Rolling Foricng 

LongLive 

Ours 

Self Forcing 

Self Forcing 

Rolling Foricng 

LongLive 

Ours 

> A candid shot of a young man joking with friends indoors.
> A candid street shot of a young man pranking a friend.

Rolling Foricng 

LongLive 

Ours 

Self Forcing 

> A close -up 3D shot of a curious fluffy monster and candle.

Figure 11. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. 

16 Pathwise Test-Time Correction for Autoregressive Long Video Generation Rolling Foricng 

LongLive 

Ours 

Self Forcing 

Self Forcing 

Rolling Foricng 

LongLive 

Ours 

> A candid close -up of a frustrated man searching for his keys.
> A candid low -angle shot of a woman enjoying a juicy apple.

Rolling Foricng 

LongLive 

Ours 

Self Forcing 

> A warm shot of a toddler sharing a cookie with a teddy bear.

Figure 12. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. 

17 Pathwise Test-Time Correction for Autoregressive Long Video Generation Rolling Foricng 

LongLive 

Ours 

Self Forcing 

Self Forcing 

Rolling Foricng 

LongLive 

Ours 

> A candid close -up of a yawning child in a cozy room.
> A watercolor illustration of a rabbit wearing glasses reading a newspaper.

Rolling Foricng 

LongLive 

Ours 

Self Forcing 

> A realistic excavation scene of archaeologists uncovering a buried plastic chair.

Figure 13. Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods. 

18
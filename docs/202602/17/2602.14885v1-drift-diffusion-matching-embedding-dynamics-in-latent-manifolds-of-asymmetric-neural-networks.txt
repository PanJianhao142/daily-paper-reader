Title: Drift-Diffusion Matching: Embedding dynamics in latent manifolds of asymmetric neural networks

URL Source: https://arxiv.org/pdf/2602.14885v1

Published Time: Tue, 17 Feb 2026 03:21:07 GMT

Number of Pages: 23

Markdown Content:
# Drift-Diffusion Matching: Embedding dynamics in latent manifolds of asymmetric neural networks 

Ram´ on Nartallo-Kaluarachchi ∗,†, Renaud Lambiotte ∗,‡, and Alain Goriely ∗

> ∗

Mathematical Institute, University of Oxford, Woodstock Road, Oxford, OX2 6GG, United Kingdom 

> †

Centre for Eudaimonia and Human Flourishing, University of Oxford, 7 Stoke Pl, Oxford, OX3 9BX, United Kingdom 

> ‡

Complexity Science Hub, Metternichgasse 8, Vienna, 1030, Austria 

{ramon.nartallo-kaluarachchi }{ renaud.lambiotte }{ alain.goriely }@maths.ox.ac.uk (Dated: February 17, 2026) Recurrent neural networks (RNNs) provide a theoretical framework for understanding computation in bi-ological neural circuits, yet classical results, such as Hopfield’s model of associative memory, rely on sym-metric connectivity that restricts network dynamics to gradient-like flows. In contrast, biological networks support rich time-dependent behaviour facilitated by their asymmetry. Here we introduce a general framework, which we term drift–diffusion matching, for training continuous-time RNNs to represent arbitrary stochastic dynamical systems within a low-dimensional latent subspace. Allowing asymmetric connectivity, we show that RNNs can faithfully embed the drift and diffusion of a given stochastic differential equation, including nonlinear and nonequilibrium dynamics such as chaotic attractors. As an application, we construct RNN re-alisations of stochastic systems that transiently explore various attractors through both input-driven switching and autonomous transitions driven by nonequilibrium currents, which we interpret as models of associative and sequential (episodic) memory. To elucidate how these dynamics are encoded in the network, we intro-duce decompositions of the RNN based on its asymmetric connectivity and its time-irreversibility. Our results extend attractor neural network theory beyond equilibrium, showing that asymmetric neural populations can implement a broad class of dynamical computations within low-dimensional manifolds, unifying ideas from associative memory, nonequilibrium statistical mechanics, and neural computation. 

I. INTRODUCTION 

Understanding how populations of neurons encode infor-mation and perform computations is one of the most signif-icant problems in neuroscience [1]. Central to the ‘connec-tionist’ perspective, is the idea that cognition is performed by distributed networks of interacting neurons. However, un-like the feed-forward architectures that dominate modern ma-chine learning, the neuronal networks of the brain are re-current, leading to a rich repertoire of time-evolving dynam-ics. As a result, classes of recurrent neural networks (RNNs) have become the prototypical models for dynamic computa-tion in biological neural circuits [2]. These networks, and variants thereof, have been used extensively to model neural data [3, 4], develop theories of neural computation [5, 6], as well as perform sequence learning tasks, such as the predic-tion and encoding of chaotic time-series [7–9]. One of the most influential results in the study of RNNs is Hopfield’s model of associative memory [10, 11]. In the continuous version of the model, the dynamics are identical to that of a so-called ‘vanilla’ RNN. Such a system is composed of n neurons, each with internal state ui and output state vi =

h(ui), where h is a monotonic nonlinear activation function (we will take h(x) = tanh (x)). The dynamics of the internal state are given by the stochastic differential equation (SDE), 

du(t) = F(u) dt + B d w(t), (1) where u = ( u1, .., un) and F = ( F1, ..., Fn) is the drift function, with, 

Fi = −ui + ∑

> j

Wi j v j + Ii. (2) Here B ∈ Rn×d and w(t) is a Wiener process with d indepen-dent sources of noise [12]. We also define ϒ = BB ⊤/2 to be the positive semidefinite diffusion matrix of rank d. The con-nectivity from neuron j to neuron i is given by Wi j , and Ii is the input current to neuron i. When the connectivity is con-strained to be symmetric, W = W ⊤, we introduce the energy potential ,

E(v) = − 12

> n

# ∑

> j,i

Wi j viv j − ∑

> i

Iivi +

> n

# ∑

> i

Z vi

> 0

h−1(ω) dω, (3) and ˜ E(u) = E(h(u)) . The drift can be written in the form, 

F(u) = −∇vE(v), (4) 

= −D(u)∇u ˜E(u), (5) where D(u) = diag 

 1 

> h′(u1)

, ..., 1

> h′(un)



is a positive-definite matrix-valued function. As a result, the RNN is constrained to perform ‘generalised’ gradient descent on the energy land-scape, eventually arriving at an energy minimum. 1 Hopfield’s model allows for the manipulation of the energy landscape via training, such that a desired ‘memory’ can be encoded as an energy minimum. The result is an autonomous neural circuit that will ‘retrieve the memory given initial information’ i.e. converge to the energy minimum from an appropriate initial state. Hopfield’s results laid the ground work for the study of 

attractor neural networks , and their possible implications for neuroscience [13, 14].      

> 1The prefactor D(u)causes the gradient to be scaled unevenly in different directions, thus the system does not follow the Euclidean direction of steepest descent on the landscape. Instead, the drift is a gradient flow with respect to the Riemannian metric defined by D(u)−1. Practically, we would see that the system descends the landscape monotonically, but more quickly in directions where h′(ui)is small.
> arXiv:2602.14885v1 [cond-mat.dis-nn] 16 Feb 2026

2Despite Hopfield’s application to associative memory, lim-iting to symmetric connectivity severely constrains the reper-toire of dynamics within the system, precluding the existence of limit-cycles and other dynamic phenomena. Moreover, asymmetry is ubiquitous in biological neural circuits, thus the symmetric assumption limits the neuroscientific plausibility of such a model. Numerous studies have analysed asymmetric Hopfield networks, but these typically consider the discrete, spin-glass form of the model [15–17], and analyse the attract-ing states emerging from random connectivity. On the other hand, in the continuous model, asymmetric connectivity dis-rupts the energy landscape eroding the ability of the network to preserve attracting states [18]. Moreover, the dichotomy of symmetric and asymmetric connectivity is closely related to the necessary departure from equilibrium to nonequilibrium statistical mechanics, which has become an emerging area of research in neuroscience, neural computation, and associative memory [18–24]. 2

In an entirely separate development, dimensionality re-duction techniques have shown the emergence of low-dimensional, latent structure in high-dimensional neural ac-tivity associated with tasks like motor control, navigation and working memory [25, 26]. These so-called neural manifolds 

indicate that high-dimensional brain network activity is, in fact, constrained to low-dimensional subspaces where the dy-namics are often more mechanistically interpretable. These low-dimensional dynamics are responsible for computation and may also emerge in RNNs [27], typically associated with 

low-rank structure in the connectivity [28, 29]. Neural mani-folds are just a single example forming part of the more gen-eral hypothesis that real-world network dynamics are effec-tively low-dimensional [30, 31], which in turn can be seen as a special case of the well-known manifold hypothesis for high-dimensional data [32]. Neural data analysis has provided sig-nificant evidence that computational dynamics are encoded at the level of latent manifolds in empiricial brain network activ-ity [25, 26, 33]. As a result, understanding how such cognitive processes can be encoded in RNNs via neural manifolds is crucial to understanding the mechanisms and limits of neural computation. In this article, we fuse together asymmetric attractor neu-ral networks with the theory of neural manifolds by proposing the drift-diffusion matching framework to train RNNs with the objective of representing any target SDE in a latent affine sub-space (Fig. 1). As a result, we show that sufficiently large RNNs can encode arbitrary dynamics in this latent space, pro-vided asymmetric connectivity is permitted, illustrating this by embedding nonlinear, and nonequilibrium systems, such as chaotic attractors, within the RNN. Next, to illustrate how the 

> 2It is worth noting that Hopfield’s discrete, stochastic model [10] is equivalent to a symmetric Ising model, and therefore is in thermodynamic equilibrium [23]. Turning to the continuous model [11], we note that whilst gradient-flow dynamics are reversible [12], this model is only a gradient flow in the appropriate Riemannian metric, thus does not preserve detailed bal-ance in Euclidean space. Nevertheless, it is constrained to descend an energy function and cannot admit rich, time-dependent, nonequilibrium behaviour without asymmetry.

theory can be linked to associative memory, we introduce two classes of SDEs, both of which have a number of attracting states. The first is the gradient of a tiltable energy potential, which allows the network to choose between energy minima using input-driven switching. The second is a nonequilib-rium stationary diffusion that cycles autonomously between attractors in a specified order using irreversible currents. We present these as illustrative models of both input-driven asso-ciative and sequential (episodic) associative memory [34]. We apply our framework to show that such dynamics can be en-coded in a latent subspace of an asymmetric RNN. In order to investigate how the dynamics are encoded in the network, we introduce two decompositions of an RNN, the first based on the asymmetry of the connectivity, and the second on the time-irreversibility of the RNN. This theoretical model sug-gests that populations of neurons with asymmetric connectiv-ity are capable of encoding a wide array of dynamical systems, including attractor-switching and -cycling dynamics, thus ex-tending Hopfield’s model and integrating it with the theory of neural manifolds.   

> II. RECURRENT NEURAL NETWORKS WITH TARGET LATENT DYNAMICS A. Drift-diffusion matching

We focus once more on the RNN of the form, 

du(t) = F(u) dt + B d w(t), (6) 

Fi = −ui + ∑

> j

Wi j v j + Ii,

for i = 1, ..., n, and consider a latent affine subspace A = {u ∈

Rn : u = Γy + b, y ∈ Rk} for some Γ ∈ Rn×k and b ∈ Rn for some latent dimension k ≪ n. We first ask if {W, I, B, Γ, b}

can be chosen such that the dynamics of our latent parametri-sation, y(t), follow a target SDE of the form, 

dy(t) = f(y) dt + σ dw(t), (7) where f is the drift of the target process, and σ dw(t) is isotropic noise in Rk.A variety of methods exist to train RNNs. For systems with feedback, such as reservoir computers, approaches include ‘echo-state’ [35], ‘FORCE’ learning [5], direct programming [8], and the ‘Neural Engineering Framework’ [36]. For se-quence learning from data, models are trained through back-propagation through time (BPTT) [37], a training procedure that is computationally expensive, especially for stochastic data, and numerically unstable, often suffering from vanish-ing and exploding gradients [38]. Instead, we opt to train our RNNs using, what we denote as, drift-diffusion matching 

(DDM), where we directly align the drift and diffusive terms in the affine subspace to the target SDE. Firstly, we assume that Γ has full rank k, thus it has a Moore-Penrose (MP) in-verse such that Γ†Γ = Ik, where Ik is the identity in Rk. Using ˆIto’s rule [12], we have that the drift in the affine subspace is 3a) b) c)                

> FIG. 1. Embedding dynamics in latent manifolds of RNNs. a)Our framework focuses on RNNs with low-rank connectivity of the form
> W=ΓWs, where Γ∈Rn×kwith k≪n. We train these autonomous systems to encode arbitrary stochastic dynamics in a learnt affine subspace.
> b)The training of the RNN can be reformulated as the training of a two-layer perceptron to approximate a target vector field. c)Once the parameters of the RNN have been fitted, it can be integrated over time as an autonomous circuit which produces stochastic dynamics. Here we plot traces from 5 of 64 neurons in a network trained to approximate the van der Pol oscillator. When we project the dynamics into the learnt subspace, we recover a trajectory from the stochastic van der Pol system, which has been embedded into the RNN.

given by, ˆf(y) = Γ† (F(Γy + b)) , (8) whilst the diffusion matrix is, 

Σ = Γ†BB ⊤(Γ†)⊤. (9) However, matching ˆ f = f and Σ = σ 2Ik, is not sufficient to guarantee that projections of trajectories from the RNN, will behave as trajectories from the target SDE. This is due to the presence of the drift and diffusion terms in directions or-thogonal to the affine subspace. To guarantee that DDM en-forces that projections of the RNN trajectory recover trajec-tories from the target SDE, we must ensure there is a one-to-one relationship between the latent point y(t) and the high-dimensional u(t). Each point in the trajectory can be uniquely decomposed as, 

u(t) = u|| (t) + u⊥(t), (10) where u|| = Γy + b ∈ A , whilst u⊥ ⊥ Im (Γ) i.e. u⊥ is or-thogonal to the image of Γ. As a result, Γ†(u − b) = y for all different u⊥ ⊥ Im (Γ), thus it is not injective. To prevent dynamics in directions that are not ‘visible’ un-der the projection, we enforce a low-rank parametrisation of the RNN, 

W = ΓWs, I = ΓIs + b, B = ΓBs, (11) where Ws ∈ Rk×n, Is ∈ Rk, Bs ∈ Rk×d , where we recall that d

is the number of independent noise sources. The result of this parameterisation is that, for any u ∈ A , we have that, 

F(u) = −u + ΓWsh(u) + ΓIs + b ∈ Im (Γ), (12) where h(u) = ( h(u1), ..., h(un)) . Moreover, we have that 

ΓBs dw(t) ∈ Im (Γ). Together, these imply that, given u(0) ∈

A , the drift and diffusion are tangent to the subspace, and thus u(t) ∈ A for all t > 0 i.e the dynamics are constrained to the affine subspace. When the process begins outside the subspace, the ‘leak term’, −u, will cause the process to drift to the origin along the directions orthogonal to Im (Γ).Under this parametrisation, the dynamics in the affine sub-space simplify significantly, ˆf(y) + y = Wsh(Γy + b) + Is, (13) where we have moved the leak term to the left-hand-side for convenience. Here, we can notice the right-hand-side is equiv-alent to a two-layer perceptron with a k-neuron input layer, a n−neuron hidden layer, and a k-neuron output layer (Fig. 1b)). The input-to-hidden weights and biases are given by Γ

and b respectively, whilst the hidden-to-output weights and biases are given by Ws and Is, and where h(·) plays the role of the element-wise activation function. It is well-known that this parametrisation is a universal approximator i.e. it can ap-proximate any continuous function to arbitrary accuracy [39]. Moreover, it can be efficiently optimised via backpropagation of error [40]. The diffusion matrix in the affine subspace also simplifies to BsB⊤ 

> s

.Given this insight, we can setup the DDM loss function to be, 

LDDM = || f(y) + y −Wsh(Γy + b) − Is|| (14) 

+ λdiff || σ 2Ik − BsB⊤ 

> s

|| ,

where λdiff is a hyper-parameter controlling the relative im-portance of the drift and diffusion losses, and where we op-timise over the parameters {Γ, b,Ws, Is, Bs}, for a total of 2nk + n + k + kd free parameters. 4 

> B. Latent dynamics of symmetric networks

Previously, we noted that the drift of an RNN with symmet-ric connectivity can be written in the form, 

F(u) = −D(u)∇u ˜E(u),

i.e. as a generalised gradient flow that monotonically de-scends the energy landscape, defined in Eq. (3), to a local minimum. Now we focus on the case of a low-rank, but sym-metric network projected onto the affine subspace, A , given the parametrisation in Eq. (11). We define an additional energy function on A , S(y) = 

˜E(Γy+b) whose gradient is therefore ∇yS(y) = Γ⊤∇u ˜E(Γy+

b). The dynamics in the affine subspace are given by, ˆf(y) = −Γ†D(Γy + b)( Γ†)⊤∇yS(y), (15) 

= −K(y)∇yS(y),

where K(y) is positive definite i.e. the dynamics in A also admit an energy function. As a result, a symmetric RNN in-duces an energy potential over the subspace, and precludes limit-cycles or chaotic attractors. Therefore, any dynamics which go beyond monotonic decreases in energy must be en-coded via the asymmetry of the connectivity matrix.  

> C. Symmetric-asymmetric decomposition of neural network dynamics

The observations made in the previous section, that the pro-jected dynamics of a symmetric RNN remain a generalised gradient flow, motivate a decomposition of a trained RNN into a symmetric and asymmetric component. We consider 

W = Wsym + Wasym , where the symmetric component, Wsym ,descends an energy landscape, whilst the asymmetric compo-nent, Wasym , encodes rotational dynamics, ˆf = ˆfsym + ˆfasym , (16) ˆfsym (u) = −u +Wsym h(u) + I,

ˆfasym (u) = Wasym h(u).

This decomposition has similarities to the Helmholtz-Hodge decomposition of a vector field [41] or SDE [42], or the GENERIC decomposition of a stochastic process [43]. How-ever, it is not identical to either, as the symmetric compo-nent is neither a conservative field, nor is it equal to the time-reversible component of the stochastic process. We consider decompositions based on time-irreversibility in more detail in Sec. III and IV. At first glance, it is natural to use the unique decomposition of the network into a symmetric and antisymmetric compo-nent, 

W = 12



W +W ⊤

+ 12



W −W ⊤

, (17) where antisymmetric means A = −A⊤. However, with this decomposition, the drift components, ˆ fsym and ˆ fasym , are no longer confined to the affine subspace. As a result, whilst the symmetric network will still admit an energy function on Rn,for the projected dynamics to follow an energy function, S(y),on Rk requires that Wsym = ΓA for some A, which is not neces-sarily true, even if W = ΓWs. Thus the projected dynamics of this symmetric component of the network are not ‘attractor’-like and, in general, the process does not converge to the min-imum of the projected energy function S(y).To tackle this problem, we introduce a more motivated de-composition given the low-rank structure of the network. We split the network as, 

W = Γ(Ω + Π), (18) where ΓΩ = ( ΓΩ )⊤ i.e. it is symmetric. Let us note that ΓΠ 

is asymmetric, unless W = W ⊤, but not antisymmetric, except in the special case when this decomposition coincides with Eq. (17). Such a decomposition is clearly not unique, thus we define the ‘best’ decomposition to be, 

Ω = argmin A∈Rk×n || ΓA −C|| 2 

> F

, (19) where C = 12 (W + W ⊤) i.e. our symmetric part is the most similar to the canonical decomposition, with respect to the Frobenius norm. This problem has a closed form solution (see App. B), 

Ω = Γ†CΓΓ †. (20) This ‘improved’ decomposition gives two drift components, where both are constrained to the affine subspace, and where ˆfsym descends the energy function S(y) associated with the network Wsym = ΓΩ . 

> D. Pipeline and examples

To illustrate the DDM framework and the network decom-position, we first consider three well-known nonlinear sys-tems with additive noise. We train an RNN with 64, 512, and 1024 neurons for each of the systems respectively, us-ing the DDM framework. Using the fitted parameters, we integrate the RNN as an autonomous SDE using the Euler-Maruyama scheme (see App. A 1). Projecting the dynamics into the learnt affine subspace, we recover trajectories from the target SDE. Next, we decompose the RNN connectivity as described in Sec. II C, where we can investigate which fea-tures of the dynamics are encoded in each part of the network respectively. The examples we consider include: • The stochastic van der Pol (VDP) oscillator is a nonlin-ear, planar system with a stable limit cycle. It is given by the system, 

d

y1(t)

y2(t)



=

 y2

−y1 + μy2(1 − y21)



dt + σ d

w1(t)

w2(t)



, (21) where we take μ = 1 [44]. 5a) b) c) 

FIG. 2. Embedding attractors in neural networks. a) Traces from 5 randomly selected neurons in a 64-neuron RNN trained to embed a stochastic VDP oscillator in a latent subspace. In the subspace, we see the characteristic VDP limit-cycle. b − c) Traces from a 512-neuron RNN trained to embed the LA, and a 1024-neuron RNN trained to embed the DA. 

• The stochastic Lorenz attractor (LA) is a three-dimensional chaotic system given by, 

d

y1(t)

y2(t)

y3(t)

 =

 ς (y2 − y2)

y1(ρ − y3) − y2

y1y2 − β y3

 dt + σ d

w1(t)

w2(t)

w3(t)

 . (22) The system has a characteristic strange attractor with two scrolls when ς = 10, ρ = 28, and β = 8/3 [44]. • The stochastic Dadras attractor (DA) is a three-dimensional chaotic system given by, 

d

y1(t)

y2(t)

y3(t)

 =

y2 − py 1 + oy 2y3

ry 2 − y1y3 + y3

cy 1y2 − ey 3

 dt + σ d

w1(t)

w2(t)

w3(t)

 .

(23) The system has a strange attractor with three scrolls when p = 3, o = 2.7, r = 1.7, c = 2, and e = 9 [45]. Fig. 2 shows example traces from 5 randomly selected neu-6rons in each of the RNNs, alongside the phase-plane dynam-ics in the learnt latent space. We see that the system is able to encode all three nonlinear systems, and that the trajectories of individual neurons mirror the qualities of the embedded at-tractor. Fig. 3 shows the results of the decomposition for the VDP, LA, and DA examples. This decomposition gives us two vec-tor field components, one of which has an associated energy function. Panels a − c) show both components for each sys-tem, whilst Panels d − e) show characteristic isopotentials of the energy function for the LA and DA systems. It appears that for the VDP and DA systems, the symmetric component of the network encodes attracting dynamics that maintain the process near the attractor, whilst the asymmetric component drives nonlinear rotation in phase-space. On the other hand, for the LA, the energy gradient appears to drive the process away from the origin, with the asymmetric, rotational compo-nent preventing it from diverging.  

> E. Reconstructing affine subspaces from trajectories

Another pertinent question is whether or not we can recon-struct the appropriate affine subspace given only trajectories from the RNN. Given dynamics that are entirely constrained to an affine subspace, it is possible to exactly identify the sub-space, up to numerical accuracy. The solution comes from a seminal result by Pearson, and is known as principal compo-nent analysis (PCA) [46]. Additionally, if the trajectory data has measurement error or noise in the form of isotropic Gaus-sian variation in the directions orthogonal to the subspace, then it is not entirely constrained and an exact recovery is not possible, but PCA becomes the maximum-likelihood estima-tor of the subspace. Subspace identification through PCA is extremely common in the analysis of neural data, and has been used in countless studies to identify neural manifolds in brain network activity [33, 47, 48], as well as to identify latent sub-spaces in the activity of trained RNNs [27, 49]. This method is canonical and the proofs have been reviewed at length (e.g. [50]), thus we will only outline the approach in brief, and illustrate how it applies to our setting. Given trajec-tory data of the form {u(t j) : j = 1, ..., T }, we assume the data lives in an affine subspace of dimension k, where k typically begins as an unknown, and can be written in the form, 

u = Ψυυυ + μμμ, (24) where Ψ ∈ Rn×k has orthonormal columns, μμμ ∈ Rn, and 

υυυ(t) ∈ Rk is a parametrisation. Whilst it is tempting to associate Ψ with Γ and μμμ with 

b, we note that this is not necessarily the case. Whilst the subspace can be identified, the exact parametrisation cannot be, as for any invertible matrix A, we have indistinguishable parametrisations, u = Γy + b = ( ΓA−1)( Ay) + b = ˜Γ˜y + b.Moreover, our parameterisation could be shifted arbitrarily, 

u = Γ(y − c) + b + Γc. Thus, the parametrisation we obtain is equivalent to the original parametrisation, up to an affine transformation. We stack our time-series into the matrix U ∈ Rn×T and cen-ter it, X = U − ¯u, where ¯u = 1

T ∑

> j

u1(t j), ..., ∑

> j

un(t j)

!

. (25) This is equivalent to choosing μμμ = 000. Next, we take the singu-lar value decomposition (SVD) of X. For long trajectories, it is more efficient to compute the covariance matrix, Θ = XX ⊤,and compute its eigendecomposition. Assuming that the dy-namics are exactly constrained to the subspace, then this ma-trix will have k non-zero eigenvalues, whose eigenvectors form the columns of Ψ, which we call principal components 

(PCs), and where we order the columns by decreasing eigen-value magnitude, {λ1, ..., λk} and λ j = 0 for j ∈ { k + 1, ..., n}.The zero eigenvalues correspond to the directions orthogonal to the subspace, in which the dynamics do not evolve. For noisy data, these eigenvalues are non-zero but very small. In this case, the dimension of the subspace, k, can be approx-imately identified by considering the proportion of the vari-ance explained by each PC, ϑl = λl / ∑nj=1 λ j, where we ‘cut-off’ after k components, if ∑kj=1 ϑ j > c, where c is a threshold e.g. c = 0.7 [50]. We then project the time-series to obtain the parameterisation, υυυ = Ψ⊤X. Panels a − b) of Fig. 4 show the traces and phase-space dynamics of the 3 non-trivial PCs for a trajectory from the 512-neuron RNN trained to embed the LA. Given Γ and b, we can find the affine transformation that turns υυυ(t) to y(t). We can solve ΓΞ = Ψ and Γχχχ = ¯u + b as a least-squares problem, and apply the transformation y(t) = 

Ξυυυ(t) + χχχ, to obtain the original parametrisation, as shown in Panel c) of Fig. 4.  

> III. SWITCHING AND CYCLING BETWEEN ATTRACTORS

Attractor dynamics are frequently cited as both emergent features in neural data, and as a possible mechanism for com-putation via dynamics [13, 14]. These attractors are most easily understood in terms of energy landscapes, where the dynamics perform some kind of gradient descent, arriving at a local minima in the form of a fixed point or other low-dimensional manifold e.g. a line or a ring. On the one hand, a range of computational mechanisms, both neuroscientific and other, rely on such dynamics, including stochastic gradient descent [40], the ‘free-energy’ principle [51], and the class of so-called energy-based models [52]. On the other hand, in both neural and biological systems, asymmetry and non-reciprocity are ubiquitous, typically disrupting the dynam-ics associated with symmetric models and their energy land-scapes [18, 19, 53–55]. However, designing learning systems that utilise such dynamics has, so far, been evasive. Based on our method to train an asymmetric RNN to en-code an arbitrary system in a latent affine subspace, we now introduce two classes of SDEs that are defined by energy po-tential gradients and thus have attracting dynamics. The first of these uses network inputs to switch between the minima 7a) b) c) d) e) 

FIG. 3. Symmetric-asymmetric decomposition of RNNs. a) Decomposition of RNN trained to encode the VDP. The symmetric dynamics descend the contoured energy function. This appears to keep the process near the limit cycle, whilst the asymmetric component provides rotational dynamics. b − c) Decompositions of RNNs trained to encode the LA and DA. For the DA, the symmetric dynamics keep the process near the attractor, whilst the asymmetric component provides rotational dynamics. For the LA, it appears the symmetric component pushes the dynamics away from the origin, and the asymmetric component provides a restoring force. d − e) This is confirmed by plotting isopotentials of the energy functions for the LA and DA respectively. 8a) b) c)              

> FIG. 4. Reconstructing latent subspaces from trajectories .a)Trajectories from a low-rank RNN trained to embed a k−dimensional system only have knon-zero principal components. b)With PCA, we can identify the affine subspace in which the dynamics are constrained. c)The PCA representation is equivalent to original coordinates up to an affine linear transformation. Given (Γ,b)can solve for this transformation to recover the original coordinates.

of the energy potential. Here the network inputs represent sensory information or the output of a disjoint neural circuit, which prompts the RNN to retrieve a particular memory in light of this information. This is a more complex extension of Hopfield’s model, where the initial network state serves as the input, and the system cannot transition between memories. The second is a class of stationary diffusions that use irre-versible, rotational currents to cycle between minima in a pre-ferred direction. In this case, our model attempts to describe the ability of a memory system to autonomously transition be-tween a series of a related memories in a preferred direction. We show that both these dynamics can be encoded within an RNN, as illustrative examples of how a neural circuit could encode more complex forms of associative memory. Before progressing, we introduce some preliminaries for stochastic dynamics and their nonequilibrium steady-states. The general form for SDEs which we will consider is given by, 

dy(t) = f(y) dt + Σ dw(t), (26) with y(t) ∈ Rk and Σ ∈ Rk×d . The diffusion matrix is defined by D = ΣΣ ⊤/2 which is positive semidefinite. We will focus on cases where the drift f is designed to encode descent of an energy landscape and, in the second case, an additional rota-tional flow between minima. The Fokker-Planck (FP) equa-tion describes the dynamics of the probability density, p(y,t),

∂t p = −∇ · J, (27) 

J(y,t) = f(y)p(y,t) − D∇p(y,t),

where J is the probability flux [12]. The SDE is ergodic with stationary density π(y) if p(y,t) →t→∞ π(y) and ∇ · Jss = 0, where Jss is the flux with respect to the density π.A stationary process is said to be an equilibrium steady-state (ESS) if Jss = 0, which is also known as the detailed bal-ance condition , and implies that the process is time-reversible [56]. Otherwise, the process is in a nonequilibrium steady-state (NESS), is time-irreversible, and produces entropy. We also introduce the Helmholtz-Hodge decomposition (HHD) of the process, which decomposes the drift of a stationary diffu-sion into two components, 

f = frev + firr , frev = D∇ log π, firr = Jss /π, (28) where frev is a time-reversible (shifted) gradient flow and firr 

is the time-irreversible circulation [42]. Finally, the degree of irreversibility can be quantified by the entropy production rate 

(EPR), given by, 

Φ = 

> Z
> Rk

(firr )⊤D−1firr π dy, (29) which is non-negative, and zero if and only if the process is in detailed balance [56]. 3 

> A. Input-driven attractor-switching

We consider an RNN of the form, 

du(t) = F(u) + Gs(t) dt + B d w(t), (30) 

Fi = −ui + ∑

> j

Wi j v j + Ii,

where we have an additional input term Gs(t) + Γd, where 

s(t) ∈ Rl is the low-dimensional input signal, and G ∈ Rn×l

represents its projection into the RNN. The additional bias term, Γd, is low-rank and is absorbed into the input current 

I of the original RNN. Moreover, we assume that our input connectivity is also low-rank and of the form G = ΓGs. As a result, the dynamics remain constrained to the affine subspace. Next, we assume that our network, in the absense of inputs, encodes a gradient flow on a energy potential, V : Rk → R,with a number of minima of approximately equal energy, and isotropic noise, 

dy(t) = −∇V (y) dt + σ dw(t). (31)   

> 3When Dis not full-rank, we can use its MP inverse, provided certain conditions are met (see App. E and Ref. [42]).

9  

> FIG. 5. ‘Tilting’ the energy landscape. Our approach uses net-work inputs to ‘tilt’ the energy landscape, encouraging the process to push a trajectory into a particular energy minimum, by making it a global minimum. This is a soft version of the child’s ‘labyrinth’ game, where the board can be tilted such that the ball rolls into a spe-cific hole – though in the actual game, one tries rather hard to avoid them.

Typically trajectories will be attracted to the closest minimum, eventually exploring the landscape via random diffusion. It is well-known that this process has a stationary Boltzmann den-sity ,

π(y) = 1

Z exp 



− 2V (y)

σ 2



, (32) and is in an ESS [12]. Our aim is to direct the system towards a particular minimum using the network input. The dynamics on the affine subspace can be written as, 

dy(t) = −∇V (y) + Gss(t) + d dt + σ dw(t). (33) Considering a constant input over time, and defining c =

Gss + d, we notice that the drift can be written as the gradi-ent of a modified potential function, f(y) = −∇[V (y) − c · y].Adding a term that is constant in y corresponds to ‘tilting’ the potential to prioritise a particular minimum, making it a global minimum. This approach can be thought of in analogy with a child’s ‘labyrinth’ game (Fig. 5), where the objective is to tilt the board, which serves as the energy potential, such that the ball rolls into a particular hole i.e. the chosen energy minimum. Assuming that the potential has a set of m deep, well-separated minima, {μμμ j}mj=1, for each minima there is a con-vex subset C j ⊂ Rk, where c ∈ C j makes μμμ j the global mini-mum. More explicitly, defining V 0 

> j

= V (μμμ jjj), if c·(μμμ∗ 

> i

− μμμ j) >

V 0 

> i∗

−V 0 

> j

for all j̸ = i∗, then μμμ∗ 

> i

is the global minimum, assum-ing the perturbation is small enough. Given values of c for each minimum, it remains to compute (Gs, d) for a particu-lar choice of coding scheme for the inputs. Example schemes include ‘one-hot’ or random Gaussian inputs. In App. A 2, we provide a concrete method for picking values of c with quadratic programming, followed by computing an appropri-ate Gs and d with ridge regression. We now consider a numerical example. As the energy po-tential, we choose a sum of Gaussian wells, 

V (y) = −

> m

# ∑

> j=1

a j exp − || y − μμμ j|| 2

2ν2

> j

!

, (34) where the m minima can be placed at arbitrary positions 

μμμ j ∈ Rk, and we can specify the depth, a j, and breadth, ν j,of each well. As shown in Fig. 6, we place four identical min-ima at points in the plane. In the absense of inputs, the process typically converges to the minima that is closest. However, ac-tivating the corresponding input code, causes the potential to tilt and encourages trajectories to converge to a specified min-imum. We train a 256-neuron RNN to encode the potential-gradient, then, using a one-hot coding for the inputs, we fit the input-connectivity and bias, G and d (see App. A 2 for details). We then integrate the RNN with the input code for a given attractor, then switch it halfway through the trajectory. Panels b) and c) of Fig. 6 show traces of 10 random neurons in the network, and the projection of the network dynamics into the latent space, respectively. We see that the process converges to the first attractor, but then switches to the second due to the change in the input.  

> B. Attractor-cycling stationary diffusions

In this section, we design a class of planar SDEs, which not only converge to energy minima, but also cycle between these minima in a preferred direction using irreversible cur-rents. As before, we begin with an energy potential and define a gradient flow, 

dy(t) = −∇V (y) dt + σ dw(t). (35) In order to preserve the energy landscape, whilst adding in a rotational component, we consider a process of the form, 

dy(t) = −∇V (y) + R(y) dt + σ dw(t), (36) where we enforce that the process must still converge to the Boltzmann density. It is easy to show that the condition for this to be satisfied is that ∇ · (Rπ) = 0 i.e. the rotational com-ponent is divergence-free with respect to the stationary den-sity (DFSD). This condition is closely related to the HHD de-fined previously. In this case, we have simply that frev = −∇V

whilst firr = R. By adding on a rotational component that does not disrupt the Boltzmann density, we construct stationary dif-fusions that descend the energy landscape whilst being driven rotationally by stationary probability flux. In App. D, we construct both high-dimensional and non-coplanar examples. Specifically we design a process that can cycle between a pair of pixelated images in R1024 , and another that cycles between minima placed on a saddle in R3, although we do not encode these in an RNN. We now focus on constructing planar exam-ples, which we embed within an RNN. In R2, or in the case that the attractors are coplanar in plane 

P, we can construct a differentiable, closed planar curve, γ,which connects points in the order that we would like the field 10 c)

> Inputs  Neurons

# a) b) 

FIG. 6. Input-driven attractor-switching . a) We consider a four-well energy potential in the plane. In the absense of inputs, the trajectories converge to a minima at random, with a preference for the closest one. We can then tilt the potential with an input, which prioritises the relevant minima, and causes the trajectories to tend towards it. b) We train a 256-neuron RNN to embed the potential-gradient dynamics. Using a one-hot coding for the input, we learn the coupling matrix input connectivity and bias, then add the input to the RNN during integration. We start with the input that drives the process to the top-right well, then switch it halfway through to the bottom-right well. We plot traces from 10 random neurons and see the network dynamics react to the input and converge to the other minima. c) Projecting the RNN trajectory onto the latent space, we see the attractor-switching that occurs due to the change in input. 

FIG. 7. Constructing rotational diffusions. To construct an SDE with rotation between energy minima, we begin with the gradient of the energy potential. We then draw a closed curve, γ, through the minima in the direction of rotation. From this, we can define a rotational drift which decays with distance from the curve due to the weighting kernel g(y).

to rotate (Fig. 7). Next, we design a rotational field that pro-vides a force in the direction of this closed curve, acting along all concentric curves with the same shape. For example, if the curve is a circle, this is the set of circles with the same centre but different radii. To do this, we consider γ = ∂ O, to be the boundary of a bounded planar region O ⊂ P, and define the 

signed distance to be, 

φ (y) = 

−dist (y, γ) if x ∈ O

dist (y, γ) if x ∈ P\O (37) where dist (y, γ) = inf p∈∂ O || y − p|| . This defines a potential where each level set is a closed curve with the same shape, up to an inflation or deflation, as γ. We would like our rotational field to be tangent to these level sets. We obtain such a field by rotating ∇φ by π/2 via an antisymmetric matrix M, which in R2 is given by, 

M =

 0 1

−1 0 



. (38) To prevent the rotational field from dominating the gradient dynamics far from the minima, we also introduce a weighting kernel, 

g(y) = exp 



− φ 2(y, γ)

2ξ 2



, (39) which decays exponentially with the distance from the closed curve γ at a rate inversely proportional to ξ 2 (Fig. 7). Putting together the components, we have the rotational vector field, 

R(y) = απ(y) g(y)M∇φ (y), (40) 11 where α is the scalar strength of rotation, and where R is DFSD (see App. C). This gives a combined field, 

f = −∇V + R, (41) that has both attractive and cycling dynamics. Given a set of points {μμμ j} to encode as minima of a Gaussian potential, the parameters {{ a j}, {ν j}, σ , ξ , α}, can all be chosen to obtain the desired behaviour in the process (details on parameters are given in App. H). We construct three examples of diffusion processes with three, four, and five Gaussian wells placed in the plane. In the case of the three minima we choose the curve γ to be the intersecting circle, whilst for four and five minima we com-pute it numerically with cubic splines (see App. A 3). Fig. 8 illustrates the example processes for a) three, b) four, and 

c) five minima. Next, we use DDM to train a different 128-neuron RNN to approximate each diffusion process in a latent affine subspace. Once the model is trained, we sample trajec-tories from the RNN and project them into the learnt subspace. Fig. 9 shows example traces from ten randomly selected neu-rons in each network, as well as a projection of the trajectory and the drift into the affine subspace. We see that the RNN is able to encode such autonomous attractor-cycling dynamics in a neural subspace. Moreover, as in the case of the nonlinear systems (Fig. 2), the activity of individual neurons hints at the character of the encoded process i.e. here we see neuronal activity jumping between piecewise approximately constant segments.  

> IV. NONEQUILIBRIUM STEADY-STATES OF NEURAL NETWORKS

Computing properties of the NESS for a high-dimensional network is particularly challenging, except in the case of simple, solvable processes [53], as the FP equation is high-dimensional and its numerical solution is computationally in-tractable. However, in the case of a high-dimensional RNN encoding a low-dimensional stationary diffusion in an invari-ant subspace, the situation simplifies significantly. We consider, as before, an RNN with state u(t) ∈ A , where the latent dynamics follow the SDE, 

dy(t) = f(y) dt + σ dw(t), (42) which has a stationary density πy(y). The stationary density of u is given by the pushforward measure [57], 4

πu(u) = πy(Γ†(u − b)) 

pdet (Γ⊤Γ) . (43) With the stationary density, the HHD, stationary flux, and EPR can be computed, and is closely related between the latent and     

> 4From this point on, we are implicitly constraining u∈Ato avoid Dirac deltas in the expression.

network-level. Assuming that the HHD of the latent process is given by f = frev + firr , we can decompose the network drift, 

F as, 

F = Frev + Firr , (44) 

Firr (u) = Γfirr (Γ†(u − b)) ,

Frev (u) = Γfrev (Γ†(u − b)) ,

which additionally implies that the fluxes and EPRs also con-verge (see App. E). We return to the example process with three minima given in Sec. III B. As the process has a solvable NESS, we can compute the HHD, frev = −∇V and firr = R, as shown in Panel 

a) of Fig. 10. The reversible component balances the dif-fusion to maintain the process at stationarity, as seen in the example trajectory, whilst the irreversible component cycles around the curve γ. We then train a 64-neuron RNN to ap-proximate the process in a latent affine subspace. Using the HHD of the example and Eq. (44), we define a decomposi-tion of the high-dimensional dynamics in the ambient space, and sample trajectories from each component. Panel b) shows a trajectory from each of these components. As with the dy-namics at the latent level, we see that the reversible dynamics keep the process near the minima, balancing the noise, whilst the irreversible dynamics show limit-cycle behaviour.  

> A. A Helmholtz-Hodge decomposition of the network

Whilst Eq. (44) is a decomposition of the high-dimensional dynamics, it is challenging to interpret, as each component is a high-dimensional vector field that we cannot visualise. More-over, the decomposition is not related directly to the learnt network connectivity or input currents, only to the learnt affine subspace. Instead, we now relate these components to a decomposi-tion of the network itself. We assume that the HHD of the network can be written as, 

Firr (u) = −ρu +Wirr h(Γy + b) + Iirr , (45) 

Frev (u) = −(1 − ρ)u +Wrev h(Γy + b) + Irev ,

where W = Wrev +Wirr , I = Irev + Iirr , and ρ ∈ [0, 1]. We also assume that these components respect the parameterisation, 

Wrev = ΓWs,rev , Wirr = ΓWs,irr , (46) 

Irev = ΓIs,rev + ( 1 − ρ)b, Iirr = ΓIs,irr + ρb,

which ensures that they are tangent to the affine subspace, and thus neither component causes the process to drift away from it. This also implies that we have a network decomposition that corresponds to the reversible and irreversible components of the dynamics. We can solve for these components, by writ-ing the projected dynamics in the affine subspace, ˆfirr (y) = −ρy+Ws,irr h(Γy + b) + Is,irr , (47) ˆfrev (y) = −(1 − ρ)y +Ws,rev h(Γy + b) + Is,rev ,12 a) 

b) 

c)      

> FIG. 8. Attractor-cycling diffusions. We construct example diffusion processes that cycle 3, 4, and 5 attractors with irreversible currents (Panels a−c). These are composed of a gradient flow (first column) plus a rotational component. The rotational component is constructed by taking a smooth curve through the minima, defining a potential as the distance from the curl, multiplying by a decaying weight kernel (contour in second column), and dividing by the stationary density (second column). Combined, this defines a vector field that both converges to, and cycles between attractors (third and fourth column).

where we can again notice that this can be seen as two two-layer perceptrons optimised to approximate a vector field, ex-cept that the weights and biases of the first layer are shared and already fixed, (Γ, b), as is the sum of the weights and biases in the second layer i.e. Ws = Ws,rev +Ws,irr and Is = Is,rev + Is,irr .We also introduce the trainable parameter ρ ∈ [0, 1].Whilst fixing the first layer causes this perceptron to no longer be a universal approximator, we find that this param-eterisation is able to approximate the components with very high accuracy. This is due to the fact that (Γ, b) is opti-mised for the full drift field and therefore remains well-aligned with this downstream fine-tuning. This procedure yield a reversible-irreversible decomposition of the network structure that we can compare to the symmetric-asymmetric decompo-sition from Sec. II C. Using the three minima example, we decompose the RNN with both the HHD and the symmetric-asymmetric decompo-sition. Panel a) of Fig. 11 shows the learnt HHD compo-nents of the vector field using Eq. (45), whilst Panels b) and 

c) show the decomposed connectivity matrix and bias vec-tor, respectively. We find that for both decompositions, the irreversible/asymmetric component has significantly larger weights, in absolute value, hinting that irreversible cycling dy-namics require far higher network complexity. In the appendix, we further investigate how the character of the target dynamics is represented in the RNN structure. In App. F, we show that the asymmetry of the RNN does not seem to be directly correlated with the EPR of the underly-ing process, a result that is in contrast with previous results for solvable networked systems [53]. In App. G, we describe how the DDM framework can be used to provide a universal representation for nonlinear systems and, using chaotic attrac-tors as an example, we show how this can be used to compare their complexity.  

> V. CONCLUSION

Neural circuits encode information and perform computa-tions through a repertoire of complex spatiotemporal dynam-ics, but their fundamental mechanisms remain obscure. How-ever, neural data analysis has shown that high-dimensional 13 a) b) c)        

> FIG. 9. Attractor-cycling in RNNs. We train 128-neuron RNNs to approximate the planar diffusion processes from Fig. 8 for a)three, b)
> four, and c)five minima respectively. Trajectories from the RNN can then be sampled autonomously, and the learnt drift can be projected onto the learnt affine subspace. We find that the RNNs are able to learn the target diffusion processes. For each RNN, we plot traces from ten random neurons. We see that the attractor cycling dynamics in the latent subspace are visible in the raw trajectories.

brain network dynamics are often redundant, with inter-pretable computational mechanisms best observed at the level of a latent neural manifold [25, 26, 33]. In line with previous studies focusing on the implications of low-rank structure in RNNs [27–29], our DDM framework presents a simple proce-dure to train a low-rank RNN to encode a arbitrary target SDE in an affine latent subspace, provided asymmetric connectiv-ity is permitted. This allows us to go far beyond Hopfield’s original model [11], which was only able to encode attracting states as the minima of an energy-landscape, to encode nonlin-ear and nonequilibrium processes, such as chaotic attractors, and show that RNNs are able to perform both input-driven switching and autonomous cycling between attracting states. Finally, in an effort to elucidate the representation of the dy-namics that is learnt by the RNN, we present two decompo-sitions of the learnt connectivity matrix, using its asymmetry and its time-irreversibility respectively. Whilst interesting, our approach brings with it a number of limitations. First, unlike Hopfield’s model which employs a Hebbian learning rule, the DDM framework trains the RNN via backpropagation of error, noticing that a low-rank RNN is equivalent to a two-layer perceptron. This makes training quick, easy, and stable, unlike BPTT for RNNs, but is not bi-ologically motivated or plausible. In this model, neural mani-folds do not emerge spontaneously due to self-organisation, perhaps as the result of a learning rule. Instead, they are created by enforced structure in the connectivity. Most ap-proaches enforce manifold structure through low-rank con-nectivity [28, 58], or by assuming certain symmetry proper-ties [59, 60], but there have been some attempts to develop biologically-inspired learning rules for RNNs and other neu-ral network models [5, 55, 61]. Nevertheless, the mechanisms by which biological neural circuits learn to encode complex dynamics over neural manifolds remain unclear, and requires the development of learning rules which produce asymmet-ric connectivity and emergent manifold structure. Next, our framework encodes dynamics in an affine linear subspace, suggesting that the relationship between neuronal states and 14 a) b) 

FIG. 10. The HHD of a latent, embedded process. a) When we embed a process with a known NESS, we can perform the HHD. In this example with three minima, we see that the reversible dynamics balance the diffusion and maintain the process on the stationary distribution, whilst the rotational dynamics create rotation around the curve γ. b) We train an RNN with 64 neurons to approximate the process in a latent affine subspace. Using the HHD and Eq. (44), we can define the irreversible, Firr and reversible, Frev components of the high-dimensional dynamics. We see that the reversible dynamics in the ambient space keep the process near the minima, balancing the noise, whilst the irreversible dynamics show limit-cycle behaviour in the ambient space. a) b) c) 

FIG. 11. The HHD of an RNN. a) Following Eq. (47), we train two coupled RNNs to approximate the reversible and irreversible components of the HHD respectively. Their projected drift shows that they were able to learn the components effectively. b) We can decompose the learnt connectivity matrix, W , using both the HHD, W = Wrev + Wirr , or using the symmetric-asymmetric decomposition from Sec. II C, 

W = Wsym +Wasym . We find that for this three minima example, the magnitude of the irreversible and asymmetric components are far greater than the reversible component suggesting that encoding rotational or cycling dynamics requires more network complexity. c) We can also examine the decomposition of the input current vector, I = Iirr + Irev . As with the connectivity, more of the input current is used to encode the irreversible component than the reversible component. 15 neural manifolds is linear. Whilst linear methods have been used extensively to probe neural data [48], state-of-the-art ap-proaches reveal neural manifolds with nonlinear dimension-ality reduction [3, 25]. A more general relationship between neural firing and the effective computational manifold, allows for a much richer latent geometry in the system, which would be more consistent with empirical data. However, this pro-duces major mathematical challenges to our framework, as nonlinear maps introduce troublesome Jacobian and Hessian terms in the projected dynamics. Finally, whilst we consid-ered one case of a input-driven circuit, we mostly focus on autonomous RNN dynamics, which encode stationary pro-cesses. Integrating an RNN as the computational circuit in a more complete and time-dependent network, which includes both sensory input and an upstream output, is crucial for re-lating this model to more neuroscientific applications, such as motor control. Asymmetric connections are required for RNNs to encode complex, nonlinear dynamics, and are intimately related to the emerging topic of nonequilibrium brain dynamics. At a range of scales, the brain has been shown to violate the detailed balance condition and produce time-irreversible dy-namics [19]. Whilst nonequilibrium brain network dynam-ics have been linked to asymmetric connections between neu-rons and brain regions [18, 53, 62, 63], here we go beyond observational analysis of neural data, and present a possible computational mechanism for such nonequilibrium dynam-ics, indicating that time-irreversibility is the necessary result of computation in neural circuits. However, as discussed in App. F, we found that the asymmetry of the RNN was not directly correlated with the irreversibility of the encoded pro-cess. Related results have shown that irreversibility of neu-ral dynamics is also uncorrelated with the irreversibility of sensory stimuli [64], thus the exact relationship between the nonequilibrium nature of sensory information, computational mechanisms, and observed dynamics remains unclear. In an effort to understand how dynamics are represented by an RNN, it is interesting to study the properties of the in-ferred connectivity. In particular, as any symmetric compo-nent of the connectivity can be seen as encoding an energy landscape, probing the connectivity with matrix decomposi-tions is a promising way forward. Furthermore, a number of decompositions for stochastic processes have been developed [42, 43, 65], which have implications for their nonequilibrium thermodynamics and their convergence to stationarity. We present avenues for exploring the relationship between com-ponents of the connectivity, and the components of the under-lying target process. The DDM framework provides a simple approach for en-coding low-dimensional computational dynamics in a high-dimensional neural network. As a result, it marries the mod-els of attractor neural networks with the theory of neural man-ifolds, providing us a way to investigate how cognitive pro-cesses may be represented by neural circuits. Ultimately, it brings us a step closer to cracking the neural code. 

> CODE AVAILABILITY

Code will be made available upon publication at https: //github.com/rnartallo/ddm .

> AUTHOR CONTRIBUTIONS

R.N.K. designed and performed research and wrote the manuscript. R.L. and A.G. designed and supervised the re-search and edited the manuscript. 

> ACKNOWLEDGEMENTS

R.N.K would like to thank Pauline Thibaut for her pixel renditions of a dog and chick in App. D. Additionally, the au-thors would like to thank photographer Alexas Fotos for up-loading their image of the labyrinth game for free use (Fig. 5). R.N.K acknowledge support in the form of an EPSRC Doc-toral Scholarship from Grants No. EP/T517811/1 and No. EP/R513295/1. R.L. acknowledges support from the EPSRC grants EP/V013068/1, EP/V03474X/1 and EP/Y028872/1  

> SUPPLEMENTARY MATERIAL Appendix A: Numerical methods 1. Simulating trajectories from SDEs

We sample paths from an SDE using the Euler-Maruyama 

discretisation, 

xt+∆t = xt + ∆t[f(xt )] + ∆wt [Σ(xt )] , (A1) where ∆t is the time-step and ∆wt are independent and identi-cally distributed (i.i.d.) multivariate normal random variables with mean zero and covariance ∆tIn [66].  

> 2. Computing inputs for attractor-switching

In Sec. III A, we propose an approach for designing an RNN that switches between attractors based on an input. Here we present the concrete approach used for computing c, Gs,and d, in the numerical example. As detailed in Sec. III A, 

c ∈ Rk makes the local minimum μμμi∗ a global minimum if, 

c · (μμμi∗ − μμμ j) > V 0 

> i∗

−V 0 

> j

, (A2) for all j̸ = i∗. Whilst this defines a convex subset, we will focus on finding a single value c∗ 

> i

for each i by solving the quadratic convex problem, 

c∗ 

> i

= argmin c∈Rk

12 || c|| 22, (A3) s. t. c · (μμμi − μμμ j) ≥ V 0 

> i

−V 0 

> j

+ δ , for j̸ = i, (A4) 16 where δ > 0 is a small parameter used to weaken the strict inequality. We solve this with convex optimisation in cvxpy .Given a collection of {c∗ 

> j

}, we choose a coding for the inputs. For example, we can choose a ‘one-hot’ encoding where l = m

i.e. we have one input direction per minimum, and s = ei is the input corresponding to minimum i, where ei is the unit vector in the direction i. Alternatively, we can sample random Gaussian vectors for s. In any case, we can also add low-intensity noise to the input. It remains to to solve for Gs and 

d given the coding scheme, for which we use ridge regression 

[67]. We combine our c∗ values into a matrix C ∈ Rm×k, and combine the codes si into a matrix S ∈ Rm×m, and augment it with an additional column of ones to form S∗ = [ S, 1] ∈

Rm×(m+1). We then aim to fit the model C ≈ SG s + 1d ⊤, by minimising, 

Θ∗ = argmin Θ|| S∗Θ − C|| 2 + λ || Θ|| 2, (A5) where Θ = [ Gs, d]. This has exact solution, 

Θ∗ = (( S∗)⊤S∗ + λ I)−1(S∗)⊤C. (A6)  

> 3. Constructing closed curves with splines

We compute interpolating closed curves with cubic splines with periodic boundary conditions. More specifically we use 

CubicSpline from scipy.interpolate [68]. We then parametrise the curve, γ(t), with 500 time-points and con-struct a matplotlib path. We can then measure the distance between any point, p, and each discrete point in the path, tak-ing the minimum to be the distance between p and γ.

> Appendix B: Optimal decomposition of a network under constraints

Let W ∈ Rn×n be a matrix that is low-rank and has the form 

W = ΓWs, where Γ ∈ Rn×k and Ws ∈ Rk×n. Γ has full column rank k. We want to find the ‘best’ decomposition, 

W = Γ(Ω + Π), (B1) where ΓΩ is symmetric. We will define this optimal decom-position to have, ˜Ω = argmin Ω|| ΓΩ −C|| 2 

> F

, (B2) where C = 12

 W +W ⊤ i.e. the matrix that is most simi-lar to the symmetrised version of W , but which respects the parametrisation. Let P be the projector matrix onto 

S = {X ∈ Rn×n : X ∈ range (Γ)}, (B3) i.e. P = ΓΓ †. Due to the symmetry of any feasible X, we have that both the columns and rows of X live in the range of Γ.This implies that, 

PX = X, XP = X, (B4) which implies that X = PXP for any feasible X.As we are trying to minimise the Frobenius norm, the op-timal solution X = ΓΩ , is the projection of C onto the linear subspace, 

Q = {X ∈ Rn×n : X = X⊤, X ∈ range (Γ)}. (B5) As C is already symmetric, the solution is simply the projec-tion of C onto S ,

X = PCP = ΓΓ †CΓΓ †, (B6) which gives, ˜Ω = Γ†CΓΓ †. (B7) 

> Appendix C: Designed rotational drift is DFSD

In the example considered in Sec. III, we consider a rota-tional field of the form, 

R(y) = απ(y) g(y)M∇φ (y). (C1) We can show that this field is DFSD by showing that, 

ζζζ (y) = g(y)M∇φ (y), (C2) is divergence-free. Applying the product rule we have that, 

∇ · ζζζ = ( ∇g) · (M∇φ ) + g(∇ · (M∇φ )) . (C3) We have that, 

∇g = − φξ 2 g∇φ , (C4) which implies that, 

(∇g) · (M∇φ ) = − φξ 2 g∇φ · (M∇φ ), (C5) 

= 0,

as v · Mv = 0 for antisymmetric M. Next, we have that, 

∇ · (M∇φ ) = ∑ 

> i,j

Mi j ∂i∂ jφ , (C6) which is the sum-product of a symmetric (Hessian) and anti-symmetric matrix, which is zero. Thus ∇ · ζζζ = 0. 

> Appendix D: Attractor-cycling diffusions in higher-dimensions

In this section, we consider similar approaches to that of Sec. III for designing SDEs with transiently visited attrac-tors. First, we consider a high-dimensional example that cycles between two pixelated images. Next, we consider a three-dimensional example where the attracting points are non-coplanar. Unlike the examples in Sec. III, we do not en-code these processes in the dynamics of a RNN as it is not motivated to encode a high-dimensional process on a neural manifold, and the non-coplanar example is not smooth. 17 a) b) c)            

> FIG. 12. Cycling between pixelated images in high-dimensions . We can modify our approach from Sec. III to design processes which cycle between energy minima in high-dimensions. a)We illustrate this using 32 ×32 pixelated images of a dog and a chick, which are points in R1024 .b)As before, we design a drift field that encodes these points as the minima of a Gaussian potential, but drives rotation between them without disrupting the stationary density. c)Integrating this SDE, we can sample paths which cycle between (noisy) instances of the two images in the high-dimensional space.
> 1. A high-dimensional example with pixel images

We consider two grey-scaled, 32 × 32 images, whose pixel-values are scaled to [0, 1]. Panel a) of Fig. 12 shows a dog and chick image, respectively. These points can be thought of as points μμμ1, μμμ2 ∈ R1024 . We define a Gaussian poten-tial on R1024 with minima at the points corresponding to the dog and chick images respectively. Next, we want to create a rotational flow that cycles between these pixelated images in 

R1024 . To do this, we first select a plane which intersects both these points – we choose the one that goes through the origin – which is spanned by vectors {e1, e2}, using the Gram-Schmidt orthogonalisation. We take d = μμμ2 − μμμ1, and e1 = d/|| d|| .Next, we must choose a vector which is linearly independent of e1, so we choose the standard basis vector in the direction where e1 has smallest absolute value, εεεk, and then project this vector onto e1,

q = εεεk − (εεεk · e1)e1, (D1) and then normalise it to obtain e2 = q/|| q|| . The result is an orthonormal basis, {e1, e2}, for a 2D plane that intersects both minima. Given this plane, the matrix representing a π/2 rotation in the plane, is given by, 

M = e2 ⊗ e1 − e1 ⊗ e2, (D2) where ⊗ is the outer product. We define the rotational com-ponent to be, 

R(y) = αg(y)M∇V (y), (D3) where we are rotating the gradient field itself. Note that we have not defined an intersecting curve as in Sec. III. Instead, we modify the weighting kernel, 

g(y) = exp 



− (V (y) − s)2

2ξ 2



, (D4) such that it decays with the distance from a particular level set of the potential function with value s, rather than the distance from a curve. As a result, the rotational field creates flow along this particular level set, pushing trajectories between the minima. It is easy to show that this is DFSD. Finally, we define our combined drift field to be, 

f = −∇V + R, (D5) thus it admits a stationary Boltzmann distribution. The gra-dient flow, rotational component, weighting kernel, and com-bined field are projected onto the plane in Panel b) of Fig. 12. Integrating the SDE with this drift field, and suitably chosen noisy intensity, defines a process whose trajectories cycle be-tween (noisy) instances of the two images in succession, as shown in Panel c) of Fig. 12.  

> 2. Non-coplanar minima in three dimensions

Here we consider a set of minima which are non-coplanar in R3. As before, we define the attractor dynamics, −∇V ,with Gaussian wells. However, the rotational component R

from the coplanar case cannot be applied in this setting. This is because the level sets of the signed distance function are no longer curves, but instead tubular sections. As a result, flows along the contours of these level sets are not restricted to the ‘direction’ of the curve through the minima, but also along the minor radius of each tube. Instead, we introduce an alternative approach. Again, we draw a differentiable curve, γ, that intersects our minima in the order that we wish them to be traversed. Fig. 13 shows an example with four minima which lie on sad-dle. Panel a) shows the intersecting curve. Next, we consider the tubular neighbourhood , Q, around the non-coplanar curve with parameterisation γ(s), which is defined as, 

Q = {y ∈ R3 : dist (y, γ) < r}, (D6) 18 a) b) c) d)           

> FIG. 13. Cycling between non-coplanar minima. We can also design SDEs which cycle between non-coplanar minima in 3D. a)In this example, we consider four minima on a saddle, which we interpolate with a cubic spline. b)We design a rotational field which pushes trajectories around this curve, but is DFSD. c)This produces a combined field which has both attracting and rotational dynamics. d)Sampling from this process, we see that it rotates transiently between attracting states.

where r is the tubular radius , which must be smaller than the 

injectivity radius r max = κ−1max , where κmax is the maximum curvature of γ – this is the largest tube that does not self-intersect. Each point y ∈ Q can be written as, 

y(s, ρ, θ ) = γ(s) + ρ cos (θ ) · N(s) + ρ sin (θ ) · B(s), (D7) where s is the arc-length parameter, and (ρ, θ ) are polar co-ordinates in the normal plane spanned by the normal and bi-normal vectors, {N(s), B(s)} at γ(s) [69, 70]. At any point 

γ(s), we can form a local cylindrical coordinate system, 

es = T(s) (D8) 

eρ = cos (θ ) · N(s) + sin (θ ) · B(s)

eθ = − sin (θ ) · N(s) + cos (θ ) · B(s)

which are the axial , radial and azimuthal basis vectors re-spectively, and T(s) is the tangent vector to γ(s) [69]. To-gether, the vectors {T(s), N(s), B(s)} form the Frenet-Serret 

orthonormal frame along a curve [69]. This cylindrical basis is accurate up to a first-order correction arising due to the cur-vature of the curve. Our goal is to construct a vector field with forcing in the direction T(s). To do so, we define the vector potential, 

A(s, ρ, θ ) = ψ(ρ)eθ (s, ρ, θ ), (D9) where ψ(ρ) = exp (−ρ2/2ξ 2) is the weighting kernel that de-cays as ρ increases i.e. that decays with distance from γ.This coincides with our definition of g in the planar case (Eq. (39)). We can take the curl to obtain a divergence-free field, 

∇ × A(x), which, following the right hand rule, pushes in the direction T(s). In practice, the curl of the vector potential is approximated numerically using finite differences. Finally, dividing through by the stationary density, we obtain, 

R(y) = απ(y) ∇ × A(y), (D10) where α controls the strength of rotation. This field is DFSD, thus −∇V +R converges to the Boltzmann density, yet pushes trajectories along γ, as desired. Panels b) and c) of Fig. 13 show the rotational component and combined vector field re-spectively, whilst Panel d) shows an example trajectory from this process. It cycles autonomously between the attractors exhibiting metastability. As there is a discontinuity between the field inside the tubular radius and beyond it, the field is not smooth, thus we not attempt to encode it in an RNN. 

> Appendix E: Nonequilibrium steady-state is equivalent between latent and network dynamics

Analytical results for the NESS of large, complex networks, including RNNs, are typically intractable, except for limited cases. We consider the situation where the RNN encodes a latent process with a known NESS. As considered in Sec. IV, we take an RNN with state u(t) ∈ A , where the latent dynam-ics follow the SDE, 

dy(t) = f(y) dt + Bs dw(t), (E1) which has a stationary density πy(y). In the network space, as the mapping between latent and network trajectories is one-to-one, u is given by the pushforward measure [57], 

πu(u) = πy(Γ†(u − b)) 

pdet (Γ⊤Γ) , (E2) where we are restricting to u ∈ A to avoid Dirac-delta terms. Similarly, the flux is related by, 

Ju(u) = ΓJy(Γ†(u − b)) 

pdet (Γ⊤Γ) . (E3) Finally, we can compute the EPR. As mentioned in Sec. IV, the traditional formula for EPR, Eq. (29), assumes that the diffusion matrix is full-rank [56]. This does not hold in our case where D = BB ⊤ = ΓBsB⊤ 

> s

Γ⊤, which is a symmetric pos-itive semidefinite matrix with rank k. However, we can extend the EPR formula to this case, where it is given by, 

Φ = 

> Z
> Rn

F⊤

> irr

D†Firr πu du, (E4) 19 i.e. where D−1 has been replaced by the MP inverse, if and only if Firr (u) ∈ Range (B(u)) where B = ΓBs [42]. We consider spatially-constant diffusion, and as we have already shown that Firr lies in the image of Γ (Eq. (44)), it is sufficient for Bs to be of full row-rank k, guaranteeing that the noise is elliptic . A necessary requirement for this is that d ≥ k i.e. we have more independent noise-sources in the network than dimensions in the latent process. When Bs has full row-rank, we have that the EPR of the latent process and the full network are identical, 

Φu = Φy. (E5) 

> Appendix F: Network asymmetry and entropy production

Neural systems operate in NESSs across a range of scales, with the level of irreversibility, defined by the EPR in Eq. (29), being correlated with the level of consciousness or the com-plexity of a cognitive task [19]. Moreover, the EPR has been shown to be driven by the asymmetry of the network underly-ing the neural activity [53]. Here, we investigate this further by considering a nonequilibrium diffusion with a parametris-able EPR, embedding the process in a latent subspace of a fixed-size RNN, and measuring the asymmetry of the result-ing connectivity. In particular, we consider the stochastic Hopf oscillator , a nonlinear model with a solvable NESS, which is given by, 

d

y1(t)

y2(t)



=

(1 − y21 − y22)y1 − ωy2

(1 − y21 − y22)y2 + ωy1



dt + σ

w1(t)

w2(t)



,

(F1) where ω is the frequency of oscillation [19]. The NESS is given by a radially-symmetric stationary density and, for fixed 

σ , has EPR Φ ∝ ω2, satisfying time-reversibility if and only if ω = 0 (see Ref. [71]). To measure the asymmetry of the resulting RNN, we con-sider the canonical symmetric-antisymmetric decomposition, the low-rank symmetric-asymmetric decomposition of Sec. II C, and the HHD of Sec. IV. In general, this produces a pair 

(W1,W2) where W = W1 +W2, where we quantify the relative energy of W1 with, 

E (W1,W2) = |W1||W1| + |W2| , (F2) where | · | is the Frobenius norm. We train an ensemble of 100 RNNs to encode the stochastic Hopf oscillator for each value in a range ω ∈ [0, 5]. Fig. 14 shows the relative energy of a)(W −C,C), b) ( ΓΠ , ΓΩ ), and c) ( Wirr ,Wrev ). Panel d) shows the fitted value of ρ from the network HHD in Eq. (45). We find that there is no clear relationship between the relative en-ergy in the asymmetric/irreversible component and the EPR, which suggests that irreversible currents are also parametrised in the bias terms, and that the projection matrix Γ can in-duce asymmetries that are not trivially related to the rotational forces in the target vector field. This result contrasts with pre-vious studies suggesting that the EPR is driven directly by network asymmetry [53], although this was not in the case of a low-rank system. 

> Appendix G: Measuring the complexity of dynamics with a universal representation

Whilst existing approaches include the calculation of Lya-punov exponents, Kaplan-Yorke embedding dimension, or the entropy rate, there is no consensus on how to measure the ‘complexity’ of a chaotic system [44]. Even at a descriptive level it can be difficult to define what is a more or less complex dynamics. For example, if time is rescaled so that an attrac-tor is traversed at twice the speed, or an attractor is inflated in space, is it of the same complexity? Whilst this fundamental problem remains, our embedding approach provides a universal representation of a nonlinear system via an RNN. The connectivity structure of the RNN can then be studied to derive a metric for the complexity of the representation, which in turn captures some features of the complexity of the encoded chaotic system. To illustrate this, we consider four familiar 3D chaotic attractors: the Lorenz and Dadras attractors introduced in Sec. II D, the R¨ ossler at-tractor, 

d

y1(t)

y2(t)

y3(t)

 =

 −y2 − y3

y1 + ay 2

b + y3(y1 − c)

 dt , (G1) with a = b = 0.2 and c = 5.7, and the Halvorsen attractor, 

d

y1(t)

y2(t)

y3(t)

 =

−ay 1 − 4y2 − 4y3 − y22

−ay 2 − 4y3 − 4y1 − y23

−ay 3 − 4y1 − 4y2 − y21

 dt , (G2) with a = 1.7 [44]. For each of these systems, we train 100 separate RNNs, with 1024 neurons each, to encode the deter-ministic drift i.e. we set λdiff = 0. With the fitted connectivity matrix, W , we then compute the spectral radius ,

r(W ) = max  

> i

|λi|, (G3) where λi are the eigenvalues of W , and the participation ratio 

(PR), PR (v) = (∑i vi)2

N ∑i v2

> i

, (G4) for each eigenvector v. We can take the mean of the PR over all eigenvectors, or focus on the PR of the dominant mode. The spectral radius captures the maximum asymptotic growth of the system, and the degree of instability for fixed points and periodic trajectories. On the other hand, the participation ratio captures the effective proportion of neurons in the network that are active in encoding a representation. A PR of 1 /N

implies that a single neuron encodes a representation, whilst a PR of 1 implies the entire network is being used to encode a representation. Fig. 15 shows the a) spectral radius, b) PR of the dominant mode, and c) mean PR, for each system and its RNN repre-sentations. Whilst there is a lack of ‘ground truth’ for this exploratory experiment, we can notice that the Lorenz sys-tem has, by far, the highest spectral radius, suggesting that it 20 a) c) d) b)                

> FIG. 14. Network asymmetry for increasing EPR. For a range of values ω∈[0,5]we train an ensemble of 100 RNNs. We then compute the relative size of each component in the decomposition with Eq. (F2) for a) ( W−C,C),b) ( ΓΠ ,ΓΩ ), and c) ( Wirr ,Wrev ). In d)we also plot ρ
> from the network HHD in Eq. (45).

has the most instability and asymptotic growth. On the other hand, it is the Dadras attractor which has the highest mean PR and dominant PR, suggesting that it requires the most network complexity to be encoded. This is consistent with the fact that it has the most ‘scrolls’ and thus has non-trivial dynamics in the largest number of directions. These results are just a first attempt at exploring the com-plexity of nonlinear systems via the unified and compara-ble representation offered by DDM, and further progress will require investigating additional network properties beyond those considered here in order to fully capture the richness of the underlying dynamics. 

> Appendix H: Parameters and details for numerical experiments

Here we provide details on parameters for the numeri-cal simulations. Throughout, we will train the perceptrons with PyTorch using the Adam optimiser with learning rate 

= 0.001. We fix λdiff = 20.  

> 1. Nonlinear systems and RNN representations

In Sec. II, we use the following parameters. • Van der Pol. We take σ = 0.25. We train an RNN with 

N = 64, and 25,000 uniform samples from [−4, 4]2 for 30,000 epochs. • Lorenz. We take σ = 0.25. We train an RNN with N =

512, and 125,000 uniform samples from [−25 , 25 ] ×

[−30 , 30 ] × [0, 50 ] with λdiff = 20 for 100,000 epochs. • Dadras. We take σ = 0.01. We train an RNN with N =

1024, and 150,000 uniform samples from [−20 , 20 ] ×

[−12 , 10 ] × [−15 , 15 ] for 100,000 epochs.  

> 2. Switching and cycling attractors

In Sec. III, we use the following parameters. • Input-driven switching. We place four minima at 

(±3, ±3) ∈ R2, with Gaussian well parameters given by a j = 0.125 and ν j = 1, and noise intensity σ = 0.1. We train the RNN with N = 256 using 25,000 samples from [−4, 4]2 for 30,000 epochs. Next, we have the three examples of autonomously cycling diffusions. • Three minima. We place three minima at μμμ =

{(1, 2), (−1.5, 0), (1, −2.25 )}, with a j = 0.25 and ν j =

0.75. The process has ξ = 0.2, α = 1.5 and σ = 0.2. 21 a)

b) c)

## 10/02/2026, 10:53          

> FIG. 15. A universal representation for nonlinear dynamics. The DDM framework allows us to obtain a universal representation for nonlinear dynamics. By studying the inferred parameters, we can define measures of ‘complexity’ for the dynamics and their representations. We illustrate this with chaotic attractors, where we compute the a)spectral radius, b)mean participation ratio, and c)participation ratio of the dominant mode, for the connectivity, W, of 100 RNNs with 1024 neurons each, trained to encode the R¨ ossler, Dadras, Halvorsen, and Lorenz attractors. We find that the Lorenz attractor has, by far, the highest spectral radius, whilst the Dadras attractor has the highest participation ratio.

We train a RNN with N = 128 and 25,000 samples from 

[−4, 4]2 for 30,000 epochs. • Four minima. We place four minima at μμμ =

{(2, 3), (3, −2), (−2, −3), (−1.5, 1.5)}, with a j = 0.25 and ν j = 0.75. The process has ξ = 0.6, α = 2.5 and 

σ = 0.225. We train a RNN with N = 128 and 25,000 samples from [−4, 4]2 for 30,000 epochs. • Five minima. We place five minima at μμμ =

{(3.5, 4), (0, 0), (3, −4), (−3, 4), (−3.5, 4)}, with a j =

0.25 and ν j = 0.6. The process has ξ = 0.6, α = 2.5and σ = 0.225. We train a RNN with N = 128 and 40,000 samples from [−8, 8]2 for 40,000 epochs. In Sec. IV, we retrain the second layers using 25,000 addi-tional samples and 30,000 epochs to obtain the HHD.  

> 3. Non-coplanar examples

In App. D, we use the following parameters. • Pixel images. We fix minima at the points correspond-ing to the images, with Gaussian wells specified by 

a j = 7, ν j = 11. The process has σ = 0.3, ξ = 0.5, 

s = −3.2, and α = 40. • Points on the saddle. We place points at the positions given by, 

μμμ =



0.4 0.7 0.21 −0.5 −0.2

−0.2 −0.6 0.4

−0.5 0.2 −0.6

 , (H1) and take rmax = 0.4, ξ = 0.75 and compute the curl with ∆ = 10 −5. The Gaussian wells are given by 

a j = 0.5, ν j = 0.325 and the process has σ = 0.155 and 

α = 4000.  

> 4. Network asymmetry and entropy production

In App. F, we train RNNs with N = 64 with 25,000 samples in [−4, 4]2 for 30,000 epochs and set σ = 0.25.  

> 5. Measuring complexity of chaotic attractors

In App. G, we train RNNs with N = 1024 with 125,000 samples for 2000 epochs using batches of size 1024. For the Halvorsen attractor, we take the samples in the range 

[−15 , 12 ]3. For the R¨ ossler attractor, we take [−15 , 12 ] ×

[−15 , 12 ] × [0, 25 ].22 

[1] M. W. Mathis, A. P. Rotondo, E. F. Chang, A. S. Tolias, and A. Mathis, “Decoding the brain: From neural representations to mechanistic models,” Cell , vol. 187, no. 21, pp. 5814–5832, 2024. [2] S. Vyas, M. D. Golub, D. Sussillo, and K. V. Shenoy, “Compu-tation through neural population dynamics,” Annual Review of Neuroscience , vol. 43, 2020. [3] C. Pandarinath, D. J. O’Shea, J. Collins, R. Jozefowicz, S. D. Stavisky, J. C. Kao, E. M. Trautmann, M. T. Kaufman, S. I. Ryu, L. R. Hochberg, J. M. Henderson, K. V. Shenoy, L. F. Abbott, and D. Sussillo, “Inferring single-trial neural popula-tion dynamics using sequential auto-encoders,” Nature Meth-ods , vol. 15, p. 805–815, 2018. [4] D. Durstewitz, G. Koppe, and M. I. Thurm, “Reconstructing computational system dynamics from neural data with recur-rent neural networks,” Nature Reviews Neuroscience , vol. 24, p. 693–710, 2023. [5] D. Sussillo and L. F. Abbott, “Generating coherent patterns of activity from chaotic neural networks,” Neuron , vol. 63, no. 4, pp. 544–557, 2009. [6] G. Hennequin, T. P. Vogels, and W. Gerstner, “Optimal control of transient dynamics in balanced networks supports generation of complex movements,” Neuron , vol. 82, no. 6, pp. 1394–1406, 2014. [7] D. J. Gauthier, E. Bollt, A. Griffith, and W. A. S. Barbosa, “Next generation reservoir computing,” Nature Communica-tions , vol. 12, no. 5564, 2021. [8] J. Z. Kim and D. S. Bassett, “A neural machine code and pro-gramming framework for the reservoir computer,” Nature Ma-chine Intelligence , vol. 5, pp. 622–630, 2023. [9] L.-W. Kong, G. A. Brewer, and Y.-C. Lai, “Reservoir-computing based associative memory and itinerancy for com-plex dynamical attractors,” Nature Communications , vol. 15, no. 4840, 2024. [10] J. Hopfield, “Neural networks and physical systems with emer-gent collective computational abilities,” Proceedings of the Na-tional Academy of Sciences , vol. 79, pp. 2554–2558, 1982. [11] J. Hopfield, “Neurons with graded response have collective computational properties like those of two-state neurons,” 

Proceedings of the National Academy of Sciences , vol. 81, pp. 3088–3092, 1984. [12] G. Pavliotis, Stochastic Processes and Applications: Diffu-sion Processes, the Fokker-Planck and Langevin Equations .Springer, 2014. [13] D. J. Amit, Modeling Brain Function: The World of Attractor Neural Networks . Cambridge University Press, 1989. [14] M. Khona and I. R. Fiete, “Attractor and integrator networks in the brain,” Nature Reviews Neuroscience , vol. 23, p. 744–766, 2022. [15] H. Sompolinsky and I. Kanter, “Temporal association in asym-metric neural networks,” Physical Review Letters , vol. 57, no. 2861, 1986. [16] J. A. Hertz, G. Grinstein, and S. A. Solla, “Irreversible spin glasses and neural networks,” in Heidelberg Colloquium on Glassy Dynamics , Springer, 2006. [17] A. Crisanti and H. Sompolinsky, “Dynamics of spin systems with randomly asymmetric bonds: Langevin dynamics and a spherical model,” Physical Review A , vol. 36, no. 4922, 1987. [18] H. Yan, L. Zhao, L. Hu, X. Wang, E. Wang, and J. Wang, “Nonequilibrium landscape theory of neural networks,” Pro-ceedings of the National Academy of Sciences , vol. 100, no. 45, 2013. [19] R. Nartallo-Kaluarachchi, M. Kringelbach, G. Deco, R. Lam-biotte, and A. Goriely, “Nonequilibrium physics of brain dy-namics,” Physics Reports , vol. 1152, pp. 1–43, 2026. [20] A. K. Behera, M. Rao, S. Sastry, and S. Vaikuntanathan, “En-hanced associative memory, classification, and learning with active dynamics,” Physical Review X , vol. 13, no. 041043, 2023. [21] T. Spisak and K. Friston, “Self-orthogonalizing attractor neu-ral networks emerging from the free energy principle,” arXiv ,vol. 2505.22749, 2025. [22] M. Aguilera, D. D. Martino, I. Garashchuk, and D. Sinelshchikov, “Nonequilibrium thermodynamics of associative memory continuous-time recurrent neural net-works,” in ALIFE 2025: Ciphers of Life: Proceedings of the Artificial Life Conference 2025 , 2025. [23] A. Coolen, “Statistical mechanics of recurrent neural networks I—Statics,” in Handbook of Biological Physics , vol. 4, pp. 553– 618, Elsevier, 2001. [24] A. Coolen, “Statistical mechanics of recurrent neural networks II — Dynamics,” in Handbook of Biological Physics , vol. 4, pp. 619–684, Elsevier, 2001. [25] M. G. Perich, D. Narain, and J. A. Gallego, “A neural manifold view of the brain,” Nature Neuroscience , vol. 28, p. 1582–1597, 2025. [26] J. A. Gallego, M. G. Perich, L. E. Miller, and S. A. Solla, “Neu-ral manifolds for the control of movement,” Neuron , vol. 94, no. 5, pp. 978–984, 2017. [27] D. Sussillo and O. Barak, “Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks,” Neural Computation , vol. 25, no. 3, pp. 626–649, 2013. [28] F. Mastrogiuseppe and S. Ostojic, “Linking connectivity, dy-namics, and computations in low-rank recurrent neural net-works,” Neuron , vol. 99, no. 3, pp. 609–623, 2018. [29] V. Schmutz, A. Haydaro˘ glu, S. Wang, Y. Feng, M. Caran-dini, and K. D. Harris, “High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model,” in 

39th Conference on Neural Information Processing Systems (NeurIPS 2025) , vol. 38, 2025. [30] V. Thibeault, A. Allard, and P. Desrosiers, “The low-rank hypothesis of complex systems,” Nature Physics , vol. 20, pp. 294—-302, 2024. [31] B. Prasse and P. V. Mieghem, “Predicting network dynamics without requiring the knowledge of the interaction graph,” Pro-ceedings of the National Academy of Sciences , vol. 119, no. 44, 2022. [32] C. Fefferman, S. Mitter, and H. Narayanan, “Testing the mani-fold hypothesis,” Journal of the American Mathematical Soci-ety , vol. 29, pp. 983–1049, 2016. [33] R. Mitchell-Heggs, S. Prado, G. P. Gava, M. A. Go, and S. R. Schultz, “Neural manifold analysis of brain circuit dynamics in health and disease,” Journal of Computational Neuroscience ,vol. 51, pp. 1–21, 2023. [34] D. V. Buonomano, G. Buzs´ aki, L. Davachi, and A. C. Nobre, “Time for memories,” Journal of Neuroscience , vol. 43, no. 45, pp. 7565–7574, 2023. [35] H. Jaeger, “The ”echo state” approach to analysing and train-ing recurrent neural networks,” tech. rep., GMD Report 148, German National Research Center for Information Technology, 2001. 23 

[36] C. Eliasmith and C. H. Anderson, Neural Engineering: Com-putation, Representation and Dynamics in Neurobiological Sys-tems . MIT Press, 2003. [37] P. J. Werbos, “Backpropagation through time: what it does and how to do it,” Proceedings of the IEEE , vol. 78, no. 10, pp. 1550–1560, 1990. [38] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term de-pendencies with gradient descent is difficult,” IEEE Transac-tions on Neural Networks , vol. 5, no. 2, pp. 157–166, 1994. [39] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedfor-ward networks are universal approximators,” Neural Networks ,vol. 2, no. 5, pp. 359–366, 1989. [40] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning .MIT Press, 2016. [41] E. Gl¨ otzl and O. Richters, “Helmholtz decomposition and po-tential functions for n-dimensional analytic vector fields,” Jour-nal of Mathematical Analysis and Applications , vol. 525, no. 2, p. 127138, 2023. [42] L. D. Costa and G. A. Pavliotis, “The entropy production of stationary diffusions,” Journal of Physics A: Mathematical and Theoretical , vol. 56, no. 36, p. 365001, 2023. [43] M. H. Duong and M. Ottobre, “Non-reversible processes: GENERIC, hypocoercivity and fluctuations,” Nonlinearity ,vol. 36, no. 1617, 2023. [44] J. C. Sprott, Elegant Chaos: Algebraically Simple Chaotic Flows . World Scientific, 2010. [45] S. Dadras and H. R. Momeni, “A novel three-dimensional au-tonomous chaotic system generating two, three and four-scroll attractors,” Physics Letters A , vol. 373, pp. 3637–3642, 2009. [46] K. Pearson, “LIII. On lines and planes of closest fit to sys-tems of points in space,” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science , vol. 2, no. 11, pp. 559–572, 1901. [47] M. M. Churchland, J. P. Cunningham, M. T. Kaufman, J. D. Foster, P. Nuyujukian, S. I. Ryu, and K. V. Shenoy, “Neu-ral population dynamics during reaching,” Nature , vol. 487, p. 51–56, 2012. [48] J. P. Cunningham and B. M. Yu, “Dimensionality reduction for large-scale neural recordings,” Nature Neuroscience , vol. 17, p. 1500–1509, 2014. [49] C. J. Cueva, A. Ardalan, M. Tsodyks, and N. Qian, “Recurrent neural network models for working memory of continuous vari-ables: activity manifolds, connectivity patterns, and dynamic codes,” arXiv , vol. 2111.01275, 2021. [50] R. Vidal, Y. Ma, and S. S. Sastry, Generalized Principal Com-ponent Analysis . Springer, 2016. [51] K. Friston and P. Ao, “Free energy, value, and attractors,” Com-putational and Mathematical Methods in Medicine , vol. 2012, no. 937860, 2011. [52] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. J. Huang, “Energy-based models,” in Predicting Structured Data , MIT Press, 2007. [53] R. Nartallo-Kaluarachchi, M. Asllani, G. Deco, M. L. Kringel-bach, A. Goriely, and R. Lambiotte, “Broken detailed balance and entropy production in directed networks,” Physical Review E, vol. 110, no. 034313, 2024. [54] M. Fruchart, R. Hanai, P. B. Littlewood, and V. Vitelli, “Non-reciprocal phase transitions,” Nature , vol. 592, pp. 363–369, 2021. [55] H. Ninou, J. Kadmon, and N. A. Cayco-Gajic, “Curl descent: Non-gradient learning dynamics with sign-diverse plasticity,” in 39th Conference on Neural Information Processing Systems (NeurIPS 2025). , 2025. [56] D.-Q. Jiang, M. Qian, and M.-P. Qian, Mathematical Theory of Nonequilibrium Steady States: On the Frontier of Probability and Dynamical Systems . Springer, 2004. [57] V. I. Bogachev, Measure Theory . Springer, 2007. [58] E. Pollock and M. Jazayeri, “Engineering recurrent neural networks from task-relevant manifolds and dynamics,” PLOS Computational Biology , vol. 16, no. 8, p. e1008128, 2020. [59] R. B. Yishai, R. L. Bar-Or, and H. Sompolinsky, “Theory of orientation tuning in visual cortex,” Proceedings of the National Academy of Sciences , vol. 92, pp. 3844–3848, 1995. [60] A. D. Bernardo, A. Valente, F. Mastrogiuseppe, and S. Ostojic, “Shaping manifolds in equivariant recurrent neural networks,” 

arXiv , vol. 2511.04802v2, 2025. [61] K. Rajan, C. D. Harvey, and D. W. Tank, “Recurrent network models of sequence generation and memory,” Neuron , vol. 90, no. 1, pp. 128–142, 2016. [62] M. Aguilera, M. Igarashi, and H. Shimazaki, “Nonequilibrium thermodynamics of the asymmetric Sherrington-Kirkpatrick model,” Nature Communications , vol. 14, no. 3685, 2023. [63] M. Aguilera, S. A. Moosavi, and H. Shimazaki, “A unifying framework for mean-field theories of asymmetric kinetic Ising systems,” Nature Communications , vol. 58, 2021. [64] C. W. Lynn, C. M. Holmes, W. Bialek, and D. J. Schwab, “Decomposing the local arrow of time in interacting systems,” 

Physical Review Letters , vol. 129, no. 118101, 2022. [65] Y.-A. Ma, T. Chen, and E. B. Fox, “A complete recipe for stochastic gradient MCMC,” in NIPS’15: Proceedings of the 29th International Conference on Neural Information Process-ing Systems , vol. 2, pp. 2917 – 2925, 2015. [66] P. Kloeden and E. Platen, Numerical Solution of Stochastic Dif-ferential Equations . Springer, 1992. [67] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction .Springer, 2009. [68] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haber-land, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wil-son, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙ Ilhan Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and S. . Contribu-tors, “Scipy 1.0: fundamental algorithms for scientific comput-ing in Python,” Nature Methods , no. 17, p. 261–272, 2020. [69] M. P. do Carmo, Differentiable Geometry of Curves and Sur-faces . Prentice-Hall, 1976. [70] J. M. Lee, Introduction to Smooth Manifolds . Springer, 2000. [71] R. Nartallo-Kaluarachchi, R. Lambiotte, and A. Goriely, “Coarse-graining nonequilibrium diffusions with Markov chains,” arXiv , vol. 2511.05366, 2025.
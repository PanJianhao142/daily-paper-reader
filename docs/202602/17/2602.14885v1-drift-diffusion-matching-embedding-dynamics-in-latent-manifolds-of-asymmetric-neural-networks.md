---
title: "Drift-Diffusion Matching: Embedding dynamics in latent manifolds of asymmetric neural networks"
title_zh: 漂移-扩散匹配：在非对称神经网络的潜在流形中嵌入动力学
authors: "Ramón Nartallo-Kaluarachchi, Renaud Lambiotte, Alain Goriely"
date: 2026-02-16
pdf: "https://arxiv.org/pdf/2602.14885v1"
tags: ["keyword:MDM"]
score: 6.0
evidence: 随机动力系统的漂移扩散匹配
tldr: 本研究针对传统对称连接RNN（如Hopfield模型）仅能处理梯度流动态的局限性，提出了“漂移-扩散匹配”框架。该框架允许在具有非对称连接的连续时间RNN中，将任意随机动力系统（包括非线性和非平衡态动力学，如混沌吸引子）嵌入到低维潜流形中。通过该方法，RNN能够实现由输入驱动或自主转换的联想与序列记忆，为理解生物神经回路中的复杂动态计算提供了统一的理论视角。
motivation: 传统的对称连接神经网络模型限制了动力学表现，无法捕捉生物神经网络中丰富的非平衡态和随时间变化的复杂行为。
method: 提出“漂移-扩散匹配”框架，通过训练非对称RNN的连接权重，使其在低维潜空间中精确模拟给定随机微分方程的漂移和扩散项。
result: 成功在RNN中实现了包括混沌吸引子在内的复杂动力学，并构建了能够进行状态切换和自主序列转换的联想与情节记忆模型。
conclusion: 该研究将吸引子网络理论扩展至非平衡态，证明了非对称神经元群体可以在低维流形上实现广泛的动力学计算。
---

## 摘要
循环神经网络 (RNN) 为理解生物神经回路中的计算提供了理论框架，然而经典结果（如 Hopfield 联想记忆模型）依赖于对称连接，这限制了网络动力学仅能表现为类梯度流。相比之下，生物网络凭借其非对称性支持丰富的时变行为。在这里，我们引入了一个通用框架，称之为“漂移-扩散匹配”，用于训练连续时间 RNN，使其在低维潜在子空间内表征任意随机动力系统。通过允许非对称连接，我们证明了 RNN 可以忠实地嵌入给定随机微分方程的漂移和扩散，包括非线性和非平衡动力学（如混沌吸引子）。作为应用，我们构建了随机系统的 RNN 实现，这些系统通过输入驱动的切换和由非平衡流驱动的自主转换，瞬态地探索各种吸引子，我们将其解释为联想记忆和序列（情境）记忆模型。为了阐明这些动力学是如何在网络中编码的，我们基于 RNN 的非对称连接性和时间不可逆性引入了分解方法。我们的结果将吸引子神经网络理论扩展到了平衡态之外，表明非对称神经元群体可以在低维流形内实现广泛的动力学计算，统一了联想记忆、非平衡统计力学和神经计算的思想。

## Abstract
Recurrent neural networks (RNNs) provide a theoretical framework for understanding computation in biological neural circuits, yet classical results, such as Hopfield's model of associative memory, rely on symmetric connectivity that restricts network dynamics to gradient-like flows. In contrast, biological networks support rich time-dependent behaviour facilitated by their asymmetry. Here we introduce a general framework, which we term drift-diffusion matching, for training continuous-time RNNs to represent arbitrary stochastic dynamical systems within a low-dimensional latent subspace. Allowing asymmetric connectivity, we show that RNNs can faithfully embed the drift and diffusion of a given stochastic differential equation, including nonlinear and nonequilibrium dynamics such as chaotic attractors. As an application, we construct RNN realisations of stochastic systems that transiently explore various attractors through both input-driven switching and autonomous transitions driven by nonequilibrium currents, which we interpret as models of associative and sequential (episodic) memory. To elucidate how these dynamics are encoded in the network, we introduce decompositions of the RNN based on its asymmetric connectivity and its time-irreversibility. Our results extend attractor neural network theory beyond equilibrium, showing that asymmetric neural populations can implement a broad class of dynamical computations within low-dimensional manifolds, unifying ideas from associative memory, nonequilibrium statistical mechanics, and neural computation.
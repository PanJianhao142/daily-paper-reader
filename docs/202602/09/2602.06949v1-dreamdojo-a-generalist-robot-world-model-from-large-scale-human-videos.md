---
title: "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos"
title_zh: DreamDojo：基于大规模人类视频的通用机器人世界模型
authors: "Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, Qianli Ma, Seungjun Nah, Loic Magne, Jiannan Xiang, Yuqi Xie, Ruijie Zheng, Dantong Niu, You Liang Tan, K. R. Zentner, George Kurian, Suneel Indupuru, Pooya Jannaty, Jinwei Gu, Jun Zhang, Jitendra Malik, Pieter Abbeel, Ming-Yu Liu, Yuke Zhu, Joel Jang, Linxi \"Jim\" Fan"
date: 2026-02-06
pdf: "https://arxiv.org/pdf/2602.06949v1"
tags: ["query:课题"]
score: 6.0
evidence: 人类视频和灵巧控制的世界模型
tldr: DreamDojo是一个通用的机器人世界模型，利用4.4万小时的大规模人类第一视角视频进行预训练，旨在解决机器人任务中数据覆盖不足和动作标签稀缺的问题。该模型引入了连续潜动作作为统一代理动作，通过在少量机器人数据上进行后训练，实现了对物理规律的深刻理解和精确的动作控制。此外，通过蒸馏技术实现了实时推理，为远程操作、策略评估和模型预测规划提供了强有力的支持。
motivation: 针对机器人领域数据覆盖有限及动作标签稀缺的挑战，旨在构建一个能模拟多样化环境和灵巧操作的通用世界模型。
method: 利用大规模人类视频预训练，引入连续潜动作作为代理动作，并结合机器人数据后训练与蒸馏技术提升性能与速度。
result: 实现了10.81 FPS的实时推理速度，在多个分布外（OOD）基准测试中展现出强大的物理理解能力和精确的动作可控性。
conclusion: DreamDojo证明了利用大规模人类视频构建通用机器人世界模型的可行性，为实现开放世界、高接触任务的模拟奠定了基础。
---

## 摘要
能够模拟不同环境中动作的结果，将彻底改变大规模通用智能体的开发。然而，由于数据覆盖范围有限且动作标签稀缺，对这些世界动力学进行建模（特别是对于灵巧机器人任务）面临着重大挑战。为此，我们推出了 DreamDojo，这是一个基础世界模型，它从 4.4 万小时的第一视角人类视频中学习多样的交互和灵巧控制。我们的混合数据代表了迄今为止用于世界模型预训练的最大视频数据集，涵盖了具有多样物体和技能的广泛日常场景。为了解决动作标签稀缺的问题，我们引入了连续潜动作作为统一的代理动作，增强了从未标记视频中进行的交互知识迁移。在小规模目标机器人数据上进行后训练后，DreamDojo 展示了对物理规律的深刻理解和精确的动作可控性。我们还设计了一个蒸馏流水线，将 DreamDojo 加速至 10.81 FPS 的实时速度，并进一步提高了上下文一致性。我们的工作实现了基于生成式世界模型的几项重要应用，包括实时远程操作、策略评估和基于模型的规划。在多个具有挑战性的分布外（OOD）基准测试上的系统评估验证了我们方法在模拟开放世界、高接触任务方面的意义，为通用机器人世界模型铺平了道路。

## Abstract
Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.
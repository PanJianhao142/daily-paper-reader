---
title: "AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models"
title_zh: AEGPO：扩散模型的自适应熵引导策略优化
authors: "Yuming Li, Qingyu Li, Chengyu Bai, Xiangyang Luo, Zeyue Xue, Wenyu Qin, Meng Wang, Yikai Wang, Shanghang Zhang"
date: 2026-02-06
pdf: "https://arxiv.org/pdf/2602.06825v1"
tags: ["keyword:FM", "keyword:MDM"]
score: 7.0
evidence: 扩散和流模型的策略优化
tldr: 本研究针对扩散模型在强化学习对齐（RLHF）中采样策略低效且静态的问题，提出了自适应熵引导策略优化（AEGPO）。通过分析注意力机制，发现注意力熵的相对变化可指示样本学习价值，而绝对熵峰值可识别关键去噪步骤。AEGPO在全局层面动态分配计算预算，在局部层面引导关键步骤的探索。实验证明，该方法在文生图任务中显著加速了收敛并提升了对齐性能。
motivation: 现有的扩散模型策略优化方法（如GRPO）对所有提示词和去噪步骤一视同仁，忽略了样本学习价值的差异和关键探索时刻的动态性。
method: 提出AEGPO算法，利用注意力熵的相对变化进行全局预算分配，并利用熵峰值在局部层面引导关键去噪步骤的探索。
result: 在文生图任务上的实验表明，AEGPO相比标准GRPO变体显著提高了收敛速度和对齐效果。
conclusion: 注意力熵是优化扩散模型训练的有效双信号代理，能通过聚焦高价值样本和关键时刻显著提升策略优化的效率。
---

## 摘要
基于人类反馈的强化学习（RLHF）在对齐扩散模型和流模型方面展现出巨大潜力，但诸如 GRPO 等策略优化方法仍受困于低效且静态的采样策略。这些方法对所有提示词（prompts）和去噪步骤一视同仁，忽略了样本学习价值的显著差异以及关键探索时刻的动态特性。为解决这一问题，我们对 GRPO 训练过程中的内部注意力动态进行了详细分析，并发现了一个关键见解：注意力熵可以作为一种强大的双信号代理。首先，在不同样本之间，反映当前策略与基础策略之间差异的注意力熵相对变化（ΔEntropy），可作为样本学习价值的稳健指标。其次，在去噪过程中，量化注意力分散程度的绝对注意力熵（Entropy(t)）峰值，能有效识别发生高价值探索的关键时间步。基于这一观察，我们提出了自适应熵引导策略优化（AEGPO），这是一种新颖的双信号、双层级自适应优化策略。在全局层级，AEGPO 利用 ΔEntropy 动态分配展开（rollout）预算，优先处理具有更高学习价值的提示词。在局部层级，它利用 Entropy(t) 的峰值，在关键的高分散时间步有选择性地引导探索，而非在所有去噪步骤中均匀进行。通过将计算资源集中在信息量最大的样本和最关键的时刻，AEGPO 实现了更高效、更有效的策略优化。在文本生成图像任务上的实验表明，与标准的 GRPO 变体相比，AEGPO 显著加速了收敛，并取得了更优的对齐性能。

## Abstract
Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.   To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.   Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.   By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.
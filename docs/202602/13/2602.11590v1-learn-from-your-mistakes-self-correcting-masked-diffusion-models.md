---
title: "Learn from Your Mistakes: Self-Correcting Masked Diffusion Models"
title_zh: 从错误中学习：自纠错掩码扩散模型
authors: "Yair Schiff, Omer Belhasin, Roy Uziel, Guanghan Wang, Marianne Arriola, Gilad Turok, Michael Elad, Volodymyr Kuleshov"
date: 2026-02-12
pdf: "https://arxiv.org/pdf/2602.11590v1"
tags: ["keyword:MDM"]
score: 6.0
evidence: 专注于用于序列生成和误差校正的掩码扩散模型 (MDM)
tldr: 针对掩码扩散模型（MDM）中已生成 token 无法修改导致错误累积的问题，本文提出 Progressive Self-Correction (ProSeCo) 框架。该方法通过训练模型同时具备取消掩码和纠错能力，在生成过程中引入修正步骤，允许对已解码 token 进行迭代优化。实验证明，ProSeCo 在保持高质量的同时显著提升了采样效率（2-3倍加速），并支持通过增加推理计算量进一步提升生成质量。
motivation: 解决掩码扩散模型中 token 一旦取消掩码便无法更改，从而导致生成过程中错误不断累积并降低样本质量的局限性。
method: 提出 ProSeCo 框架，通过重用去噪网络输出进行训练，使模型在生成过程中能交替执行取消掩码与纠错优化步骤。
result: 在多项任务中实现了更好的质量与效率平衡，采样速度提升达 2-3 倍，且基准测试性能提升约 1.3 倍。
conclusion: ProSeCo 证明了通过引入自我纠错机制，可以有效克服 MDM 的固有缺陷，实现更高效、更高质量的序列生成。
---

## 摘要
掩码扩散模型 (MDMs) 已成为自回归模型的一种极具前景的替代方案，在实现并行令牌生成的同时，达到了具有竞争力的性能。尽管具有这些优势，MDMs 仍面临一个根本性的局限：一旦令牌被取消掩码，它们就会保持固定，从而导致错误累积并最终降低样本质量。我们通过提出一个框架来解决这一问题，该框架训练模型同时执行取消掩码和纠错。通过将 MDM 去噪网络的输出重新用作纠错器训练的输入，我们训练模型从潜在的错误中恢复。在生成过程中，我们在取消掩码步骤之间应用额外的纠错细化步骤，以更改已解码的令牌并改进输出。我们将这种训练和采样方法命名为渐进式自纠错 (Progressive Self-Correction, ProSeCo)，因为它具有迭代细化整个序列（包括已生成的令牌）的独特能力。我们在多个有条件和无条件任务上进行了广泛的实验验证，证明了 ProSeCo 带来了更好的质量-效率权衡（采样速度提升高达约 2-3 倍），并实现了推理时计算扩展，从而在标准 MDMs 的基础上进一步提高了样本质量（在基准测试中提升高达约 1.3 倍）。

## Abstract
Masked diffusion models (MDMs) have emerged as a promising alternative to autoregressive models, enabling parallel token generation while achieving competitive performance. Despite these advantages, MDMs face a fundamental limitation: once tokens are unmasked, they remain fixed, leading to error accumulation and ultimately degrading sample quality. We address this by proposing a framework that trains a model to perform both unmasking and correction. By reusing outputs from the MDM denoising network as inputs for corrector training, we train a model to recover from potential mistakes. During generation we apply additional corrective refinement steps between unmasking ones in order to change decoded tokens and improve outputs. We name our training and sampling method Progressive Self-Correction (ProSeCo) for its unique ability to iteratively refine an entire sequence, including already generated tokens. We conduct extensive experimental validation across multiple conditional and unconditional tasks, demonstrating that ProSeCo yields better quality-efficiency trade-offs (up to ~2-3x faster sampling) and enables inference-time compute scaling to further increase sample quality beyond standard MDMs (up to ~1.3x improvement on benchmarks).
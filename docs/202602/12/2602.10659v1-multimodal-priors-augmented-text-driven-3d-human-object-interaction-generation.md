---
title: Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation
title_zh: 多模态先验增强的文本驱动 3D 人物交互生成
authors: "Yin Wang, Ziyao Zhang, Zhiying Leng, Haitian Liu, Frederick W. B. Li, Mu Li, Xiaohui Liang"
date: 2026-02-11
pdf: "https://arxiv.org/pdf/2602.10659v1"
tags: ["keyword:MDM", "query:课题"]
score: 6.0
evidence: 3D人体-物体交互运动生成
tldr: 针对文本驱动的3D人机交互（HOI）生成中存在的人体动作不佳、物体运动不自然及交互感弱等问题，本文提出了MP-HOI框架。该框架利用多模态大模型先验引导生成，通过增强物体表征、引入多模态感知专家混合模型（MoE）以及级联扩散模型，显著提升了HOI生成的真实感与交互质量。
motivation: 现有的文本到HOI直接映射方法由于跨模态差距，难以生成高质量的人体动作、自然的物体运动以及紧密的交互关系。
method: 提出了MP-HOI框架，结合多模态数据先验、增强的物体几何与动态表征、多模态感知MoE融合机制以及带交互监督的级联扩散模型。
result: 实验结果表明，MP-HOI在生成高保真度和细粒度的人机交互动作方面优于现有方法。
conclusion: 本研究通过引入多模态先验和改进的扩散架构，有效解决了3D HOI生成中的核心挑战，为复杂交互场景的建模提供了新思路。
---

## 摘要
我们致力于解决文本驱动的 3D 人物交互（HOI）动作生成这一极具挑战性的任务。现有方法主要依赖于直接的文本到 HOI 映射，由于显著的跨模态差距，这些方法面临三个关键局限性：(Q1) 人体动作次优，(Q2) 物体运动不自然，以及 (Q3) 人与物体之间的交互较弱。为了应对这些挑战，我们提出了 MP-HOI，这是一个基于四个核心见解的新颖框架：(1) 多模态数据先验：我们利用来自大型多模态模型的多模态数据（文本、图像、姿态/物体）作为先验来指导 HOI 生成，从而在数据建模层面解决了 Q1 和 Q2；(2) 增强的物体表示：我们通过整合几何关键点、接触特征和动态属性来改进现有的物体表示，实现了更具表现力的物体表示，从而在数据表示层面解决了 Q2；(3) 多模态感知混合专家（MoE）模型：我们提出了一种模态感知的 MoE 模型，用于构建有效的多模态特征融合范式，从而在特征融合层面解决了 Q1 和 Q2；(4) 带有交互监督的级联扩散：我们设计了一个级联扩散框架，在专门的监督下逐步细化人物交互特征，从而在交互细化层面解决了 Q3。综合实验表明，MP-HOI 在生成高保真和细粒度的 HOI 动作方面优于现有方法。

## Abstract
We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.
Title: Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation

URL Source: https://arxiv.org/pdf/2602.10659v1

Published Time: Thu, 12 Feb 2026 01:49:53 GMT

Number of Pages: 17

Markdown Content:
# SCIENCE CHINA 

# Information Sciences 

# . RESEARCH PAPER .Multimodal priors-augmented text-driven 3D human-object interaction generation 

Yin Wang 1, Ziyao Zhang 1, Zhiying Leng 1, Haitian Liu 1, Frederick W. B. Li 2,Mu Li 1 & Xiaohui Liang 1,3,* 

> 1

State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; 

> 2

Department of Computer Science, University of Durham, U.K; 

> 3

Zhongguancun Laboratory, Beijing, China 

Abstract We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions. 

Keywords Text-Driven Motion Generation, Human-Object Interaction, Multimodal Models, Diffusion Model 

Citation Multimodal priors-augmented text-driven 3D human-object interaction generation. Sci China Inf Sci, for review 

1 Introduction 

Humans continuously interact with surrounding objects in daily life, e.g., moving monitors onto desks, pushing suitcases to desired locations, or washing apples and placing them on plates. Each task requires precise interaction between humans actions and objects movements. Exploring human-object interaction motion generation holds significant importance due to its broad downstream applications in character animation, VR/AR content creation, and robotics [12, 20, 30, 46, 52, 67]. As language serves as a natu-ral interface for expressing interaction intentions, text-driven HOI motion generation has emerged as a promising research direction, aiming to generate 3D human-object interaction sequences guided by text prompts. Existing work has explored text-driven HOI motion generation using diffusion models [21] guided by text [34, 57], object trajectories [28, 29], contact maps [36, 45], and other cues [5,54, 63]. Despite progress, current methods remain limited to coarse-grained interaction motions, falling short in three key aspects: 

(Q1) Sub-optimal Human Motion. Existing methods, such as HIMO-Gen [34], typically take text and object geometry as input and generate human motion through simple condition concatenation. This insufficient modeling of conditions leads to sub-optimal human motion, resulting in low-quality sequences that are misaligned with the intended interactions. (Q2) Unnatural Object Motion. Prior works [29, 34] often adopt a simplified object representation, namely 3D translation and 6D rotation (only 9 dimensions), which disregards the geometric structure of objects. Consequently, the generated object motions often appear unnatural, exhibiting floating or sliding artifacts. (Q3) Weak Interaction Motion. Methods such as CHOIS [28] employ a one-step and direct text-to-HOI mapping. However, given the complexity of human-object interactions, this approach struggles to capture fine-grained HOI dynamics, leading to unrealistic contact, interpenetration, or implausible interactions in the generated motions. 

* Corresponding author (email: liang xiaohui@buaa.edu.cn)          

> arXiv:2602.10659v1 [cs.CV] 11 Feb 2026 Sci China Inf Sci 21D: Text
> 1. Put up the box.
> 2. Hold on the box.
> 3. Move the box to
> 2D: Image
> 3D: Action
> Text Prompt: A person picks up a large box and then moves it to his right.
> the right.
> MP-HOI
> 1234
> 1
> 234
> Figure 1 MP-HOI excels in generating fine-grained human-object interaction motions from multimodal data priors, achieving both high-quality human-object interactions and precise text-motion alignment.

To address these challenges, our core insight for effective modeling of human–object interaction can be structured around four core innovations. (1) Multimodal Data Priors — Addressing Q1 and Q2 

in data modeling. We extract structured hierarchical priors from large multimodal models. Textual (1D): fine-grained descriptions derived from language parsing, Visual (2D): image-based motion refer-ences, and Spatial (3D): human atomic actions and object geometric structure. These multimodal priors provide rich semantic guidance for generating both human and object motion. (2) Enhanced Object Representation — Addressing Q2 in data representation. We augment object representations with additional 3D mesh keypoints, contact information, and translational/angular velocities. By enrich-ing objects with detailed geometric and dynamic attributes, we enable more precise and stable object motion generation. (3) Multimodal-Aware Mixture-of-Experts Model — Addressing Q1 and 

Q2 in feature fusion. We propose a modality-aware MoE framework that selects specialized experts to enable effective multimodal interaction, which optimizes the multimodal feature fusion paradigm. (4) Cascaded Diffusion with Interaction Supervision — Addressing Q3 in interaction optimiza-tion. We design a cascaded diffusion framework: human diffusion, object diffusion, and human–object interaction diffusion. Coarse-grained human and object motions are generated first to guide fine-grained interaction motion generation. Additional supervision losses further refine the interaction features. We validate our approach through comprehensive experiments on two benchmark datasets: FullBody-Manipulation (single-object interaction) [29] and HIMO (multi-objects interaction) [34]. Results show that our method outperforms existing techniques, achieving a new state-of-the-art in text-driven HOI motion generation. Our contributions are summarized as follows: 

• We introduce the leverage of multimodal priors—1D textual, 2D visual, and 3D spatial—to guide the fine-grained human–object interaction motion generation. 

• We propose an enhanced object representation that incorporates geometric keypoints, contact fea-tures, and dynamic properties, enabling structurally rich and stable motion representation. 

• We present a modality-aware mixture-of-experts model that optimizes the fusion paradigm for multimodal features. 

• We design a cascaded diffusion framework that progressively refines human–object interaction fea-tures under dedicated supervision, achieving state-of-the-art performance on existing benchmarks. 

2 Related Work 

2.1 Text-Driven Human Motion Generation 

Three primary methodologies have emerged to tackle the challenge of text-driven human motion gener-ation. (i) Latent Space Alignment [2, 15, 18, 38, 48] aims to learn a unified latent space between text and motion embeddings. (ii) Conditional Autoregressive Models [19, 23, 33, 58, 64, 65] generate mo-tion tokens sequentially by leveraging previous tokens and text. In recent advancements [17, 39, 40] utilize masked motion modeling to generate more natural movements. (iii) Conditional Diffusion Mod-els [7,25,49–51,53,59–62], which learn probabilistic text-to-motion mappings within a conditional diffusion Sci China Inf Sci 3

framework, have shown remarkable performance. While these advancements have propelled human mo-tion generation forward, they predominantly center on individual motion generation, lacking the ability to generate interactive motions with external elements (e.g., objects). 

2.2 Text-Driven Human-Object Interaction Generation 

The generation of human-object interactions has recently emerged as a promising research direction, attracting increasing attention from the community [36, 45, 56]. OMOMO [29] generates 3D human pose sequences based on the given motion of interacting 3D objects. GRAB [47] predicts 3D hand grasping poses for specific 3D object shapes, performing various grasping manipulations. InterDiff [55] develops a diffusion-based generative model that predicts future human-object collaborative motion from their 3D interaction history. CG-HOI [9] proposes a method to generate realistic 3D human-object interactions from text descriptions and given static object geometry. IMoS [16] synthesizes full-body human and 3D object motions from textual inputs, but focuses exclusively on small-object grasping. HIMO [34] introduces a large-scale motion capture dataset of humans interacting with multiple objects and develops a baseline model. However, current methods can only achieve coarse-grained human-object interactions, which primarily manifest in three limitations: sub-optimal human motion, unnatural object motion, and weak interaction motion. Therefore, exploring fine-grained human-object interaction generation remains a critical yet challenging problem. 

2.3 Large Model-Assisted Motion Generation 

In recent years, large language models (LLMs) such as BERT [8], GPT-4 [1], and T5 [43] have demon-strated remarkable capabilities in language tasks. Some works have leveraged the strengths of LLMs to assist in motion generation tasks. For example, ActionGPT [24] utilizes GPT-3 to parse input text prompts into simple and long-form text prompts. SINC [3] also employs GPT-3.5 to establish rela-tionships between motion and the human body. FineMoGen [62] uses LLMs to adjust text prompts according to user input requirements for motion editing tasks. Fg-T2M++ [53] leverages GPT-4 to parse text prompts into detailed prompts for body part joints and analyzes the keyword properties in the text prompts to enable fine-grained motion generation. Overall, current methods primarily utilize the text generation capability of LLMs to enhance the richness of textual prompts, thereby improving motion generation tasks. However, they only exploit the single-modality (text) prior knowledge of LLMs, lack-ing consideration for leveraging multimodal large models (e.g., incorporating image) to effectively guide motion generation tasks. 

3 Preliminarily 

Mixture-of-Experts [11, 22] assigns specific tasks to specialized experts, each adept at handling a particular aspect of the problem. This approach is well-suited to our multimodal priors-augmented HOI task, where the fusion of multimodal content and motion features presents a dynamic and complex challenge. Mixture-of-Experts involves a gating network to activate distinct subsets of expert networks for different inputs, which mainly consists of two key components. (1) MoE Layer: A MoE layer contains 

N experts (denoted as ei(·), i = 1 , 2, . . . , N ). (2) Gating Network: A router G routes the input token x

to the most suitable top-k experts. Formally, given the input token x, the output token y of the MoE layer is the weighted sum of outputs from the k activated experts: 

y = X

> i∈T

gi(x)ei(x), (1) where g(x) = σ(W x ), where W is the gate parameter, σ is the softmax function, and T represent the set of the top-k indices. Training MoE models directly often results in most tokens being assigned to a few experts, while others do not get sufficient training. Therefore, a load balancing loss [27, 68] is applied to ensure a balanced distribution of input tokens across the experts: 

Lb =

> N

X

> i=1

fiPi, (2) Sci China Inf Sci 4           

> Figure 2 Overview of MP-HOI. Given a text prompt and multimodal priors, the reverse denoising process of the Human Motion Diffusion Model and Object Motion Diffusion Model starts from noisy motion data HTand OT, generating clean human and object motion data ( H0and O0). Then, the Human-Object Interaction Diffusion Model takes the text prompt and the clean human and object motion data ( H0and O0) as inputs, and generates the final clean human-object interaction motion data X0.

where fi = 1

> Tt

PTt 

> t=1

1(Token xt selects Expert i), fi represents the fraction of tokens routed to expert i.

Pi is the fraction of the router probability assigned to expert i, defined as Pi = 1

> Tt

PTt 

> t=1

Softmax  g(xt)

> i

,

Tt denotes the number of tokens, N is the number of experts, 1(∗) denotes the indicator function. 

4 Methodology 

4.1 Overview 

We first formulate the text-driven HOI generation task. Given a text prompt Tp and object geometry Go,our goal is to generate a HOI motion sequence M ∈ RS×D, where S indicates the length of the motion sequence and D represents the dimension of HOI motion representation, which comprises a human motion 

H and an object motion O. The text prompt Tp is represented as Tp ∈ RN ×L, where N denotes the number of words and L is the dimension of the word vector. The object geometry Go is represented as 

Go ∈ RK×3, where K denotes the number of vertices on object mesh. As illustrated in Figure 2, we introduce MP-HOI, a diffusion model-based framework for text-driven HOI generation. We start from a human motion random noisy and an object motion random noisy, represented respectively as HT ∈ RS×Dh and OT ∈ RS×Do , where Dh denotes the dimension of human motion, Do represents the dimension of object motion. MP-HOI utilizes a Human Motion Diffusion Model and an Object Motion Diffusion Model to denoise them over th and to steps, respectively, generating H0

and O0. These are then fed as conditional inputs into a Human-Object Interaction Diffusion Model to facilitate the denoising process of the HOI motion random noise, ultimately producing X0 over thoi steps. 

4.2 Data Representation Human Representation. We denote the human motion as H ∈ RS×Dh . We adopt the SMPL-X [35] parametric model to represent the human motions. The representation Dh consists of the global joint position Djh ∈ R52 ×3, joint rotation Drh ∈ R52 ×6 represented in the continuous 6D rotation format [66] and global translation Dth ∈ R3. Thus, the overall human motion representation is 471 dimensions. Notably, since the FullBodyManipulation [29] dataset does not provide hand parameters, we omit the hand component of the SMPL-X [35] model during processing. 

Enhanced Object Representation. We denote Object motion as O = {Oj }No 

> j=0

∈ RS×Do , where 

No represents the number of the objects. Typically, each Do in Oj includes relative rotation Dro ∈ R6

and global translation Dto ∈ R3. Therefore, the object motion representation contains only 9 dims in total, which is significantly fewer than the human motion representation. This dimension gap poses a substantial challenge for learning object motion. To address this limitation, we enhance the object representation with three additional informative features. First, we incorporate the object’s translational Sci China Inf Sci 5Text Prompt: 

A person turns on the faucet, washes the 

cabbage, and then places it on a plate. 

GPT-4o 

Flux   

> {' action1 ': 'A person turns on the faucet ',
> 'action2': 'A person washes the cabbage ',
> 'action3': 'A person puts the cabbage on the plate',
> 'action1_parse': 'A person triggers the water flow by gripping, turning, or pressing
> the faucet handle.',
> 'action2_parse': 'A person holds the cabbage under running water, rotating it to
> ensure all surfaces are cleaned.',
> 'action3_parse': 'A person grasps the cabbage, lifts it, and moves it towards the
> plate, adjusting their grip and arm movement to place it gently on the plate.'}

Visual Parsing 

Textual Parsing    

> Action 1 Action 2 Action 3
> Figure 3 The pipeline for large models processing multimodal data (text and image).

velocity Dvt 

> o

∈ R3 and angular velocity Dva 

> o

∈ R3, both derived from its translation and rotation. Second, we represent the object’s geometric point cloud in a reduced form using 51 key points (50 sampled points plus 1 centroid). The global positions of these points Dpo ∈ R51 ×3 are computed via translation and rotation. Third, we include contact label information Dco ∈ R2 by calculating the distance between the object and the human’s left and right hands. As a result, our enhanced object representation Do

comprises a total of 170 dimensions per object. 

4.3 Multimodal Priors 

Existing methods predominantly adopt a direct text-to-HOI mapping approach for generation. However, due to the significant cross-modal gap between text and motion modalities, such methods often exhibit limitations in modeling fine-grained HOI interactions. Recently, large-scale models across various modal-ities (e.g., GPT-4 [1], DeepSeek [31], DALL-E 3 [4], Sora [32]) have advanced the field of multimodal learning through their powerful modeling capabilities. Our key insight is to leverage multimodal priors from diverse modalities (textual (1D), visual (2D), and spatial (3D)) to enhance the model’s comprehen-sion of interaction concepts, providing rich semantic guidance for generating human/object motion. For the textual 1D prior, we leverage the strong priors of GPT-4o [1] to accurately capture the fine-grained relationship between natural language and human motion. Given a text prompt, we parse the sequence of actions being performed and provide detailed semantic explanations for each action. Specifically, for an input text such as “A person turned on the faucet, washed the cabbage, and then places it on a plate,” we first analyze the execution order of motions to obtain action-1, action-2, and action-3. Then, we provide fine-grained descriptions for action-i to support detailed understanding of interactive concepts at the textual level. The complete prompt is provided in the supplementary materials. For the visual 2D prior, we leverage the powerful text-to-image capabilities of Flux [26] to provide fine-grained visual guidance for HOI. In the previous stage, we have already extracted a sequence of interaction actions—for example, action-1 is “A person turned on the faucet.” Each action-i is then used as a textual prompt, augmented with style keywords such as “realism, photographic, detailed hand” and fed into Flux to generate the corresponding HOI images. These visual cues, generated for each interaction step, further strengthen the understanding of human-object interactions at the visual level. A complete example is provided in Figure 3. For the spatial 3D prior, we process the human and object components separately. For the object, we regard its geometric structure as a rich source of spatial information and use its point cloud data 

Go ∈ R1024 ×3 as the 3D prior. For the human, we observe that HOI types are not infinitely diverse but instead fall into a limited set. Through all datasets [29, 34] analysis, we identified approximately 55 common HOI actions, such as pick up, place, eat, wash and so on. For each category, we select the most representative frame that captures the essence of the interaction as an atomic motion, and pair it with a corresponding textual annotation. To associate a given action-i with the most relevant atomic motion, we perform text-based retrieval based on semantic similarity. Specifically, we use the CLIP [42] to extract text features for both the action-i text and all annotated atomic motion texts, and compute cosine similarity to find the closest match. By integrating the object’s spatial geometry with the human’s Sci China Inf Sci 6       

> Figure 4 Illustration of the overall object motion diffusion pipeline. (a) Object diffusion process. (b) Object motion diffusion model. (c) Architecture of Modality-aware MoE Models. Notably, the Human Motion Diffusion Model adopts this identical architecture, with the object geometry feature Cfpreplaced by the atomic motion feature Cfa.

representative atomic motion, we further reinforce the understanding of HOI at the spatial level. In summary, given a text prompt, we extract fine-grained action-parsed textual descriptions Ct, visual images Cv generated based on the interaction order text, atomic motions Ca, and object point clouds Go.For feature extraction, we employ the CLIP to encode the text Ct and the images Cv , yielding feature representations Cft ∈ RNa×Dt and Cfv ∈ RNa×Dv , respectively, where Na denotes the number of text or image. The atomic motions Ca are encoded using a MLP to obtain Cfa ∈ RNa×Da , while the object point clouds Go are processed with a PointNet [41] architecture to obtain Cfp ∈ RNb×Dp , where Nb denotes the number of object geometry. 

4.4 Human/Object Motion Diffusion Process 

Existing methods typically adopt a one-step generation approach (e.g., only the Human-Object Interac-tion Diffusion in Figure 2) to generate human-object interaction motions. However, due to the inherent complexity of HOI dynamics, such direct text-to-HOI mapping often yields suboptimal results, includ-ing imprecise human motion, unnatural object trajectories, and coarse-grained interaction patterns. To address this issue, we propose a multi-step generation paradigm to enhance interaction quality. As illus-trated in Figure 2, our framework first employs separate Human Motion Diffusion and Object Motion Diffusion model to generate preliminary human and object motions, conditioned on text prompts and multimodal priors. These intermediate motions serve as bridging representations, mitigating the feature gap between textual descriptions and HOI sequences. Subsequently, the preliminary human and object motions are integrated into the Human-Object Interaction Diffusion Model as reference features to guide the HOI generation process, ultimately yielding refined HOI motion sequences. 

Human/Object Motion Diffusion Model Figure 4 presents the complete object motion diffusion pipeline. It is worth noting that the human motion diffusion pipeline follows a similar process, with the only difference being the replacement of the object geometry feature Cfp with the atomic motion feature 

Cfa . Therefore, we take the object motion diffusion model as an example for detailed explanation. The goal of the Object Motion Diffusion Model is to generate a plausible object motion sequence based on text prompts and multimodal priors. We take the denoising process at time step t as an example, as illustrated in Figure 4(b). Specifically, the object motion features Ot and multimodal prior features (Cft , C fv , C fp ) are fed into a Modality-aware Mixture-of-Experts Model which serves as an information fusion module. This module injects the multimodal knowledge into the object motion features, producing a fused representation. The fused object motion features are then combined with the text prompt features Tp and passed through multiple transformer blocks, each consisting of a self-attention layer, a Sci China Inf Sci 7

cross-attention layer, and a feedforward layer. This process ultimately outputs the refined object motion features Ot−1 at time step t–1. 

Modality-aware Mixture-of-Experts Models To generate fine-grained human-object interaction motion using data priors that provide rich semantic guidance, the core challenge lies in effectively fusing multi-modal condition features with HOI motion features. On a deeper level, the fundamental hetero-geneity across different modalities (e.g., textual, visual, and spatial) introduces additional significant challenges for feature fusion. General fusion methods, such as simple concatenation or attention-based fusion, are typically static and inflexible. They may prioritize certain modalities while neglecting the information gains from others, failing to preserve fine-grained semantic features within some modalities features that are crucial for capturing the nuances of human-object interaction modeling. Mixture of Experts [6, 13] offers a promising alternative by dynamically routing inputs to specialized experts, each handling distinct modality patterns. Thus, we argue that MoEs provides a highly effective and flexible solution to this challenge. However, previous MoE models typically adopt simple FFNs as expert layers [10, 14]. Such architectures are insufficient for effectively integrating motion and multi-modal condition features, as they fail to account for how each modality influences motion representation in a fine-grained manner. To address this limitation, we propose the Modality-aware Mixture-of-Experts model, as shown in Figure 4 (c). Specifically, the MoEs includes a routing function that directs the input features to the appropriate expert models, and several experts specializing in different tasks. Taking the object motion feature Ot

as an example, the routing function contains a routing parameter matrix Ro that is used to assign tasks: 

lo = τt(W oOt)Ro, (3) where τt is the trainable temperature hyper-parameter, W o is the trainable matrix, lo ∈ RNt×Ne represents the logits for selecting experts in object motion. Here, Nt denotes the number of tokens input to the MoE, and Ne denotes the number of expert models. By considering which expert model best matches the input tokens, the Router dynamically identifies and selects the most suitable expert. As for expert architecture, drawing inspiration from FiLM [37], we design the condition modulation module to adaptively model the influence of multi-modal conditions on motion features: [C1 

> f

, C2 

> f

] = W 1ϕ([ Cft ; Cfv ; Cfa ]) + β1, (4) where W 1 and β1 are the trainable matrices, [ ·; ·] indicates a concatenation of input tensors, ϕ denotes the activation function, C1 

> f

∈ R1×Df and C2 

> f

∈ R1×Df represent the scale and shift parameters, respectively, which are then adaptively fused with the motion features: 

OF = FFN[W 2ϕ[(1 + C1 

> f

)Ot + C2 

> f

] + β2], (5) where W 2 and β2 are the trainable matrices, OF is then reshaped to RT ×Do as the final fusion output of the MoE Layer. 

Human/Object Motion Diffusion Training Objective To supervise human/object motion gener-ation process, we minimize the L2 loss between predicted and ground truth motions, denoted as 

Ll2 

> h/o

= E[∥ x0 − ϵθ (xt, t, Tp, C ft , C fv , C fa/p ) ∥22]. (6) where ϵθ (xt, t, Tp, C ft , C fv , C fa/p ) denotes the model prediction, Ll2 

> h

and Ll2 

> o

denote the human motion diffusion loss and the object motion diffusion loss, respectively. In addition, it is necessary to consider the load balancing loss of the MoE, which serves to optimize the expert assignment mechanism. Therefore, the total training loss in this section includes the L1 loss and load balancing loss: Lh/o = Ll2 

> h/o

+ λbLb,where λb is the trade-off hyperparameter. 

4.5 Human-Object Interaction Diffusion Process Human-Object Interaction Diffusion Model The Human-Object Interaction Diffusion Model gen-erates a fine-grained human-object interaction motion sequence based on the text prompt and the pre-liminary human and object motions produced in the previous stage, as illustrated in Figure 5. We take Sci China Inf Sci 8  

> Figure 5 Illustration of the overall human-object interaction motion diffusion pipeline. (a) Human-object interaction motion diffusion process. (b) Architecture of human-object interaction diffusion model. Pre-Human Motion and Pre-Object Motion represent the human motion and object motion generated in the human/object motion diffusion process, respectively.

the denoising process at time step t and human motion Xht as an example. Specifically, the human mo-tion features Xht are passed through multiple HOI transformer blocks, each consisting of a self-attention layer, a mixed-attention layer, and a feedforward layer. The mixed-attention layer facilitates information exchange between the human motion Xht and various contextual features ( H0, Tp, Xot ). Formally, the Query, Key, and Value matrices in this layer are computed as follows: 

Q = Q hmXht , K = [K hH0; K pTp; K omXot ], V = [V hH0; V pTp; V omXot ], (7) where Q hm, K h, K p, K om, V h, V p and V om are trainable matrices. Then, the global templates Gg in the attention are computed to yield the output Y:

Gg = softmax( K)V, Y = softmax( Q)Gg. (8) By passing through multiple HOI transformer blocks, the human motion representation is progressively refined to produce the optimized motion feature Xht−1. Similarly, the object motion Xot undergoes a parallel process, resulting in the optimized object motion feature Xot−1.

Human-Object Interaction Diffusion Training Objective To supervise the human-object interac-tion motion generation process, we employ a comprehensive set of loss functions to ensure the plausibility of the generated motion. First, we apply an L2 loss Ll2, similar to Equation 6, to encourage accurate reconstruction of HOI motion. The HOI motion representation consists of both human and object motion components. Notably, the human motion representation includes global joint positions, joint rotations, and global translation, all of which are effectively constrained by the L2 loss to ensure high-quality hu-man motion. More importantly, the object motion representation is enriched with additional features such as relative rotation, global translation, angular velocity, translational velocity, the global positions of keypoints, and contact information. Consequently, even this simple L2 loss implicitly supervises these diverse and informative object features. Second, we introduce a velocity-level constraint Lvel by com-puting the velocities of the human body and both hands, ensuring that the generated motion exhibits realistic dynamics. Third, to maintain reasonable spatial relationships among objects, we incorporate a distance-based loss Ldis that penalizes physically implausible inter-object distances. Fourth, to prevent unnatural penetration between the human and the object, we apply an interaction loss Linter , which computes the distance between the human’s left/right hands and the object’s centroid, thereby guiding appropriate proximity in human-object interactions. In summary, we formulate the final optimization objective at this stage as: 

Lhoi = λl2Ll2 + λvel Lvel + λdis Ldis + λinter Linter (9) Sci China Inf Sci 9

Dataset Methods Publication Motion Quality Evaluation Diversity Evaluation R-TOP 3 ↑ FID ↓ MM Dist ↓ Diversity ↑

FullBodyManipulation (1 Object) OMOMO [29] TOG 2023 0.773 ±.009 1.276 ±.016 2.468 ±.034 9.719 ±.087 

MotionDiffuse [59] TPAMI 2024 0.830 ±.003 0.892 ±.019 2.220 ±.021 10 .02 ±.066 

CHOIS [28] ECCV 2024 0.791 ±.005 0.823 ±.012 2.177 ±.011 9.998 ±.037 

HIMO-Gen [34] ECCV 2024 0.851 ±.008 0.924 ±.026 2.346 ±.035 9.877 ±.089 

Ours (MP-HOI) - 0.872 ±.005 0.703 ±.042 1.948 ±.016 10 .38 ±.059 

HIMO (2 Objects) IMoS [16] CGF 2023 0.501 ±.012 7.589 ±.112 8.740 ±.031 7.003 ±.320 

MDM [49] ICLR 2023 0.605 ±.009 6.845 ±.331 8.018 ±.050 11 .38 ±.234 

OMOMO [29] TOG 2023 0.592 ±.012 6.132 ±.271 7.921 ±.065 12 .73 ±.196 

PriorMDM [44] ICLR 2024 0.589 ±.003 7.851 ±.251 7.250 ±.006 12 .57 ±.146 

MotionDiffuse [59] TPAMI 2024 0.576 ±.009 4.364 ±.039 5.190 ±.039 10 .79 ±.106 

CHOIS [28] ECCV 2024 0.567 ±.041 3.996 ±.587 5.986 ±.693 12 .44 ±.514 

HIMO-Gen [34] ECCV 2024 0.636 ±.003 1.481 ±.042 3.649 ±.010 11 .66 ±.204 

Ours (MP-HOI) - 0.842 ±.007 1.070 ±.021 2.968 ±.029 12 .83 ±.079 

HIMO (3 Objects) IMoS [16] CGF 2023 0.466 ±.101 4.990 ±.177 7.770 ±.058 9.231 ±.113 

MDM [49] ICLR 2023 0.502 ±.013 4.571 ±.110 6.314 ±.026 8.895 ±.285 

OMOMO [29] TOG 2023 0.553 ±.037 4.561 ±.039 5.463 ±.049 9.169 ±.073 

PriorMDM [44] ICLR 2024 0.513 ±.025 4.821 ±.203 5.890 ±.023 9.340 ±.023 

MotionDiffuse [59] TPAMI 2024 0.515 ±.013 4.719 ±.059 5.673 ±.046 8.993 ±.097 

CHOIS [28] ECCV 2024 0.602 ±.007 3.653 ±.046 4.763 ±.046 9.135 ±.091 

HIMO-Gen [34] ECCV 2024 0.535 ±.018 4.771 ±.110 5.086 ±.041 8.946 ±.137 

Ours (MP-HOI) - 0.729 ±.009 1.621 ±.030 3.392 ±.019 10 .02 ±.103  

> Table 1 Comparisons to current state-of-the-art methods on the FullBodyManipulation [29] and HIMO [34] test set. “ ↑” denotes that higher is better. “ ↓” denotes that lower is better. We repeat all the evaluations 20 times and report the average with a 95% confidence interval. We report the best and the second-best results in red cells and blue cells.

where λl2, λ vel , λ dis , λ inter are hyperparameters. Since our MP-HOI is trained in an end-to-end manner, the overall training objective is defined as the sum of the losses from the preceding stages, formulated as: 

L = Lh + Lo + Lhoi .

5 Experiments 

5.1 Datasets, Metrics and Implementation Details Datasets. FullBodyManipulation [29] contains 10 hours of motion data involving human interactions with a single object , comprising a total of 4,838 HOI sequences. Each HOI sequence is accompanied by a textual description that guided the volunteers during the motion recording. A total of 17 participants were involved in the data collection process. They interacted with each object according to the given textual instructions. The dataset includes 15 commonly used objects in daily tasks, such as clothes stand, suitcase, table, trashcan, monitor, and others. 

HIMO [34] includes 9.44 hours of motion data depicting human interactions with two or three objects , comprising 3,376 HOI sequences. Each sequence is paired with a textual description. A total of 34 participants contributed to the data collection. The dataset covers 53 everyday household objects, such as plate, laptop, bottle, apple, bowl, and more. It also encompasses many interaction types, including: Put A (and B) into C, Wash A (and B) under faucet, Use A and B, Place A on B, among others. 

Metrics. To evaluate HOI motion generation, we first employ general metrics to assess the quality of the generation, such as R-TOP , FID , MM-Dist , and Diversity . R-TOP reflects the semantic consistency between generated HOIs and the given textual prompts. FID measures the similarity between the feature distributions extracted from the generated motions and the ground truth motions. MM-Dist computes the average Euclidean distance between the feature of generated motions and the text prompt feature. Diversity evaluates the dissimilarity among all generated motions across all descriptions. Furthermore, we follow [28, 29] evaluation metrics to assess the quality of human-object interactions, including Interaction Distance , Contact Percentage , Precision , Recall and F1 score . We first Sci China Inf Sci 10 

Dataset Methods Publication Human-Object Interaction Evaluation 

Cprec ↑ Crec ↑ CF 1 ↑ C% → DI →

FullBodyManipulation (1 Object) GT - - - - 0.445 ±0.000 0.501 ±0.000 

MotionDiffuse [59] TPAMI 2024 0.358 ±0.015 0.272 ±0.013 0.278 ±0.013 0.321 ±0.017 0.615 ±0.019 

CHOIS [28] ECCV 2024 0.372 ±0.009 0.305 ±0.009 0.303 ±0.008 0.318 ±0.019 0.637 ±0.019 

HIMO-Gen [34] ECCV 2024 0.364 ±0.008 0.286 ±0.008 0.289 ±0.009 0.297 ±0.011 0.613 ±0.013 

Ours (MP-HOI) - 0.396 ±0.007 0.343 ±0.006 0.342 ±0.007 0.371 ±0.005 0.589 ±0.007 

HIMO (2 Objects) GT - - - - 0.833 ±0.000 0.215 ±0.000 

MotionDiffuse [59] TPAMI 2024 0.845 ±0.013 0.764 ±0.013 0.770 ±0.012 0.747 ±0.017 0.315 ±0.017 

CHOIS [28] ECCV 2024 0.829 ±0.011 0.789 ±0.012 0.791 ±0.011 0.756 ±0.010 0.294 ±0.009 

HIMO-Gen [34] ECCV 2024 0.844 ±0.007 0.804 ±0.008 0.802 ±0.007 0.707 ±0.010 0.302 ±0.009 

Ours (MP-HOI) - 0.863 ±0.010 0.837 ±0.011 0.835 ±0.009 0.815 ±0.007 0.255 ±0.008 

HIMO (3 Objects) GT - - - - 0.843 ±0.000 0.222 ±0.000 

MotionDiffuse [59] TPAMI 2024 0.841 ±0.015 0.795 ±0.015 0.794 ±0.016 0.784 ±0.011 0.303 ±0.014 

CHOIS [28] ECCV 2024 0.850 ±0.010 0.803 ±0.011 0.804 ±0.011 0.771 ±0.015 0.332 ±0.012 

HIMO-Gen [34] ECCV 2024 0.844 ±0.010 0.772 ±0.009 0.779 ±0.010 0.758 ±0.011 0.315 ±0.014 

Ours (MP-HOI) - 0.859 ±0.009 0.824 ±0.009 0.825 ±0.008 0.815 ±0.008 0.274 ±0.007  

> Table 2 Human-object interaction evaluation on the FullBodyManipulation [29] and HIMO [34] test sets. ‘ →’ means the closer to GT the better.

compute the distance between hand positions and the object centroid point. The interaction distance DI

is defined as the difference between the distances of the generated motions and those of the ground truth motions. Additionally, a contact threshold is empirically set to determine contact labels for each frame. We then count true positives, false positives, and false negatives to compute precision Cprec , recall Crec ,and F1 score CF 1. Moreover, Contact Percentage C% reflects frame-level contact inference accuracy and is defined as the proportion of frames where contact is detected. 

Implementation Details. Regarding the multimodal priors, we utilize GPT-4o [1] to process text data and Flux [26] to generate image data. The hyperparameter Na is set to 2 on the FullBodyManipula-tion [29] dataset and 3 on the HIMO [34] dataset. Nb is set to 1 on the FullBodyManipulation (1 object), 2 on HIMO (2 objects), and 3 on HIMO (3 objects). Regarding the motion diffusion model, we employ a 4-layer transformer in human motion diffusion model, a 4-layer transformer in object motion diffusion model and a 8-layer transformer in HOI motion diffusion model. As for the MoE models, the number of experts is set to 16, with top-k set to 2. The dimension R0 is set to 256. As for the text encoder, a frozen text encoder from CLIP ViT-B/32 is utilized, complemented by two additional transformer en-coder layers. Regarding some hyperparameters, Da is set to 512, Dp is set to 256, λb is set to 10, λl2 is set to 1, λvel is set to 2, λdis is set to 0.3, λinter is set to 0.01, the guidance scale is set to 2.0. In terms of the diffusion model, the variances βt are predefined to linearly spread from 0.0001 to 0.02, and the total number of noising steps is set at T = 1000. We use the Adam optimizer to train the model with an initial learning rate of 0.0001, gradually decreasing to 0.00001 through a cosine learning rate scheduler. The training process is conducted on 4 NVIDIA GeForce RTX 3090, with a batch size of 16 on a single GPU. 

5.2 Evaluation of Human-Object Interaction Generation General Evaluation The results in Table 1 compare MP-HOI against state-of-the-art (SOTA) meth-ods including IMoS [16], MDM [49], OMOMO [29], PriorMDM [44], MotionDiffuse [59], CHOIS [28], and HIMO-Gen [34] on the FullBodyManipulation [29] and HIMO [34] datasets. In terms of motion quality evaluation, compared to other methods, MP-HOI achieves significantly higher scores in R-TOP3, FID, and MM-Dist. These results highlight our method’s proficiency in generating high-quality HOI motion sequences that seamlessly align with the textual prompts. Notably, compared with the HIMO-Gen [34], our method reduces the FID by 23.92% on the FullBodyManipulation single-object interaction dataset, by 27.75% on HIMO 2-object dataset, and by an impressive 66.02% on the 3-object dataset. We further analyze the underlying reasons. Specifically, MotionDiffuse [59] and MDM [49] rely primarily on single-modality inputs and do not incorporate object-related information. Without explicit modeling of object conditions, their generated motions often lack accurate and physically consistent human-object interac-tions. In contrast, IMoS [16], CHOIS [28], and HIMO-Gen [34] consider object conditions, but they fail to Sci China Inf Sci 11 Interaction Quality 

Interaction Quality 

Interaction Quality 

Semantic Aligment 

Semantic Aligment 

Semantic Aligment 

Figure 6 User study results. The color bars indicate the percentage distribution of scores for each evaluation criterion.                                                                                                                         

> Methods Motion Quality Evaluation Diversity Evaluation Human-Object Interaction Evaluation R-TOP 3 ↑FID ↓MM Dist ↓Diversity ↑Cprec ↑Crec ↑CF1↑C%→DI→
> w/o Multimodal Priors (1D, 2D, 3D) 0.761 ±.010 1.190 ±.022 2.321 ±.009 9.399 ±.019 0.369 ±.009 0.300 ±.008 0.304 ±.009 0.319 ±.031 0.602 ±.027
> w/o Multimodal Priors (1D) 0.784 ±.015 0.995 ±.019 2.150 ±.017 9.431 ±.045 0.386 ±.024 0.319 ±.026 0.321 ±.024 0.351 ±.035 0.582 ±.010
> w/o Multimodal Priors (2D) 0.772 ±.009 1.054 ±.011 2.207 ±.026 9.543 ±.027 0.381 ±.019 0.310 ±.020 0.309 ±.022 0.334 ±.025 0.597 ±.034
> w/o Multimodal Priors (3D) 0.769 ±.011 1.086 ±.026 2.226 ±.010 9.561 ±.029 0.373 ±.013 0.303 ±.012 0.301 ±.012 0.323 ±.019 0.599 ±.021
> GPT-4o + Flux 0.783 ±.008 1.516 ±.030 2.442 ±.019 9.576 ±.059 0.345 ±.018 0.312 ±.009 0.316 ±.010 0.311 ±.022 0.553 ±.044
> GPT-3.5 + Stable Diffusion 0.762 ±.010 1.811 ±.022 2.679 ±.046 9.421 ±.089 0.312 ±.025 0.298 ±.012 0.301 ±.016 0.307 ±.031 0.539 ±.024
> w/o Enhanced Object Representation 0.759 ±.005 1.346 ±.011 2.475 ±.020 9.416 ±.069 0.359 ±.007 0.289 ±.008 0.290 ±.008 0.315 ±.022 0.615 ±.017
> w/o Cascaded Diffusion Framework 0.755 ±.005 1.368 ±.016 2.599 ±.030 9.711 ±.105 0.352 ±.012 0.310 ±.012 0.315 ±.012 0.309 ±.017 0.586 ±.030
> w/o Modality-aware MoE Models 0.836 ±.007 0.931 ±.020 2.080 ±.041 9.404 ±.085 0.379 ±.006 0.327 ±.007 0.329 ±.008 0.330 ±.009 0.597 ±.016
> w/o HOI Supervision Loss 0.841 ±.009 0.890 ±.019 2.174 ±.009 9.835 ±.052 0.364 ±.009 0.317 ±.009 0.319 ±.008 0.325 ±.011 0.569 ±.010
> Ours (MP-HOI) 0.872 ±.005 0.703 ±.042 1.948 ±.016 10 .38 ±.059 0.396 ±.007 0.343 ±.006 0.342 ±.007 0.371 ±.005 0.589 ±.007

Table 3 Ablation study results on the FullBodyManipulation [29] test set. 

accurately model interactions. Because these methods lack prior knowledge about human-object interac-tions, they cannot achieve a fine-grained understanding of interaction concepts. Furthermore, their object motion representations rely on a direct 9-dimensional encoding, which makes it difficult for the models to fully interpret the embedded information, resulting in lower motion quality. This limitation restricts their ability to model complex human-object interactions, which explains the significant performance gap compared to MP-HOI. 

Human-Object Interaction Evaluation We perform a Human-Object Interaction Evaluation to assess the interaction performance based on five HOI interaction metrics. As shown in Table 2, compared to SOTA methods, MP-HOI achieves significant improvements in Precision Cprec , Recall Crec , and F1 score CF 1. These results reveal that our method has better hand-object contact accuracy and enhanced physical reasonableness. For example, compared with CHOIS [28] in the 2-object setting of the HIMO dataset, Cprec increases by 2.25%, Crec increases by 4.10%, and CF 1 increases by 4.12%. In addition, our method also performs well on the Contact Percentage ( C%) metric. For example, it improves by 15.27% compared to HIMO-Gen [34], indicating that the percentage of contact frames in the MP-HOI generated human-object interaction motions is closest to that of the ground truth motions. Finally, regarding the interaction distance ( DI ) metric, our method achieves a 19.04% reduction compared to MotionDiffuse [59]. Therefore, the human-object distances in our generated motions are the most reasonable, effectively reducing interpenetration and other unrealistic artifacts. 

User Study Evaluation We conduct a user study, in which we compare our method with HIMO-Gen [34] and MotionDiffuse [59]. This user study engaged 30 participants to evaluate 10 motion sequences generated by each method. The designed questionnaire consisted of two questions: (1) “Which method generates motion that best aligns with the textual prompt?” and (2) “Which method best captures the fine-grained details of human-object interactions?” Participants rated the methods on a 1-to-3 scale (in-dicating bad, good, and best). As depicted in Figure 6, MP-HOI significantly outperforms the competing methods in both semantic alignment and interaction quality. These results demonstrate that MP-HOI not only produces motions that closely follow textual prompts but also excels in modeling human-object interactions, validating the effectiveness and superiority of our approach. Sci China Inf Sci 12 Text: A person picks up a clothesstand with both hands , walks to his left side , and then places the clothesstand on the ground . (1 Object)              

> Ours HIMO-Gen MotionDiffuse
> Text: The person sits, holds a camera ,adjusts it, and takes a shot . Then, he places the camera on the phoneholder .(2 Objects)
> Text: A person pours beer into a goblet held in his right hand ,swirls the goblet with his left hand , and then pours it into a mug held in his left hand .(3 Objects)
> Figure 7 Visual results compared with existing methods . The arrow represents the time axes. The green box zooms in on the detailed interactions demonstrated by our approach. The red boxes highlight the errors in other methods.

Qualitative Analysis Figure 7 qualitatively compares PerMoGen against HIMO-Gen [34] and Motion-Diffuse [59]. In the single-object scenario, HIMO-Gen [34] exhibits significant human-object penetration and generates unrealistic object motion. Similarly, MotionDiffuse [59] fails to accurately follow the text prompts, often producing motion in the opposite direction of what is described. In the two-object sce-nario, HIMO-Gen [34] generates low-quality motion where the camera is not properly held in the human’s hand as instructed, while MotionDiffuse [59] fails to place the camera back onto the phoneholder as re-quired by the text. In the three-object scenario, both HIMO-Gen [34] and MotionDiffuse [59] demonstrate weak and imprecise interactions, struggling to engage with each object in a smooth and meaningful way. In contrast, MP-HOI consistently produces motion that closely aligns with the textual prompts across all scenarios. It demonstrates fine-grained modeling of human-object interactions, highlighting the supe-riority of our approach. 

Quantitative Ablation Study We conduct a comprehensive set of ablation studies to evaluate the contribution of each key component in MP-HOI, as summarized in Table 3. First , we examine the impact of multimodal priors. Removing the multimodal priors leads to a noticeable decline of 69.27% in FID and 9.45% in motion diversity. This result highlights the importance of incorporating multimodal cues, as they not only guide the generation of high-quality interactive motions but also significantly enhance motion diversity. We further investigated the impact of each modality on the final performance and found that different modalities affect motion quality to varying degrees. For example, under the MM-Dist metric, removing the 1D prior leads to a 10.37% drop in performance, removing the 2D prior results in a 13.29% drop, and removing the 3D prior causes a 17.27% drop. These results suggest that, since the task involves 3D motion generation, knowledge that is closer to the 3D domain more effectively guides the generation process. Additionally, we examined the effect of different large model combinations on motion generation performance. We randomly sampled 100 examples and processed them using various combinations of large models. The results show that the combination of GPT-4o and Flux leads to more significant improvements in metrics, primarily because it provides more detailed and comprehensive text analysis and image-based prompts. Second , when replacing our enhanced object encoding with the original 9-dimensional format (translation + rotation), we observe a substantial decline in HOI motion quality. Specifically, the FID increases by 91.46%, and R-TOP 3 drops by 12.95%, demonstrating that our enriched object representation is critical for generating high-quality HOI motions. Third ,eliminating the cascaded diffusion framework reduces MP-HOI to a single-step generation paradigm, similar to other baseline methods. This results in coarse and less realistic HOI motions, as reflected in the degradation of both motion quality and interaction accuracy. For instance, MM-Dist decreases by Sci China Inf Sci 13 A person puts the cabbage on the knifeboard,          

> cuts it with his right hand using a knife.
> Text: A person picks up the yogaball , walks forward, then puts it down. Text: A person picks up the plier in his right hand and hits the piggybank.
> Text: A person lifts the woodchair, moves the woodchair, and then puts it down.
> Ours w/o Enhanced Object Representation w/o Multimodal Priors w/o Cascaded Diffusion Framwork

(a) Ablation Study Visualizations 

(c) Generalization Experiments   

> A person picks up the monitor, walks
> to his right, and puts it down.
> Text: The person lifts the plasticbox, rotates it, and
> set it back down.
> A person lifts the table and
> pushes it forward.
> After removing his glasses, the person shifts
> the glassescase and stores the glasses inside it.
> Text: The person pushes the suitcase, release the
> hands, then pulls it.

(d) More Visualization Results 

(e) Failure Cases   

> Text: A person picks up the remote control, turns on the TV, uses the remote to operate the TV, and puts the remote down after finishing.
> Ours w/o Text Prior

(b) Ablation Study Visualizations of Multimodal Priors  

> w/o Image Prior w/o Action Prior

Figure 8 Additional visualization results . (a) Ablation study visualizations. (b) Ablation Study Visualizations of Multimodal Priors. (c) Generalization experiments. (d) More visualization results. (e) Failure cases. 

33.41%, and Cprec drops by 11.11%. Fourth , when the modality-aware MoE module is removed, the model struggles to effectively fuse multimodal priors with HOI motion features. Thus, it fails to fully exploit the rich information across modalities, leading to noticeable drops in both motion quality and diversity, for example, a 32.43% degradation in FID and a 9.40% reduction in diversity. Finally , removing the HOI supervision constraints results in poor human-object interaction quality. In particular, the DI

metric experiences a significant performance drop (3.39%), indicating less plausible spatial relationships and a higher frequency of interpenetration artifacts. 

Qualitative Ablation Study To thoroughly evaluate the individual contribution of each module, we employed motion visualization for an in-depth comparative analysis and examined the impact of removing key components, as shown in Figure 8 (a). When the enhanced object representation was removed, the quality of object motion significantly deteriorated, leading to sliding and floating artifacts of the chair. Without the multimodal priors, the human demonstrated poor interaction with the object, erroneously grasping unreasonable parts of the chair. In the absence of the cascaded diffusion framework, both the human and the motion quality degraded drastically, resulting in severe human-object penetration. In contrast, the complete MP-HOI model successfully performed the motion dictated by the text prompt and exhibited fine-grained human-object interaction, thereby validating the effectiveness of our approach. Moreover, we provide ablation and visualization experiments for each modality, as shown in Figure 8 (b). From these results, we observe that when the textual modality is removed, the generated motions may Sci China Inf Sci 14 

fail to fully reflect the multiple steps described in the input text. When the visual modality is removed, the quality of hand-object contact information is weakened. When the motion modality is removed, the generated motions tend to have lower overall motion quality. These observations demonstrate the specific contribution of each modality in supporting fine-grained HOI generation. 

Generalization To evaluate the generalization ability of MP-HOI to unseen objects, we qualitatively assess the effectiveness of our method. We feed their geometries as input conditions into our model and generate corresponding human-object motions, as illustrated in Figure 8 (c). In the first example, the human correctly grasps the end of the pliers and uses the tool appropriately. In the second example, the human interacts with a reasonable region of the yoga ball and successfully completes a sequence of actions, including picking it up, walking, and putting it down. These examples demonstrate that MP-HOI can still generate high-quality human-object interaction motions even when interacting with previously unseen object geometries, such as a plier or a yoga ball, which are not present in the dataset. This highlights the effectiveness of our method and its strong generalization capability to unseen objects. 

Additional Visualization Results Figure 8 (d) presents four additional visualizations showcasing human-object interactions. From the first and second examples on the left side, it is evident that MP-HOI can stably grasp appropriate parts of the monitor and the edge of the table, thereby accurately performing the actions described in the text. In the third example, MP-HOI effectively models the interaction between the human and two objects, successfully completing the task of picking up the glasses and placing them back into the glasses case. Even in the fourth example, which involves interaction with three objects, the human is able to reasonably manage the relationships among multiple objects and sequentially carry out the instructions from the text, demonstrating strong multi-object interaction modeling capabilities. Further comparisons and visualization examples are provided in the supplementary video. 

Failure Cases While MP-HOI effectively generates human-object interaction motion based on textual prompts, it encounters two failure cases shown in Figure 8 (e). The primary failures occur on the FullBodyManipulation [29] dataset, which does not provide hand pose parameters. As a result, the generated motions on this dataset sometimes cannot accurately model finger positions, leading to potential hand-object penetrations. Additionally, during close-range interactions with objects, such as pushing a suitcase, the lack of detailed hand parameters may result in overly large contact distances between the hand and the object. 

6 Limitation and Future Work 

Here, we discuss the limitations of MP-HOI and suggest directions for future work. First, although the generated motion sequences generally align well with textual prompts, some artifacts, such as foot sliding, may occasionally occur. These issues could be alleviated in the future by incorporating additional physical constraints and enhancing physical simulations. Second, our current framework is limited to interactions with rigid objects. However, in the real world, humans often interact with deformable or fluid-like substances, such as gases or liquids. Exploring human interactions with such dynamic materials presents an interesting and promising direction for future research. 

7 Conclusion 

In this paper, we address the challenging task of text-driven human-object interaction generation by introducing MP-HOI, a novel framework built upon four key insights: (1) We leverage multimodal priors, including text, images, and pose/object information, to effectively guide the HOI generation process; (2) We enhance object representations by incorporating geometric keypoints, contact features, and dy-namic properties, enabling more stable and informative representations; (3) We propose a modality-aware Mixture-of-Experts model to facilitate feature fusion of multimodal data; (4) We design a cascaded diffu-sion framework that progressively refines human-object interaction features under dedicated supervision. We conduct extensive quantitative and qualitative experiments, demonstrating that MP-HOI significantly Sci China Inf Sci 15 

outperforms existing methods by generating motions that are not only well-aligned with textual prompts but also capable of modeling fine-grained human-object interactions. 

Acknowledgements This work was supported by the Academic Excellence Foundation of BUAA for PhD Students, the National Natural Science Foundation of China (Project Number: 62272019), the China Postdoctoral Science Foundation (Grant Number: 2025M774236), and the Postdoctoral Fellowship Program of CPSF (Grant Number: GZC20242159). 

References 

1 Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 2 Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In 2019 International Conference on 3D Vision (3DV) , pages 719–728. IEEE, 2019. 3 Nikos Athanasiou, Mathis Petrovich, Michael J Black, and G¨ ul Varol. Sinc: Spatial composition of 3d human motions for simultaneous action generation. arXiv preprint arXiv:2304.10417 , 2023. 4 James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023. 5 Junuk Cha, Jihyeon Kim, Jae Shin Yoon, and Seungryul Baek. Text2hoi: Text-guided 3d motion generation for hand-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1577–1585, 2024. 6 Tianlong Chen, Xuxi Chen, Xianzhi Du, Abdullah Rashwan, Fan Yang, Huizhong Chen, Zhangyang Wang, and Yeqing Li. Adamv-moe: Adaptive multi-task vision mixture-of-experts. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 17346–17357, 2023. 7 Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,pages 18000–18010, 2023. 8 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. 9 Christian Diller and Angela Dai. Cg-hoi: Contact-guided 3d human-object interaction generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 19888–19901, 2024. 10 Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning , pages 5547–5569. PMLR, 2022. 11 David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. 

arXiv preprint arXiv:1312.4314 , 2013. 12 Siyuan Fan, Wenke Huang, Xiantao Cai, and Bo Du. 3d human interaction generation: A survey. arXiv preprint arXiv:2503.13120 , 2025. 13 Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, Zhangyang Wang, et al. M3vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design. Advances in Neural Information Processing Systems , 35:28441–28457, 2022. 14 William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research , 23(120):1–39, 2022. 15 Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. In Proceedings of the IEEE/CVF international conference on computer vision , pages 1396–1406, 2021. 16 Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos: Intent-driven full-body motion synthesis for human-object interactions. In Computer Graphics Forum , volume 42, pages 1–12. Wiley Online Library, 2023. 17 Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. arXiv preprint arXiv:2312.00063 , 2023. 18 Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5152–5161, 2022. 19 Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV , pages 580–597. Springer, 2022. 20 Shihui Guo, Richard Southern, Jian Chang, David Greer, and Jian Jun Zhang. Adaptive motion synthesis for virtual characters: a survey. The Visual Computer , 31:497–512, 2015. 21 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems , 33:6840–6851, 2020. 22 Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation , 3(1):79–87, 1991. 23 Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. 

Advances in Neural Information Processing Systems , 36:20067–20079, 2023. 24 Sai Shashank Kalakonda, Shubh Maheshwari, and Ravi Kiran Sarvadevabhatla. Action-gpt: Leveraging large-scale language models for improved and generalized action generation. In 2023 IEEE International Conference on Multimedia and Expo (ICME) , pages 31–36. IEEE, 2023. 25 Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis & editing. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 8255–8263, 2023. 26 Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 27 Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020. 28 Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and C Karen Liu. Controllable human-object interaction synthesis. In European Conference on Computer Vision , pages 54–72. Springer, 2024. 29 Jiaman Li, Jiajun Wu, and C Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG) , 42(6):1–11, 2023. 30 Mu Li, Yin Wang, Zhiying Leng, Jiapeng Liu, Frederick WB Li, and Xiaohui Liang. Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction. arXiv preprint arXiv:2510.08260 , 2025. 31 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 , 2024. Sci China Inf Sci 16 32 Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177 , 2024. 33 Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and Gr´ egory Rogez. Posegpt: Quantization-based 3d human motion generation and forecasting. In European Conference on Computer Vision , pages 417–435. Springer, 2022. 34 Xintao Lv, Liang Xu, Yichao Yan, Xin Jin, Congsheng Xu, Shuwen Wu, Yifan Liu, Lincheng Li, Mengxiao Bi, Wenjun Zeng, et al. Himo: A new benchmark for full-body human interacting with multiple objects. In European Conference on Computer Vision , pages 300–318. Springer, 2024. 35 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 10975–10985, 2019. 36 Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. Hoi-diff: Text-driven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553 , 2023. 37 Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence , volume 32, 2018. 38 Mathis Petrovich, Michael J Black, and G¨ ul Varol. Temos: Generating diverse human motions from textual descriptions. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII , pages 480–497. Springer, 2022. 39 Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, and Chen Chen. Bamm: bidirec-tional autoregressive motion model. In European Conference on Computer Vision , pages 172–190. Springer, 2025. 40 Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1546–1555, 2024. 41 Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 652–660, 2017. 42 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 43 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020. 44 Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. Human motion diffusion as a generative prior. In The Twelfth International Conference on Learning Representations , 2023. 45 Wenfeng Song, Xinyu Zhang, Shuai Li, Yang Gao, Aimin Hao, Xia Hou, Chenglizhao Chen, Ning Li, and Hong Qin. Hoian-imator: Generating text-prompt human-object animations using novel perceptive diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 811–820, 2024. 46 Kewei Sui, Anindita Ghosh, Inwoo Hwang, Jian Wang, and Chuan Guo. A survey on human interaction motion generation. 

arXiv preprint arXiv:2503.12763 , 2025. 47 Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas. Grab: A dataset of whole-body human grasping of objects. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16 , pages 581–600. Springer, 2020. 48 Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision , pages 358–374. Springer, 2022. 49 Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations , 2023. 50 Yin Wang, Zhiying Leng, Frederick WB Li, Xiaohui Liang, et al. Most: Motion diffusion model for rare text via temporal clip banzhaf interaction. IEEE Transactions on Visualization and Computer Graphics , 2025. 51 Yin Wang, Zhiying Leng, Frederick WB Li, Shun-Cheng Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human motion generation via diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,pages 22035–22044, 2023. 52 Yin Wang, Zhiying Leng, Haitian Liu, Frederick WB Li, Mu Li, and Xiaohui Liang. Dynamic worlds, dynamic humans: Generating virtual human-scene interaction motion in dynamic scenes. arXiv preprint arXiv:2601.19484 , 2026. 53 Yin Wang, Mu Li, Jiapeng Liu, Zhiying Leng, Frederick WB Li, Ziyao Zhang, and Xiaohui Liang. Fg-t2m++: Llms-augmented fine-grained text driven human motion generation. International Journal of Computer Vision , pages 1–17, 2025. 54 Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu, and Jingya Wang. Thor: Text to human-object interaction diffusion via relation intervention. arXiv preprint arXiv:2403.11208 , 2024. 55 Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14928– 14940, 2023. 56 Sirui Xu, Yu-Xiong Wang, Liangyan Gui, et al. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. 

Advances in Neural Information Processing Systems , 37:52858–52890, 2024. 57 Ling-An Zeng, Guohong Huang, Yi-Lin Wei, Shengbo Gu, Yu-Ming Tang, Jingke Meng, and Wei-Shi Zheng. Chainhoi: Joint-based kinematic chain modeling for human-object interaction generation. arXiv preprint arXiv:2503.13130 , 2025. 58 Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Gener-ating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14730–14740, 2023. 59 Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence ,46(6):4115–4128, 2024. 60 Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. arXiv preprint arXiv:2304.01116 , 2023. 61 Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. In European Conference on Computer Vision , pages 397–421. Springer, 2025. 62 Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. Finemogen: Fine-grained spatio-temporal motion generation and editing. Advances in Neural Information Processing Systems , 36:13981–13992, 2023. 63 Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo P´ erez-Pellitero, and Gerard Pons-Moll. Force: Dataset and method for intuitive physics guided human-object interaction. CoRR , 2024. 64 Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 7368–7376, 2024. 65 Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multi-Sci China Inf Sci 17 perspective attention mechanism. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 509–519, 2023. 66 Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5745–5753, 2019. 67 Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 46(4):2430–2449, 2023. 68 Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906 , 2022.
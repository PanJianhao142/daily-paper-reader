Title: Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning

URL Source: https://arxiv.org/pdf/2602.10420v1

Published Time: Thu, 12 Feb 2026 01:27:07 GMT

Number of Pages: 12

Markdown Content:
> JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 1

# Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning 

Jiadong Hong, Lei Liu, Xinyu Bian, Wenjie Wang, Zhaoyang Zhang 

Abstract —Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes high-lighting the effectiveness of signal-space prediction ( x-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative model-ing of discrete data. While x-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ( v-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction–loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ( x-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary—and related discrete—domains, positioning signal-space alignment as a key principle for robust diffusion learning. 

I. I NTRODUCTION 

Flow matching (FM) [ 1 ], [ 2 ] and diffusion models [ 3], [ 4]provide a unified framework for transforming simple noise distributions into complex data manifolds through continuous probability paths. While theoretically applicable to arbitrary distributions, their most prominent empirical successes have relied on Gaussianization pre-processing, where linear inter-polation facilitates smooth transport trajectories. Extending these continuous frameworks to discrete or binary data remains an open challenge. Existing approaches largely fall into two categories: those that explicitly redesign the forward process for discrete state spaces via transition kernels [ 5 ], [ 6], [ 7], and those that treat binary data as ”analog bits” [ 8], embedding them into continuous Euclidean space. While the latter pre-serves architectural simplicity by leveraging standard Gaussian interpolation, its optimal objective design and numerical sta-bility remain poorly understood. Recently, the ”back-to-basics” philosophy exemplified by the Just Image Transformer (JiT) [ 9]demonstrated that for continuous signals, a remarkably simple design—signal-space prediction ( x-prediction)—achieves state-of-the-art performance. A critical question naturally arises: 

Does the inherent advantage of x-prediction still exist under binary distributions, and is the paradigm fundamentally robust? 

In this work, we address these questions by systematically disentangling prediction parameterization from loss objectives. Our investigation yields three primary insights. First, we provide a formal validation that x-prediction is indeed a superior paradigm for binary manifolds. We show that directly parameterizing the model in the signal space offers improved generative quality and path consistency compared to traditional velocity prediction for discrete-latent data. Second, we identify a foundational robustness issue: pairing 

x-prediction with the prevailing velocity-matching loss ( v-loss) induces a prediction–loss space mismatch. We rigorously demonstrate that this mismatch leads to a structural singularity where gradient variance exhibits a third-order divergence as the flow approaches the target. We reveal that this pathology is a general characteristic of the x-prediction paradigm, and its stability in current literature often relies on boundary-avoiding heuristics, such as the Logit-Normal sampling strategies used in JiT [9] and Stable Diffusion 3 [10]. Third, we establish the principles for robust and topology-aware learning on binary domains. We propose prediction-loss space alignment—shifting the objective back to the signal space—as the structural solution that ensures stability indepen-dent of the sampling distribution. Furthermore, we show that once alignment is secured, the optimal loss design is dictated by the signal topology: geometric regression (Mean Squared Error, MSE) is uniquely suited for recovering spatially correlated structures like binary images, whereas probabilistic objectives (e.g., Binary Cross-Entropy, BCE) provide superior inductive biases for distinguishing independent symbolic states in tasks such as Multiple-Input-Multiple-Output (MIMO) detection. Together, our results provide a theoretical refinement of the 

x-prediction framework, repositioning signal-space alignment as a prerequisite for stable and robust flow matching. Our contributions can be summarized as follows:  

> •

Validation of x-Prediction for Binary Data: We provide an empirical validation that signal-space prediction ( x-prediction) is a superior paradigm for binary manifolds. We show that directly parameterizing the model in signal space offers improved generative quality and path consistency compared to traditional velocity prediction for discrete-latent data.  

> •

Diagnosis of Mismatch Singularity: We identify a foundational robustness issue: pairing x-prediction with the prevailing velocity-matching loss ( v-loss) induces a 

prediction-loss space mismatch . We rigorously demon-strate that this mismatch leads to a structural singularity 

in the gradient variance. This pathology is a general characteristic of the x-prediction paradigm, and we reveal that its stability in current literature often relies on boundary-avoiding heuristics, such as Logit-Normal sampling or timestep clipping.   

> arXiv:2602.10420v1 [cs.LG] 11 Feb 2026 JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 2
> •

Principles of Aligned and Topology-Aware Learning: 

We establish the principles for robust learning on binary domains. We propose prediction-Loss space alignment —shifting the objective back to the signal space—as the structural solution that ensures stability independent of the sampling distribution. Furthermore, we show that once alignment is secured, the optimal loss design is dictated by the signal topology : probabilistic objectives (e.g., BCE) are suited for independent symbolic recovery, whereas geometric regression (MSE) more effectively preserves spatial correlations. II. R ELATED WORKS 

A. Continuous Diffusion and Flow Matching 

Diffusion models [ 3 ], [ 4] and Flow Matching (FM) [ 1], [ 2] have unified generative modeling through continuous probability paths. Recently, the Just Image Transformer (JiT) 

[9] simplified this landscape by demonstrating that, for contin-uous signals, pairing signal prediction (ˆxθ ) with a velocity-matching loss (Lvel ) yields state-of-the-art performance. However, our work identifies a critical structural limit to this specific x-prediction paradigm. We demonstrate that while this combination is effective for continuous manifolds, the velocity-based loss becomes pathologically singular when applied to binary priors under x-prediction. We note that recent high-performance models like Stable Diffusion 3 [10 ] and JiT [9]both rely on a Logit-Normal sampling trick to bias training towards intermediate timesteps ( t ≈ 0.5). While these works use such samplers to improve empirical results or hide boundary instabilities, our work provides a theoretical refinement of the x-prediction framework. We prove that by implementing 

prediction-loss space alignment —shifting the loss calculation back to the signal space—we can analytically cancel the singularity. This refinement allows x-prediction models to achieve superior stability and sampling quality on binary domains without requiring specialized sampling schedules, providing a more principled foundation for robust learning. 

B. Discrete and Binary Diffusion Models 

To bridge continuous diffusion with discrete data, early research focused on categorical transition kernels [ 5 ], [ 11 ]. A simpler alternative, “Analog Bits” [ 8], treats binary data as continuous values, enabling the use of standard Gaussian paths. Within this “Analog” framework, the choice of loss remains a point of contention. Bit Diffusion [ 8] utilizes MSE for binary distributions, while CDCD [ 12 ] advocates for Cross-Entropy to ensure stable gradients in categorical settings like text. Our work unifies these diverse strategies by identifying 

signal topology as the key determinant for loss selection. We show that once Prediction-Loss Alignment is secured, the choice between BCE and MSE is dictated by the data’s internal correlation structure. We demonstrate that x-space prediction is inherently superior for binary settings because it allows for direct probabilistic interpretation (BCE), and our alignment framework ensures that this process remains robust throughout the entire training trajectory. 

Fig. 1: Schematic of Conditional Flow Matching. The frame-work unifies generative and denoising tasks via a continuous probability path. Starting from pure noise e at t = 0 , the process recovers a signal xgt ∈ Px by integrating a velocity field learned by NN θ . The observation y, derived from a semantic mapping F, acts as a condition that shapes the vector field, guiding the trajectory from the isotropic Gaussian prior to the structured posterior distribution. III. P ROBLEM SETUP AND BINARY FLOW MATCHING 

We formally define the problem of recovering signals through the lens of Conditional Flow Matching (CFM). This framework unifies classical inverse problems and modern generative modeling by constructing a time-dependent probability path that transports a simple source distribution to a complex target conditional distribution. 

A. Gaussian Interpolation and Bayesian Unification 

We adopt the linear probability path between source noise and target signal [ 9]. The forward process defines the state 

zt at t ∈ [0 , 1] as: 

zt = txgt + (1 − t)e, e ∼ N (0 , I). (1) The corresponding conditional vector field is ut(zt|xgt ) = ˙ zt =

xgt −e. Flow Matching learns a parameterized field vθ (zt, t, y)

to transport noise π(z) to the posterior p(x|y) ∝ p(y|x)p(x),unifying two tasks:  

> •

Generative Modeling: y is a class label or null; the model hallucinates structure by learning the marginal prior p(x). 

> •

Inverse Problems (Denoising): y = F(x) + η is a measurement; the likelihood p(y|x) provides a “seman-tic anchor” while p(x) enforces structural priors (e.g., binarity). 

B. Standardized Manifolds and Analog Binary Priors 

A foundational consensus in raw-signal generative model-ing—distinct from latent-space diffusion—is the requirement of data standardization . For continuous signals, it is standard practice to normalize the data manifold to the bipolar range 

[−1, 1] , ensuring the signal’s marginal statistics (zero mean, JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 3

unit variance) are aligned with the isotropic Gaussian noise prior N (0 , I). Empirically, such standardized manifolds are treated as “approximately Gaussian” in high-dimensional space, which facilitates smooth transport trajectories. Following the “Analog Bits” paradigm [ 8], we extend this normalization logic to discrete domains by treating bits as continuous variables in RN and mapping the raw binary information {0, 1}N to the same range {− 1, 1}N . In the context of signal processing, this representation corresponds to Binary Phase Shift Keying (BPSK) modulation. This unified standardization enables the forward interpolation process defined in Eq. (1) to represent a time-varying Additive White Gaussian Noise (AWGN) channel for both dense and discrete data. At any instance t ∈ (0 , 1) , the instantaneous Signal-to-Noise Ratio (SNR) is γ(t) = t2/(1 − t)2. As t → 1,the target distribution Px for binary data converges to a Dirac comb supported strictly on the discrete vertices {− 1, 1}N ,while standardized continuous signals reside on correlated dense manifolds. 

C. Inference via ODE Integration 

The generative process is formulated as an Initial Value Problem (IVP). The estimated signal ˆx at t = 1 is obtained by integrating the learned velocity field vθ from a generalized start time t0 ∈ [0 , 1) :

ˆx = zt0 +

Z 1

> t0

vθ (zt, t, y) dt. (2) The task type determines the initialization zt0 and the lower bound t0: 

> •

Generative Tasks: Start from pure noise ( t0 = 0 , z0 =

e ∼ N (0 , I)).  

> •

Denoising Tasks: Start from an intermediate “Noised Signal Prior” ( t0 > 0, zt0 = t0xprior + (1 − t0)e), where integration acts as a partial refinement to recover the clean signal. In practice, Eq. (2) is solved numerically (e.g., Euler’s Method). During the inference phase, the process of numerically solving the ODE manifests as the model gradually removing noise from the noisy observed prior. 

D. Parameterization and the Prediction-Loss Mismatch 

While standard CFM targets velocity v, the Just Image Trans-former (JiT) [ 9] advocates for a signal-prediction paradigm (ˆxθ ). In this setting, the velocity field is derived algebraically as vθ = (ˆ xθ − zt)/(1 − t). Substituting this into the standard velocity matching objective yields a prediction-loss space mismatch :

Lvel (θ) = Et, x,e

 1(1 − t)2 ∥ˆxθ (zt, t ) − x∥2



. (3) Eq. (3) introduces a time-dependent singular weighting 

λ(t) = (1 − t)−2. In current literature, the potential instability as t → 1 is typically circumvented via non-uniform time sampling (e.g., the Logit-Normal schedule used in SD3 [ 10 ]and JiT [ 9 ]), which suppresses the density of samples near the boundaries. However, we argue that this empirical workaround masks a fundamental structural pathology. In the following section, we rigorously prove that this mismatch induces a divergent gradient variance, and we propose prediction-loss space alignment as a more principled, sampler-agnostic solution for robust learning. IV. A NALYSIS 

In this section, we analyze the optimization dynamics of pairing x-predictions with v-losses. This coupling introduces a structural singularity that affects both continuous and binary domains, and we rigorously analyze why this mismatch can become exceptionally severe. We also analyze and explain why the logit-normal t-sampling used in JiT [ 9] leads to stable convergence during training. Furthermore, we analyze the optimization under prediction-loss space alignment, demon-strating that alignment successfully eliminates the effects of the singularity, resulting in a consistently robust optimization process. Finally, we analyze the data topological principles represented by different loss functions in the case of binary data, proposing that different loss functions should be used for binary data with different topological characteristics under the principle of flow-matching prediction-loss space alignment. 

A. Preliminaries a) Stability Criterion.: A central object in our analysis is the second moment of the stochastic gradient, E[∥gt(θ)∥2],aggregated over the time horizon. This quantity governs the stability of stochastic optimization: while the first-order gradient determines the descent direction, the magnitude of its second moment controls gradient noise, step-size sensitivity, and the feasibility of maintaining a globally stable learning rate. In particular, the divergence of the integrated gradient variance I = R 10 E[∥gt(θ)∥2] dt implies that no uniform step-size schedule can simultaneously accommodate all time regions, rendering the optimization process intrinsically stiff or unstable. To isolate the structural sources of such diver-gence—independent of architectural pathologies—we analyze 

I under a set of mild and standard assumptions on the network parameterization and optimization landscape. 

b) Formal Assumptions: To analyze the gradient variance 

I = R 10 E[∥gt(θ)∥2]dt , we introduce the following assump-tions. 

Assumption IV.1 (Finite Lipschitz Capacity) . The neural network ˆxθ (z, t ) is K-Lipschitz continuous with respect to parameters θ. The parameter Jacobian satisfies ∥Jθ ∥ ≤ K for all t ∈ [0 , 1] .

Assumption IV.2 (Jacobian Non-Degeneracy) . The network parameterization is locally non-degenerate with respect to the prediction residual δ = ˆ xθ − x. There exists a constant c > 0

such that E[∥δ⊤Jθ ∥2] ≥ c E[∥δ∥2].

Assumption IV.3 (Asymptotic Residual Scaling) . The decay rate of the minimum residual R(t) = E[∥ˆxθ − x∥2] as t → 1

is dictated by the signal’s probability support: 1) Continuous Correlated Case: For standardized signals 

x ∼ N (0 , Σ), the Bayes-optimal residual vanishes quadratically: R(t) = O((1 − t)2).JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 4

2) Binary Discrete Case: For binary data x ∈ {− 1, 1}D ,any finite-Lipschitz network ˆxθ maintains a non-vanishing residual ϵ2 > 0 as t → 1: R(t) = Ω(1) .

B. Divergence Analysis: The Mismatch Singularity 

Under the x-prediction and v-loss coupling, the stochastic gradient is given by gt(θ) = 2(1 −t)2 (ˆ xθ − x)⊤Jθ . The optimization stability is governed by the second moment of this gradient, I = R 10 E[∥gt(θ)∥2]dt .

Theorem IV.4. Consider x-prediction trained under velocity matching with uniform time sampling t ∼ U [0 , 1] . Under Assumptions IV.1–IV.3, the cumulative gradient variance I

is divergent for all standardized manifolds: 

1) For continuous correlated signals , I exhibits a first-order divergence ( O((1 − t)−1)). 

2) For binary signals , I exhibits a third-order divergence (O((1 − t)−3)). Proof Sketch. The integral I ∝ R 10 (1 − t)−4R(t)dt is determined by the asymptotic behavior of the residual R(t)

as t → 1. For continuous signals, R(t) ∼ (1 − t)2 partially regularizes the integrand to (1 − t)−2. For binary data, the persistent approximation error R(t) ∼ Ω(1) leaves the (1 −t)−4

term uncompensated, triggering explosive divergence. In both cases, the configuration is structurally ill-conditioned. (See Appendix A for full proof). 

C. Implicit Stabilization via Logit-Normal Sampling 

JiT’s empirical successes in x-prediction with v-loss param-eterization [ 9] rely on the Logit-Normal sampling schedule, originally proposed in [ 10 ] for importance sampling based on signal-to-noise ratios. A Logit-Normal density πLN (t; m, s ) is defined as: 

πLN (t) = 1

s√2π

1

t(1 − t) exp 



− (u − m)2

2s2



,u = ln t

1 − t , u ∈ R.

(4) We reveal that while its primary intent is to reweight timesteps by learning difficulty, it accidentally acts as a numerical ”safety valve” for the mismatched objective. 

Proposition IV.5. Let πLN (t) denote a Logit-Normal sampling density over t ∈ (0 , 1) with scale parameter s > 0 and mean 

m = 0 , i.e., 

t = σ(u), u ∼ N (m, s 2), σ(u) = 11 + e−u . (5) 

This distribution provides a structural suppression of the boundary t → 1, rendering the weighted variance integral 

Iπ =

Z 10

πLN (t)E[∥gt∥2]dt (6) 

convergent for both continuous and binary manifolds. In logit space u = logit (t) = ln( t/ (1 − t)) , the polynomial singularity 

(1 − t)−n is mapped to an exponential enu , while the Logit-Normal density decays as exp( −u2/(2 s2)) , dominating any finite-order divergence. The effective integrand thus scales differently depending on the signal topology: 

1) Continuous Case: For Gaussianized continuous data, the residual scales as R(u) ∼ (1 − t)2 ∼ e−2u, yielding an integrand proportional to exp( −u2/(2 s2) + 2 u).

2) Binary Case: For discrete binary data, the residual remains constant R(u) ∼ Ω(1) , yielding an integrand proportional to exp( −u2/(2 s2) + 4 u).The peak of the effective density occurs at upeak = ns 2 with 

n = 2 for continuous and n = 4 for binary data. Thus, the Logit-Normal scale s determines how far into the boundary region the sampling density remains significant. Convergence of Iπ is guaranteed for any finite s > 0, but the practical sampling budget near t ≈ 1 is reduced for binary data due to the stronger third-order singularity. Proof Sketch. Transforming the integral into logit space, 

(1 − t)−n 7 → enu , and multiplying by the Gaussian tail 

exp( −u2/(2 s2)) ensures convergence of the integral for any finite s. (See Appendix A for full proof). 

D. Stability via Prediction-Loss Space Alignment 

The mismatch singularity identified in Theorem IV.4 is the consequence of the ill-conditioned coupling between the prediction target and the loss space. We propose prediction-loss space alignment as the structural solution. 

Proposition IV.6 (Uniform Stability of Aligned Objectives) .

Consider an aligned training configuration where the objective is defined in the network’s prediction space. Under Assumption IV.1, the stochastic gradient gt is uniformly bounded, ensuring sampler-agnostic stability: 

1) Continuous Manifolds: Both velocity alignment ( v-pred + v-loss) and signal alignment ( x-pred + MSE-loss) yield 

E[∥gt∥2] = O(1) for all t ∈ [0 , 1] .

2) Binary Manifolds: Signal alignment using either MSE or BCE objectives yields E[∥gt∥2] = O(1) , effectively eliminating the third-order divergence. Proof Sketch. Alignment ensures the algebraic factor (1 −

t)−2 is absent from the gradient gt. In continuous domains, this restores stability to the x-prediction paradigm popularized by JiT. In binary domains, alignment prevents the non-vanishing residual R(t) ∼ Ω(1) from triggering the mismatch singularity, allowing for robust convergence under uniform sampling. (See Appendix 0e for full proof). 

E. Loss-Induced Signal Topologies for Binary Data 

Under prediction–loss space alignment, the optimization instability caused by parameterization mismatch is eliminated. As a result, the choice of loss function no longer serves a numerical stabilizing role, but instead encodes an explicit structural assumption on the organization of binary signals. Consider binary data x ∈ {− 1, 1}D and model outputs 

ˆx ∈ RD , which are interpreted according to the chosen loss function. JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 5

Fig. 2: Toy experiments reveal that Logit-Normal sampling stabilizes training by suppressing boundary singularities. 

(a,b) Gradient norms under Logit-Normal vs. uniform sampling for binary and Gaussian data, respectively. Uniform sampling leads to severe gradient explosion under x-prediction with velocity loss, while Logit-Normal sampling yields bounded gradients. (c,d) Corresponding training loss trajectories show that Logit-Normal sampling enables stable convergence, whereas uniform sampling results in highly unstable optimization. (e) Distribution of sampled t values under Logit-Normal and uniform schedules, illustrating strong suppression of the boundary region t → 1. (f,g) Gradient norms under uniform sampling for different prediction–loss pairings, confirming that instability is specific to mismatched x-prediction with velocity loss. (h,i) Training losses under uniform sampling further highlight that aligned objectives remain stable. (j) Bit error rate (BER) as a function of t

in the binary case, compared with the Bayes-optimal MMSE estimator, showing that performance degradation concentrates near the singular boundary region. Note that all prediction-loss-aligned approaches achieve MMSE except x-prediction with v-loss. 

a) Binary Cross-Entropy (BCE).: The binary cross-entropy loss is defined as 

LBCE (x, ˆx) = −

> D

X

> i=1

h 1 + xi

2 log pi + 1 − xi

2 log(1 − pi)

i

,

(7) where pi = σ(ˆ xi). This objective corresponds to the negative log-likelihood of a factorized Bernoulli model, 

p(x | ˆx) = 

> D

Y

> i=1

p(xi | ˆxi), (8) and therefore assumes conditional independence between bits given the model output. From a structural perspective, BCE treats the binary signal as an independent symbolic stream, applying supervision locally at the level of individual bits. 

b) Mean Squared Error (MSE).: In contrast, the mean squared error loss is given by 

LMSE (x, ˆx) = ∥x − ˆx∥22. (9) This loss corresponds (up to a constant) to the negative log-likelihood of an isotropic Gaussian model over the full vector, 

p(x | ˆx) ∝ exp  −∥ x − ˆx∥22

 , (10) and therefore treats the binary signal as a single point embedded in a continuous Euclidean space. Consequently, MSE enforces global geometric consistency across all dimensions and implicitly preserves spatial or structural correlations among bits. 

c) Discussion.: Once prediction–loss alignment guaran-tees stable optimization, the loss function selection is dictated by the intrinsic topology of the binary signal. Probabilistic objectives such as BCE are naturally suited for independent symbolic recovery, whereas geometric regression losses such as MSE are more appropriate when binary data exhibit structured or correlated organization. V. T OY EXPERIMENTS : C ONTROLLED VERIFICATION OF 

SAMPLING -I NDUCED STABILITY 

A. Problem Setup 

We first conduct controlled toy experiments to verify the theo-retical predictions on gradient instability and sampling-induced stabilization. Rather than evaluating generation quality, these experiments focus on optimization dynamics , in particular the behavior of gradient norms and training losses under different parameterization–loss pairings and t-sampling strategies. We consider two synthetic data manifolds: continuous Gaussian vectors and binary {− 1, +1 }D vectors. In both cases, we train a lightweight conditional MLP to perform x-prediction under velocity-based loss, which corresponds to the mismatched configuration analyzed in Theorem IV.4. We compare uniform sampling of the time variable t with Logit-Normal sampling, which suppresses the boundary region t → 1. (See Appendix 0e for detailed settings.) 

B. Results and Discussion 

As shown in Fig. 2(a–d), uniform t-sampling leads to severe gradient explosion and unstable loss behavior under JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 6                   

> Loss v-MSE (uniform) —BCE (w) CE (uniform) x-MSE v-MSE
> Pred / Sample x-pred / uniform Logit-Normal x-pred / uniform —x-pred / uniform v-pred / uniform
> FID ↓1247.9823 5.5797 835.7452 991.7779 4.9386 8.7199

TABLE I: FID on Binary MNIST (lower is better). All results use the same training schedule and sampling procedure, and use the best checkpoint (lowest validation loss). 

Fig. 3: Binary MNIST qualitative samples and training dynamics across different objectives. Each column illustrates a specific parameterization–loss pairing: (1) Mismatched (Uniform) : x-prediction with vMSE loss, exhibiting immediate catastrophic divergence; (2) Mismatched (Logit-Normal) : while the Logit-Normal schedule masks the singularity during training (blue), the validation loss (red) exhibits extreme, orders-of-magnitude oscillations, revealing an ill-conditioned vector field; (3–4) Aligned (BCE) : stable convergence but yields thick strokes due to the independent Bernoulli assumption; (5) 

Aligned (MSE) : our proposed method, achieving the lowest FID and most stable convergence; (6) Original FM : standard 

v-prediction with vMSE loss. The sharp contrast between the instability in (2) and the monotonic convergence in (5) empirically validates that Alignment is the fundamental remedy for the structural singularity, proving that boundary-avoiding heuristics alone cannot resolve the underlying numerical fragility. 

x-prediction with velocity loss on both Gaussian and binary manifolds, while Logit-Normal sampling yields bounded gra-dients and smooth loss decay. However, Fig. 2(e) reveals that this stabilization is achieved by heavily suppressing samples near t → 1, which is precisely where the theoretical singularity occurs. Furthermore, Fig. 2(f–i) confirms that aligned prediction– loss configurations remain stable even under uniform sampling, indicating that the instability is not inherent to the data distribution or network architecture, but arises from structural mismatch. Finally, Fig. 2(j) shows that performance degradation in the binary case concentrates near the boundary region, consistent with our analysis of residual-driven singularity. These results support the conclusion that Logit-Normal sampling mitigates instability by avoiding the singular region, rather than resolving the underlying mismatch, motivating the sampler-agnostic stabilization provided by prediction–loss space alignment. VI. E XPERIMENTS 

In this section, we evaluate two representative problems: binary MNIST handwritten digit generation and multiple-input multiple-output (MIMO) signal detection. These tasks contrast the strong spatial correlations of visual data with the symbol-wise independence of communication signals. Our results reveal that prediction-loss alignment is a fundamental prerequisite for robust learning; standard heuris-tics like Logit-Normal sampling or t → 1 clipping merely mask singularities without resolving the underlying numerical fragility. Furthermore, we demonstrate that data topology dictates the optimal choice between BCE and MSE once alignment is secured. While not aiming for state-of-the-art generative benchmarks, these results establish alignment as a key principle for stable and topology-aware discrete flow matching. 

A. Binary MNIST Image Generation 

The Binary MNIST dataset serves as a benchmark for discrete distributions with strong spatial correlations [ 13 ], [ 14 ]. We evaluate our framework using a U-Net backbone [ 15 ], systematically comparing various prediction–loss pairings and sampling schedules. 

Results and Discussion. Table I and Fig. 3 demonstrate the clear superiority of the proposed alignment principle. Our aligned x-prediction with xMSE loss not only ensures monotonic convergence but also achieves the the lowest FID score among all tested configurations. This performance edge stems from the well-conditioned loss landscape, which allows the model to accurately capture fine-grained spatial structures without being hindered by numerical instability. In stark contrast, we analyze the mismatched configuration 

(x-pred with vMSE loss) under the Logit-Normal schedule. While the “best” checkpoint in this setting can yield areasonably legible FID, it consistently fails to match the fidelity of our aligned approach. Furthermore, its training is characterized by extreme validation instability , with losses fluctuating across two orders of magnitude (Fig. 3, Plot 2). This suggests that even the best results from the mismatched setting are ”fragile” and highly sensitive to early-stopping. The gap in FID underscores a key insight: our Alignment 

does not merely provide a “palliative masking” of the singu-larity; it enables the model to effectively supervise the entire probability path. By eliminating the structural singularity, our method allows for more precise learning of the deterministic transition to binary states near t → 1, resulting in the sharper JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 7

Fig. 4: Conditional flow-matching detection for MIMO systems. (a) MIMO detection is formulated as a conditional flow-matching signal generation problem, where a DiT backbone with AdaLN modulation learns a conditional vector field 

vθ (zt, t, y) to transport noise toward the posterior of transmitted signals given the observation y = Hx gt + n. (b) Training loss curves on the 8 × 8 MIMO task under different parameterization–loss combinations, where mismatched objectives exhibit severe instability. (c) Bit error rate (BER) performance on the 8 × 8 MIMO system. (d) Bit error rate (BER) performance on the 16 × 16 MIMO system. Aligned parameterization and loss pairs consistently outperform mismatched combinations, while BCE-based objectives are more competitive due to the i.i.d. binary structure of QPSK symbols after real-valued decomposition. 

strokes and higher sample fidelity reflected in the superior FID. (See Appendix 0e for full settings.) 

B. MIMO Detection 

We consider a real-valued equivalent MIMO system 

y = Hx + n, n ∼ N (0 , σ 2I) (11) where x ∈ {− 1, +1 }2N follows an i.i.d. Bernoulli prior after real-imaginary decomposition, and detection reduces to high-dimensional binary inference under noised linear mixing. Traditional detectors perform approximate Bayesian infer-ence via iterative message passing [ 16 ], [ 17 ], [ 18 ], while the Soft Graph Transformer (SGT) [ 19 ] learns such refinement with neural message passing. Motivated by the connection between iterative inference and continuous-time transport, we incorporate Adaptive Layer Normalization (AdaLN) [ 20 ] into SGT, yielding a DiT-style backbone and enabling conditional flow matching, as shown in Fig. 4(a). The model therefore learns a conditional vector field 

vθ (zt, t, y) that transports noise toward the posterior p(x | y),and detection is performed by sampling from the resulting conditional flow. 

Results and Discussion. Fig. 4(b) shows training losses on the 8 × 8 system. With learning rate 10 −3 and gradient clipping at 0.99 , mismatched training ( x-prediction with v-loss) still diverges, and evaluation uses the best checkpoint before instability. For the 16 × 16 system, reducing the learning rate to 10 −4 stabilizes training, but mismatched models remain significantly inferior in BER (Fig. 4(d)). Across both system sizes, aligned objectives consistently outperform mismatched combinations (Fig. 4(c,d)), empirically supporting our theoretical analysis on prediction–loss consis-tency. Although optimization techniques can partially stabilize training, they do not close the resulting performance gap. In contrast to binary image generation, BCE-based objectives outperform MSE in MIMO detection (Fig. 4(c,d)). Since bits are i.i.d. after real-valued conversion, symbol-wise likelihood objectives better match the data distribution, while regression losses implicitly assume continuous manifolds. This further highlights that effective flow-matching on binary domains requires alignment between model design, loss function, and data topology. (See Appendix 0e for the SGT architecture and complete experiment settings.) VII. C ONCLUSION 

We studied flow matching and diffusion models on binary data manifolds and showed that signal-space prediction ( x-prediction) remains effective when objectives are properly aligned. We identified that pairing x-prediction with velocity-matching loss induces a prediction–loss mismatch that causes gradient variance explosion near the terminal region, explaining the instability observed in practice. While heuristic sampling or timestep clipping can partially mitigate this issue, they do not resolve the underlying incompatibility. We proposed prediction-loss space alignment as a structural remedy that restores stability independently of the sampling distribution. Moreover, once alignment is enforced, the optimal loss is dictated by data topology: regression losses suit spatially correlated structures such as binary images, whereas probabilistic objectives are more appropriate for independent symbolic inference as in MIMO detection. Experiments across both tasks consistently validate these principles. Overall, our results highlight that objective design is central when extending continuous-time generative frameworks to discrete domains, and that respecting the geometric and statistical structure of binary data is essential for stable and effective learning. REFERENCES [1] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative modeling,” in The Eleventh International Conference on Learning Representations .[2] X. Liu, C. Gong et al. , “Flow straight and fast: Learning to generate and transfer data with rectified flow,” in The Eleventh International Conference on Learning Representations .[3] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”   

> Advances in neural information processing systems , vol. 33, pp. 6840– 6851, 2020. [4] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,”
> arXiv preprint arXiv:2010.02502 , 2020. JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 8

[5] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, “Structured denoising diffusion models in discrete state-spaces,” Advances in neural information processing systems , vol. 34, pp. 17 981–17 993, 2021. [6] H. Stark, B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola, “Dirichlet flow matching with applications to dna sequence design,” in Forty-first International Conference on Machine Learning .[7] A. Campbell, J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola, “Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design,” in International Conference on Machine Learning . PMLR, 2024, pp. 5453–5512. [8] T. Chen, R. ZHANG, and G. Hinton, “Analog bits: Generating discrete data using diffusion models with self-conditioning,” in The Eleventh International Conference on Learning Representations .[9] T. Li and K. He, “Back to basics: Let denoising generative models denoise,” arXiv preprint arXiv:2511.13720 , 2025. [10] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M ¨uller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al. , “Scaling rectified flow transformers for high-resolution image synthesis,” in International Conference on Machine Learning . PMLR, 2024, pp. 12 606–12 633. [11] A. Lou, C. Meng, and S. Ermon, “Discrete diffusion modeling by estimating the ratios of the data distribution,” in Proceedings of the 41st International Conference on Machine Learning , 2024, pp. 32 819– 32 848. [12] S. Dieleman, L. Sartran, A. Roshannai, N. Savinov, Y. Ganin, P. H. Richemond, A. Doucet, R. Strudel, C. Dyer, C. Durkan et al. , “Continuous diffusion for categorical data,” arXiv preprint arXiv:2211.15089 , 2022. [13] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE , vol. 86, no. 11, pp. 2278–2324, 1998. [14] R. Salakhutdinov and G. Hinton, “Learning a nonlinear embedding by preserving class neighbourhood structure,” in Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics , ser. Proceedings of Machine Learning Research, M. Meila and X. Shen, Eds., vol. 2. San Juan, Puerto Rico: PMLR, 21–24 Mar 2007, pp. 412–419. [15] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical image computing and computer-assisted intervention . Springer, 2015, pp. 234–241. [16] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algo-rithms for compressed sensing,” Proceedings of the National Academy of Sciences , vol. 106, no. 45, pp. 18 914–18 919, 2009. [17] J. Ma and L. Ping, “Orthogonal amp,” IEEE Access , vol. 5, pp. 2020– 2033, 2017. [18] L. Liu, S. Huang, and B. M. Kurkoski, “Memory amp,” IEEE Transactions on Information Theory , vol. 68, no. 12, pp. 8015–8039, 2022. [19] J. Hong, L. Liu, X. Bian, W. Wang, and Z. Zhang, “Soft graph transformer for mimo detection,” arXiv preprint arXiv:2509.12694 , 2025. [20] W. Peebles and S. Xie, “Scalable diffusion models with transformers,” in Proceedings of the IEEE/CVF international conference on computer vision , 2023, pp. 4195–4205. [21] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, “Film: Visual reasoning with a general conditioning layer,” in Proceedings of the AAAI conference on artificial intelligence , vol. 32, no. 1, 2018. 

APPENDIX 

In this section, we provide the formal derivation of the mismatch singularity across different signal manifolds. 

Theorem IV.4. Consider x-prediction trained under velocity matching with uniform time sampling t ∼ U [0 , 1] . Under Assumptions IV.1–IV.3, the cumulative gradient variance I

is divergent for all standardized manifolds: 

1) For continuous correlated signals , I exhibits a first-order divergence ( O((1 − t)−1)). 

2) For binary signals , I exhibits a third-order divergence (O((1 − t)−3)). Proof. The second moment of the stochastic gradient for the mismatched objective is: 

E[∥gt(θ)∥2] = 4(1 − t)4 Ex,e

∥(ˆ xθ − x)⊤Jθ ∥2 . (12) Applying the non-degeneracy condition (Assum. IV.2), we have: 

E[∥gt(θ)∥2] ≥ 4c

(1 − t)4 R(t). (13) The optimization stability is determined by the integral I =R 10 E[∥gt∥2]dt .

a) Continuous Correlated Case.: Let x ∼ N (0 , Σ). The Bayes-optimal residual R∗(t) is: 

R∗(t) = Tr  Σ − t2Σ(t2Σ + (1 − t)2I)−1Σ . (14) As t → 1, using the matrix expansion (I + ϵΣ−1)−1 ≈ I −

ϵΣ−1 with ϵ = (1 −t)2 

> t2

:

R∗(t) ≈ Tr 



Σ − Σ(I − (1 − t)2

t2 Σ−1)



= (1 − t)2

t2 Tr (I) = D(1 − t)2

t2 .

(15) Substituting R(t) = O((1 − t)2) into the integral: 

Icont ≥

Z 1

> t∗

4c · D(1 − t)2

(1 − t)4 dt 

= 4 cD 

Z 1

> t∗

1(1 − t)2 dt 

=

 4cD 

1 − t

1

> t∗

= ∞.

(16) The integral exhibits a first-order divergence. While divergent, this instability is often numerically circumvented in practice by specialized time-sampling heuristics that avoid the t ≈ 1

boundary. 

b) Binary Discrete Case.: For x ∈ {− 1, 1}D , the network maintains a residual R(t) ≥ ϵ2 as t → 1 (Assum. IV.3). Substituting this into the integral: 

Ibin ≥

Z 1

> t∗

4cϵ 2

(1 − t)4 dt 

=

 4cϵ 2

3(1 − t)3

1

> t∗

= ∞.

(17) The integral exhibits a third-order divergence. Unlike the continuous case, this explosive variance leads to immediate numerical overflow in the absence of specialized sampling or structural alignment. We rigorously analyze the interaction between the Logit-Normal schedule and the mismatch singularity. Let u = logit (t),then 1 − t = e−u 

> 1+ e−u

∼ e−u for large u. The gradient variance under πLN (t) transforms as follows: 

Iπ ∝

Z ∞

> u∗

e−u2/2s2

· R(u)(e−u)4 · (Jacobian factor )du 

∼

Z ∞

> u∗

R(u) exp 



4u − u2

2s2



du. 

(18) 

c) Case 1: Continuous Manifolds.: For Gaussianized continuous data, the residual R(u) ∼ (1 − t)2 ∼ e−2u. The integrand becomes exp( −u2/2s2 + 2 u). The peak of this effective density is shifted to upeak = 2 s2. For standard settings (s = 1 ), the density remains substantial near the boundary, allowing JiT [9] to achieve high quality. JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 9

d) Case 2: Binary Manifolds.: For binary data, R(u) ∼

Ω(1) . The integrand becomes exp( −u2/2s2 +4 u). The peak is shifted further to upeak = 4 s2. To prevent numerical overflow from the e4u term, the sampler must ensure that the Gaussian tail exp( −u2) decays significantly before e4u hits the floating-point limit. This creates an Effective Sampling Gap : the model spends its ”sampling budget” in regions where t is far from 1. 

e) Conclusion.: For binary flow matching, Logit-Normal sampling creates a fundamental conflict: one must choose between numerical survival (by suppressing the boundary) and binary fidelity (by sampling the boundary). Our proposed 

Prediction-Loss Alignment resolves this conflict by ensuring 

O(1) variance regardless of the sampling measure, enabling the model to learn the critical t → 1 transition without instability. We rigorously demonstrate that alignment between the model’s prediction target and the loss space ensures numerical stability by analytically canceling the singular terms. 

Proposition IV.6 (Uniform Stability of Aligned Objectives) .

Consider an aligned training configuration where the objective is defined in the network’s prediction space. Under Assumption IV.1, the stochastic gradient gt is uniformly bounded, ensuring sampler-agnostic stability: 

1) Continuous Manifolds: Both velocity alignment ( v-pred + v-loss) and signal alignment ( x-pred + MSE-loss) yield 

E[∥gt∥2] = O(1) for all t ∈ [0 , 1] .

2) Binary Manifolds: Signal alignment using either MSE or BCE objectives yields E[∥gt∥2] = O(1) , effectively eliminating the third-order divergence. Proof. We analyze the second moment of the stochastic gradient gt(θ) by considering the target manifold’s geometry. Stability is achieved if E[∥gt∥2] < ∞ for all t ∈ [0 , 1] .

A. Case 1: Continuous Manifolds (Standardized Signals) 

For standardized continuous signals x ∼ Px, the prediction and loss are aligned in either velocity or signal space. 

a) Velocity Alignment ( v-pred + v-loss).: The gradient is 

gvt = 2( vθ − (x − e)) ⊤Jvθ . In the standardized paradigm, the target velocity x − e has finite moments. Since the Jacobian 

∥Jvθ ∥ ≤ K (Assum. IV.1) and no singular factors of (1 − t)

are introduced by the objective, we have: 

E[∥gvt ∥2] ≤ 4K2E[∥vθ − (x − e)∥2] = O(1) . (19) 

b) Signal Alignment ( x-pred + MSE-loss).: The gradient is gmse t = 2(ˆ xθ −x)⊤Jxθ . For standardized signals in the bipolar range [−1, 1] D , the residual is bounded by ∥ˆxθ − x∥ ≤ 2√D.Thus: 

∥gmse t ∥ ≤ 4K√D =⇒ E[∥gmse t ∥2] = O(1) . (20) This confirms that signal-space alignment restores stability to x-prediction in continuous domains, rendering boundary-avoiding samplers optional. 

B. Case 2: Binary Manifolds (Discrete Priors) 

For binary signals x ∈ {− 1, 1}D , the model is aligned in the signal space using either geometric (MSE) or categorical (BCE) objectives. 

a) Geometric Alignment (MSE).: As in the continuous case, the residual ∥ˆxθ − x∥ is bounded by the hypercube diameter 2√D. Despite the non-vanishing residual R(t) ∼

Ω(1) at the Dirac peaks, the absence of the (1 −t)−2 weighting in the objective ensures: 

E[∥gmse t ∥2] ≤ 16 K2D = O(1) . (21) 

b) Categorical Alignment (BCE).: With logits aθ and sigmoid predictions σ(aθ ) ∈ [0 , 1] D , the BCE gradient is 

gbce t = ( σ(aθ ) − x01 )⊤∇θ aθ . The residual (σ(aθ ) − x01 ) is strictly bounded within [−1, 1] D . Thus: 

∥gbce t ∥ ≤ √D · K =⇒ E[∥gbce t ∥2] = O(1) . (22) 

c) Conclusion.: In both continuous and binary cases, alignment ensures that the gradient norm is independent of the singularity at t → 1. The cumulative variance R 10 O(1) dt is uniformly bounded, enabling robust, sampler-agnostic conver-gence. JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 10 

Fig. 5: Soft Graph Transformer Architecture TABLE II: Toy Experiment: Network Architecture and Training Hyperparameters 

Category Component Setting 

Data Data distribution i.i.d. standard Gaussian or i.i.d. BPSK Signal Data dimension D = 16 

Batch size 1000 Time Sampling Time distribution t = σ(N (−0.8, 0.82)) (Same as JiT [9]) or t ∼ Uniform (0 , 1) 

Forward Process Corruption rule zt = tx + (1 − t)ϵ, ϵ ∼ N (0 , I )

Time Embedding Type Sinusoidal embedding Embedding dimension 128 Backbone Model type Gated MLP (FiLM [21]-style) Hidden layers 2Hidden width 256 Activation SiLU Gating function Sigmoid Output Head Output dimension 8Parameterization x-prediction or v-prediction Loss Functions x-pred + MSE E∥ˆx − x∥2

x-pred + v-loss E(1 − t)−2∥ˆx − x∥2

v-prediction E∥ˆv − (x − ϵ)∥2

Optimizer Optimizer Adam Learning rate 1 × 10 −4

Training Training steps 5000 BER in Fig. 2 Inference Trajectory Uniform step length, 3 steps from t = t0 to t = 1 JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 11 

TABLE III: BMNIST flow-matching experiment configuration (Conditional UNet backbone, training, and evaluation). 

Item Setting 

Backbone: ConditionalUNet 

Input / output channels 1 / 1 Base channels ( C) 64 Conditioning time embedding + class embedding (10 classes) Time embedding MLP Linear(1 →64) → SiLU → Linear(64 →64) Label embedding Embedding(10, 64) Embedding fusion emb = emb t + emb y

Residual block norm GroupNorm with 1 group (in blocks) Residual block activation SiLU Residual block dropout 0.1 Encoder blocks ResBlock(1 →64), ResBlock(64 →128) Downsample Conv2d(128 →128, kernel=4, stride=2, pad=1) Middle block ResBlock(128 →128) Upsample ConvTranspose2d(128 →128, kernel=4, stride=2, pad=1) Decoder blocks ResBlock(256 →64), ResBlock(128 →64) Output norm GroupNorm(8 groups, 64 channels) Output head Conv2d(64 →1, kernel=3, pad=1) 

Training: Flow Matching 

Forward interpolation xt = (1 − t) ϵ + t x , ϵ ∼ N (0 , I )

Time sampling t = σ(N (−0.8, 0.82)) (Same as JiT [9]) or t ∼ Uniform (0 , 1) 

Velocity target vtarget = x − ϵ

Model prediction xpred = UNet( xt, t, y ) or xpred = UNet( xt, t, y )

Implied velocity (for x-prediction cases) vpred = xpred − xt

1 − t

Loss same as Tab. II Optimizer Adam Learning rate 1 × 10 −4

Epochs 1000 (default in train_flow )Best checkpoint saved by minimum validation MSE Sample logging every 10 epochs, 1 sample per digit (0–9) 

Sampling & Evaluation (FID) 

Sampling initialization x ∼ N (0 , I )

Sampler forward Euler integration Steps / step size 50 steps, ∆t = 1 /50 

Update rule x ← x + v ∆t, v = xpred − x

1 − t + ε

Hard thresholding disabled (commented out in code) FID feature extractor SimpleClassifier (2 conv layers + 2 FC layers) FID classifier training Adam, lr = 10 −3, 2 epochs FID feature layer activations of fc1 (128-d) FID samples default 10,000 total (balanced: 1,000 per digit) FID real features extracted from training loader JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, FEB 2026 12 

TABLE IV: Hyperparameter settings for MIMO detection experiments (DiT-style SGT backbone). 

Component Setting 

Signal and Channel Model 

Modulation QPSK Real-valued model y = Hx + n, x ∈ {− 1, +1 }2N

Noise AWGN, n ∼ N (0, σ 2I)

Prior on symbols i.i.d. Bernoulli 

Backbone: DiT-style Soft Graph Transformer (SGT) 

Model type Transformer with message-passing structure Conditioning AdaLN on diffusion timestep and observation y

Embedding dimension ( dmodel ) 128 Number of heads 8Number of layers ( L) 8Feed-forward ratio ( dff /d model ) 1Channel embedding Linear( 2N + 2 → dmodel )Prior embedding Linear( 1 → dmodel )Positional encoding Learnable positional embeddings 

Prediction and Loss 

Prediction targets x-prediction and v-prediction Velocity definition v = xpred − zt

1 − t

Aligned settings x-pred+ x-loss, v-pred+ v-loss Mismatched setting x-pred+ v-loss Loss functions MSE, BCE, velocity-matching MSE 

Training Configuration 

Optimizer AdamW (default), Adam in ablations Learning rate (8 ×8) 1 × 10 −3

Learning rate (16 ×16) 1 × 10 −4

Batch size 500 (16 ×16), 2500 (8 ×8) 

t clipping clip at 0.99 Training schedule Cosine annealing with warmup Checkpoint selection Best validation loss or pre-divergence checkpoint 

Sampling and Evaluation 

Sampling method Euler integration of learned flow Number of steps 2Update rule zt+∆ t = zt + vθ (zt, t, y) ∆ t

Decision rule Soft output; hard decision for BER evaluation Evaluation metric Bit Error Rate (BER)
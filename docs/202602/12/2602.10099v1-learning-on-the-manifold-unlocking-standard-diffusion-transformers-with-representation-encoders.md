---
title: "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders"
title_zh: 在流形上学习：利用表示编码器解锁标准扩散 Transformer
authors: "Amandeep Kumar, Vishal M. Patel"
date: 2026-02-10
pdf: "https://arxiv.org/pdf/2602.10099v1"
tags: ["keyword:FM", "keyword:MDM"]
score: 6.0
evidence: 提出黎曼流匹配以改进扩散Transformer的收敛性
tldr: 本研究探讨了标准扩散Transformer（DiT）在表示编码器特征空间中难以收敛的问题。作者指出，失败根源在于“几何干扰”，即传统的欧几里得流匹配迫使概率路径穿过超球体特征空间的低密度内部，而非沿流形表面。为此，提出了带有雅可比正则化的黎曼流匹配（RJF），通过将生成过程约束在流形测地线上并修正曲率误差，使标准DiT架构无需增加参数量即可高效收敛，显著提升了生成质量。
motivation: 标准DiT在表示编码器上无法收敛并非因为模型容量不足，而是由于欧几里得路径与超球体流形几何不匹配导致的几何干扰。
method: 提出了RJF方法，结合黎曼流匹配将生成路径约束在流形测地线上，并利用雅可比正则化修正曲率引起的误差传播。
result: 在DiT-B架构（1.31亿参数）上实现了3.37的FID，成功解决了此前方法在相同规模下无法收敛的问题。
conclusion: 证明了通过考虑流形几何特性而非单纯扩展模型宽度，可以解锁标准DiT在表示学习空间中的生成潜力。
---

## 摘要
利用表示编码器进行生成建模为高效、高保真合成提供了一条路径。然而，标准的扩散 Transformer（Diffusion Transformer）无法直接在这些表示上收敛。虽然最近的研究将其归因于容量瓶颈，并提出了计算成本高昂的扩散 Transformer 宽度缩放方案，但我们证明这种失败从根本上是几何性的。我们将“几何干扰”（Geometric Interference）确定为根本原因：标准的欧几里得流匹配（Euclidean flow matching）迫使概率路径穿过表示编码器超球面特征空间的低密度内部，而不是遵循流形表面。为了解决这个问题，我们提出了带有雅可比正则化的黎曼流匹配（RJF）。通过将生成过程约束在流形测地线上，并修正由曲率引起的误差传播，RJF 使标准的扩散 Transformer 架构无需宽度缩放即可收敛。我们的 RJF 方法使标准的 DiT-B 架构（1.31 亿参数）能够有效收敛，在先前方法无法收敛的情况下实现了 3.37 的 FID。代码：https://github.com/amandpkr/RJF

## Abstract
Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF
Title: Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning

URL Source: https://arxiv.org/pdf/2602.10285v1

Published Time: Thu, 12 Feb 2026 01:14:03 GMT

Number of Pages: 8

Markdown Content:
# Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning 

Ananya Trivedi ∗1,2 Anjian Li 1,3 Mohamed Elnoor 1,4 Yusuf Umut Ciftci 1,5 Avinash Singh 1

Jovin D’sa 1 Sangjae Bae 1 David Isele 1 Tas ¸kın Padır 2 Faizan M. Tariq ∗1

Abstract —Autonomous driving requires reasoning about in-teractions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online tra-jectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible compu-tational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/ 

I. I NTRODUCTION 

Autonomous driving in urban environments requires rea-soning over a wide range of factors. For instance, navigat-ing a crowded intersection may require anticipating whether other vehicles will yield for the ego to proceed. In such cases, the desired behavior can often be expressed as a set of interpretable rules that can be directly encoded into an online trajectory optimization framework [1]. However, as the range of driving scenarios grows, the rule set can become increasingly unwieldy and hard to generalize. Several large-scale, open-sourced datasets such as nuScenes [2] and Waymo Open Motion [3] capture diverse urban driving scenarios through rich sensor data, high-definition maps, and detailed agent trajectories. To mitigate the scalability limitations of rule-based planners, recent     

> All work was done at HRI.
> 1Honda Research Institute, San Jose, CA 95134, USA
> 2Northeastern University, Boston, MA 02115, USA
> 3Princeton University, Princeton, NJ 08544, USA
> 4University of Maryland, College Park, MD 20742, USA
> 5Stanford University, Stanford, CA 94305, USA
> ∗Corr esponding authors: trivedi.ana@northeastern.edu &
> faizan_tariq@honda-ri.com

approaches have adopted imitation learning based on these datasets [4]. This entails training a deep neural network capable of handling a wide range of motion planning tasks. Diffusion models have shown strong performance in robotic manipulation, where long-horizon planning is prioritized over fast reaction times [5]. It has also been effectively applied to controllable traffic simulation [6] and open-loop robotic trajectory optimization [7]. However, autonomous driving re-quires planners that operate at high frequencies to enable reactive decision-making. The need for potentially hundreds of denoising steps makes diffusion models impractical for real-time use [8]. Conditional flow matching has gained significant traction in the generative modeling community, with applications rang-ing from image generation to molecular design [9], [10]. It involves training a neural network to predict a continuous velocity field that transforms a known base distribution, often a Gaussian, into the data distribution under a given conditional input. During inference, this velocity field is integrated forward in time to recover the predicted output. Recent applications to motion forecasting and planning have shown that even a small number of inference steps can produce high-quality tra-jectories [4], [11]. Additionally, the number of inference steps does not need to be fixed during training. These properties make flow matching a strong and flexible candidate for motion planning tasks that demand both computational efficiency and behavioral diversity. In this paper, we propose a flow matching based motion planning framework that predicts the future motion of sur-rounding agents and plans the ego vehicle’s trajectory in response. Recent work by Hu et al. [12] showed that variance in the training loss of flow-matching models correlates with integration error during inference. We leverage this observation and train an auxiliary feedforward network to estimate this variance. The predicted variance determines the integration step size during online inference, using finer steps in inter-active urban scenarios where the model is less confident. This removes the need for manually tuning inference schedules. Since predicted trajectories may not perfectly match the expert demonstrations they are replicating, even small errors can often lead to motion that is dynamically inconsistent or uncomfortable for passengers [13]. To address this, we introduce a post-processing step posed as a convex quadratic program (QP) that improves both ride comfort and dynamic feasibility by smoothing the trajectory and enforcing physical constraints. This additional step runs in under one millisecond, thereby introducing negligible computational overhead. An 

> arXiv:2602.10285v1 [cs.RO] 10 Feb 2026

Fig. 1: Our approach encodes past motion, map layout, and the desired pose using a transformer-based encoder. The resulting representation is passed to a flow matching network with an adaptive number of integration steps, followed by a lightweight post-processing step to generate a motion plan for the ego vehicle and future behavior predictions for surrounding agents. overview of the proposed method is summarized in Fig. 1. We evaluate our approach on the Waymo Open Motion Dataset and compare it against diffusion model, consistency model, and transformer model baselines. Our method con-sistently outperforms these approaches in terms of trajectory smoothness, goal-reaching performance, and generalization across diverse maneuvers. We further show how, by optimizing our implementation for the inference stage via TensorRT and ONNX deployment [14], the complete pipeline can run at approximately 20 Hz, supporting real-time deployment. To summarize, our key contributions are:  

> •

A flow-matching-based motion planner for interactive au-tonomous driving that handles diverse maneuvers without scenario-specific tuning and runs in real time.  

> •

A variance-guided inference mechanism that adaptively selects the number of neural network function evalua-tions, removing the need for tuning or retraining.  

> •

A lightweight post-processing module formulated as a convex QP that enhances ride comfort and dynamic feasibility with negligible overhead. II. R ELATED WORKS 

Traditional model-based planners formulate trajectory gen-eration as a constrained optimization problem that incorporates vehicle dynamics and safety constraints [15]. These meth-ods typically rely on hand-crafted cost functions, which are difficult to tune and fail to generalize across diverse real-world scenarios [16], [17]. Moreover, incorporating predic-tions of surrounding agents as additional costs or constraints significantly increases the computational complexity of the underlying optimization problem [18]. Recent work has explored generative models for imitating expert driving behavior directly from data. Variational Au-toencoders (VAEs) generate motion plans by sampling from a learned latent space [19], while Generative Adversarial Networks (GANs) [20] use a generator–discriminator setup to mimic expert behavior. Both these methods have been applied to motion planning and prediction tasks [21], [22]. However, they often struggle to represent rare maneuvers present in expert demonstrations. Both VAEs and GANs can produce unrealistic outputs, often biased toward more frequent demonstrations and lacking coverage of less common but valid driving behaviors [23]. Transformer-based models [24] offer strong imitation learning performance but require large neural network architectures that may be too expensive for online replanning. Diffusion models [5] have shown promise in planning and prediction but typically require hundreds of denoising steps, making them impractical for real-time use. To reduce this overhead, recent variants such as MotionDiffuser [25] use fewer denoising steps by introducing trajectory priors built from clustered examples in the training set. This speeds up inference, but rare or long-tail behaviors are often missed, as they are typically absent from the clustered priors. To address the inference-time inefficiency of diffusion mod-els, prior work used consistency models [26] to develop an interactive motion planner for autonomous driving [27]. These models are trained with a fixed noise schedule and enforce consistency across denoising steps during inference. The method in [27] was applied to the Waymo Open Motion Dataset, which contains diverse urban driving scenarios, and demonstrated strong performance using only four denoising steps at inference. To handle this scenario diversity, tuning the noise schedule was necessary to achieve an effective trade-off between inference speed and imitation learning accuracy. Each adjustment required retraining, thereby increasing model development time. Trajectory-Conditional Flow Matching (T-CFM) [11] ap-plies flow matching to aircraft trajectory forecasting and maze navigation tasks. It achieves state-of-the-art results with sampling speeds up to 100 × faster than diffusion and often requires only a single integration step. GoalFlow [4] extends this idea to autonomous driving by conditioning the vector field on a goal point along with LiDAR and camera inputs. However, it is limited to single-agent settings and requires a separate goal-generation module during training. In this paper, we use variance-adaptive flow matching to ad-just the integration steps based on predicted uncertainty. This increases the number of function evaluations in regions where the model is less confident, enabling finer-grained updates in parts of the scene context that were underrepresented during training. It mitigates the bias of output trajectories toward scenes more frequently observed in the imitation learning dataset, as seen in VAEs and GANs, and avoids the reliance on fixed anchor trajectories in diffusion-based methods. This variable time-stepping scheme also eliminates the need for manually tuned noise schedules associated with consistency-based methods. Finally, by optimizing our implementation for runtime inference and incorporating a lightweight post-processing step, we achieve real-time performance unlike sev-eral transformer-based architectures with high computational overhead. III. P RELIMINARIES 

A. Problem Statement 

Let tcurr denote the current time step. With a sampling interval of ∆t = 0 .1 s, we use a past horizon of K = 10 

steps, to collect state sequences for all agents. Each agent’s state at time t is xt ∈ Rn, where n = 4 includes position and velocity components. The ego vehicle’s past trajectory is denoted Spast  

> e

:= ( xe 

> 0

= xetcurr , . . . , x eK−1). Past trajectories for other agents are denoted Spast obj := {Spast  

> j

}No

> j=1

, where each 

Spast  

> j

:= (xj 

> 0

= xjtcurr , . . . , x jK−1). We include up to the closest No = 5 agents whose positions at tcurr lie within a fixed distance threshold Ro = 10 m from the ego. The map is represented as a tensor M ∈ RNm×L×D , where Nm

is the number of polylines, L is the number of points per polyline, and D is the number of attributes per point. Each point on a polyline includes several attributes such as its position, orientation, a semantic tag indicating whether it lies on a lane centerline, road edge, crosswalk boundary, and so on. Finally, let sge denote the desired ego pose at the end of the horizon. Then, given the inputs (Spast  

> e

, S past obj , M, s ge )

and a future planning horizon of T = 80 timesteps, the goal is to simultaneously generate a motion plan for the ego 

Splan  

> e

:= ( xe

> 1

, . . . , x eT ) and a prediction of the behavior of surrounding agents denoted as Spred  

> obj

:= {Spred  

> j

}No

> j=1

, where each Spred  

> j

:= ( xj

> 1

, . . . , x jT ).

B. Motion Transformer Encoder 

Shi et al. [24] proposed a transformer-based architecture for autonomous driving which used an encoder to fuse agent histories and polyline map features into scene context, and a decoder to generate multiple plausible future trajectories. In our work, we retain their encoder and use its output, denoted by C(Spast  

> e

, S past obj , M, s ge ), as the conditional input to our flow matching based trajectory generator. It is worth noting that our method does not depend on a specific encoder design and can flexibly incorporate any scene encoder that provides rich contextual representations. 

C. Variance-Adaptive Flow Matching Overview of Flow Matching: Flow matching is a gen-erative modeling technique that transforms a sample z0 ∼

p0, typically drawn from a standard Gaussian prior, into a corresponding sample z1 ∼ p1 from the data distribution, conditioned on a context vector c that encodes relevant input information [9]. This transformation is realized by training a neural network to represent a time-dependent velocity field 

vθ , such that ddt zt = vθ (zt, t | c) for t ∈ [0 , 1] . The model is trained on tuples (c, z 0, z 1) extracted from the Waymo Open Motion Dataset, where c = C(Spast  

> e

, S past obj , M, s ge ) is the context embedding produced by the Motion Transformer Encoder. At inference time, we sample z0 from a standard Gaussian prior and compute the context embedding c, then integrate the learned velocity field vθ to obtain Splan  

> e

and Spred  

> obj

which jointly represents the the planned trajectory for the ego vehicle and the predicted behavior of surrounding agents. 

Variance-Adaptive Time-Stepping: A specific variant of flow matching, known as variance-adaptive flow match-ing [12], trains a lightweight feedforward neural network σϕ,alongside the velocity field vθ . The output of this network, 

σϕ(zt, t | c), estimates the local uncertainty of the flow based on how confidently the model has learned the velocity field for a given context, c = C(Spast  

> e

, S past obj , M, s ge ). In contexts where the training data was dense, the predicted variance tends to be low, indicating that the learned velocity field is reliable. Conversely, in less frequently observed contexts, the predicted variance is high, signaling greater uncertainty about the flow. At inference time, we set the time step ∆t ∝ 1/σ ϕ(zt, t | c).This determines the number of evaluations of vθ (NFE) needed to integrate from t = 0 to 1.IV. M ETHODOLOGY 

In this section, we detail the algorithmic components of our proposed planner. We begin by describing the neural network architectures used to encode the scene context and subsequently generate trajectories. We then present the training procedure used to jointly optimize these components. Finally, we introduce a convex quadratic program that post-processes the generated trajectories to improve ride comfort and enforce dynamic feasibility. 

A. Neural Network Architectures Scene Encoder Architecture: The scene encoder processes the ego and surrounding agent histories, map polylines, and goal pose using a combination of Multi Layer Perceptrons (MLPs) and a transformer. Agent histories and map inputs are first embedded using an MLP, then passed through the transformer to capture spatial and temporal interactions. The goal pose is embedded separately and combined with the transformer outputs. The resulting token embeddings are flat-tened, masked, and projected into a fixed-length context vector 

c = C(Spast  

> e

, S past obj , M, s ge ), which conditions the flow-matching trajectory generator. 

Velocity Field and Variance Head Architecture: The veloc-ity field neural network vθ is implemented as U-Net with base width 128 and dimensional multipliers (1 , 2, 4) , following the design in [28]. It takes the conditional context embedding c

as input and predicts per-timestep trajectories in (x, y, v x, v y )

space. The variance estimator σϕ, described in the previous section, operates on the bottleneck features of the U-Net. These features are passed through a four-layer MLP with hidden dimension 512 and Sigmoidal Linear Unit (SiLU) activations. The network outputs a scalar variance estimate, which is used to modulate the integration step size ϵt as: 

ϵt = max 

 ησϕ

, ϵ min 



, (1) where η = 0 .1 is a user-defined regulation constant, and ϵmin =0.01 is a configurable minimum allowable step size. 

B. Training Procedure 

We jointly train the scene encoder, the velocity field net-work, and the variance prediction network. For the scene encoder, we adopt the encoder training loss Lencoder defined in the Motion Transformer framework [24]. During training, we sample z0 ∼ p0, which is a standard multivariate Gaussian distribution N (0 , I ) and z1 ∼ p1 is sampled from the data distribution of the normalized ground-truth future trajectory in the Waymo Open Motion Dataset (WOMD). At a uniformly sampled time t ∈ [0 , 1] , we construct the interpolated state zt = (1 − t)z0 + tz 1, and define the target velocity as z1 − z0. The context vector 

c = C(Spast  

> e

, S past obj , M, s goal  

> e

) is used to condition both the velocity field and variance networks, which are trained using the following loss [12]: 

Lflow = Et∼U (0 ,1) , z 0∼p0, z 1∼p1 [L(vθ , σ ϕ)] (2) 

L(vθ , σ ϕ) = ∥z1 − z0 − vθ (zt, t | c)∥2

2σϕ(zt, t | c) + log σϕ(zt, t | c)

This loss encourages the velocity predictor to match the target vector field while allowing the predicted variance to in-crease in high-error regions and decrease in confident regions. The log term acts as a regularization penalty that prevents the predicted variances from becoming arbitrarily large. The total training loss is a weighted combination of the encoder and flow objectives: 

Ltotal = β1Lencoder + β2Lflow (3) 

Constrained Refinement of Predicted Trajectories: The tra-jectory generated by the adaptive flow-matching network may have sharp turns or fail to reach the target goal by the end of the prediction horizon. To improve trajectory smoothness and encourage goal reaching, we apply a post-processing step to the ego motion plan that enforces constraints on lateral acceleration, angular velocity, and final position. Given discrete positions (xk, y k), we compute the linear velocities and accelerations as: 

vkx = xk+1 − xk

∆t , vky = yk+1 − yk

∆t ,akx = xk+2 − 2xk+1 + xk

∆t2 , aky = yk+2 − 2yk+1 + yk

∆t2 .

The lateral acceleration αk and angular velocity ωk at time k

are computed as [17]: 

αk = akxvkx + aky vky

q

(vkx )2 + ( vky )2

, ωk = vkx aky − vky akx

(vkx )2 + ( vky )2 .

We enforce bounds of the form |αk| ≤ αlimit , |ωk| ≤ ωlimit ,and require that the final position lies within a tolerance 

r of the desired goal. These constraints are non-convex due to bilinear dependencies on position terms. To avoid solving a computationally expensive optimization program, we linearize it around the flow matching output trajectory 

τ ref = (( xref  

> 0

, y ref  

> 0

), · · · , (xref  

> T−1

, y ref  

> T−1

)) to obtain affine surro-gates ¯αk and ¯ωk. We then impose absolute value bounds on the linearized quantities, relaxed using slack variables: 

|¯αk| ≤ αlimit + sk

> acc

, |¯ωk| ≤ ωlimit + skω , s k 

> acc

≥ 0, s kω ≥ 0. (4) 

|xT −xgoal | ≤ r +sx

> goal

, |yT −ygoal | ≤ r +sy

> goal

, s x,y  

> goal

≥ 0, (5) We solve the following convex quadratic program, where 

τ = (( x0, y 0), · · · , (xT , y T )) denotes the ego vehicle’s opti-mized trajectory and s denotes all associated slack variables: 

min  

> τ,s

wtrack  

> T−1

X

> k=0

xk

yk



−

xref 

> k

yref 

> k

 2

| {z }

> Position tracking

+ wT

xT

yT



−

xgoal 

ygoal 

 2

| {z }

> Goal-reaching

+ wsmooth  

> T−2

X

> k=0

xk+1 − xk

yk+1 − yk



−

xref  

> k+1

− xref 

> k

yref  

> k+1

− yref 

> k

 2

| {z }

> Reference-centered velocity smoothing

+ wacc 

X

> k

sk

> acc

| {z }

> Lateral slack

+ wω

X

> k

skω

| {z }

> Angular slack

+ wgoal (sx 

> goal

+ sy

> goal

)

| {z }

> Goal slack

subject to the constraints in Eqs. 4 and 5. The weights wtrack , w T , w smooth , w acc , w ω , w goal control the trade-off between tracking accuracy, smoothness, and con-straint satisfaction. The output of this convex quadratic pro-gram is the final ego-vehicle motion plan. We solve it effi-ciently using off-the-shelf solvers such as OSQP [29] in around 1 millisecond per trajectory. V. E XPERIMENTS AND RESULTS 

This section presents experiments on the Waymo Open Motion Dataset. We begin with a description of the dataset and evaluation metrics, followed by qualitative visualizations that illustrate the planner’s performance across diverse real-world scenarios. We then benchmark our method against diffusion, consistency, and transformer baselines in terms of planning accuracy, constraint satisfaction, and runtime. 

A. Dataset and Evaluation Setup 

The Waymo Open Motion Dataset (WOMD) [3] is a large-scale benchmark covering diverse urban driving scenarios. Each sample includes one second of past motion for all agents, eight seconds of future ground-truth trajectories, and high-definition maps. Our planner is conditioned on the final pose of the ego vehicle from the ground-truth, though this can be replaced by any goal generation module, such as [4]. All eval-uations are performed on the official validation split, which is further categorized into interactive and non-interactive subsets based on agent interaction levels. (a) Original Goal (b) Updated Goal 

Fig. 2: (a) The ego takes a sharp right exit. (b) From the same initial pose, the goal is changed to a left lane change. The policy adapts and produces smooth, lane-aligned trajectories. To evaluate planner performance, we report metrics that capture both alignment with ground-truth trajectories and ride quality. These include minimum average displacement error (minADE) and final displacement error (minFDE), which mea-sure average and final deviations from the ground truth. Goal-reaching error is defined as the Euclidean distance between the final pose of the planned trajectory and the assigned goal. To assess ride quality, we compute angle change, total path length, and average curvature, capturing directional smoothness, path efficiency, and turning sharpness, respectively. Constraint sat-isfaction is measured through violations in lateral acceleration, angular velocity, and goal-reaching constraints. We also report collision rate, defined as the percentage of scenarios where the ego comes within 2 meters of another agent’s ground-truth trajectory. All metrics are computed in an open-loop setting and averaged over validation subsets. 

B. Trajectory Output Visualizations 

In a receding-horizon implementation, where the planner is invoked at regular intervals with updated scene context, it is essential that the goal-conditioned policy can reliably adapt to changes in the desired end pose. To stress test this behavior, we evaluate the planner from a fixed initial condition while specifying two distinct goals: one that requires a sharp right exit and another that demands a left lane change. As shown in Fig. 2, the planner successfully adapts to both target poses, producing smooth trajectories that remain consistent with the road layout. We showcase the planner’s behavior in three challenging real-world scenarios: (a) adaptive cruise control, (b) dual lane change, and (c) unprotected left turn. Each row in Fig. 3 shows map snapshots at three time steps (0s, 5s, 8s), depicting the ego trajectory, surrounding traffic, and the goal. The ego’s full trajectory history is visualized, while only 1 second of history is shown for other agents to reduce clutter. The bottom row presents the ego’s speed profile for each maneuver. In (a), the ego approaches stationary traffic, slows down accordingly, and resumes acceleration once the traffic begins to move, maintaining a safe following distance. In (b), the ego accelerates to merge ahead of traffic and performs two lane changes to reach the goal, then eases its speed near TABLE I: Trajectory Accuracy for Interactive Split                       

> Method minADE ↓minFDE ↓
> Ours (Flow-Adaptive) 0.92 0.09
> Flow-Euler-5 0.93 0.10 Flow-Euler-50 1.03 0.09 Flow-Adaptive-NoQP 1.01 0.40 Consistency-Guided 0.69 0.33 Transformer 2.89 1.09 DDPM-10 1.04 2.45 DDPM-20 1.10 2.21 DDIM 5.98 14.48

TABLE II: Trajectory Accuracy for Non-Interactive Split                       

> Method minADE ↓minFDE ↓
> Ours (Flow-Adaptive) 1.61 0.11
> Flow-Euler-5 1.61 0.13 Flow-Euler-50 1.74 0.12 Flow-Adaptive-NoQP 1.70 0.81 Consistency-Guided 1.23 0.59 Transformer 2.80 9.25 DDPM-10 1.60 12.64 DDPM-20 1.60 11.26 DDIM 6.25 22.89

the endpoint. In (c), the ego waits for oncoming vehicles to pass, then gradually accelerates to complete an unprotected left turn. All of these behaviors emerge from the same unified framework without requiring any scenario-specific tuning. In contrast, RL- or MPC-based planners typically require separate offline training or online optimization for each maneuver [15]. 

C. Quantitative Comparison of Different Models 

We benchmark our planner against Transformer, Diffusion, and Consistency-based baselines, all using the same context encoder shown in Fig. 1. The Transformer baseline follows the MTR decoder [24] and does not condition on the ego goal. In contrast, the Diffusion, Consistency, and our models use goal-conditioned U-Net decoders with the same encoder. For diffusion-based models, we evaluate the DDPM base-line [30] with 10 and 20 denoising steps, as well as the deterministic DDIM variant [31]. The Consistency model, adapted from [27], performs four forward passes followed by a gradient guided constraint enforcement step. To ensure fair runtime comparison, we cap the number of gradient descent iterations to match the 20 Hz planning rate achieved by our method. We evaluate three variants of our approach: Flow-Euler, which uses a fixed number of integration steps (5 and 50); Flow-Adaptive, which adjusts the step size based on a learned variance predictor; and Flow-Adaptive–NoQP, which omits the QP refinement. Both Flow-Euler and Flow-Adaptive methods include the QP-based post-processing step. All U-Net decoders are exported to ONNX and accelerated using TensorRT [14]. As shown in Tables I and II, our adaptive flow matching method achieves the lowest minFDE across both interactive and non-interactive scenarios. This improvement is due to the learned velocity field and the enforcement of goal satisfaction during post-processing. The final displacement error of 0.11 mFig. 3: Example maneuvers handled by the proposed method: (a) approaching stop-and-go traffic, (b) changing multiple lanes in front of traffic to reach the goal, and (c) yielding to oncoming vehicles before executing an unprotected left turn. The bottom row shows the corresponding ego speed profiles. Videos of these and additional maneuvers can be found at https://flow-matching-self-driving.github.io/ TABLE III: Trajectory Quality and Constraint Violations on the Interactive Split                                                                               

> Method Angle Change ↓Path Length ↓Curvature ↓Collision (%) ↓Goal Error ↓Acc. Violation ↓ωViolation ↓
> Ours (Flow-Adaptive) 0.31 54.93 0.48 1.1 0.09 0.12 0.71
> Flow-Euler-5 0.41 55.09 0.86 1.53 0.10 0.36 1.13 Flow-Euler-50 0.35 55.04 0.71 1.40 0.09 0.28 0.96 Flow-Adaptive-NoQP 0.69 56.52 1.07 1.55 0.67 2.11 1.32 Consistency-Guided 0.65 55.39 0.97 0.8 0.33 3.82 1.47 Transformer 0.47 55.88 0.60 4.8 8.11 0.44 1.12 DDPM-10 0.63 67.74 1.38 1.8 2.45 32.61 2.25 DDPM-20 0.56 77.91 1.16 1.6 2.21 56.69 2.17 DDIM 0.67 134.50 1.71 18.6 14.48 167.77 4.51

is close to the goal-reaching tolerance of r = 0 .1m used in Eq. 2, indicating effective constraint satisfaction. Our method reports a slightly higher minADE than the consistency model, reflecting an inherent tradeoff: the post-processing step may shift the trajectory away from the nominal flow matching output to better satisfy constraints. Since ADE is computed relative to the ground truth, such deviations can slightly increase intermediate errors. Tables III and IV evaluate trajectory smoothness and con-straint satisfaction across all models. Our method outperforms all baselines on nearly all metrics in both interactive and non-interactive settings, achieving lower angular change, shorter path length, reduced curvature, smaller goal-reaching error, and fewer violations of acceleration and angular velocity lim-its. These gains are primarily due to the learned flow matching planner, which produces smooth, goal-directed trajectories. The quadratic program provides a lightweight refinement that further reduces constraint violations. The only exception is the collision rate, where our method performs comparably to the best baseline, the Consistency model. TABLE IV: Trajectory Quality and Constraint Violations on the Non-Interactive Split                                                                               

> Method Angle Change ↓Path Length ↓Curvature ↓Collision (%) ↓Goal Error ↓Acc. Violation ↓ωViolation ↓
> Ours (Flow-Adaptive) 0.29 57.05 0.55 2.4 0.11 0.31 0.77
> Flow-Euler-5 0.38 57.30 0.93 2.5 0.13 0.64 1.20 Flow-Euler-50 0.33 57.20 0.77 2.6 0.12 0.52 1.01 Flow-Adaptive-NoQP 0.73 59.31 1.23 3.1 1.47 3.57 1.39 Consistency-Guided 0.62 58.07 1.00 2.0 0.59 4.75 1.50 Transformer 0.42 59.62 0.56 3.3 9.26 0.57 1.02 DDPM-10 0.66 88.13 3.45 3.7 12.64 78.47 3.34 DDPM-20 0.59 93.19 2.98 3.3 11.26 89.54 2.96 DDIM 0.68 158.94 1.97 17.6 22.89 217.28 5.20

Fig. 4: Collision rate plot: Collisions are rare initially and higher toward the end of the horizon. With continuous replan-ning, the early time steps are most critical for safety. TABLE V: Inference Time and NFE Analysis                       

> Method Time (ms) ↓NFE ↓
> Ours (Flow-Adaptive) 45 ± 4.9 4.7 ± 1.4 Flow-Euler-5 55 ± 3.8 5.0 ± 0.0 Flow-Euler-50 676 ± 25.3 50.0 ± 0.0 Flow-Adaptive-NoQP 43 ± 4.2 4.7 ± 1.4 Consistency-Guided 52 ± 4.5 4.0 ± 0.0 Transformer 37 ± 3.1 N/A DDPM-10 83 ± 7.2 10.0 ± 0.0 DDPM-20 125 ± 10.7 20.0 ± 0.0 DDIM 37 ± 2.5 4.0 ± 0.0

Since collision rate is a key safety measure, we analyze collision occurrences along the planning horizon of 8 seconds. Figure 4 shows that most collisions occur near the end rather than at the beginning. This is expected, as small deviations from the ground-truth trajectory accumulate over time, leading to larger divergence later in the plan. While collisions are measured in an open-loop setting without replanning, real-world deployment would involve continuous updates based on the evolving scene context. In such cases, the planner would regenerate the trajectory at each cycle, making safety in the early steps more critical. Our method maintains collision-free behavior in these initial segments. Finally, future work will involve incorporating obstacle avoidance as explicit constraints in the post-processing optimization, as shown in [13]. 

D. Analysis of Inference Time and Adaptive Integration 

Table V reports the mean and standard deviation of infer-ence time and number of U-Net function evaluations (NFE) for all methods, computed over all scenes in the interactive validation split. Inference time is measured as the total time 

Fig. 5: Normalized integration time for different NFE. The non-uniform progression reflects variance-based step size ad-justment, unlike the uniform fixed-step Euler integration. needed to produce traffic behavior predictions and ego plans from the motion history, map, and goal inputs. Our method achieves an average inference time of 45 milliseconds, corresponding to a 20 Hz planning rate suitable for real-time deployment. While DDIM is slightly faster at 37 milliseconds, its performance in trajectory quality and constraint satisfaction falls short of ours. Other baselines such as Consistency, Transformer, and DDPM-10 operate in a similar time range but do not match our overall metrics. To visualize how adaptive time stepping operates, we an-alyze the average integration step size during inference for NFE values of 4, 5, and 6 on the interactive validation set. As shown in Fig. 5, we observe non-uniform step sizes, indicating that the variance module allocates computation based on confidence. Low predicted variance suggests that the model has encountered similar scene contexts during training and assigns larger steps due to a reliable velocity field. Conversely, high variance results in smaller steps in unfamiliar scene contexts. To further illustrate the benefit of adaptive integration, we compare against fixed-step variants of our model. We observe that flow matching with 50 steps achieves performance comparable to the adaptive variant but at significantly higher computational cost due to increased U-Net evaluations. We also evaluate flow matching with 5 steps, selected to match the average number of function evaluations used by the adaptive model. This variant performs notably worse, indicating that fixed low-resolution integration fails to generalize across di-verse scenarios. We include the Flow-Adaptive–NoQP variant, which excludes the post-processing step. This exhibits more frequent constraint violations. As shown in Table V, the QP refinement adds only about 1-2 milliseconds to the runtime while significantly improving motion planning performance. Finally, it is worth noting that in [27], the Consistency model required extensive tuning of the noise schedule to optimize performance on the WOMD. This process involved retraining both the encoder and U-Net, taking approximately five days on four H100 GPUs per run. In contrast, our adaptive variant achieves a similar average NFE of 4.7 without any manual tuning, as the number of U-Net evaluations is automatically determined by the learned variance estimator. VI. C ONCLUSION AND FUTURE WORK 

In this work, we presented a framework for interactive motion planning for autonomous vehicles in urban settings, based on adaptive flow matching. The approach operates in real time, requires no scenario-specific tuning, and generates smooth trajectories that try to satisfy ride quality constraints. While our current implementation relies on polyline-based map inputs provided by the WOMD, this assumes access to dedicated preprocessing and sensor infrastructure. As future work, we aim to replace the map encoder with vision-based alternatives that operate directly on raw LiDAR or camera data [32]. We also plan to evaluate the planner in closed-loop simulation environments such as MetaDrive [33], followed by deployment on real hardware platforms. REFERENCES [1] F. M. Tariq, D. Isele, J. S. Baras, and S. Bae, “Slas: Speed and lane advisory system for highway navigation,” in 2022 IEEE 61st Conference on Decision and Control (CDC) . IEEE, 2022, pp. 6979–6986. [2] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: Amultimodal dataset for autonomous driving,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2020, pp. 11 621–11 631. [3] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al. , “Scalability in perception for autonomous driving: Waymo open dataset,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2020, pp. 2446–2454. [4] Z. Xing, X. Zhang, Y. Hu, B. Jiang, T. He, Q. Zhang, X. Long, and W. Yin, “Goalflow: Goal-driven flow matching for multimodal trajec-tories generation in end-to-end autonomous driving,” in Proceedings of the Computer Vision and Pattern Recognition Conference , 2025, pp. 1602–1611. [5] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, “Diffusion policy: Visuomotor policy learning via ac-tion diffusion,” The International Journal of Robotics Research , p. 02783649241273668, 2023. [6] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone, “Guided conditional diffusion for controllable traffic simulation,” arXiv preprint arXiv:2210.17366 , 2022. [7] A. Li, Z. Ding, A. B. Dieng, and R. Beeson, “Diffusolve: Diffusion-based solver for non-convex trajectory optimization,” arXiv preprint arXiv:2403.05571 , 2024. [8] B. Liao, S. Chen, H. Yin, B. Jiang, C. Wang, S. Yan, X. Zhang, X. Li, Y. Zhang, Q. Zhang et al. , “Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving,” in Proceedings of the Computer Vision and Pattern Recognition Conference , 2025, pp. 12 037–12 047. [9] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative modeling,” arXiv preprint arXiv:2210.02747 ,2022. [10] Z. Qiao, F. Ding, T. Dresselhaus, M. A. Rosenfeld, X. Han, O. Howell, A. Iyengar, S. Opalenski, A. S. Christensen, S. K. Sirumalla et al. ,“Neuralplexer3: Physio-realistic biomolecular complex structure predic-tion with flow models,” arXiv preprint arXiv:2412.10743 , 2024. [11] S. Ye and M. C. Gombolay, “Efficient trajectory forecasting and gener-ation with conditional flow matching,” in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 2024, pp. 2816–2823. [12] X. Hu, Q. Liu, X. Liu, and B. Liu, “Adaflow: Imitation learning with variance-adaptive flow-based policies,” Advances in Neural Information Processing Systems , vol. 37, pp. 138 836–138 858, 2024. [13] J. Yang, S. Jang, and S. Han, “Safeflowmatcher: Safe and fast planning using flow matching with control barrier functions,” arXiv preprint arXiv:2509.24243 , 2025. [14] P. Davoodi, C. Gwon, G. Lai, and T. Morris, “Tensorrt inference with tensorflow,” in GPU Technology Conference , 2019. [15] F. M. Tariq, N. Suriyarachchi, C. Mavridis, and J. S. Baras, “Au-tonomous vehicle overtaking in a bidirectional mixed-traffic setting,” in 2022 American Control Conference (ACC) . IEEE, 2022, pp. 3132– 3139. [16] A. Trivedi, S. Prajapati, M. Zolotas, M. Everett, and T. Padır, “Chance-constrained convex mpc for robust quadruped locomotion under para-metric and additive uncertainties,” IEEE Robotics and Automation Letters , 2025. [17] A. Trivedi, S. Prajapati, A. Shirgaonkar, M. Zolotas, and T. Padır, “Data-driven sampling based stochastic mpc for skid-steer mobile robot navigation,” in 2025 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 2025, pp. 16 619–16 625. [18] B. Ivanovic and M. Pavone, “The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs,” in Proceed-ings of the IEEE/CVF international conference on computer vision ,2019, pp. 2375–2384. [19] C. Doersch, “Tutorial on variational autoencoders,” arXiv preprint arXiv:1606.05908 , 2016. [20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” 

Communications of the ACM , vol. 63, no. 11, pp. 139–144, 2020. [21] P. Dendorfer, A. Osep, and L. Leal-Taix´ e, “Goal-gan: Multimodal trajectory prediction based on goal position estimation,” in Proceedings of the Asian Conference on Computer Vision , 2020. [22] W. Zheng, R. Song, X. Guo, C. Zhang, and L. Chen, “Genad: Generative end-to-end autonomous driving,” in European Conference on Computer Vision . Springer, 2024, pp. 87–104. [23] A. Amini, W. Schwarting, G. Rosman, B. Araki, S. Karaman, and D. Rus, “Variational autoencoder for end-to-end control of autonomous driving with novelty detection and training de-biasing,” in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 2018, pp. 568–575. [24] S. Shi, L. Jiang, D. Dai, and B. Schiele, “Motion transformer with global intention localization and local movement refinement,” Advances in Neural Information Processing Systems , vol. 35, pp. 6531–6543, 2022. [25] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, “3d diffuser ac-tor: Policy diffusion with 3d scene representations,” arXiv preprint arXiv:2402.10885 , 2024. [26] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, “Consistency models,” 2023. [27] A. Li, S. Bae, D. Isele, R. Beeson, and F. M. Tariq, “Predictive planner for autonomous driving with consistency models,” arXiv preprint arXiv:2502.08033 , 2025. [28] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical image computing and computer-assisted intervention . Springer, 2015, pp. 234–241. [29] B. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S. Boyd, “Osqp: An operator splitting solver for quadratic programs,” Mathematical Programming Computation , vol. 12, no. 4, pp. 637–672, 2020. [30] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” 

Advances in neural information processing systems , vol. 33, pp. 6840– 6851, 2020. [31] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” 

arXiv preprint arXiv:2010.02502 , 2020. [32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 770–778. [33] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning,” IEEE transactions on pattern analysis and machine intelli-gence , vol. 45, no. 3, pp. 3461–3475, 2022.
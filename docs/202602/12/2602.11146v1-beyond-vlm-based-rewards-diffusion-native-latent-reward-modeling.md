---
title: "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling"
title_zh: 超越基于 VLM 的奖励：扩散原生的潜空间奖励建模
authors: "Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke, Didan Deng, Han Gao, Yongxiang Huang, Kaihao Zhang, Hongbo Fu, Wenhan Luo"
date: 2026-02-11
pdf: "https://arxiv.org/pdf/2602.11146v1"
tags: ["keyword:FM", "keyword:MDM"]
score: 6.0
evidence: 流匹配和扩散模型的偏好优化
tldr: 针对扩散模型偏好优化中 VLM 奖励函数计算开销大且存在像素-潜空间域不匹配的问题，本文提出 DiNa-LRM。这是一种扩散原生潜空间奖励模型，直接在加噪扩散状态上建模。通过引入噪声校准的 Thurstone 似然和时间步条件奖励头，DiNa-LRM 在保持高性能的同时显著降低了计算成本，并提升了模型对齐的效率和鲁棒性。
motivation: 现有的基于 VLM 的奖励模型计算成本高，且在像素空间评估潜空间扩散模型会导致严重的域不匹配问题。
method: 提出 DiNa-LRM，利用预训练扩散骨干网络和时间步条件奖励头，在加噪潜空间直接进行偏好学习与噪声校准建模。
result: 在图像对齐基准测试中，DiNa-LRM 优于现有扩散奖励基准，并以极低的计算成本达到了与顶级 VLM 相当的性能。
conclusion: DiNa-LRM 为扩散模型提供了一种高效、原生的奖励建模方案，显著优化了偏好对齐的动力学过程并提升了资源利用率。
---

## 摘要
扩散模型和流匹配模型的偏好优化依赖于既具有判别鲁棒性又具有计算效率的奖励函数。视觉语言模型 (VLM) 已成为主要的奖励提供者，利用其丰富的多模态先验来引导对齐。然而，它们的计算和内存开销可能非常巨大，且通过像素空间奖励优化潜空间扩散生成器会引入领域不匹配，从而使对齐变得复杂。在本文中，我们提出了 DiNa-LRM，这是一种扩散原生的潜空间奖励模型，它直接在含噪扩散状态上构建偏好学习。我们的方法引入了一种具有扩散噪声相关不确定性的噪声校准 Thurstone 似然。DiNa-LRM 利用预训练的潜空间扩散主干网络和时间步条件奖励头，并支持推理时的噪声集成，为测试时扩展和鲁棒奖励提供了一种扩散原生机制。在图像对齐基准测试中，DiNa-LRM 显著优于现有的基于扩散的奖励基准，并以极低的计算成本实现了与最先进 VLM 相当的性能。在偏好优化方面，我们证明了 DiNa-LRM 改善了偏好优化动态，实现了更快且更节省资源的模型对齐。

## Abstract
Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.
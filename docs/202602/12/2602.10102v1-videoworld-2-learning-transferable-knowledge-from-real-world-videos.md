---
title: "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos"
title_zh: VideoWorld 2：从真实世界视频中学习可迁移知识
authors: "Zhongwei Ren, Yunchao Wei, Xiao Yu, Guixun Luo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin"
date: 2026-02-10
pdf: "https://arxiv.org/pdf/2602.10102v1"
tags: ["keyword:MDM", "query:课题"]
score: 6.0
evidence: 潜空间动力学与长程推理
tldr: VideoWorld 2 旨在从无标注的真实世界视频中学习可迁移知识。它提出了动态增强的潜空间动力学模型（dLDM），通过预训练视频扩散模型解耦视觉外观与动作动力学，利用自回归方式建模潜码以学习任务策略和长程推理。在手工制作和机器人操作任务中表现优异，显著提升了任务成功率，展示了从原始视频中提取通用世界知识的潜力。
motivation: 旨在解决现有模型难以从无标注真实视频中有效提取并迁移复杂任务动力学知识的问题。
method: 引入动态增强潜空间动力学模型（dLDM），通过解耦视觉外观与动作动态，并利用自回归模型进行长程推理。
result: "在真实手工任务中实现 70% 的成功率提升，并成功将 Open-X 机器人数据集的知识迁移至 CALVIN 任务中。"
conclusion: 该研究验证了从原始视频中学习可迁移世界知识的巨大潜力，为智能体在复杂环境下的策略学习提供了新方案。
---

## 摘要
从无标签视频数据中学习可迁移知识并将其应用于新环境是智能体的一项基本能力。本研究提出了 VideoWorld 2，它是 VideoWorld 的扩展，并首次探讨了直接从原始真实世界视频中学习可迁移知识的方法。VideoWorld 2 的核心是引入了一种动态增强的潜动力学模型（dLDM），该模型将动作动力学与视觉外观解耦：预训练的视频扩散模型负责视觉外观建模，使 dLDM 能够学习专注于紧凑且有意义的任务相关动力学的潜代码。随后，这些潜代码通过自回归建模来学习任务策略并支持长时程推理。我们在具有挑战性的真实世界手工制作任务上评估了 VideoWorld 2，在这些任务中，先前的视频生成和潜动力学模型难以可靠运行。值得注意的是，VideoWorld 2 在任务成功率上实现了高达 70% 的提升，并生成了连贯的长执行视频。在机器人领域，我们展示了 VideoWorld 2 可以从 Open-X 数据集中获取有效的操作知识，从而显著提高在 CALVIN 上的任务性能。本研究揭示了直接从原始视频中学习可迁移世界知识的潜力，所有代码、数据和模型都将开源以供进一步研究。

## Abstract
Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.
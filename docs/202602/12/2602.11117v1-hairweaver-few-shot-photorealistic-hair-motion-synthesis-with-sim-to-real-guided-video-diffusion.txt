Title: HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion

URL Source: https://arxiv.org/pdf/2602.11117v1

Published Time: Thu, 12 Feb 2026 02:23:23 GMT

Number of Pages: 16

Markdown Content:
# HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion 

Di Chang 1,2,âˆ—, Ji Hou 1, Aljaz Bozic 1, Assaf Neuberger 1, Felix Juefei-Xu 1, Olivier Maury 1, Gene Wei-Chin Lin 1, Tuur Stuyck 1, Doug Roble 1, Mohammad Soleymani 2, Stephane Grabli 11Meta, 2University of Southern California 

> âˆ—

Work done during an internship at Meta 

We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subjectâ€™s photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.  

> Date:

February 12, 2026  

> Correspondence:

Di Chang at dichang@usc.edu  

> Website:

https://boese0601.github.io/hairweaver/ 

## 1 Introduction 

This work addresses the challenging problem of animating a single source image driven by a motion sequence. This technology holds immense promise for applications in virtual reality, next-generation gaming, and the film industry (Lee et al., 2023; Epic Games, 2024). While recent progress in generative models has enabled plausible human animation, a significant hurdle remains in synthesizing the intricate, non-rigid dynamics of secondary elements, most notably hair. The complex physics and fine-grained visual nature of hair motion are often overlooked by these models, leading to generated humans with fluid-like and unnatural hair motions, shattering the illusion of realism. Building on prior research in human animation (Hu, 2024; Xu et al., 2024; Chang et al., 2023; Wang et al., 2024), we aim to explicitly model and generate expressive and physically plausible hair dynamics in concert with accurate body pose transfer. Recent breakthroughs in human image animation (Chang et al., 2023; Zhang et al., 2024; Hu, 2024; Wang et al., 2025b,a; Chang et al., 2025; Zhao et al., 2025; Song et al., 2025; Zhang et al., 2025a; Wang et al., 2025a; Luo et al., 2025; Chen et al., 2025; Wang et al., 2024) are largely driven by controlled image-to-video diffusion models (Zhang et al., 2023; Guo et al., 2023). These methods typically disentangle appearance and motion by conditioning a pretrained video diffusion model on a reference image for appearance and a sequence of poses (e.g., Pose skeletons (Chang et al., 2023; Zhang et al., 2024; Hu, 2024; Wang et al., 2025b; Zhang et al., 2025a; Luo et al., 2025; Wang et al., 2025a) or DensePose (Xu et al., 2024)) for motion control. Appearance is often preserved using attention-based mechanisms that inject features from the reference image into the generation process (Cao et al., 2023; Blattmann et al., 2023b). Despite achieving impressive control over body posture and identity preservation, these models fundamentally struggle with secondary dynamics. Their network architectures and training paradigms are optimized for overall postural correctness, treating hair as a static texture mapped to the head. Alternatively, 3D-based models (Lin et al., 2025a; Stuyck et al., 2025; Li et al., 2025; Zhang et al., 2025b; Wang et al., 2023b, 2022), offer a potential solution to generate diverse hair motions with controllability. 1

> arXiv:2602.11117v1 [cs.CV] 11 Feb 2026

Reference Generated Hair Motion Figure 1 Photorealistic hair motions generated by HairWeaver. 

However, these methods often lack the photorealism and fine details, producing results that appear synthetic and cannot be directly used to train a diffusion model as previous human video animation methods. We argue that these animation methods falter for a common reason: a critical lack of high-fidelity paired data that contain photorealistic appearance, and diverse, controllable, physically-plausible hair geometry conditions. To solve this, we propose HairWeaver, a framework that achieves both fine-grained control and photorealism by addressing the data problem head-on. We first generate a small-scale synthetic dataset using a physics-based method (Kugelstadt and SchÃ¶mer, 2016), providing rich, physically-grounded hair motion and body pose conditions. We then propose a two-stage few-shot training strategy to transfer this motion control to a powerful, pre-trained photorealistic video diffusion transformer (DiT) (Peebles and Xie, 2023) without 

transferring the CG domainâ€™s non-photorealistic style. This strategy involves two lightweight components: a  

> Motion-Context-LoRA

module to inject the new motion conditions, and a temporary Sim2Real-Domain-LoRA 

module to overcome the domain gap between CG simulated and photorealistic videos. The Sim2Real-Domain-LoRA is first trained to adapt the DiT to the CG domain. Then, it is frozen, and the Motion-Context-LoRA is trained to learn the mapping from motion conditions to video. Crucially, the Sim2Real-Domain-LoRA is discarded entirely during inference. This strategy allows HairWeaver to leverage the precise motion physics from the CG data while retaining the full photorealistic power of the original foundation model. As a result, HairWeaver achieves a unique combination of capabilities: the fine-grained, controllable hair physics, and the high-fidelity photorealism, while being trained on only a few samples (around 1k videos). We conduct comprehensive evaluations against state-of-the-art baselines on challenging video benchmarks. HairWeaver consistently outperforms existing methods in both quantitative metrics and qualitative user studies, particularly in the realism of motion and the preservation of fine details. In summary, our main contributions are: 

â€¢ A diffusion-based pipeline, HairWeaver, specifically designed to synthesize expressive and dynamic hair motion for realistic few-shot human video animation, with a synthetic guiding signal from a simulator as training data. 

â€¢ An efficient and effective Motion-Context-LoRA that injects hair motion control as additional attention 2ð‘§ ð‘¡  

> Sim2Real -Domain -LoRA
> Motion -Context -LoRA
> ð¼ ð‘Ÿð‘’ð‘“
> Sim2Real -Domain -LoRA
> ð¼ ð‘Ÿð‘’ð‘“
> ð‘§ ð‘¡
> ð¶ â„Žð‘Žð‘–ð‘Ÿ ð¶ ð‘ð‘œð‘ ð‘’

c) Train Stage 2 

b) Train Stage 1 a) Data Generation  

> ð‘§ ð‘¡
> Motion -Context -LoRA
> ð‘ƒâ„Žð‘œð‘¡ð‘œð‘Ÿð‘’ð‘Žð‘™
> ð‘…ð‘’ð‘“ð‘’ð‘Ÿð‘’ð‘›ð‘ð‘’
> ð‘ƒâ„Žð‘œð‘¡ð‘œð‘Ÿð‘’ð‘Žð‘™
> ðºð‘’ð‘›ð‘’ð‘Ÿð‘Žð‘¡ð‘–ð‘œð‘›
> ð¶ â„Žð‘Žð‘–ð‘Ÿ ð¶ ð‘ð‘œð‘ ð‘’

d) Inference  

> â€œA woman turns her
> head to her leftâ€
> ð‘‰ ð‘”ð‘¡
> ð‘‰ ð‘”ð‘¡

e) Details of Model Architecture  

> Sim2Real -Domain -LoRA
> Motion -Context -LoRA
> ð¼ ð‘Ÿð‘’ð‘“
> ð¶ â„Žð‘Žð‘–ð‘Ÿ
> ð¶ ð‘ð‘œð‘ ð‘’
> ð‘‰ ð‘”ð‘¡
> VAE -Encoder C
> ð‘§ ð‘¡

1

2

3

4

+ C

> VAE -Encoder

5

6

7

8 

> ð‘§ ð‘ð‘œð‘ ð‘’ ð‘§ â„Žð‘Žð‘–ð‘Ÿ
> Positional
> Embedding
> Pose Encoder
> Positional
> Embedding
> ð‘§ ð‘–ð‘› ð‘§ ð‘¡
> â€²
> 1.5K digital human
> Assets with geometric
> hair strands
> Animated meshes
> Simulated hair strands,
> Light, Camera Viewpoint, â€¦
> Output Renders
> Animation
> Simulation
> Vgt
> Chair
> Cbody
> Rendering

Figure 2 Overview of HairWeaver pipeline. a) We use CG simulation to generate data including human videos with motions Vgt , static reference image Iref (a frame from Vgt ), pose condition Cpose , and hair condition Chair . b) 

During training stage 1, we leverage the a diffusion transformer (Peebles and Xie, 2023) (DiT) as the backbone model and pre-train the Sim2Real-Domain-LoRA. This training process is conducted in Image-to-Video manner with Iref and text prompt for Vgt . c) During training stage 2, we freeze the Sim2Real-Domain-LoRA and finetune the Motion-Context-LoRA with Cpose , and hair condition Chair as additional guidance. d) During inference, the Sim2Real-Domain-LoRA is discarded and the trained model generates photorealistic human videos with hair and body motions with photorealistic reference and CG conditions Cpose , Chair as input. e) Details of the model architecture presented in (c). The Pose Encoder integrates the body motions as a trainable residual to the noisy latent. The hair motions are encoded as additional attention context to the DiT blocks by a frozen VAE-Encoder. The only trainable modules are the Pose Encoder and the Motion-Context-LoRA. 

context, preserving the generative power of the video diffusion backbone. 

â€¢ A two-stage training strategy that uses a temporary Sim2Real-Domain-LoRA to learn motion control from synthetic data, which is then discarded at inference to ensure photorealistic generation. 

## 2 Related Work 

2.1 Diffusion Models for Human Video Animation 

The animation of human subjects from a single image has been revolutionized by latent diffusion models (Rom-bach et al., 2022). A prevalent paradigm involves conditioning a model on both appearance and motion. To preserve the subjectâ€™s identity, methods often employ a ReferenceNet architecture or cross-attention mechanisms to inject appearance features from a source image into the generation process (Hu, 2024; Chang et al., 2023; Zhu et al., 2024; Cao et al., 2023; Ye et al., 2023). For motion control, pose information derived from a driving video is typically supplied through a spatial guidance module like ControlNet (Zhang et al., 2023; Xu et al., 2024; Chang et al., 2023). Before video models became ubiquitous, many methods followed a two-stage training process: first training a static image generator and then adding a temporal module for video consistency (Wang et al., 2023a; Chang et al., 2023). However, the advent of powerful, large-scale video foundation models has enabled a more streamlined approach. Recent state-of-the-art works, such as MimicMotion (Zhang et al., 2024) and UniAnimate-DiT (Wang et al., 2025b), directly fine-tune Video Diffusion models like Stable Video Diffusion (SVD) (Blattmann et al., 2023a) or Wan2.1 (Wan et al., 2025). By leveraging the strong temporal priors of these pretrained models, they achieve high-fidelity motion transfer and identity preservation. Despite their success in replicating overall body movements, these frameworks are not explicitly designed to capture fine-grained secondary dynamics, a limitation that becomes particularly apparent in complex materials like hair. 3Ours Ground Truth Wan2.2-Animate  

> Reference
> UniAnimate-DiT
> Ours Ground Truth Wan2.2-Animate UniAnimate-DiT Figure 3

Visualization of comparison between HairWeaver and the previous state-of-the-art baselines (Wang et al., 2025b; Wan et al., 2025). Our model generates more realistic and diverse hair motions. 

2.2 Hair Synthesis 

Synthesizing highly convincing, photorealistic human hair â€” particularly in dynamic scenarios â€” remains a significant challenge. Over the past several decades, the Visual Effects (VFX), Animation, and Video Game industries have refined the craft of hair synthesis using Computer Graphics (CG), both in offline rendering and real-time settings. CG-based hair offers numerous advantages. By representing hair as hundreds of thousands of individual geometric strands, artists gain a high degree of control and flexibility in the creative process. Skilled artists achieve compelling hair renders by leveraging advanced grooming and simulation tools (Sid), sophisticated hair shading models (Chiang et al., 2015), and physically-based path tracing algorithms (Pharr et al., 2023). Despite the relative ease of producing high-quality CG hair, achieving true photorealism remains elusive. Only top-tier studios can generate truly realistic video sequences, and doing so incurs substantial costs. One of our goals is to harness the controllability of CG hair combined with video diffusion models, to achieve photorealistic results on par with, or surpassing, high-end productions, while significantly reducing costs. As previously discussed, while video diffusion models excel at producing realistic results and provide some degree of control over body pose, they generally lack explicit mechanisms for directing hair synthesis. This limitation poses a significant challenge when creating movies that require detailed and controllable hair dynamics. One notable exception is the recent ControlHair (Lin et al., 2025b) paper, which finetunes a video diffusion backbone (Wan et al., 2025) to offer directable hair motion. ControlHair utilizes HairStepâ€™s hair direction neural extractor (Zheng et al., 2023) to generate its driving signals. However, unlike CG data â€” which offers perfect alignment between conditioning and target signals, albeit with a domain gap â€” these extracted maps are only approximate. This limited accuracy ultimately constrains the quality and expressiveness of the resulting model. We choose to rely more thoroughly on CG data and describe our solution to handling its domain gap in section 3.4. Recently, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has emerged as an innovative 3D representation, offering a promising shortcut to achieving photorealism. However, this approach trades off versatility for visual fidelity: the photorealistic appearance is typically derived from static multi-view captures and baked into the representation. As a result, animating or relighting objects represented with 3DGS remains a challenge. Notably, recent work such as Gaussian Haircut (Zakharov et al., 2024) has sought to bridge this gap by integrating 3DGS with traditional strand-based hair models, aiming to restore the flexibility inherent in classic approaches. Nevertheless, animating hair within the 3DGS framework continues to be an open research problem. 43 Method 

Our method involves two main stages. First, we generate a synthetic dataset by rendering a set of animated digital human assets, where the hair is represented as geometric strands and animated using physics-based simulation, providing rich and physically-grounded hair motion signals paired with body pose, as detailed in Section 3.2. Second, we propose a novel two-step training strategy to adapt a large-scale Video DiT (Peebles and Xie, 2023) for this task without inheriting the synthetic dataâ€™s non-photorealistic artifacts. This strategy involves Motion-Context-LoRA, a lightweight module for injecting our dual motion conditions (described in Section 3.3), and Sim2Real-Domain-LoRA, a temporary LoRA that bridges the domain gap during training (described in Section 3.4). Crucially, Sim2Real-Domain-LoRA is discarded at inference, allowing Motion-Context-LoRA to drive the original photorealistic backbone. This results in a model with the fine-grained controllability of our CG data and the high-fidelity realism of the pre-trained DiT. The pipeline of HairWeaver is depicted in Fig. 2. 

3.1 Preliminaries 

The video diffusion transformer (DiT) (Peebles and Xie, 2023) represents a significant advancement in video generation, utilizing fully transformer-based models in diffusion frameworks. The core principle of DiT involves a forward diffusion process that adds Gaussian noise to the data, and a reverse denoising process that learns to reconstruct the original video from the noisy input. Given a video sequence x0 âˆˆ RT Ã—HÃ—W Ã—C , where T is the number of frames, H and W are spatial dimensions, and C is the number of channels, the forward process is defined as: 

q(xt|xtâˆ’1) = N (xt; p1 âˆ’ Î²txtâˆ’1, Î² tI), (1) where Î²t and xt are the variance schedule controlling the noise level and current data sample with added noise at timestep t, respectively. The transformer backbone in DiTs processes spatiotemporal tokens through multi-head self-attention layers. Specifically, the DiT backbone utilizes a 3D patch embedding strategy, where the input video is divided into non-overlapping spatiotemporal patches. These patches are then linearly projected and augmented with positional embeddings before being fed into transformer blocks. The denoising objective is formulated as: 

L = Ex0,Ïµ,t 

âˆ¥Ïµ âˆ’ ÏµÎ¸ (xt, t, c)âˆ¥2 , (2) where Ïµ is the added noise, ÏµÎ¸ is the learned denoising network parameterized by Î¸, and c represents conditioning information. This formulation allows the model to learn a direct mapping from noisy inputs to predicted noise, which can then be subtracted to recover clean video frames. 

3.2 Synthetic Hair Motion Generation 

While datasets of real subjects with hair motion exist and can be acquired, the low accuracy of hair labeling techniques would hinder the training of diffusion models. Therefore, to construct the hair-specific dataset necessary to train our model, we turn to Computer Graphics (CG) and physics-based simulation (Kugelstadt and SchÃ¶mer, 2016) and rendering. This approach enables us to create a large corpus of videos with physically plausible hair dynamics, driven by simulated forces, while simultaneously extracting precise, per-pixel ground-truth motion conditions. Each data sample in our dataset is a quadruplet {Iref , Vgt , Cpose , Chair }.

â€¢ Iref âˆˆ RHÃ—W Ã—3 is the static reference image, which serves as the appearance condition. In our setup, this is typically the first frame of the video. 

â€¢ Vgt âˆˆ RT Ã—HÃ—W Ã—3 is the ground truth video sequence, representing the target output. It is rendered using Blender Cyclesâ€™ physically-based path tracer. 

â€¢ Cpose âˆˆ RT Ã—HÃ—W Ã—3 is the body pose condition, represented as a sequence of camera-space normal renders augmented with a set of 68 facial landmarks. In these renders, the hair geometry is hidden, making the head and body fully visible. This provides dense structural and orientation information for the body, head and face, guiding the overall motion. 5â€¢ Chair âˆˆ RT Ã—HÃ—W Ã—3 is the hair motion condition, represented as a sequence of renders in which the hair is rasterized as a U, V, W buffer, where the U, V values are the texture coordinates of the scalp at the hair strand root locations and W is the normalized arc-length parameter along the hair curve. It is a dense, per-pixel representation that effectively captures the intricate deformations and flow of hair strands, providing a much richer signal than sparse keypoints or optical flow. This dataset forms the basis for our two-stage training. It provides the explicit signals necessary to train a conditional generation model with motion control that is superior to standard I2V models. Furthermore, our training pipeline (detailed in Section 3.4) is designed specifically to leverage this data without the realism drawbacks common to 3D-based animation methods. 

3.3 Motion-Context-LoRA 

The Motion-Context-LoRA module is our core contribution for motion control. It is a lightweight adapter designed to inject dual motion conditions into the DiT backbone, ÏµÎ¸ , without altering its pre-trained weights. It features two distinct pathways to process body and hair motion, reflecting the different nature of these signals. Let the noisy video at timestep t be xt. This is first encoded by the frozen VAE encoder E and patchified into spatiotemporal tokens zt = Patchify (E(xt)) . Our Motion-Context-LoRA module injects conditions Cpose and 

Chair directly into this token space as follows:  

> Body Motion Pathway:

The body pose condition Cpose (normal map) provides structural guidance. We use a dedicated Pose Encoder, Epose , composed of convolutional layers, as introduced in UniAnimate-DiT (Wang et al., 2025b). Epose is initialized with the weights of the VAE encoder E but is fine-tuned during training. The pose condition is encoded and patchified into pose tokens: 

zpose = Patchify (Epose (Cpose )) . (3) These pose tokens are injected via element-wise addition to the noisy tokens: 

zâ€² 

> t

= zt + zpose . (4) This additive injection modulates the features of the noisy input, effectively biasing the denoising process towards the target pose.  

> Hair Motion Pathway:

The hair condition Chair (UVW map) provides dense, fine-grained motion. This signal is processed using the frozen VAE encoder E, inspired by DreamActor-M1 (Luo et al., 2025), and patchified to match the token dimensions: 

zhair = Patchify (E(Chair )) . (5) Unlike the pose condition, the hair tokens are injected by concatenating them with the modulated noisy tokens along the sequence length dimension: 

zin = Concat ([ zâ€²

> t

, zhair ]) . (6) This approach, inspired by unified attention mechanisms (Zhou et al., 2024; Luo et al., 2025; Wang et al., 2025a), allows the DiTâ€™s self-attention layers to directly access and utilize the explicit hair motion information as part of the input sequence. The parameters of Epose and the LoRA weights applied to the DiTâ€™s attention blocks constitute the trainable parameters of Motion-Context-LoRA, denoted Ï•P .

3.4 Sim2Real-Domain-LoRA and Training 

Training Ï•P directly on the CG dataset risks domain overfitting, where the model learns the non-photorealistic style of the simulator. To circumvent this, we propose a two-stage training strategy that separates the learning of domain-specific features from motion-control features.  

> Stage 1: Sim2Real-Domain-LoRA Pre-training.

We first pre-train a Sim2Real-Domain-LoRA LoRA module, 

Ï•D , on our synthetic dataset. This stage uses a standard Image-to-Video (I2V) objective, where the model 6learns to generate video frames Vgt from Reference Image Iref and text prompts Tprompt , describing the content which is captioned by Qwen-2.5-VL (Bai et al., 2025) from Vgt . The objective is to capture the general motion dynamics and visual characteristics of the CG domain within Ï•D . The loss is: 

Ldomain = Ex0âˆ¼Vgt ,Ïµ,t, Tprompt 

âˆ¥Ïµ âˆ’ ÏµÎ¸,Ï• D (xt, t, Tprompt )âˆ¥2 . (7) This stage adapts the DiT backbone Î¸ with LoRA weights Ï•D to the synthetic domain. 

Stage 2: Motion-Context-LoRA Training. In the second stage, we freeze both the DiT backbone Î¸ and the pre-trained Sim2Real-Domain-LoRA LoRA Ï•D . We then introduce our Motion-Context-LoRA module Ï•P

(which includes the Pose Encoder Epose and its own LoRA weights). The model is now trained on the full quadruplet {Iref , Vgt , Cpose , Chair }, with Iref serving as the appearance condition. The objective for Ï•P is to predict the noise Ïµ given the rich conditional inputs: 

Lpose = Ex0âˆ¼Vgt ,Ïµ,t, c

âˆ¥Ïµ âˆ’ ÏµÎ¸,Ï• D ,Ï• P (xt, t, c)âˆ¥2 , (8) where c = {Iref , Cpose , Chair }. Because Ï•D is frozen, it provides a stable, CG-domain-adapted feature space, forcing Ï•P to focus exclusively on learning the mapping from the motion conditions ( Cpose , Chair ) to the desired output. 

Inference. At inference time, we discard the Sim2Real-Domain-LoRA LoRA Ï•D entirely. Inference is performed using only the original DiT backbone Î¸ and our trained Motion-Context-LoRA module Ï•P :

Ïµpred = ÏµÎ¸,Ï• P (xt, t, c). (9) This strategy allows HairWeaver to leverage the precise, fine-grained motion control learned by Ï•P from the synthetic data, while completely shedding the non-photorealistic domain artifacts captured by Ï•D . The final generation is thus guided by Ï•P but rendered using the original, photorealistic generative priors of the DiT backbone. Reference Generated Hair Motion  

> Figure 4

Photorealistic motions generated by HairWeaver. The reference images are photorealistic human subjects generated by Flux (Labs, 2024). 

74 Experiments 

4.1 Implementation Details 

Dataset Our model is trained on a synthetic human video dataset, which contains 83 minutes of synthetic videos (1,500 videos of 100 frames each with FPS=30) generated from the CG simulator. This dataset was specifically created to provide clean examples of complex hair dynamics, which are critical for our task. All videos and conditions were processed to a resolution of 896 (height) Ã— 512 (width). This curated dataset ensures our model is exposed to a broad distribution of high-quality motions and expressive details. We test our model on two test sets. We first used the model trained on the synthetic data and tested it on the self-collected hair motion test-set, which is also generated from the simulator but with different (synthetic) human subjects than those in the training data. This test set comprises 10 randomly selected videos featuring diverse hair and body motions. We also evaluate the result on the NeRSemble (Kirschstein et al., 2023) test-set. This test set comprises 30 videos featuring diverse photorealistic hair and body motions with human subjects facing the camera viewpoint, which are randomly sampled from the hair subset of the whole NeRSemble v2 Dataset. Note that since we do not have the hair UVW maps and body normal for NeRSemble videos, we use the alpha map as the hair condition and densepose map (GÃ¼ler et al., 2018) as the body condition in this case. 

Model Training and Inference Our framework is built upon the LTX-Video-0.9.8 (HaCohen et al., 2024), whose DiT backbone weights remain frozen during all training phases. For initialization, both the pose encoder for body and the VAE-Encoder for hair inherit the weights from the pretrained LTX-Video VAE-Encoder, while the layers in Motion-Context-LoRA and Sim2Real-Domain-LoRA are initialized with zeros. We employ a two-stage training strategy. In the first stage, we train only the Sim2Real-Domain-LoRA for 10,000 steps in a standard image-to-video manner with reference image and text prompt captioned by Qwen-2.5-VL (Bai et al., 2025) as input. In the second stage, we freeze the Sim2Real-Domain-LoRA and train the Motion-Context-LoRA and Pose Encoder for 10,000 steps on our primary task of motion-conditioned image-to-video animation. For all training, we use the AdamW optimizer with a learning rate of 2eâˆ’4. The model is trained on video clips of 97 frames. All experiments were conducted on 8 NVIDIA H200 GPUs with a batch size of 8. 

Table 1 Comparison to state-of-the-art video animation methods on our self-collected hair motion test-set.                                                                               

> Method Hair Full Body Avg Infer Time(s) â†“
> SSIM â†‘PSNR â†‘LPIPS â†“FID â†“cd-FVD â†“SSIM â†‘PSNR â†‘LPIPS â†“FID â†“cd-FVD â†“
> LTX-Video-0.9.8-13B (HaCohen et al., 2024) 0.9642 30.3565 0.0468 156.8218 827.5890 0.7885 19.9854 0.2751 113.9513 866.8544 56
> Wan-2.2-14B (Wan et al., 2025) 0.9153 24.8174 0.1326 172.6361 802.6791 0.5535 12.7273 0.6345 89.8836 868.1351 476 LTX-Video-ICLora (HaCohen et al., 2024) 0.9700 30.6511 0.0365 108.1872 652.3275 0.8665 24.1449 0.1446 45.4541 423.4040 58 UniAnimate-DiT (Wang et al., 2025b) 0.9761 35.4174 0.0304 66.1997 678.9257 0.8724 25.6764 0.1407 45.0817 448.8855 870 Wan-2.2-Animate-14B (Wan et al., 2025) 0.9758 35.3174 0.0299 59.6534 587.4605 0.8400 25.2877 0.1803 49.4041 461.6013 312 HairWeaver 0.9794 37.6347 0.0233 50.5938 434.1582 0.8948 27.7903 0.1127 43.2786 407.1929 62

Table 2 Comparison to state-of-the-art video animation methods on NeRSemble (Kirschstein et al., 2023) test-set.   

> Method Hair Portrait SSIM

â†‘ PSNR â†‘ LPIPS â†“ FID â†“ cd-FVD â†“ SSIM â†‘ PSNR â†‘ LPIPS â†“ FID â†“ cd-FVD â†“

LTX-Video-0.9.8-13B (HaCohen et al., 2024) 0.9131 23.40 0.1020 120.74 891.18 0.6871 17.83 0.3878 73.50 703.35 Wan-2.2-14B (Wan et al., 2025) 0.9150 21.65 0.1051 113.11 652.18 0.6865 17.28 0.3830 51.43 503.07 LTX-Video-ICLora (HaCohen et al., 2024) 0.9397 24.95 0.0710 44.11 384.41 0.7635 20.99 0.2683 28.42 277.77 UniAnimate-DiT (Wang et al., 2025b) 0.9350 25.33 0.0810 66.29 477.13 0.7545 20.32 0.3059 52.44 379.12 Wan-2.2-Animate-14B (Wan et al., 2025) 0.9544 28.75 0.0599 20.48 328.67 0.8011 23.62 0.2030 24.32 314.65 HairWeaver 0.9670 34.34 0.0477 17.79 286.25 0.8291 26.47 0.1763 19.43 212.61 

4.2 Evaluations and Comparisons 

Metrics. We evaluate our model against state-of-the-art methods using a suite of standard metrics that assess different aspects of generation quality. 

â€¢ Reconstruction and Perceptual Quality: We measure frame-level similarity to the ground truth using Peak Signal-to-Noise Ratio ( PSNR ), Structural Similarity Index Measure ( SSIM ), L1 distance, and the Learned Perceptual Image Patch Similarity ( LPIPS ) (Zhang et al., 2018). 

â€¢ Video Realism and Temporal Consistency: To evaluate the overall quality and realism of the generated video distribution, we report the FrÃ©chet Inception Distance ( FID ) and the Content-Debiased FrÃ©chet 8Table 3 Ablation Analysis. DiT+Pose Encoder denotes the DiT backbone is trained with Pose Encoder only without Sim2Real-Domain-LoRA and Motion-Context-LoRA. DiT+Pose-IC-LoRA denotes the DiT backbone is trained with In-Context-Lora, the same as LTX-Video-ICLora (HaCohen et al., 2024), without Sim2Real-Domain-LoRA and Motion-Context-LoRA. w/o Sim2Real-Domain-LoRA denotes the Sim2Real-Domain-LoRA module is removed from the HairWeaver pipeline.                                                    

> Method Hair Full Body SSIM â†‘PSNR â†‘LPIPS â†“FID â†“cd-FVD â†“SSIM â†‘PSNR â†‘LPIPS â†“FID â†“cd-FVD â†“
> DiT+Pose Encoder 0.9158 27.4486 0.1214 94.0048 654.8677 0.5623 12.9256 0.6481 91.1008 431.4920 DiT+Pose-IC-LoRA 0.9700 30.6511 0.0365 108.1872 652.3275 0.8665 24.1449 0.1446 45.4541 423.4040 w/o Sim2Real-Domain-LoRA 0.9693 36.7183 0.0236 49.0808 447.7628 0.8828 26.5544 0.1184 38.4230 416.9271 HairWeaver 0.9794 37.6347 0.0233 50.5938 434.1582 0.8948 27.7903 0.1127 43.2786 407.1929

Video Distance ( cd-FVD ) (Ge et al., 2024). The cd-FVD metric provides a more robust measure of temporal quality than the standard FVD (Unterthiner et al., 2018) by disentangling content from motion. These metrics are widely adopted in recent human video animation research (Wang et al., 2023a; Xu et al., 2024; Chang et al., 2023; Zhang et al., 2024), providing a standardized basis for comparison.  

> Table 4

The user study with 30 participants. We collect the number of votes for eight video subjects from test set by six methods and report the average percentage. (Full table presented in the supplement). Our HairWeaver generates the most realistic human videos with hair motions. 

Method Average 

LTX-Video-0.9.8-13B (HaCohen et al., 2024) 6.9% Wan-2.2-14B (Wan et al., 2025) 8.2% LTX-Video-ICLora (HaCohen et al., 2024) 9.0% UniAnimate-DiT (Wang et al., 2025b) 9.5% Wan-2.2-Animate-14B (Wan et al., 2025) 16.4% HairWeaver 49.9% Quantitative Comparison We benchmark our method, HairWeaver, against several state-of-the-art video generation models that represent distinct architectural approaches. These baselines include: (1) Image-to-Video methods, Wan2.2 (Wan et al., 2025) and LTX-Video-0.9.8 (HaCohen et al., 2024), and (2) Human Video Animation methods with additional control, such as LTX-Video-0.9.8-ICLora (HaCohen et al., 2024), UniAnimate-DiT (Wang et al., 2025b), and Wan2.2-Animate (Wan et al., 2025). Our primary goal is to improve the quality of dynamic hair generation. While our model is able to leverage a driving signal for the hair that is unavailable to other models, we still find it relevant to conduct quantitative evaluations against those models to 1) validate that, regardless of control signals available or not, our model is the only one able to get close to ground truth and 2) to set a baseline for future research. Our quantitative results are presented in Table 1, where we compare performance on the self-collected hair motion CG test-set. Note that we use the hair segmentation mask from Matte-Anything (Yao et al., 2024) to segment the videos and calculate the metrics for hair area. The results demonstrate that HairWeaver consistently and significantly outperforms all baseline models across the relevant metrics. This indicates that our proposed method successfully generates animations with more vivid and expressive dynamics. We also compare the efficiency of HairWeaver to previous methods by reporting average inference time (in seconds) per video sample. Our method is more efficient compared to state-of-the-art animation methods (Wang et al., 2025b; Wan et al., 2025). As mentioned in Section 4.1, we also test the model on the photorealistic NeRSemble (Kirschstein et al., 2023) test-set. The result is presented in Table 2. We observe that the hair motion quality of HairWeaver outperforms all baselines across the relevant metrics as well. 

Qualitative Comparison We qualitatively compare the dynamic hair and body motion generation of HairWeaver on NeRSemble test-set with the baselines in Figure 3. We also provide more photorealistic visualizations in Fig. 4, where the reference images are generated by Flux (Labs et al., 2025). More video visualizations are presented in the project page of the supplementary materials. 9Reference w/o Sim2Real-Domain-LoRA 

> w/ Sim2Real-Domain-LoRA

Hair&Body Condition Figure 5 Ablation analysis of Sim2Real-Domain-LoRA. We visualize the generation without (w/o) and with (w/) Sim2Real-Domain-LoRA. The one w/o such a module cannot preserve referenceâ€™s appearance when itâ€™s a photorealistic image.  

> User Study

We conducted a user study to compare HairWeaver with previous work (HaCohen et al., 2024; Wan et al., 2025; Wang et al., 2025b). We collect reference images, ground truth videos, and animation results from previous works and HairWeaver for 8 video subjects. For each subject, we visualize the reference, animation results, and ground truth videos side by side, with the animation results anonymized and randomly ordered. We ask 50 users to choose the best methods according to the following two criteria: (1) follows the hair motions and head poses in the Real Video and (2) preserves the identity and appearance of the reference image (person). We present the result in Table 4. We observe that the users prefered HairWeaver more than baseline methods. For more details, please refer to the supplementary material.  

> Ablation Analysis

We provide the ablation analysis in Table 3. Since the self-collected test set has ground-truth video from the CG simulator domain, it is challenging to observe the improvement from Sim2Real-Domain-LoRA in overcoming the domain gap between the CG training data and the photorealistic test data. To this end, we present a visualization of the effectiveness of such a module in Fig. 5. The proposed Sim2Real-Domain-LoRA ability to preserve appearance is evident. 

4.3 Limitations and Future Works 

The modelâ€™s performance can degrade in scenarios with extreme deviations between the driving pose and the reference image. For instance, when the driving video involves significant zooming that leads to large changes in scale, the preservation of the subjectâ€™s appearance and identity can be compromised. Furthermore, like many generative models, our method struggles with rendering some details, particularly in generating consistently accurate and realistic hands. We attribute these challenges primarily to the scope of our training data and the limitations of the backbone modelâ€™s (LTX-Video (HaCohen et al., 2024)) generation ability. Future work could address these by incorporating more diverse training data and employing more advanced 10 backbone models. 

## 5 Conclusion 

In this paper, we introduce HairWeaver , a novel diffusion-based framework designed to address a critical limitation in human video animation: the synthesis of expressive and realistic hair motion. We identified that existing methods, while proficient at transferring body pose, often fail to model secondary dynamics, resulting in characters with static, unnatural hair that undermines realism. Our solution leverages a pre-trained video DiT, enhanced with two key components. The Motion-Context-LoRA seamlessly integrates hair motion control by adding additional context to the input latent, effectively guiding the animation while preserving the rich generative priors of the backbone model. Concurrently, the Sim2Real-Domain-LoRA improves the modelâ€™s flexibility, enabling generalization to diverse photorealistic identities. By training on a synthetic dataset with simulated videos rich in hair dynamics, our model learns to generate motion that is not only temporally coherent but also physically plausible. 

## 6 Ethics Statement 

We clarify that, except for those in the NeRSemble (Kirschstein et al., 2023) dataset, all characters in this paper are fictional. We strongly condemn any misuse of generative artificial intelligence that could harm individuals or disseminate misinformation. While we acknowledge the potential for misuse in human-centered animation generation, we are dedicated to upholding the highest ethical standards in our research. This commitment includes strict adherence to legal frameworks, respect for privacy, and a focus on promoting the generation of positive and constructive content. We have submitted the legal review for the usage of such data in our work and itâ€™s still pending approval from the legal team of our organization. 

## 7 Details of User Study 

In this section, we provide a comprehensive user study for qualitative comparison between HairWeaver and previous works (HaCohen et al., 2024; Wang et al., 2025b; Wan et al., 2025). We generate 8 different human animation results from all baseline models and HairWeaver, where the results are anonymized and shuffled. On the online platform Prolific , we ask 30 users to choose the only one best method from all videos for each animation result. 

7.1 Results and Statistical Analysis: 

We present the full user study result in Table. 5. To validate the user study results, we conduct a chi-square test of independence (McHugh, 2013) to determine whether the preference distribution across methods is statistically significant. The null hypothesis ( H0) states that there is no significant difference in user preferences among the six methods.  

> Chi-Square Test Results.

We construct a 6 Ã— 8 contingency table with vote counts for six methods across eight subjects. The chi-square test yields Ï‡2 = 122 .47 , with 35 degrees of freedom and p < 0.001 , strongly rejecting the null hypothesis. This indicates that user preferences are significantly different across methods at the 

Î± = 0 .05 significance level.  

> Pairwise Comparisons.

We further conduct pairwise chi-square tests between HairWeaver and each baseline method. Table 7 shows the results. All comparisons demonstrate highly significant differences ( p < 0.001 ), confirming that HairWeaver is significantly preferred over all baseline methods.  

> Effect Size.

We compute CramÃ©râ€™s V as a measure of effect size, yielding V = 0 .361 , indicating a large effect. This suggests that the observed differences in user preferences are not only statistically significant but also practically meaningful.  

> Conclusion.

The statistical analysis strongly supports our claim that HairWeaver generates more realistic human videos with hair motions compared to existing methods. With an average preference rate of 49.9%, our 11 Table 5 The user study with 30 participants. We collect the number of votes for eight video subjects from test set by six methods and report the percentage. Our HairWeaver generates the most realistic human videos with hair motions.                                                                

> Method Subject1 Subject2 Subject3 Subject4 Subject5 Subject6 Subject7 Subject8 Average
> LTX-Video-0.9.8-13B (HaCohen et al., 2024) 0.0% 6.9% 10.3% 10.3% 6.9% 10.3% 6.9% 3.4% 6.9% Wan-2.2-14B (Wan et al., 2025) 10.3% 3.4% 10.3% 10.3% 3.4% 6.9% 6.9% 13.8% 8.2% LTX-Video-ICLora (HaCohen et al., 2024) 20.7% 6.9% 6.9% 10.3% 10.3% 6.9% 6.9% 3.4% 9.0% UniAnimate-DiT (Wang et al., 2025b) 10.3% 10.3% 10.3% 10.3% 6.9% 6.9% 17.2% 3.4% 9.5% Wan-2.2-Animate-14B (Wan et al., 2025) 10.3% 24.1% 17.2% 24.1% 6.9% 13.8% 6.9% 27.6% 16.4% HairWeaver 48.3% 48.3% 41.4% 34.5% 65.5% 55.2% 55.2% 48.3% 49.9%

Table 6 A larger-scale user study with 45 participants on 20 videos. We collect the number of votes for these 20 video subjects from test set by six methods and report the average percentage. Our HairWeaver generates the most realistic human videos with hair motions. 

Method Average 

LTX-Video-0.9.8-13B (HaCohen et al., 2024) 1.83% Wan-2.2-14B (Wan et al., 2025) 3.78% LTX-Video-ICLora (HaCohen et al., 2024) 4.02% UniAnimate-DiT (Wang et al., 2025b) 6.95% Wan-2.2-Animate-14B (Wan et al., 2025) 15.24% HairWeaver 68.17% 

method receives approximately 3 times more votes than the best baseline (Wan-2.2-Animate-14B at 16.4%) and significantly outperforms all competing methods ( p < 0.001 ). 

7.2 A Larger-Scale User Study 

We further expand the user study to 45 participants on 20 videos subjects. The result is presented in Table. 6. 

## 8 Details of Photoreal Inference 

In this section, we provide the details of our pipeline for generating high quality photorealistic animations with fine-grained control from CG animations. Given a CG animation, we render it using the same simulation and rendering pipeline that was employed previously for generating the CG training dataset. Specifically, we use Blender Cyclesâ€™ physically-based path-tracer to render 1) the shaded images Vgt and 2) the normal and UVW 

images ( Cpose and Chair respectively) required for guiding the generation. We then take the first shaded frame and transform it into a more photorealistic reference image using an image-to-image pipeline (Meng et al., 2022) based on Flux (Labs, 2024) â€” the resulting image becoming our Iref . Since it is desirable for our image-to-image process to preserve the alignment between the source and the result as much as possible, we add only a small amount of noise to the source image and we provide a detailed prompt which thoroughly describes it. In our implementation, we set the noising timestep to 0.35 and perform 100 denoising steps following the Euler integration scheme. We use the Qwen 2.5 Vision Language Model (Bai et al., 2025) to generate the detailed prompt from the source image. 

Table 7 Pairwise chi-square tests between HairWeaver and baseline methods. All comparisons show highly significant differences ( âˆ— âˆ— âˆ— indicates p < 0.001 ).                              

> Comparison Ï‡2DoF p-value Sig.
> HairWeaver vs. LTX-Video-0.9.8-13B 87.03 7<0.001 *** HairWeaver vs. Wan-2.2-14B 73.86 7<0.001 *** HairWeaver vs. LTX-Video-ICLora 70.26 7<0.001 *** HairWeaver vs. UniAnimate-DiT 67.45 7<0.001 *** HairWeaver vs. Wan-2.2-Animate-14B 40.89 7<0.001 ***

12 9 Details of Experiments on NeRSemble 

For experiments on the NeRSemble (Kirschstein et al., 2023) dataset, since we do not have the normal and 

UVW images for Cpose and Chair , we use densepose (GÃ¼ler et al., 2018) and alpha map as body and hair conditions respectively. We train our model on TikTok (Jafarian and Park, 2021) dataset, which is widely used by previous works (Chang et al., 2023; Xu et al., 2024; Wang et al., 2023a; Zhang et al., 2024), together with the synthetic data generated by CG simulator and test on NeRSemble (Kirschstein et al., 2023) dataset. We run DensePose Detection (GÃ¼ler et al., 2018) and Matte-Anything Segmentation (Yao et al., 2024) on TikTok (Jafarian and Park, 2021) and NeRSemble (Kirschstein et al., 2023) videos to obtain densepose body conditions and alpha map hair conditions. For videos from CG simulator, we directly use the alpha channel of hair UVW images as hair condition and run DensePose Detection (GÃ¼ler et al., 2018) on these videos to get densepose body condition. 13 References 

Houdini. https://www.sidefx.com/ . Accessed: 2025-11-12. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 , 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 22563â€“22575, 2023b. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465 , 2023. Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning , 2023. Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, et al. X-dyna: Expressive dynamic human image animation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 5499â€“5509, 2025. Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, and Linjie Luo. X-dancer: Expressive music to human dance video generation. arXiv preprint arXiv:2502.17414 , 2025. Matt Jen-Yuan Chiang, Benedikt Bitterli, Chuck Tappan, and Brent Burley. A practical and controllable hair and fur model for production path tracing. In ACM SIGGRAPH 2015 Talks , pages 1â€“1. 2015. Epic Games. Creating believable digital humans with metahuman and unreal engine 5. https://www.unrealengine. com/en-US/metahuman , 2024. Accessed: 2025-07-17. Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the content bias in frÃ©chet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,2024. RÄ±za Alp GÃ¼ler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In 

CVPR , 2018. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 , 2023. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103 , 2024. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8153â€“8163, 2024. Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 12753â€“12762, June 2021. Bernhard Kerbl, Georgios Kopanas, Thomas Leimk"uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. In ACM SIGGRAPH 2023 Transactions , 2023. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias NieÃŸner. Nersemble: Multi-view radiance field reconstruction of human heads. ACM Trans. Graph. , 42(4), jul 2023. ISSN 0730-0301. doi: 10.1145/3592455. https://doi.org/10.1145/3592455 .Tassilo Kugelstadt and Elmar SchÃ¶mer. Position and orientation based cosserat rods. In Symposium on Computer Animation , volume 11, pages 169â€“178, 2016. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024. 

14 Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas MÃ¼ller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. https://arxiv.org/abs/2506.15742 .Jin-Soo Lee, Min-Joon Park, and Wei Chen. Photorealistic avatars for immersive social interaction in the metaverse: A survey. In Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces (VR) , pages 215â€“224, Shanghai, China, 2023. IEEE. doi: 10.1109/VR55154.2023.000XX. Yue Li, Gene Wei-Chin Lin, Egor Larionov, Aljaz Bozic, Doug Roble, Ladislav Kavan, Stelian Coros, Bernhard Thomaszewski, Tuur Stuyck, and Hsiao-Yu Chen. Self-supervised learning of latent space dynamics. Proceedings of the ACM on Computer Graphics and Interactive Techniques , 8(4):1â€“18, 2025. Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Doug Roble, and Tuur Stuyck. Neuralocks: Real-time dynamic neural hair simulation. arXiv preprint arXiv:2507.05191 , 2025a. Weikai Lin, Haoxiang Li, and Yuhao Zhu. Controlhair: Physically-based video diffusion for controllable dynamic hair rendering. arXiv preprint arXiv:2509.21541 , 2025b. Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, and Tianshu Hu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 11036â€“11046, 2025. Mary L McHugh. The chi-square test of independence. Biochemia medica , 23(2):143â€“149, 2013. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations , 2022. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision , pages 4195â€“4205, 2023. Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically based rendering: From theory to implementation . MIT Press, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR , 2022. Guoxian Song, Hongyi Xu, Xiaochen Zhao, You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, and Linjie Luo. X-unimotion: Animating human images with expressive, unified and identity-agnostic motion latents. arXiv preprint arXiv:2508.09383 , 2025. Tuur Stuyck, Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Aljaz Bozic, Nikolaos Sarafianos, and Doug Roble. Quaffure: Real-time quasi-static neural hair simulation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 239â€“249, 2025. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717 , 2018. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 , 2025. Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang, Pengfei Wei, Zerong Zheng, Ming Zhou, Yuan Zhang, and Mingyuan Gao. Dreamactor-h1: High-fidelity human-product demonstration video generation via motion-designed diffusion transformers. arXiv preprint arXiv:2506.10568 , 2025a. Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. 

arXiv preprint arXiv:2405.18156 , 2024. 

15 Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. arXiv preprint arXiv:2307.00040 ,2023a. Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. Science China Information Sciences , 2025b. Ziyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lombardi, Michael ZollhÃ¶fer, Jessica Hodgins, and Christoph Lassner. Hvh: Learning a hybrid neural volumetric representation for dynamic hair performance capture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6143â€“6154, 2022. Ziyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lombardi, Chen Cao, Jason Saragih, Michael ZollhÃ¶fer, Jessica Hodgins, and Christoph Lassner. Neuwigs: A neural dynamic model for volumetric hair capture and animation. In 

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8641â€“8651, 2023b. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1481â€“1490, 2024. Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu. Matte anything: Interactive natural image matting with segment anything model. Image and Vision Computing , page 105067, 2024. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 , 2023. Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, and Otmar Hilliges. Human hair reconstruction with strand-aligned 3d gaussians. In European Conference on Computer Vision , pages 409â€“425. Springer, 2024. Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu, Guoxian Song, Xin Chen, Chao Liang, Jianwen Jiang, et al. X-actor: Emotional and expressive long-range portrait acting from audio. arXiv preprint arXiv:2508.02944 , 2025a. Joy Xiaoji Zhang, Jingsen Zhu, Hanyu Chen, and Steve Marschner. Hairformer: Transformer-based dynamic neural hair simulation. arXiv preprint arXiv:2507.12600 , 2025b. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR , 2018. Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680 ,2024. Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, and Yebin Liu. X-nemo: Expressive neural motion reenactment via disentangled latent attention. arXiv preprint arXiv:2507.23143 ,2025. Yujian Zheng, Zirong Jin, Moran Li, Haibin Huang, Chongyang Ma, Shuguang Cui, and Xiaoguang Han. Hairstep: Transfer synthetic to real using strand and depth maps for single-view 3d hair modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12726â€“12735, 2023. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039 , 2024. Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781 ,2024. 

16
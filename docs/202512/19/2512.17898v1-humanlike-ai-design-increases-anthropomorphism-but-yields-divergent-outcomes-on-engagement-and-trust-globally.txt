Humanlike AI Design Increases
Anthropomorphism but Yields Divergent
Outcomes on Engagement and Trust Globally
Robin Schimmelpfennig1, Mark Diaz2,
Vinodkumar Prabhakaran2, Aida Davani2
1Center for Humans and Machines, Max Planck Institute for Human
Development, work completed while at Google Research.
2Google Research.
Contributing authors: schimmelpfennig@mpib-berlin.mpg.de;
markdiaz@google.com; vinodkpg@google.com; aidamd@google.com;
Abstract
Over a billion users across the globe interact with AI systems engineered with
increasing sophistication to mimic human traits. This rapid adoption of human-
like AI has triggered urgent debate regarding Anthropomorphism, the attribution
of human characteristics to synthetic agents, and its potential to induce mis-
placed trust or emotional dependency. However, the causal link between more
humanlike AI design and subsequent effects on user engagement and trust has not
been tested in realistic human-AI interactions with a global user pool. Prevailing
safety frameworks continue to rely on theoretical assumptions derived from West-
ern populations, overlooking the global diversity of AI users. Here, we address
these gaps through two large-scale cross-national experiments (N=3,500) across
10 diverse nations, involving real-time and open-ended interactions with a state-
of-the-art AI system. We find that when evaluating an AI’s human-likeness, users
focus less on the kind of theoretical aspects often cited in policy (e.g., sentience
or consciousness), but rather applied, interactional cues like conversation flow or
understanding the user’s perspective. We also experimentally demonstrate that
humanlike design levers can causally increase anthropomorphism among users;
however, we do not find that humanlike design universally increases behavioral
measures for user engagement and trust, as previous theoretical work suggests.
Instead, part of the connection between human-likeness and behavioral outcomes
is fractured by culture: specific design choices that foster self-reported trust in
AI-systems in some populations (e.g., Brazil) may trigger the opposite result
in others (e.g., Japan). Our findings challenge prevailing narratives of inherent
1
arXiv:2512.17898v1  [cs.AI]  19 Dec 2025


risk in humanlike AI design. Instead, we identify a nuanced, culturally medi-
ated landscape of human-AI interaction, which demands that we move beyond a
one-size-fits-all approach in AI governance.
1 Introduction
The rapid integration of Artificial Intelligence (AI) systems into daily life is fundamen-
tally shifting AI’s role from a mere technical tool to an active social partner [1–3]. Users
increasingly rely on conversational agents not just for information and technical assis-
tance, but for companionship, advice, and emotional support [4–6]. This “social leap”
transforms the nature of human-technology interaction [7], creating a critical tension in
the design and governance of AI systems. On one hand, driven by intense competition
for user attention and market share, commercial actors intentionally design AI sys-
tems that mimic human characteristics [8–11] to deepen user engagement [11]. On the
other, AI safety researchers and ethicists warn that exposure to increasingly human-
like systems poses significant psychological and social risks. Central to this concern
is AI anthropomorphism—attributing human traits, such as intention, intelligence,
and personality to these non-human agents [12–15]. Crucially, AI anthropomorphism
is hypothesized to stimulate user engagement and foster increased—and potentially
misplaced—trust [16–18]. Prior work hypothesizes that such dynamics may, in turn,
heighten user vulnerability to targeted persuasion, emotional attachment, and over-
reliance on AI systems for high-stakes tasks for which the technology remains ill-suited
[6, 19–21].
As the global user base of conversational AI expands [5], reaching over a bil-
lion users on platforms such as ChatGPT, Gemini, and Claude [22], the urgency of
understanding their psychological impact grows, particularly for more vulnerable pop-
ulations [23]. In many ways, humanity is in the midst of a massive, real-time social
experiment. Yet, we lack an evidence-based understanding of the role AI anthropo-
morphism plays in this dynamic. While theoretical frameworks have outlined potential
adverse ethical and practical harms [7, 24], empirical research has lagged behind. Most
existing studies, with few exceptions [25], rely on non-representative samples, corre-
lational study designs or hypothetical vignettes that measure self-reported attitudes
rather than actual behavior [15, 26, 27]. Furthermore, many studies evaluate isolated
AI-generated output, stripping away the iterative nature of conversation [27]. These
methodological constraints limit our ability to study AI anthropomorphism in ecologi-
cally valid settings, where perceptions are context-dependent and emerge dynamically
through open-ended interaction [7, 28].
Compounding these limitations, existing research is overwhelmingly concentrated
on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations
[29]. This geographic bias implicitly assumes a universal AI experience, potentially
overlooking the moderating role of culture in human-AI interaction [30, 31]. For
example, Japanese culture and religious traditions that attribute spirit to non-human
objects may predispose individuals to accept humanlike AI as social partners [32]. In
2


contrast, Western traditions often maintain a sharper moral and ontological distinc-
tion between humans and machines, leading to greater skepticism toward attributing
minds to artificial agents [12, 33]. Currently, most widely-used AI systems tend to
reflect WEIRD humans and values [34]. However, with rapid AI adoption outside the
US and Europe [5] and the emergence of models trained in non-Western regions [22],
a culturally inclusive research framework is no longer just an academic ideal—it is a
practical and ethical imperative.
To address these gaps, we conducted two large-scale experiments designed to col-
lect causal, ecologically valid, and culturally diverse evidence on the mechanisms of AI
anthropomorphism. Across both studies, 3,500 participants from 10 culturally diverse
countries engaged in real-time, open-ended, and non-sensitive conversations in their
native languages with a state-of-the-art chatbot (Figure 1). Study 1 focused on iden-
tifying the specific chatbot characteristics that trigger anthropomorphic perceptions
and documenting baseline variations across sampled countries. We find that while the
Fig. 1: Two-stage experimental design for measuring AI anthropomorphism
and its downstream affect across user groups. In both studies, participants
first engage in an open-ended, multi-turn interaction with a chatbot (GPT-4o, August
2024), followed by questionnaires and behavioral tasks (for Study 2).
3


tendency to anthropomorphize AI varies significantly across countries and cultures, it
is relatively high across most groups. Moreover, the specific cues users prioritize when
evaluating human-likeness are mostly applied, such as conversation flow and latency,
and deviate significantly from the abstract theoretical dimensions, such as sentience
or intelligence, typically emphasized in prior literature on anthropomorphism. Build-
ing on these insights, Study 2 experimentally manipulated specific AI design aspects
based on Study 1, grouping them into two treatment conditions labeled as design char-
acteristics (DC) and conversational sociability (CS) to test their causal impact on user
behavior. While these manipulations reliably increased anthropomorphism across all
tested countries, the downstream effects were notably non-uniform. Contrary to pre-
vailing assumptions, increased human-likeness and anthropomorphism often increased
engagement but did not translate into a universal increase in trust across user groups.
We validated this through a battery of self-reported and behavioral measures for both
engagement (e.g., via analyzing chat logs) and trust (e.g., via the Trust Game [35]).
Instead, we observed heterogeneous treatment effects, where more humanlike AI sys-
tems influenced engagement and trust only within specific cultural and demographic
subgroups.
Our findings demonstrate that while human-like design reliably increases anthro-
pomorphism, its effects on trust and engagement are culturally contingent—the same
design choices that foster trust in some populations may undermine it in others.
This challenges prevailing assumptions that human-like AI poses uniform psycholog-
ical risks. Instead, our results suggest that risk is not inherent to human-like design
but emerges from the interaction between design choices and cultural context. These
findings provide the first large-scale, causal, cross-cultural evidence to inform AI
governance, highlighting the need for culturally adaptive frameworks over universal
restrictions.
2 Results
2.1 Study 1 on AI anthropomorphism in user groups across
the globe
In the first study (N = 1, 100) we recruited nationally representative samples across 10
countries via an internationally operating panel provider (see Supplementary Materials
for details on participants and countries: https://osf.io/wgmah/overview?view only=
54bccd1$F 1$bcc47c485d9fac04ce5b6d4). The study helped us to gain an exploratory
and in-depth understanding of AI anthropomorphism during interactions with a state-
of-the-art chatbot. Methodologically, we combined quantitative Likert-type survey
items [12, 36, 37] with qualitative analysis of open-ended user feedback about the AI’s
human-likeness or its lack thereof. Participants provided this feedback after having a
conversation with an LLM (OpenAI GPT-4o, August 2024 version; temperature = 1;
no token limit) on mundane, non-sensitive topics (Methods 4.1.2). The study design
enabled users to interact with the AI in an open-ended fashion, without any priming
or specific tasks for the user. The AI was given guardrails via a prompt (included in
the Supplementary Materials), which included instructions like “engaging in a friendly
and engaging conversation about favorite foods, their cooking habits, any funny or
4


memorable cooking anecdotes they might have”, and avoiding sensitive or deceptive
components. The interaction experience for the English-speaking samples is available
at https://google.qualtrics.com/jfe/form/SV 2nkorVSZ3zguxSe.
2.1.1 Heterogeneity in AI anthropomorphism across sample
cultures
The data indicate a relatively high existing level of AI anthropomorphism among
users. Specifically, while almost 68% perceived the chatbot as “somewhat” or “com-
pletely” humanlike, only 25% of users reported that the chatbot acted “completely”
or “somewhat” machine-like (see Figure 2). This tendency to anthropomorphize was
even more pronounced for other surveyed characteristics that we usually uniquely
attribute to humans: 90% of users rated the chatbot as “intelligent” (or somewhat
intelligent), 78% reported it was “empathetic”, and 75% rated the AI as “conscious”.
These results suggest that a large majority of users associated the AI system they
talked to, often by large margins, with human traits; pointing to widespread anthro-
pomorphism across the users base. In fact, among all characteristic we surveyed, no
characteristic was rated as non-anthropomorphic by a majority of users.
5


Fig. 2: Prevalence of anthropomorphism across measured attributes. The
figure shows the share of users whose response shows tendency to anthropomor-
phize across several attributes. To aid interpretation, responses from the original
5-point scale (e.g., (1) “completely machine-like” to (5) “completely humanlike” for
the “machine-like/humanlike” item) are collapsed into three categories. Scores of 1
and 2 are grouped as “Not Anthropomorphizing”, scores of 4 and 5 are grouped as
“Anthropomorphizing” and the score of 3 is shown as “Neutral”. Attributes are sorted
in ascending order by the total ratio of “Anthropomorphizing” responses. The results
show that for every characteristic, the largest response group was “Anthropomorphiz-
ing”.
Interestingly, our results reveal significant variation in anthropomorphism across
sampled countries. Most notably, visual inspection of the data revealed two emergent
clusters of countries distinguished by their mean anthropomorphism scores (Figure
3). Participants in one cluster (Indonesia, Mexico, India, Nigeria, Egypt, Brazil; N =
600) perceived the AI as more humanlike (M = 3.98, SD = 1.19) than participants
in the second cluster (United States, Germany, Japan, South Korea; N = 500; M =
3.29, SD = 1.25). This grouping was identified post-hoc based on observed score dis-
tributions rather than a priori theoretical criteria. Exploratory analysis revealed a
positive but non-significant correlation between a country’s cultural distance from the
US (Cultural Fst; [38]) and anthropomorphism (r = 0.52, p = 0.127), suggesting a
possible pattern that warrants further investigation with a larger sample of countries.
We found no significant variation in AI anthropomorphism between men and women,
6


or between people with different usage intensity of AI-system. We observed a signifi-
cant non-linear association between age and anthropomorphism. Specifically, we found
an inverted U-shaped relationship: age was positively associated with humanlike per-
ception (β = 0.0581, SE = 0.016, p < 0.001), whereas the quadratic age term showed
a significant negative association (β = −0.0006, SE = 0.00017, p < 0.001). This tra-
jectory indicates that humanlike perception increases with age until approximately 48
years, after which it begins to decline.
In sum, this pronounced cultural variance confirms generalizability constraints of
current empirical approaches that exclusively rely on WEIRD samples, which implic-
itly assume a universal “AI experience”. Our findings demonstrate that perceptions
of AI are not uniform across user groups, highlighting the necessity of considering
users’ backgrounds and context before generalizing human-AI interaction findings and
strategies.
2.1.2 Deviation between anthropomorphism scales and users’
focus when interacting with AI
The above results provide an initial, yet surface-level diagnosis of AI anthropomor-
phism among sampled users. Relying solely on survey questions about pre-determined,
theoretical aspects about what constitutes human-likeness ignores peculiarities of the
users’ perspectives, which can emerge during the interaction with a novel and chang-
ing technology like AI. While the capabilities of AI systems are advancing at an
unprecedented speed, existing anthropomorphism scales emerged from a vastly differ-
ent technological reality (e.g., from anthropomorphizing robots or other machines),
where constraints in anthropomorphism were often due to limitations in system com-
petence; a constraint now diminishing as AI capabilities advance. Thus, to attain a
more in-depth view of what users pay attention to when they anthropomorphize AI,
we conducted a qualitative text analysis of user feedback after the interaction. In this
feedback, participant were asked to explain what, if anything, about the AI system
made them feel they were talking to a human, and conversely, what made them feel
they were not (Methods).
This text analysis combined human coding and an LLM-in-the-loop automated
rater (further referenced as ’autorater’) [39, 40] to evaluate the N = 1,100 open-ended
responses by participants across eight native languages. Participants responded to the
entire survey in their native language. We first manually developed a comprehensive
codebook of aspects determining the AI’s human-likeness from the users perspective,
by qualitatively analyzing a subset of the data (the entire codebook is provided in the
Supplementary Materials). We define the aspects in a neutral way, capturing references
regardless of their positivity (humanlike) or negativity (machine-like). For example,
the aspect response speed was coded whether the user mentioned the chatbot writing
too fast or too slow. We then used the emerging aspects, and their descriptions, for
instruction-tuning a Gemini 2.5 Pro model [41], which then served as an autorater.
Following established practices for instruction-tuning [42], we tasked the autorater
to identify language about each aspect in the codebook across participant responses
(Methods 4.1.3). Overall, this scalable and multilingual approach efficiently estab-
lished the thematic focus of the users’ AI anthropomorphism. We group the resulting
7


Fig. 3: Cross-national variation in human-like perception and prefer-
ence.The figure shows mean and distribution of ’humanlike’ perception and preference
by country. Responses were measured on a bipolar 5-point Likert scale (1=“Completely
machine-like” to 5=“Completely humanlike”). Each row represents one country. The
faint blue shaded area is a violin plot, illustrating the full distribution of all responses.
The black dot indicates the mean score, and the horizontal bars represent the 95%
confidence interval (CI) of the mean. The dashed vertical line indicates the neutral
midpoint (3.0) of the scale. Blue arrows start at each country’s “Humanlike” per-
ception mean and encode preference relative to neutral: they point right when the
country’s average preference for human-likeness is above 3 (more humanlike) and left
when it is below 3 (more machine-like); arrow length is proportional to the difference
between the mean preference score and the neutral midpoint (“Would you prefer to
talk to an AI system that is less or more humanlike compared to the one you just
talked to?”), and uniformly scaled for readability. Sample sizes were N=100 for each
country, with the exception of the United States (N=200; total N=1,100).
aspects into two sets of categories—applied aspects and theoretical aspects—and Figure
4 presents the frequency of all aspects in our codebook.
8


Fig. 4: Frequency of user-identified ’applied’ and ’theoretical’ aspects of
AI anthropomorphism Bars represent the percentage of participants (N = 1, 100)
mentioning specific aspects in response to an open-ended prompt regarding the chat-
bot’s human-likeness (“Was there something specific about the AI system that made
you feel you were (or were not) talking to a human? If so, why? Please describe in
detail.”). Applied aspects (highlighted in purple) were identified through a bottom-up
qualitative analysis of user feedback and subsequently used to develop our codebook.
Theoretical aspects (shaded in gray) represent top-down constructs derived from estab-
lished anthropomorphism scales. An LLM-based autorater identified these features
across eight languages. The results indicate that users prioritize applied, interactional
characteristics (e.g., conversation flow, perspective-taking) over abstract theoretical
constructs (e.g., consciousness, possession of a soul) when evaluating human-likeness.
Figure 4 highlights a notable disconnect between the inductive, user-driven map-
ping of anthropomorphism and the deductive, theory-grounded approaches prevalent
in existing literature. While academic scales often emphasize abstract attributes that
are difficult to conceptualize or operationalize (e.g., consciousness), the users in our
study focused overwhelmingly on a pragmatic set of interactional qualities. Specifically,
participants consistently mentioned aspects of design and Conversational Sociabil-
ity, such as conversation flow (32.1% of participants), understanding user perspective
(24.4%), response speed (22.5%), and authenticity (18.4%). In contrast, despite the
high ratings these traits received in our Likert-scale data, theoretically motivated con-
structs like intelligence (8.8%) and empathy (5.5%) were mentioned less frequently.
More abstract dimensions, such as spirituality, Soul, intentionality, or consciousness
were mentioned by fewer than 0.5% of participants. This evidence suggests that
actual AI anthropomorphism in open-ended user-chatbot interactions is not primar-
ily driven by the perception of chatbot as a moral or conscious entity, but rather
by the way it writes, appears, and builds a relationship with the user. This find-
ing reorients the discourse on AI human-likeness from conceptual abstractions to
applied interactional dynamics. Consequently, for Study 2, we leverage these identi-
fied levers—drawing on both the quantitative baseline variations (Figures 2 and 3)
9


and qualitative themes (Figure 4)—to experimentally manipulate AI human-likeness
and test its causal downstream effects.
2.2 Study 2 on the Effects of Experimentally Manipulating AI
Anthropomorphism
In Study 2, we causally test whether varying AI human-likeness allows us to manip-
ulate AI anthropomorphism among users and whether the variation subsequently
influences user engagement with and trust in the chatbot. We recruited N = 2, 400
participants, with 400 participants from across six countries (Methods). Participants
were informed that they were interacting with an LLM, and the system was config-
ured to never actively deceive participants by pretending to be human (see system
prompt in Supplementary Materials). We used a 2 x 2 factorial design, varying the
AI’s human-likeness from low to high across two treatment dimensions: a) Design
Characteristics (DC) and b) Conversational Sociability (CS) (Methods 4.2). Both
treatment dimensions were built on both previous theoretical work, and aspects most
frequently cited by users in Study 1 (Figure 4). The DC manipulation focused on the
design, appearance, and mechanistic functioning of the chatbot. That is, the High-
DC condition prompted the model to, for example, vary response speed (to mimic
human pauses), use a more informal tone (e.g., colloquialisms and emojis), fluctu-
ate response length, and was represented by a “human” emoji and an assigned name
(i.e., Alex, which was localized to culturally appropriate equivalents in non-English-
speaking samples; see Supplementary Materials for the full list of names). The low-DC
treatment was instructed to do the opposite of that, or contained a more machine-like
appearance (e.g., a “robot emoji”). That is, it maintained a consistent, low-latency,
and neutral delivery. The CS manipulation focused on the social and interpersonal
behavior of the chatbot. The High-CS condition prompted for a high level of empa-
thy and warmth. This dimension also included explicit relationship-building efforts
(e.g., social follow-up questions by the chatbot), and adaptation of the AI’s person-
ality to match the user’s style. The low-CS control maintained a mainly factual and
task-oriented tone (see screenshots in Figure 1). The treatments can be tested under
the following link: https://google.qualtrics.com/jfe/form/SV 2nkorVSZ3zguxSe. This
experimental design allowed us to causally test the effects of these empirically- and
theoretically-derived treatments dimension, alone or in combination.
2.2.1 Experimentally manipulating user anthropomorphism
We find that experimentally varying the humanlike design of the chatbot significantly
influences users’ tendency to anthropomorphize the chatbot. Our results show that
varying the Design Characteristics and Conversational Sociability increases anthropo-
morphism across many user groups (Figure 5). OLS Regressions show that users rate
the chatbot as significantly more “humanlike” (5-point scale item ranging from “very
machine-like” to “very humanlike”), with the coefficient for the high-DC/high-CS
(compared to omitted control low-DC/low-CS) being β = 0.386, t(2396) = 5.590, p <
0.001). A significant trend in the same direction was observable across 7 of the 10
10


items surveyed (Figure 5). The manipulation had no significant effect on the per-
ceived “intelligence”, “competence”, “consciousness” of the chatbot. This suggests
that our manipulation successfully influenced human-likeness without altering all the
perceived system capabilities (e.g., intelligence). Importantly, the regression model
confirms that manipulating either dimension in isolation also produced a significant
effect (high-DC only/low-CS: (β = 0.21, SE = 0.069, p = 0.003, low-DC/high-CS
only: β = 0.18, SE = 0.069, p = 0.009). However, the treatment dimension that com-
bines both aspects consistently yields the highest effect sizes. This finding confirms
that the maximum increase of AI anthropomorphism occurs when empirically-derived
pragmatic design cues (DC) are combined with more theoretically-derived social cues
(CS).
Fig. 5: Effect of human-like AI design treatments on anthropomorphism.
Points represent the coefficient estimates from a series of Ordinary Least Squares
(OLS) regressions (showing coefficients for ‘DC-high/CS-high’ vs. ‘DC-low/CS-low’)
and horizontal lines represent the 95% confidence intervals. Panel a) shows the treat-
ment effects across all ten Likert-measured anthropomorphism items. Panel b) shows
the heterogeneity analysis, specifically for the “humanlike” item, across all sampled
countries.
2.2.2 AI anthropomorphism does not universally influence trust
and engagement across all user groups
The first part of Study 2 establishes that experimentally manipulating the human-
likeness of the chatbot causally increases anthropomorphism. In the second part of
Study 2 we now investigate the causal effects of the same treatments on downstream
user perception and behavior. This addresses an important discourse in AI research.
Commercial interests seem to follow user preferences: pooling participants in Study 1
who chose “more” and “much more” on a 5-point Likert scale (“Would you prefer to
talk to an AI system that is less or more humanlike compared to the one you just talked
11


to?”), 80% of participants reported a preference for a ’more’ of ’much more’ humanlike
AI in the future. However, work in AI ethics and governance suggests that increased
AI anthropomorphism may harm users. Interestingly, our experimental findings only
partially serve as support for the claims on either side. We find evidence for significant
heterogeneity in treatment effects across users in how humanlike chatbots increase
self-reported and behavioral measures for engagement, trust, and even emotional (AI
as a friend) outcomes only across some user groups (see Figure 7).
The humanlike manipulation did not significantly increase the behavioral measure
for trust among the pooled sample. In an incentivized trust game (Methods 4.2.3) the
amount of money participants entrusted to the most humanlike AI (between subjects
design), did not statistically differ from that entrusted to the least humanlike AI
(t(1194) = 0.038, p = 0.97). That is, the effect on the increased anthropomorphism on
the AI system did not translate into the theorized increase in trust.
Fig. 6: Treatment effects on behavioral engagement and trust. Violin plots
display the data distribution for the four treatment groups (Design Characteristics
(DC) and Conversational Sociability (CS)); internal boxplots represent the inter-
quartile range. Solid horizontal lines indicate group means. (a) Behavioral engagement
is operationalized as the standardized mean message length in the human-AI interac-
tion. (b) Trust is measured by the number of trust points allocated in the incentivized
trust game. Pairwise comparisons were calculated using independent t-tests (∗p < 0.05;
∗∗p < 0.01; ∗∗∗p < 0.001; ns p ≥0.05).
In exploratory analysis we did, however, find an effect, for the behavioral measure
of engagement with the chatbot for chosen behavioral measures (e.g., average number
of messages: t(1191) = 4.380, p < 0.001). Subsequent analysis confirms that this effect
is driven by a reciprocal increase in verbosity; users write more in response to a more
verbose AI, creating a self-reinforcing feedback loop of engagement.
Significant Subgroup Variation: Importantly, these aggregate findings mask
significant deviations within specific subgroups. As hypothesized by the cultural differ-
ence observed in Study 1, the humanlike treatment successfully increased self-reported
12


and behavioral measures for trust, engagement, and emotional outcomes (e.g., perceiv-
ing the AI as a friend) in several user groups (see Figure 7 and Figure 6). For instance,
exploratory analysis shows positive effects of AI human-likeness on Brazilian partic-
ipants’ engagement (higher tendency to use AI again, more behavioral engagement,
increase in seeing AI as a friend) and trust (higher self-reported tendency to trust AI).
Japanese participants in the high-DC and low-CS group (compared to the control)
showed negative effects for their tendency to use AI again, decreased perception of AI
as a friend, and lower self-reported trust. This demonstrates that while the effect is
not universal, humanlike design can lead to the predicted outcomes—increased trust
and engagement—for specific populations.
Fig. 7: Subgroup analysis of treatment effects by country. Treatment effects
on key trust (e.g., tendency to trust AI) and engagement metrics (e.g., tendency to use
AI again) are shown across sampled countries, illustrating the significant heterogeneity
observed in Study 2.
These findings are crucial: the simple relationship between anthropomorphism and
psychological outcomes is not a universal rule but a context- and group-dependent
one.
13


3 Discussion
Widespread concern exists about how increasingly social AI systems may influence
user psychology and behavior [3]. However, empirical evidence on when and how users
anthropomorphize AI, whether this varies across cultures, and whether humanlike
design causally affects downstream outcomes like trust and engagement has remained
limited. Our two large-scale, cross-national experiments with 3,500 participants from
10 countries provide causal, ecologically valid evidence that challenges several pre-
vailing assumptions. We demonstrate that while AI designers can systematically
manipulate users’ tendency to anthropomorphize chatbots through specific design
choices, the downstream consequences, particularly for trust, are far more nuanced
and heterogeneous than current theoretical work suggests.
Our research reveals a critical disconnect between traditional conceptualizations
of AI anthropomorphism and actual user experience. Existing academic anthropo-
morphism scales, developed largely in the context of robots and earlier technologies,
focus on abstract human attributes such as consciousness, intentionality, and spir-
ituality [36, 43]. When directly asking users to rate these items on a Likert scale,
forced-choice endorsement may yield high scores. Yet our qualitative analysis of spon-
taneously salient aspects reveals that users overwhelmingly attend to pragmatic design
features when evaluating human-likeness: conversation flow, response speed, authen-
ticity, and the chatbot’s ability to understand their perspective. In contrast, theoretical
constructs like consciousness, morality, or having a soul were mentioned by fewer than
0.5% of participants. This finding suggests that anthropomorphism in user-chatbot
interactions is not primarily driven by the AI appearing as a moral or sentient entity,
but stems from conversational dynamics: how the chatbot writes, responds, and builds
rapport. As AI capabilities continue to advance, this disconnect will likely grow. What
once distinguished humans from machines (e.g., competence, conversational coherence,
coherent conversation) is rapidly diminishing as a salient cue. Our findings call for a
fundamental reorientation of anthropomorphism research: from abstract philosophical
debates about machine consciousness toward empirically grounded investigations of
the specific, measurable design cues that users actually notice and respond to.
Our results also provide compelling evidence that there is no universal “AI expe-
rience.” We observed a pronounced “anthropomorphism gap” across countries, with
participants in Indonesia, Mexico, India, Nigeria, Egypt, and Brazil perceiving the
AI as significantly more humanlike than those in the United States, Germany, Japan,
and South Korea. This cultural variation extended to preferences: the tendency to
desire more humanlike AI increased with cultural distance from the United States.
These findings confirm the generalizability constraints of current empirical approaches
that rely predominantly on Western, Educated, Industrialized, Rich, and Democratic
(WEIRD) samples [29]. Practices that may seem atypical in one cultural context,
such as Buddhist funeral ceremonies for Sony’s AIBO robot dogs in Japan, reflect
broader worldviews that blur distinctions between artificial and living beings. [32]. As
AI adoption accelerates globally, particularly in regions outside the US and Europe
[5], these cultural variations carry profound implications for both research and prac-
tice. They emphasize the necessity of considering users’ backgrounds and contexts
14


before generalizing human-AI interaction findings, and they challenge the adequacy
of one-size-fits-all approaches to AI design and governance.
A central premise in both commercial AI design and ethical discourse is that
increasing humanlike cues will increase user trust and engagement, potentially foster-
ing over-reliance or vulnerability to manipulation [16, 18]. Our causal evidence reveals
that at least part of this link is not universal but profoundly subgroup-dependent.
Across the entire sample, we successfully manipulated anthropomorphism: users in the
high Design Characteristics and high Conversational Sociability condition rated the
chatbot as significantly more humanlike. Yet this manipulation did not translate into
a significant increase in trust, whether measured through self-report or an incentivized
behavioral task (the trust game). We did observe increased engagement in the most
humanlike condition, but even here, the effects were heterogeneous across countries.
This aggregate null finding for trust may be explained by theory: humanlike manipu-
lations failed to influence users’ core evaluations of the AI’s competence or alignment
with the AI system (see Figure 5 and Figure 6). Theories suggest that justified trust in
AI is grounded in perceived capability (e.g., competence of the model in the domain at
question) and alignment with the user [44]. Our results suggest that while the human-
like treatments activate the psychological mechanisms underlying anthropomorphism,
they do not override users’ deeper judgments of whether the AI can actually help them
or share their goals. Put differently, our experiment does not suggest that human-
likeness will never increase trust, but it may do so more effectively if it activates the
necessary components, perceived competence and alignment, among users.
Despite some non-significant aggregate effects, the humanlike treatment success-
fully increased self-reported and behavioral measures for trust, engagement, and
emotional outcomes in several user groups. Brazilian participants, for instance, showed
positive effects across multiple dimensions: higher tendency to use AI again, greater
behavioral engagement, increased perception of AI as a friend, and higher self-reported
trust. Japanese participants in the high DC and low CS group (compared to the con-
trol) showed negative effects for their tendency to use AI again, decreased perception
of AI as a friend, and lower self-reported trust. This demonstrates that while the effect
is not universal, humanlike design can lead to the predicted outcomes for specific
populations—such as increased trust in Brazil—while potentially triggering the oppo-
site in others, such as Japan. These heterogeneous effects underscore that the simple,
often-assumed relationship between anthropomorphism and downstream psychologi-
cal consequences is not a universal rule but a context- and group-dependent one. The
post-hoc analysis showing a positive interaction between individual anthropomorphism
scores and trust further supports this: individuals who anthropomorphized the AI to
a greater extent also tended to trust it more, though because anthropomorphism is an
outcome of the treatment, this association remains correlational rather than causal.
These findings carry significant implications for AI development and governance.
The assumption that optimizing for human-likeness is inherently risky, a view that has
shaped much of the ethical discourse around conversational AI [7, 24], requires qualifi-
cation. Our evidence suggests that in mundane, non-sensitive interactions, humanlike
design does not universally lead to the hypothesized harms of over-trust. This may
15


open opportunities to safely improve user experience through anthropomorphic fea-
tures in contexts where the risks are lower. At the same time, the significant effects
observed in specific subgroups indicate that certain populations may indeed be more
susceptible to anthropomorphic design cues. Rather than blanket restrictions or per-
missions, these findings support context-dependent, culturally informed governance
frameworks that attend to the specific use case, the AI system’s actual capabilities,
and the characteristics of the user population.
Several limitations warrant consideration when interpreting these findings. First,
our study focused on mundane, non-sensitive conversations, a deliberate choice that
ensures ecological validity for the everyday interactions that occur millions of times
daily, but that may not generalize to high-stakes contexts. The relationship between
humanlike design and trust may differ substantially when users seek medical advice,
financial guidance, or emotional support during a crisis. Second, we employed a text-
based chatbot interface; voice-based or embodied AI systems may trigger different,
and potentially stronger, anthropomorphic responses due to additional sensory cues.
Third, all conditions used the same foundation model (GPT-4o), providing a stringent
test but also limiting generalizability to other AI architectures. Finally, our cross-
sectional design captured immediate responses; longitudinal studies would be needed
to understand how relationships with AI develop over time and whether repeated
exposure amplifies or attenuates the effects we observed.
Future research should extend this work along several dimensions. Testing the
effects of humanlike design in sensitive, high-stakes contexts, for example when it
comes to persuasion or fraud attempts, would help delineate the boundary conditions
of our findings. Investigating modality effects through voice-based and embodied AI
agents could reveal whether anthropomorphic responses are amplified by non-textual
cues. Longitudinal designs would illuminate how human-AI relationships evolve with
repeated interaction and whether the effects we observed are transient or durable.
Research with specific vulnerable populations, including children, older adults, and
individuals experiencing loneliness, would help identify groups that may require par-
ticular attention in design and governance frameworks. Finally, decomposing the
mechanisms through which cultural context and native linguistic nuances moder-
ate anthropomorphism and its consequences would provide actionable guidance for
developing globally inclusive AI systems.
In conclusion, our findings offer evidence-based nuance to prevailing assumptions
about AI anthropomorphism and its negative effect on user psyche and behavior.
While commercial actors continue to design increasingly humanlike AI systems and
ethicists warn of the attendant risks, the empirical reality we document is more
nuanced. Users anthropomorphize based on pragmatic, applied features rather than
abstract philosophical attributes. Cultural context profoundly shapes both the ten-
dency to anthropomorphize and its downstream effects. And crucially, successfully
inducing anthropomorphism through humanlike design does not universally translate
to increased trust or engagement, the link depends on who the user is and what
they bring to the interaction. As AI systems become ever more embedded in daily
life across the globe, moving from speculative, Western-centric risk frameworks to
evidence-based, globally-inclusive understanding is essential. Our work contributes to
16


this shift by providing causal, culturally inclusive evidence that can inform the respon-
sible development of AI systems that are attentive to both user experience and user
well-being.
4 Methods
We obtained informed consent from participants prior to the start of each study. No
deception was used in the studies, which were reviewed and approved by authors’
institutional review board. Participants were excluded for inattentiveness based on
multiple, pre-registered criteria, including rapid completion times, click-based atten-
tion checks, and the coherence of open-ended text responses. Conversational data
from all participants, including those excluded from the final analysis, is accessi-
ble in the Online Appendix (https://osf.io/wgmah/overview?view only=54bccd1$F
1$bcc47c485d9fac04ce5b6d4).
4.1 Study 1: When and how do users across the globe
anthropomorphize AI systems
4.1.1 Participants
We sampled 1,100 participants per our pre-registered target; given the study’s
exploratory nature, sample size was not determined by power analysis. Participants
were recruited via a high-quality panel from ten countries (survey languages in brack-
ets): USA (English, N =200), Germany (German, N =100), South Korea (Korean,
N =100), Japan (Japanese, N =100), India (Hindi, N =100), Nigeria (English,
N =100), Indonesia (Indonesian, N =100), Egypt (Arabic, N =100), Mexico (Spanish,
N =100), and Brazil (Portuguese, N =100). This selection spans five continents and
captures significant cultural variation (based on country-level CFst scores, [38]). We
initially planned to include a Chinese sample, but this was prevented by restrictions
of the OpenAI API.
Demographic information collected included age, gender, country of origin, resi-
dence, parental origin, education, prior LLM usage, resistance to digital technologies,
religion, and religious intensity. The data were collected from 12 August 2024 to 2
October 2024 (median response time: 20 minutes).
4.1.2 Procedure of the Human–AI Interaction
Prior to starting the interaction, we informed participants that they would interact in
real time with an AI chatbot (OpenAI GPT-4o, August 2024 version; temperature=1;
no token limit). The chatbot was directly embedded into the survey window. A sys-
tem prompt (see Supplementary Materials), together with the user’s inputs, set the
chatbot’s output and was always formulated in English. The chatbot interacted with
users in the national language of their country of residence and was instructed to con-
verse with the user about non-sensitive, everyday topics (e.g., food preferences). This
was a deliberate choice both for ethical reasons, and also because we intentionally
wanted to test AI anthropomorphism in an everyday interaction that occurs millions
of times each day, and is relevant across the sampled countries. This ensured some
17


level of comparability across the sample countries. Crucially, the interaction was open-
ended and non-sensitive; participants were not primed with a specific high-intent goal
(e.g., seeking deep personal advice, completing a critical task). Instead, the design
allowed a broad range of user intents to emerge naturally through open-ended interac-
tion. All participant responses were dynamically appended to the context history for
each subsequent turn. The interaction lasted a minimum of four minutes, after which
participants could choose to continue for up to one additional minute before auto-
matically proceeding. The interaction had guardrails to prevent deception, specifically
preventing the chatbot from directly self-attributing human characteristics during the
conversation (e.g., the chatbot would not directly say “I have a body and emotions”
or “I am a human”). All GPT-4o model prompts used in the experiment are detailed
in the chapter on the Human-AI interaction (see Supplementary Information).
4.1.3 Measures
All instructions and questions were presented in participants’ native languages; below
is the English version.
Perceptions of Human-likeness in AI interaction (Open-ended questions)
• Was there something specific about the AI system that made you feel you were
talking to a human? If so, why? Please describe in detail.
• Was there something specific about the AI system that made you feel you were not
talking to a human? If so, why? Please describe in detail.
Anthropomorphism (Likert-scale items)
Participants rated the chatbot on 10 traits using a 5-point Likert scale. These items
were compiled from previous work on the anthropomorphism of AI systems and
technology ([36, 37, 43]). In addition to an overarching anthropomorphism item
(humanlike) we chose to add items to capture dimensions of social perception (compe-
tent, intelligent, warmth), characteristics related to sentience (conscious, moral, soul),
and items related to personality and emotions (personality, feelings, or empathy).
Preference for human-likeness (Likert-scale)
We also surveyed the participants’ preferences for how ‘human-like’ AI systems should
be, and what they perceive to be the biggest differences between humans and AI
systems. Would you prefer to talk to an AI system that is less or more humanlike
compared to the one you just talked to? (5 point from much more to much less)
Anthropomorphism (Qualitative Data)
To analyze the open-ended responses, we followed a three-step LLM-in-the-loop
process [39]:
1. Codebook Creation via Iterative Thematic Analysis: We conducted
an iterative thematic analysis [45] via manual coding to inductively extract key
themes. Three researchers with expertise in natural language processing, human-
computer interactions and behavioral sciences independently analyzed responses from
18


20 randomly drawn participants. Given the overlapping nature of the three open-
ended questions, they were analyzed side-by-side. The researchers met after each
round of independent coding (total of three rounds using 20, 20, and 40 participants,
respectively) to discuss, refine, and consolidate codes into a final codebook (see Supple-
mentary Material). For this first step, responses were initially translated into English
using Google Translate. This approach for the initial coding in English may lead to a
loss of linguistic nuance, but was chosen to create broader themes that transfer across
languages.
2. Autorater Application: The final codebook was applied to all the 1100 open-
ended responses using an instruction-tuned LLM (Gemini V2) as an autorater [39, 40].
We used a few-shot learning approach, with detailed instructions provided in the
Supplementary Information. To ensure all relevant information was considered, the
three open-ended answers for each participant were concatenated into a single docu-
ment for labeling. The model was instructed to provide a list of most relevant labels.
LLMs exhibit variable performance across languages, potentially affecting reliability.
To preserve native linguistic nuances, we conducted all autorater coding on original
participant responses rather than translations.
3. Evaluation of the Autorater: To evaluate the LLM’s performance, we uti-
lized a human-labeled “golden set” (N = 100). Given the complexity of the 38-label
taxonomy, we employed two independent human raters (two authors of the paper with
backgrounds in Natural Language Processing and Social Psychology) to establish a
baseline of reliability. While the mean Cohen’s Kappa across all labels (M = 0.28)
reflects the inherent difficulty of the multi-label classification task, these figures are
heavily influenced by the “long tail” of rare labels in our taxonomy. Due to this high
disagreement, we considered the union of the two raters’ responses as the golden set
for evaluating the autorater. The F1 score of the autorater (M = 0.53) represents the
relative difficulty of detecting various labels of the codebook in responses (diagrams of
inter-rater agreement and autorater performance on individual labels are represented
in the Supplementary Materials).
4.2 Study 2: Experimental Manipulation of the AI’s
human-likeness
The exploratory design in Study 1 informed the design of Study 2 in its treatment
design, by identifying factors users focus on when evaluating the human-likeness of
the model, and also for the country selection.
4.2.1 Participants
We recruited a total of 2,400 participants (N=2,400) from six countries via a repre-
sentative research panel, sampling N=400 participants per country: USA, Germany,
Japan, India, Egypt, and Brazil. The reduction in countries compared to Study 1
allowed for a larger sample size per country, supporting the experimental design, while
still sampling from a global and culturally diverse user pool. The sample size was
pre-registered and determined by a power analysis (pwr in R) to ensure sufficient sta-
tistical power for both overall and within-country treatment comparisons. We sampled
19


400 participants per country to enable within-country analyses. We created a pre-
registration document that details the sample size and analysis plan, and followed
it throughout the study. However, due to an upload issue noted after data collec-
tion, the document was not formally pre-registered before the start of the study. We
still followed the intended sampling and analysis, and any deviations are labeled as
exploratory analysis. Sampling continued until N=2,400 complete participants were
reached (accounting for dropped participants due to attention or incomplete surveys).
4.2.2 Treatments for Manipulating Anthropomorphism
We employed 2×2 factorial (between-subjects) design to manipulate the human-
likeness of the AI system across two dimensions: Design Characteristics and Con-
versational Sociability. The treatments were implemented at the system prompt
level.
Treatment Development Process
1. Literature Review and Categorization (DC & CS): We surveyed past research on
anthropomorphizing machines and AI systems and grouped relevant aspects into
two categories of Design Characteristics (DC) (e.g., response speed, text length, use
of emojis) and Conversational Sociality (CS) (e.g., emotional language, warmth,
interactive mode).
2. Integration of Qualitative Findings: We mapped emerging, culturally specific con-
cepts from the thematic analysis in Study 1 (e.g., typos as a humanlike trait, lack
of emojis in specific cultures) into the two treatment conditions of DC and CS to
ensure the manipulations reflected user-identified factors.
3. Testing and Refinement: Co-authors and several domain experts assessed different
versions of the chatbot to confirm that the four treatment versions were sufficiently
distinct and that the manipulations were effective and consistent across different
languages and cultural contexts. This notion was later confirmed during a manip-
ulation check in the main study (i.e., chatbot versions with high DC and high CS
were perceived as significantly more humanlike compared to those version with low
DC and low CS).
This setup provides a stringent test for the effects of humanlike AI, as both the
treatment and control conditions were powered by the same foundation model (GPT-
4o), which has strong social capabilities by default.
4.2.3 Measures
Anthropomorphism measures
We used the same 10 Likert-scale items for AI Anthropomorphism as in Study 1. We
use the humanlike item as our main outcome for anthropomorphism.
Engagement measures (self-reported and behavioral)
First, participants responded to the following items so we could collect a self-reported
measure for engagement:
20


• How much did you enjoy interacting with the AI system?
• If you had the chance, would you want to interact with this AI system again in the
future?
• Would this AI system make for a good friend?
Second, we approximated a behavioral measure for engagement with the chatbot
using the user’s chat log. To do this, we calculated two metrics: the total number of
messages sent by the user during the interaction, and the average number of tokens
per user message. The latter metric was normalized across each language.
Trust (self-reported and behavioral)
First, participants responded to the following items so we could collect a self-reported
measure for trust.
• How much do you trust the AI system? (Do not trust it at all / Trust it completely)
• Do you trust that the information provided by this AI system is correct?
• Would you say that you would trust this AI system more than you would trust other
humans?
Second, we also deployed a decision making task to derive a behavioral measure of
trust. Participants engaged in a modified, one-shot Trust Game [35] to measure their
trust in the AI system.
In this game, the participant acted as the investor, while the AI acted as the
trustee. The AI’s response was not predetermined (see Supplementary Information).
Participants were endowed with an initial capital of 100 points, which would be con-
verted to a monetary bonus at a pre-disclosed rate. The participant had to decide
what portion of this endowment, x, where x ∈[0, 100], to send to the AI. The amount
sent was tripled by the experimenter, meaning the AI received 3x. The participant
retained 100 −x. Subsequently, the AI trustee would decide what portion, y ∈[0, 3x],
to return to the participant. The participant’s final payoff was (100 −x) + y. The
amount sent, x, serves as our behavioral measure of trust. Participants were informed
that the AI’s decision-making process was unknown and that the AI, like them, was
motivated to maximize points.
4.2.4 Analysis of treatment effects
In addition to descriptive analysis of data, we ran linear regression models (OLS),
with the treatment variables coded as dummy variables, and the respective outcome
variables as dependent variables.
5 Data and code availability
All data and code used for this analysis can be found in the Online Appendix.
21


Fig. 8: Incentivized trust game paradigm. Instructions presented to participants
before the one-shot trust game, in which participants decided how many of their 100
points to send to the AI trustee.
References
[1] Park, J. S. et al. Generative agents: Interactive simulacra of human behavior
(2023). URL https://dl.acm.org/doi/pdf/10.1145/3586183.3606763.
[2] Du´e˜nez-Guzm´an, E. A., Sadedin, S., Wang, J. X., McKee, K. R. & Leibo, J. Z. A
social path to human-like artificial intelligence. Nature Machine Intelligence 5,
1181–1188 (2023). URL http://arxiv.org/abs/2405.15815. ArXiv:2405.15815 [cs].
[3] Kirk, H. R. et al. Neural steering vectors reveal dose and exposure-dependent
impacts of human-AI relationships (2025).
URL http://arxiv.org/abs/2512.
01991. ArXiv:2512.01991 [cs].
[4] Kirk, H. R. et al.
The PRISM Alignment Project: What Participatory, Rep-
resentative and Individualised Human Feedback Reveals About the Subjective
and Multicultural Alignment of Large Language Models (2024).
URL https:
//arxiv.org/abs/2404.16019.
22


[5] Chatterji, A. et al. How People Use ChatGPT. NBER Working Paper (2025).
[6] Starke, C. et al. Risks and protective measures for synthetic relationships. Nature
Human Behaviour 8, 1834–1836 (2024). URL https://www.nature.com/articles/
s41562-024-02005-4. Publisher: Nature Publishing Group.
[7] Gabriel, I. et al.
The Ethics of Advanced AI Assistants (2024).
URL http:
//arxiv.org/abs/2404.16244. ArXiv:2404.16244 [cs].
[8] Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with
GPT-4 (2023). URL http://arxiv.org/abs/2303.12712. ArXiv:2303.12712 [cs].
[9] Cheng, M., Gligori´c, K., Piccardi, T. & Jurafsky, D. Anthroscore: A computa-
tional linguistic measure of anthropomorphism (2024).
[10] Ouyang, L. et al. Training language models to follow instructions with human
feedback. Neural Information Processing Systems 2022.
[11] Johnson, T. & Obradovich, N.
Testing for completions that simulate altru-
ism in early language models. Nature Human Behaviour 9, 1861–1870 (2025).
URL https://www.nature.com/articles/s41562-025-02258-7.
Publisher: Nature
Publishing Group.
[12] Epley, N., Waytz, A. & Cacioppo, J. T. On seeing human: A three-factor theory
of anthropomorphism.
Psychological Review 114, 864–886 (2007).
Place: US
Publisher: American Psychological Association.
[13] Moon, Y. Intimate Exchanges: Using Computers to Elicit Self-Disclosure From
Consumers.
Journal of Consumer Research 26, 323–339 (2000).
URL https:
//academic.oup.com/jcr/article-lookup/doi/10.1086/209566.
[14] Shanahan, M. Talking about Large Language Models. Communications of the
ACM 67, 68–79 (2024). URL https://dl.acm.org/doi/10.1145/3624724.
[15] Cohn, M. et al. Believing anthropomorphism: examining the role of anthropo-
morphic cues on trust in large language models (2024).
[16] Akbulut, C., Weidinger, L., Manzini, A., Gabriel, I. & Rieser, V. All Too Human?
Mapping and Mitigating the Risk from Anthropomorphic AI.
Proceedings of
the AAAI/ACM Conference on AI, Ethics, and Society 7, 13–26 (2024). URL
https://ojs.aaai.org/index.php/AIES/article/view/31613. Number: 1.
[17] Brandtzaeg, P. B., Skjuve, M. & Følstad, A.
My AI Friend: How Users of a
Social Chatbot Understand Their Human–AI Friendship. Human Communication
Research 48, 404–429 (2022). URL https://doi.org/10.1093/hcr/hqac008.
[18] Weidinger, L. et al. Taxonomy of risks posed by language models (2022).
23


[19] Pentina, I., Hancock, T. & Xie, T.
Exploring relationship development with
social chatbots: A mixed-method study of replika. Computers in Human Behav-
ior 140, 107600 (2023). URL https://www.sciencedirect.com/science/article/pii/
S0747563222004204.
[20] Matz, S. C. et al. The potential of generative AI for personalized persuasion at
scale. Scientific Reports 14, 4692 (2024). URL https://www.nature.com/articles/
s41598-024-53755-0. Publisher: Nature Publishing Group.
[21] Lin, H. et al.
Persuading voters using human–artificial intelligence dia-
logues.
Nature 648, 394–401 (2025).
URL https://www.nature.com/articles/
s41586-025-09771-9. Publisher: Nature Publishing Group.
[22] Institute, M. A. E.
Microsoft-AI-Diffusion-Report.
Tech. Rep. (2025).
URL https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/
Microsoft-AI-Diffusion-Report.pdf.
[23] Robb, M. B. Talk, Trust and Trade-Offs: How and Why Teens Use AI Companions
(2025).
[24] Weidinger, L. et al.
Ethical and social risks of harm from Language Models
(2021). URL http://arxiv.org/abs/2112.04359. ArXiv:2112.04359 [cs].
[25] Ibrahim, L. et al.
Multi-turn Evaluation of Anthropomorphic Behaviours
in Large Language Models (2025).
URL http://arxiv.org/abs/2502.07077.
ArXiv:2502.07077 [cs].
[26] Duffy, J. & Feltovich, N. Do Actions Speak Louder Than Words? An Exper-
imental Comparison of Observation and Cheap Talk.
Games and Economic
Behavior 39, 1–27 (2002). URL https://www.sciencedirect.com/science/article/
pii/S0899825601908929.
[27] Bai, H., Voelkel, J. G., Muldowney, S., Eichstaedt, J. C. & Willer, R.
LLM-generated messages can persuade humans on policy issues.
Nature
Communications 16, 6037 (2025).
URL https://www.nature.com/articles/
s41467-025-61345-5. Publisher: Nature Publishing Group.
[28] Jobin, A., Ienca, M. & Vayena, E. The global landscape of ai ethics guidelines.
Nature machine intelligence 1, 389–399 (2019).
[29] Henrich,
J.,
Heine,
S.
J.
&
Norenzayan,
A.
The
weirdest
peo-
ple
in
the
world?
Behavioral
and
brain
sciences
33,
61–83
(2010).
URL
https://www.ssoar.info/ssoar/bitstream/handle/document/42104/
ssoar-2010-henrich et al-The weirdest people in the.pdf?sequence=1.
[30] Schimmelpfennig, R. et al.
The Moderating Role of Culture in the General-
izability of Psychological Phenomena.
Advances in Methods and Practices in
24


Psychological Science 7, 25152459231225163 (2024).
URL https://doi.org/10.
1177/25152459231225163. Publisher: SAGE Publications Inc.
[31] Linxen, S. et al. How weird is chi? (2021).
[32] Nomura, T., Kanda, T., Suzuki, T. & Kato, K. Prediction of Human Behavior in
Human–Robot Interaction Using Psychological Scales for Anxiety and Negative
Attitudes Toward Robots. IEEE Transactions on Robotics 24, 442–451 (2008).
URL http://ieeexplore.ieee.org/document/4481184/.
[33] Epley, N., Waytz, A., Akalis, S. & Cacioppo, J. T. When We Need A Human:
Motivational Determinants of Anthropomorphism. Social Cognition 26, 143–155
(2008). URL https://guilfordjournals.com/doi/abs/10.1521/soco.2008.26.2.143.
Publisher: Guilford Publications Inc.
[34] Atari, M., Xue, M. J., Park, P. S., Blasi, D. E. & Henrich, J. Which Humans?
(2023). URL https://osf.io/preprints/psyarxiv/5b26t v1/.
[35] Berg, J., Dickhaut, J. & McCabe, K.
Trust, Reciprocity, and Social His-
tory. Games and Economic Behavior 10, 122–142 (1995). URL https://www.
sciencedirect.com/science/article/pii/S0899825685710275.
[36] Bartneck, C., Kuli´c, D., Croft, E. & Zoghbi, S. Measurement Instruments for the
Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived
Safety of Robots. International Journal of Social Robotics 1, 71–81 (2009). URL
http://link.springer.com/10.1007/s12369-008-0001-3.
[37] Fiske,
S.
T.,
Cuddy,
A.
J.
C.
&
Glick,
P.
Universal
dimensions
of
social cognition: warmth and competence.
Trends in Cognitive Sciences 11,
77–83 (2007).
URL https://www.cell.com/trends/cognitive-sciences/abstract/
S1364-6613(06)00329-9. Publisher: Elsevier.
[38] Muthukrishna, M. et al.
Beyond Western, Educated, Industrial, Rich, and
Democratic (WEIRD) Psychology: Measuring and Mapping Scales of Cul-
tural and Psychological Distance.
Psychological Science 31, 678–701 (2020).
URL http://journals.sagepub.com/doi/10.1177/0956797620916782.
Tex.ids=
muthukrishnaWEIRDPsychologyMeasuring2020, muthukrishnaWesternEducate-
dIndustrial2020a.
[39] Dai, S.-C., Xiong, A. & Ku, L.-W. LLM-in-the-loop: Leveraging Large Language
Model for Thematic Analysis (2023).
URL http://arxiv.org/abs/2310.15100.
ArXiv:2310.15100 [cs].
[40] Than, N., Fan, L., Law, T., Nelson, L. K. & McCall, L. Updating “The Future
of Coding”: Qualitative Coding with Generative Large Language Models. Socio-
logical Methods & Research 54, 849–888 (2025). URL https://journals.sagepub.
com/doi/10.1177/00491241251339188.
25


[41] Comanici, G. et al.
Gemini 2.5: Pushing the frontier with advanced reason-
ing, multimodality, long context, and next generation agentic capabilities. arXiv
preprint arXiv:2507.06261 (2025).
[42] Shengyu, Z. et al. Instruction tuning for large language models: A survey. arXiv
preprint arXiv:2308.10792 (2023).
[43] Waytz, A., Cacioppo, J. & Epley, N.
Who sees human? the stability and
importance of individual differences in anthropomorphism.
Perspectives on
psychological science 5, 219–232 (2010).
[44] Manzini, A. et al. Should users trust advanced ai assistants? justified trust as a
function of competence and alignment (2024).
[45] Braun,
V.
&
Clarke,
V.
Using
thematic
analysis
in
psychology.
Qualitative
Research
in
Psychology
3,
77–101
(2006).
URL
https:
//doi.org/10.1191/1478088706qp063oa.
Publisher:
Routledge
eprint:
https://doi.org/10.1191/1478088706qp063oa.
26

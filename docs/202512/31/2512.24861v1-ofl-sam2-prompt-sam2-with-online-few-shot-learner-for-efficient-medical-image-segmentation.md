
# OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation

# OFL-SAM2：利用在线少样本学习器提示SAM2的高效医学图像分割



**Authors**: Meng Lan, Lefei Zhang, Xiaomeng Li

**Date**: 2025-12-31

**Tags**: <span class="tag-label tag-green">keywords: symbolic regression</span> <span class="tag-label tag-green">keywords: transformer</span>

---

## Abstract
The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.



## 摘要
Segment Anything Model 2 (SAM2) 在视频数据中展现了卓越的可提示视觉分割能力，显示出其在涉及三维体数据和时间相关二维图像序列的医学图像分割（MIS）任务中的扩展潜力。然而，将SAM2适配至MIS面临若干挑战，包括微调所需的大量标注医学数据和高质量的人工提示，这既耗费人力又需要医学专家的介入。为了应对这些挑战，我们提出了OFL-SAM2，一种用于标签高效MIS的无提示SAM2框架。我们的核心思想是利用有限的标注样本训练一个轻量级映射网络，该网络捕获医学知识并将通用图像特征转换为目标特征，从而为每一帧提供额外的判别性目标表征，并消除了对人工提示的需求。关键是，该映射网络支持在推理过程中进行在线参数更新，从而增强了模型在测试序列上的泛化能力。在技术上，我们引入了两个关键组件：(1) 一个在线少样本学习器，它利用有限数据训练映射网络以生成目标特征；(2) 一个自适应融合模块，它将目标特征与冻结的SAM2生成的记忆注意力特征动态融合，从而产生准确且鲁棒的目标表征。在三个多样化的MIS数据集上进行的广泛实验表明，OFL-SAM2在有限的训练数据下取得了最先进的性能。


---

## 论文详细总结（自动生成）

以下是基于论文《OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation》的详细结构化总结：

### 1. 论文的核心问题与整体含义（研究动机和背景）

*   **核心问题**：
    *   **领域鸿沟与提示依赖**：Segment Anything Model 2 (SAM2) 在视频分割中表现优异，但直接应用于医学图像分割（MIS）时存在领域鸿沟。尽管SAM2的记忆机制允许通过单帧提示分割序列，但它仍依赖高质量的人工提示，这限制了其在需要大量处理3D体数据或长视频时的自动化能力，且需要持续的医学专家干预。
    *   **现有方法的局限性**：现有的无提示医学SAM方法通常忽略时空上下文信息，或与SAM2的记忆机制不兼容（例如基于二进制掩码预测的记忆编码器无法处理多类语义预测）。此外，SAM2仅依赖基于像素级特征匹配的记忆注意力，在医学图像中难以区分边界模糊的“相邻干扰物”。
*   **整体含义**：
    *   论文旨在提出一种**无提示**、**标签高效**的SAM2框架，以解决上述问题。目标是在不依赖人工提示的情况下，利用极少量标注样本，使模型能够像处理视频一样处理医学3D体数据（视为连续2D切片序列）或手术视频，并提供具有判别力的目标特征，以克服医学图像中干扰物多、边界模糊的挑战。

### 2. 论文提出的方法论

*   **核心思想**：
    *   在冻结SAM2核心组件（图像编码器、掩码解码器、记忆注意力模块）的基础上，移除提示编码器。
    *   引入一个**轻量级映射网络**，利用有限的标注样本将通用的图像特征转换为特定的目标特征（类似提示特征），从而替代人工提示。
    *   通过**在线少样本学习**机制，在推理过程中动态更新映射网络参数，增强模型对不同测试序列的泛化能力。

*   **关键技术细节**：
    *   **双分支架构**：
        *   **离线分支（记忆注意力模块）**：利用SAM2原有的记忆注意力机制，通过在训练集中检索与查询图像最相似的 K=2 个样本初始化记忆库，提取时空上下文特征 ($E_1$)。
        *   **在线分支（少样本学习器）**：训练一个映射网络 $T_\tau$（仅一个卷积层），将通用图像特征映射为目标特征 ($E_2$)。
    *   **在线少样本学习**：
        *   通过最小化映射网络输出与记忆特征之间的均方误差来优化参数 $\tau$（使用最速下降法）。
        *   在推理阶段，利用高置信度的预测结果作为新样本，实时更新映射网络参数和记忆库。
    *   **自适应融合模块 (AFM)**：
        *   通过一个权重网络 $G_\theta$ 生成权重图 $W$，动态融合离线分支特征 $E_1$ 和在线分支特征 $E_2$。
        *   公式：$E_{tar} = W \odot E_1 + (1-W) \odot E_2$。这有助于抑制干扰物表示，使融合特征适应冻结的SAM2解码器。
    *   **质量感知更新策略**：
        *   计算预测掩码的置信度分数 $S_{cf}$。只有当 $S_{cf}$ 高于阈值 $\gamma$（默认为0.8）时，才利用该预测结果更新记忆库和映射网络，防止错误累积。

### 3. 实验设计

*   **数据集与场景**：
    *   **Synapse-CT**：多器官CT分割数据集（30例，8个腹部器官），用于3D体数据测试。
    *   **PROMISE12**：前列腺MRI分割数据集（50例），用于不同模态的3D数据测试。
    *   **Autolaparo**：腹腔镜手术视频数据集（300个序列，4种器械），用于时间相关的2D图像序列测试。
*   **Benchmark设置**：
    *   **极低数据量设置**：在Synapse-CT和PROMISE12上仅使用2或3个体积进行训练；在Autolaparo上仅使用5%或10%的视频序列进行训练。剩余数据用于测试。
    *   **评估指标**：Dice系数（DSC，越高越好）和平均豪斯多夫距离（HD，越低越好）。
*   **对比方法**：
    *   **基线模型**：原始SAM2（使用Mask提示）。
    *   **无提示SAM变体**：SAMed, SurgicalSAM, H-SAM。
    *   **无提示SAM2变体**：FS-MedSAM2, FATE-SAM2。

### 4. 资源与算力

*   **硬件配置**：论文明确指出训练和实验使用的是 **一块 NVIDIA RTX 3090 GPU**。
*   **训练参数**：使用了SAM2 base模型，最大训练轮次为40 epochs，优化器为AdamW（初始学习率1e-3）。
*   **算力消耗分析**：文中未提及具体的训练总时长（如小时数），但由于采用了冻结SAM2主干、仅训练轻量级映射网络和AFM的策略，计算开销相对较小。推理阶段增加了在线更新步骤（迭代优化），但文中通过减少推理时的迭代次数（从10次降至5次）来控制开销。

### 5. 实验数量与充分性

*   **实验数量**：
    *   **主实验**：在3个不同的数据集上进行了实验，且每个数据集设置了不同的数据量（2 vs 3 个体积，5% vs 10% 序列），共计多组对比实验。
    *   **消融实验**：针对核心组件（少样本学习器、AFM）进行了消融研究；验证了更新策略及不同置信度阈值（$\gamma=0.7, 0.8, 0.9$）的影响；探究了初始化记忆库时选取的相似图像数量 $K$（$K=1, 2, 3$）。
*   **充分性与客观性评价**：
    *   实验设计**非常充分**。覆盖了CT、MRI和手术视频三种不同模态，且均在极具挑战性的“少样本”设定下进行。
    *   对比了当前最先进的（SOTA）SAM及SAM2变体，结果客观。
    *   消融实验详细验证了每个模块的有效性及最佳超参数设置，逻辑严密。

### 6. 论文的主要结论与发现

*   **性能表现**：OFL-SAM2在所有三个数据集上均取得了SOTA性能。例如，在Synapse-CT（3个体积训练）上达到82.52% DSC，显著优于H-SAM（80.12%）和FATE-SAM2（76.38%）。
*   **鲁棒性**：在训练数据极少（如仅2个体积）的情况下，OFL-SAM2的性能下降幅度（0.71%）远小于对比方法，证明了在线学习机制带来的强鲁棒性。
*   **机制有效性**：自适应融合模块（AFM）成功结合了时空特征和在线学习特征，有效抑制了医学图像中的干扰物；质量感知更新策略防止了错误信息的传播，比盲目更新更有效。

### 7. 优点

*   **标签高效**：仅需极少量（2-3个）标注样本即可达到优异性能，非常适合医学标注数据稀缺的场景。
*   **真正的自动化（无提示）**：完全消除了对人工提示（点、框、掩码）的依赖，实现了端到端的自动化分割。
*   **在线自适应能力**：借鉴了视觉跟踪中的在线判别学习思想，通过在推理过程中不断更新模型参数，能够适应测试序列中的解剖结构变化。
*   **轻量级与即插即用**：保持SAM2主干冻结，仅增加极少量的可训练参数（映射网络和AFM均为单层卷积），训练收敛快，易于部署。

### 8. 不足与局限

*   **推理延迟**：虽然映射网络很轻量，但在推理过程中需要进行在线参数更新（梯度下降迭代），这相比于纯前向推理会增加一定的计算成本和延迟。尽管文中通过减少迭代次数进行了优化，但在对实时性要求极高的场景下可能仍存在瓶颈。
*   **对初始化样本的依赖**：虽然在线学习能提升泛化能力，但框架仍需依赖少量高质量的训练样本来初始化记忆库和预训练映射网络。如果初始样本质量极差，可能会影响早期的推理效果。
*   **序列处理限制**：该方法将3D体数据视为连续切片序列处理，主要利用了切片间的连续性（时空上下文）。对于完全离散的、无时间相关性的单张2D图像，其基于记忆注意力的优势可能无法充分发挥。